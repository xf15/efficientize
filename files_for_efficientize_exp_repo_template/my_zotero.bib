
@article{_evolving_2019,
  title = {Evolving {{Images}} for {{Visual Neurons Using}} a {{Deep Generative Network Reveals Coding Principles}} and {{Neuronal Preferences}}},
  year = {2019},
  month = may,
  journal = {Cell},
  volume = {177},
  number = {4},
  pages = {999-1009.e10},
  publisher = {{Cell Press}},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2019.04.005},
  abstract = {What specific features should visual neurons encode, given the infinity of real-world images and the limited number of neurons available to represent \ldots},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/43CU9P6D/2019 - Evolving Images for Visual Neurons Using a Deep Ge.pdf;/Users/xzfang/Zotero/storage/4M9K5A9F/S0092867419303915.html}
}

@article{_intonation_2020,
  title = {Intonation Interpretations Are Talker-Sensitive, but Not Talker-Specific},
  year = {2020},
  month = may,
  publisher = {{OSF}},
  abstract = {Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9XMMU5NS/tbz8h.html}
}

@article{_musenet_2020,
  title = {Musenet : {{Music Generation}} Using {{Abstractive}} and {{Generative Methods}}},
  shorttitle = {Musenet},
  year = {2020},
  month = apr,
  journal = {International Journal of Innovative Technology and Exploring Engineering},
  volume = {9},
  number = {6},
  pages = {784--788},
  issn = {2278-3075},
  doi = {10.35940/ijitee.F3580.049620},
  abstract = {Humans have been entertained by music for millennia. For ages it has been treated as an art form which requires a lot of imagination, creativity and accumulation of feelings and emotions. Recent trends in the field of Artificial Intelligence have been getting traction and Researchers have been developing and generating rudimentary forms of music through the use of AI. Our goal is to generate novel music, which will be non-repetitive and enjoyable. We aim to utilize a couple of Machine Learning models for the same. Given a seed bar of music, our first Discriminatory network consisting of Support Vector Machines and Neural Nets will choose a note/chord to direct the next bar. Based on this chord or note another network, a Generative Net consisting of Generative Pretrained Transformers(GPT2) and LSTMs will generate the entire bar of music. Our two fold method is novel and our aim is to make the generation method as similar to music composition in reality as possible. This in turn results in better concordant music. Machine generated music will be copyright free and can be generated conditioned on a few parameters for a given use.The paper presents several use cases and while the utilization will be for a niche audience, if a easy to use application can be built, almost anyone will be able to use deep learning to generate concordant music based on their needs.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/53HR2ET8/2020 - Musenet  Music Generation using Abstractive and G.pdf}
}

@article{_resolving_2015,
  title = {Resolving the Locus of {{cAsE aLtErNaTiOn}} Effects in Visual Word Recognition: {{Evidence}} from Masked Priming},
  shorttitle = {Resolving the Locus of {{cAsE aLtErNaTiOn}} Effects in Visual Word Recognition},
  year = {2015},
  month = sep,
  journal = {Cognition},
  volume = {142},
  pages = {39--43},
  publisher = {{Elsevier}},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.05.007},
  abstract = {Determining the factors that modulate the early access of abstract lexical representations is imperative for the formulation of a comprehensive neural\ldots},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EFEU9J73/S0010027715001043.html}
}

@misc{262588213843476_code_,
  title = {Code for {{https://slides.yihui.org/xaringan/\#1}} He Didn't Seem to Host This Anywhere, and i Code by Coping and Pasting...},
  shorttitle = {Code for Https},
  author = {262588213843476},
  journal = {Gist},
  abstract = {code for https://slides.yihui.org/xaringan/\#1 he didn't seem to host this anywhere, and i code by coping and pasting... - xaringan\_example\_yihui.rmd},
  howpublished = {https://gist.github.com/xf15/de1a00c678ed54d844ce28505b6c5c8d},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FE22KPIM/de1a00c678ed54d844ce28505b6c5c8d.html}
}

@article{abbott_random_2015,
  title = {Random Walks on Semantic Networks Can Resemble Optimal Foraging.},
  author = {Abbott, Joshua T. and Austerweil, Joseph L. and Griffiths, Thomas L.},
  year = {2015},
  month = jul,
  journal = {Psychological Review},
  volume = {122},
  number = {3},
  pages = {558--569},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0038693},
  abstract = {When people are asked to retrieve members of a category from memory, clusters of semantically related items tend to be retrieved together. A recent article by Hills, Jones, and Todd (2012) argued that this pattern reflects a process similar to optimal strategies for foraging for food in patchy spatial environments, with an individual making a strategic decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that similar behavioral phenomena also emerge from a random walk on a semantic network derived from human word-association data. Random walks provide an alternative account of how people search their memories, postulating an undirected rather than a strategic search process. We show that results resembling optimal foraging are produced by random walks when related items are close together in the semantic network. These findings are reminiscent of arguments from the debate on mental imagery, showing how different processes can produce similar results when operating on different representations.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DBFW5RIA/Abbott et al. - 2015 - Random walks on semantic networks can resemble opt.pdf}
}

@article{abeille_extraction_2020,
  title = {Extraction from Subjects: {{Differences}} in Acceptability Depend on the Discourse Function of the Construction},
  shorttitle = {Extraction from Subjects},
  author = {Abeill{\'e}, Anne and Hemforth, Barbara and Winckel, Elodie and Gibson, Edward},
  year = {2020},
  month = nov,
  journal = {Cognition},
  volume = {204},
  pages = {104293},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104293},
  abstract = {In order to explain the unacceptability of certain long-distance dependencies \textendash{} termed syntactic islands by Ross (1967) \textendash{} syntacticians proposed constraints on long-distance dependencies which are universal and purely syntactic and thus not dependent on the meaning of the construction (Chomsky, 1977; Chomsky, 1995 a.o.). This predicts that these constraints should hold across constructions and languages. In this paper, we investigate the ``subject island'' constraint across constructions in English and French, a constraint that blocks extraction out of subjects. In particular, we compare extraction out of nominal subjects with extraction out of nominal objects, in relative clauses and wh-questions, using similar materials across constructions and languages. Contrary to the syntactic accounts, we find that unacceptable extractions from subjects involve (a) extraction in wh-questions (in both languages); or (b) preposition stranding (in English). But the extraction of a whole prepositional phrase from subjects in a relative clause, in both languages, is as good or better than a similar extraction from objects. Following Erteschik-Shir (1973) and Kuno (1987) among others, we propose a theory that takes into account the discourse status of the extracted element in the construction at hand: the extracted element is a focus (corresponding to new information) in wh-questions, but not in relative clauses. The focus status conflicts with the non-focal status of a subject (usually given or discourse-old). These results suggest that most previous discussions of islands may rely on the wrong premise that all extraction types behave alike. Once different extraction types are recognized as different constructions (Croft, 2001; Ginzburg \& Sag, 2000; Goldberg, 2006; Sag, 2010), with their own discourse functions, one can explain different extraction patterns depending on the construction.},
  langid = {english},
  keywords = {Cross-linguistic,Experimental syntax,Information structure,Relative clauses,Syntactic islands,Wh-questions}
}

@article{abney_what_2020,
  title = {What Are the Building Blocks of Parent\textendash Infant Coordinated Attention in Free-Flowing Interaction?},
  author = {Abney, Drew H. and Suanda, Sumarga H. and Smith, Linda B. and Yu, Chen},
  year = {2020},
  journal = {Infancy},
  volume = {25},
  number = {6},
  pages = {871--887},
  issn = {1532-7078},
  doi = {10.1111/infa.12365},
  abstract = {The present article investigated the composition of different joint gaze components used to operationalize various types of coordinated attention between parents and infants and which types of coordinated attention were associated with future vocabulary size. Twenty-five 9-month-old infants and their parents wore head-mounted eye trackers as they played with objects together. With high-density gaze data, a variety of coordinated attention bout types were quantitatively measured by combining different gaze components, such as mutual gaze, joint object looks, face looks, and triadic gaze patterns. The key components of coordinated attention that were associated with vocabulary size at 12 and 15 months included the simultaneous combination of parent triadic gaze and infant object looking. The results from this article are discussed in terms of the importance of parent attentional monitoring and infant sustained attention for language development.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/infa.12365},
  file = {/Users/xzfang/Zotero/storage/XQIRQAQ4/Abney et al. - 2020 - What are the building blocks of parentâ€“infant coor.pdf;/Users/xzfang/Zotero/storage/ZF8FFTLK/infa.html}
}

@article{abrams_intersubject_2013,
  title = {Inter-Subject Synchronization of Brain Responses during Natural Music Listening},
  author = {Abrams, Daniel A. and Ryali, Srikanth and Chen, Tianwen and Chordia, Parag and Khouzam, Amirah and Levitin, Daniel J. and Menon, Vinod},
  year = {2013},
  month = may,
  journal = {The European journal of neuroscience},
  volume = {37},
  number = {9},
  pages = {1458--1469},
  issn = {0953-816X},
  doi = {10.1111/ejn.12173},
  abstract = {Music is a cultural universal and a rich part of the human experience. However, little is known about common brain systems that support the processing and integration of extended, naturalistic `real-world' music stimuli. We examined this question by presenting extended excerpts of symphonic music, and two pseudomusical stimuli in which the temporal and spectral structure of the Natural Music condition were disrupted, to non-musician participants undergoing functional brain imaging and analysing synchronized spatiotemporal activity patterns between listeners. We found that music synchronizes brain responses across listeners in bilateral auditory midbrain and thalamus, primary auditory and auditory association cortex, right-lateralized structures in frontal and parietal cortex, and motor planning regions of the brain. These effects were greater for natural music compared to the pseudo-musical control conditions. Remarkably, inter-subject synchronization in the inferior colliculus and medial geniculate nucleus was also greater for the natural music condition, indicating that synchronization at these early stages of auditory processing is not simply driven by spectro-temporal features of the stimulus. Increased synchronization during music listening was also evident in a right-hemisphere fronto-parietal attention network and bilateral cortical regions involved in motor planning. While these brain structures have previously been implicated in various aspects of musical processing, our results are the first to show that these regions track structural elements of a musical stimulus over extended time periods lasting minutes. Our results show that a hierarchical distributed network is synchronized between individuals during the processing of extended musical sequences, and provide new insight into the temporal integration of complex and biologically salient auditory sequences.},
  pmcid = {PMC4487043},
  pmid = {23578016},
  file = {/Users/xzfang/Zotero/storage/39NLN62Y/Abrams et al. - 2013 - Inter-subject synchronization of brain responses d.pdf}
}

@article{acha_effect_2008,
  title = {The Effect of Neighborhood Frequency in Reading: {{Evidence}} with Transposed-Letter Neighbors},
  shorttitle = {The Effect of Neighborhood Frequency in Reading},
  author = {Acha, Joana and Perea, Manuel},
  year = {2008},
  month = jul,
  journal = {Cognition},
  volume = {108},
  number = {1},
  pages = {290--300},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.02.006},
  abstract = {Transposed-letter effects (e.g., jugde activates judge) pose serious models for models of visual-word recognition that use position-specific coding schemes. However, even though the evidence of transposed-letter effects with nonword stimuli is strong, the evidence for word stimuli is scarce and inconclusive. The present experiment examined the effect of neighborhood frequency during normal silent reading using transposed-letter neighbors (e.g., silver, sliver). Two sets of low-frequency words were created (equated in the number of substitution neighbors, word frequency, and number of letters), which were embedded in sentences. In one set, the target word had a higher frequency transposed-letter neighbor, and in the other set, the target word had no transposed-letter neighbors. An inhibitory effect of neighborhood frequency was observed in measures that reflect late processing in words (number of regressions back to the target word, and total time). We examine the implications of these findings for models of visual-word recognition and reading.},
  langid = {english},
  keywords = {Letter position assignment,Orthographic encoding,Transposed-letter effect},
  file = {/Users/xzfang/Zotero/storage/NY28KTJZ/Acha and Perea - 2008 - The effect of neighborhood frequency in reading E.pdf}
}

@article{acha_effects_2008,
  title = {The Effects of Length and Transposed-Letter Similarity in Lexical Decision: {{Evidence}} with Beginning, Intermediate, and Adult Readers},
  shorttitle = {The Effects of Length and Transposed-Letter Similarity in Lexical Decision},
  author = {Acha, Joana and Perea, Manuel},
  year = {2008},
  month = may,
  journal = {British Journal of Psychology},
  volume = {99},
  number = {2},
  pages = {245--264},
  issn = {00071269},
  doi = {10.1348/000712607X224478},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XAUMQH85/Acha and Perea - 2008 - The effects of length and transposed-letter simila.pdf}
}

@article{adams_predictions_2013,
  title = {Predictions Not Commands: Active Inference in the Motor System},
  shorttitle = {Predictions Not Commands},
  author = {Adams, Rick A. and Shipp, Stewart and Friston, Karl J.},
  year = {2013},
  journal = {Brain Structure \& Function},
  volume = {218},
  number = {3},
  pages = {611--643},
  issn = {1863-2653},
  doi = {10.1007/s00429-012-0475-5},
  abstract = {The descending projections from motor cortex share many features with top-down or backward connections in visual cortex; for example, corticospinal projections originate in infragranular layers, are highly divergent and (along with descending cortico-cortical projections) target cells expressing NMDA receptors. This is somewhat paradoxical because backward modulatory characteristics would not be expected of driving motor command signals. We resolve this apparent paradox using a functional characterisation of the motor system based on Helmholtz's ideas about perception; namely, that perception is inference on the causes of visual sensations. We explain behaviour in terms of inference on the causes of proprioceptive sensations. This explanation appeals to active inference, in which higher cortical levels send descending proprioceptive predictions, rather than motor commands. This process mirrors perceptual inference in sensory cortex, where descending connections convey predictions, while ascending connections convey prediction errors. The anatomical substrate of this recurrent message passing is a hierarchical system consisting of functionally asymmetric driving (ascending) and modulatory (descending) connections: an arrangement that we show is almost exactly recapitulated in the motor system, in terms of its laminar, topographic and physiological characteristics. This perspective casts classical motor reflexes as minimising prediction errors and may provide a principled explanation for why motor cortex is agranular.},
  pmcid = {PMC3637647},
  pmid = {23129312},
  file = {/Users/xzfang/Zotero/storage/GJBUEWFB/Adams et al. - 2013 - Predictions not commands active inference in the .pdf}
}

@article{addleman_experiencedriven_2019,
  title = {Experience-{{Driven Auditory Attention}}},
  author = {Addleman, Douglas A. and Jiang, Yuhong V.},
  year = {2019},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {11},
  pages = {927--937},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.08.002},
  abstract = {In addition to conscious goals and stimulus salience, an observer's prior experience also influences selective attention. Early studies demonstrated experience-driven effects on attention mainly in the visual modality, but increasing evidence shows that experience drives auditory selection as well. We review evidence for a multiple-levels framework of auditory attention, in which experience-driven attention relies on mechanisms that acquire control settings and mechanisms that guide attention towards selected stimuli. Mechanisms of acquisition include cue\textendash target associative learning, reward learning, and sensitivity to prior selection history. Once acquired, implementation of these biases can occur either consciously or unconsciously. Future research should more fully characterize the sources of experience-driven auditory attention and investigate the neural mechanisms used to acquire and implement experience-driven auditory attention.},
  langid = {english},
  keywords = {auditory attention,experience-driven attention,multiple-levels framework,selection history},
  file = {/Users/xzfang/Zotero/storage/9Q6UHH8N/S1364661319302037.html}
}

@article{adelman_behavioral_2014,
  title = {A Behavioral Database for Masked Form Priming},
  author = {Adelman, James S. and Johnson, Rebecca L. and McCormick, Samantha F. and McKague, Meredith and Kinoshita, Sachiko and Bowers, Jeffrey S. and Perry, Jason R. and Lupker, Stephen J. and Forster, Kenneth I. and Cortese, Michael J. and Scaltritti, Michele and Aschenbrenner, Andrew J. and Coane, Jennifer H. and White, Laurence and Yap, Melvin J. and Davis, Chris and Kim, Jeesun and Davis, Colin J.},
  year = {2014},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {46},
  number = {4},
  pages = {1052--1067},
  issn = {1554-3528},
  doi = {10.3758/s13428-013-0442-y},
  abstract = {Reading involves a process of matching an orthographic input with stored representations in lexical memory. The masked priming paradigm has become a standard tool for investigating this process. Use of existing results from this paradigm can be limited by the precision of the data and the need for cross-experiment comparisons that lack normal experimental controls. Here, we present a single, large, high-precision, multicondition experiment to address these problems. Over 1,000 participants from 14 sites responded to 840 trials involving 28 different types of orthographically related primes (e.g., castfe\textendash CASTLE) in a lexical decision task, as well as completing measures of spelling and vocabulary. The data were indeed highly sensitive to differences between conditions: After correction for multiple comparisons, prime type condition differences of 2.90 ms and above reached significance at the 5\% level. This article presents the method of data collection and preliminary findings from these data, which included replications of the most widely agreed-upon differences between prime types, further evidence for systematic individual differences in susceptibility to priming, and new evidence regarding lexical properties associated with a target word's susceptibility to priming. These analyses will form a basis for the use of these data in quantitative model fitting and evaluation and for future exploration of these data that will inform and motivate new experiments.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/N9VNZ274/Adelman et al. - 2014 - A behavioral database for masked form priming.pdf}
}

@article{agrawal_compositional_2020,
  title = {A Compositional Neural Code in High-Level Visual Cortex Can Explain Jumbled Word Reading},
  author = {Agrawal, Aakash and Hari, KVS and Arun, SP},
  editor = {Baker, Chris I and {de Lange}, Floris P and Baker, Chris I},
  year = {2020},
  month = may,
  journal = {eLife},
  volume = {9},
  pages = {e54846},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.54846},
  abstract = {We read jubmled wrods effortlessly, but the neural correlates of this remarkable ability remain poorly understood. We hypothesized that viewing a jumbled word activates a visual representation that is compared to known words. To test this hypothesis, we devised a purely visual model in which neurons tuned to letter shape respond to longer strings in a compositional manner by linearly summing letter responses. We found that dissimilarities between letter strings in this model can explain human performance on visual search, and responses to jumbled words in word reading tasks. Brain imaging revealed that viewing a string activates this letter-based code in the lateral occipital (LO) region and that subsequent comparisons to stored words are consistent with activations of the visual word form area (VWFA). Thus, a compositional neural code potentially contributes to efficient reading.},
  keywords = {language,orthographic processing,reading,word recognition},
  file = {/Users/xzfang/Zotero/storage/PSGBPAWH/Agrawal et al. - 2020 - A compositional neural code in high-level visual c.pdf}
}

@article{aitchison_you_2017,
  title = {With or without You: Predictive Coding and {{Bayesian}} Inference in the Brain},
  shorttitle = {With or without You},
  author = {Aitchison, Laurence and Lengyel, M{\'a}t{\'e}},
  year = {2017},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {46},
  pages = {219--227},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2017.08.010},
  abstract = {Two theoretical ideas have emerged recently with the ambition to provide a unifying functional explanation of neural population coding and dynamics: predictive coding and Bayesian inference. Here, we describe the two theories and their combination into a single framework: Bayesian predictive coding. We clarify how the two theories can be distinguished, despite sharing core computational concepts and addressing an overlapping set of empirical phenomena. We argue that predictive coding is an algorithmic/representational motif that can serve several different computational goals of which Bayesian inference is but one. Conversely, while Bayesian inference can utilize predictive coding, it can also be realized by a variety of other representations. We critically evaluate the experimental evidence supporting Bayesian predictive coding and discuss how to test it more directly.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8ZVLA2UH/Aitchison and Lengyel - 2017 - With or without you predictive coding and Bayesia.pdf}
}

@article{ajmera_textindependent_2011,
  title = {Text-Independent Speaker Identification Using {{Radon}} and Discrete Cosine Transforms Based Features from Speech Spectrogram},
  author = {Ajmera, Pawan K. and Jadhav, Dattatray V. and Holambe, Raghunath S.},
  year = {2011},
  month = oct,
  journal = {Pattern Recognition},
  series = {Semi-{{Supervised Learning}} for {{Visual Content Analysis}} and {{Understanding}}},
  volume = {44},
  number = {10},
  pages = {2749--2759},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2011.04.009},
  abstract = {This paper presents a new feature extraction technique for speaker recognition using Radon transform (RT) and discrete cosine transform (DCT). The spectrogram is compact, efficient in representation and carries information about acoustic features in the form of pattern. In the proposed method, speaker specific features have been extracted by applying image processing techniques to the pattern available in the spectrogram. Radon transform has been used to derive the effective acoustic features from the speech spectrogram. Radon transform adds up the pixel values in the given image along a straight line in a particular direction and at a specific displacement. The proposed technique computes Radon projections for seven orientations and captures the acoustic characteristics of the spectrogram. DCT applied on Radon projections yields low dimensional feature vector. The technique is computationally efficient, text-independent, robust to session variations and insensitive to additive noise. The performance of the proposed algorithm has been evaluated using the Texas Instruments and Massachusetts Institute of Technology (TIMIT) and our own created Shri Guru Gobind Singhji (SGGS) databases. The recognition rate of the proposed algorithm on TIMIT database (consisting of 630 speakers) is 96.69\% and for SGGS database (consisting of 151 speakers) is 98.41\%. These results highlight the superiority of the proposed method over some of the existing algorithms.},
  langid = {english},
  keywords = {Discrete cosine transform,Feature extraction,Radon transform,Speaker recognition,Spectrogram},
  file = {/Users/xzfang/Zotero/storage/FYUNTSM7/Ajmera et al. - 2011 - Text-independent speaker identification using Rado.pdf;/Users/xzfang/Zotero/storage/SZCURV4X/S0031320311001671.html}
}

@article{akama_decoding_2012,
  title = {Decoding Semantics across {{fMRI}} Sessions with Different Stimulus Modalities: A Practical {{MVPA}} Study},
  shorttitle = {Decoding Semantics across {{fMRI}} Sessions with Different Stimulus Modalities},
  author = {Akama, Hiroyuki and Murphy, Brian and Na, Li and Shimizu, Yumiko and Poesio, Massimo},
  year = {2012},
  journal = {Frontiers in Neuroinformatics},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1662-5196},
  doi = {10.3389/fninf.2012.00024},
  abstract = {Both embodied and symbolic accounts of conceptual organization would predict partial sharing and partial differentiation between the neural activations seen for concepts activated via different stimulus modalities. But cross-participant and cross-session variability in BOLD activity patterns makes analyses of such patterns with MVPA methods challenging. Here we examine the effect of cross-modal and individual variation on the machine learning analysis of fMRI data recorded during a word property generation task. We present the same set of living and non-living concepts (land-mammals, or work tools) to a cohort of Japanese participants in two sessions: the first using auditory presentation of spoken words; the second using visual presentation of words written in Japanese characters. Classification accuracies confirmed that these semantic categories could be detected in single trials, with within-session predictive accuracies of 80-90\%. However cross-session prediction (learning from auditory-task data to classify data from the written-word-task, or vice-versa) suffered from a performance penalty, achieving 65-75\% (still individually significant at p\&lt;\&lt;0.05). We carried out several follow-on analyses to investigate the reason for this shortfall, concluding that distributional differences in neither time nor space alone could account for it. Rather, combined spatio-temporal patterns of activity need to be identified for successful cross-session learning, and this suggests that feature selection strategies could be modified to take advantage of this.},
  langid = {english},
  keywords = {computational neurolinguistics,embodiment,fMRI,GLM,individual variability,machine learning,MVPA},
  file = {/Users/xzfang/Zotero/storage/6HCY77RM/Akama et al. - 2012 - Decoding semantics across fMRI sessions with diffe.pdf}
}

@article{alain_agerelated_2007,
  title = {Age-{{Related Differences}} in {{Neuromagnetic Brain Activity Underlying Concurrent Sound Perception}}},
  author = {Alain, Claude and McDonald, Kelly L.},
  year = {2007},
  month = feb,
  journal = {The Journal of Neuroscience},
  volume = {27},
  number = {6},
  pages = {1308--1314},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.5433-06.2007},
  abstract = {Deficits in parsing concurrent auditory events are believed to contribute to older adults' difficulties in understanding speech in adverse listening conditions (e.g., cocktail party). To explore the level at which aging impairs sound segregation, we measured auditory evoked fields (AEFs) using magnetoencephalography while young, middle-aged, and older adults were presented with complex sounds that either had all of their harmonics in tune or had the third harmonic mistuned by 4 or 16\% of its original value. During the recording, participants were asked to ignore the stimuli and watch a muted subtitled movie of their choice. For each participant, the AEFs were modeled with a pair of dipoles in the superior temporal plane, and the effects of age and mistuning were examined on the amplitude and latency of the resulting source waveforms. Mistuned stimuli generated an early positivity (60\textendash 100 ms), an object-related negativity (ORN) (140\textendash 180 ms) that overlapped the N1 and P2 waves, and a positive displacement that peaked at {$\sim$}230 ms (P230) after sound onset. The early mistuning-related enhancement was similar in all three age groups, whereas the subsequent modulations (ORN and P230) were reduced in older adults. These age differences in auditory cortical activity were associated with a reduced likelihood of hearing two sounds as a function of mistuning. The results reveal that inharmonicity is rapidly and automatically registered in all three age groups but that the perception of concurrent sounds declines with age.},
  pmcid = {PMC6673581},
  pmid = {17287505},
  file = {/Users/xzfang/Zotero/storage/6BN9S38Z/Alain and McDonald - 2007 - Age-Related Differences in Neuromagnetic Brain Act.pdf}
}

@article{alain_what_2001,
  title = {"{{What}}" and "Where" in the Human Auditory System},
  author = {Alain, C. and Arnott, S. R. and Hevenor, S. and Graham, S. and Grady, C. L.},
  year = {2001},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {98},
  number = {21},
  pages = {12301--12306},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.211209098},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8LN6CISS/Alain et al. - 2001 - What and where in the human auditory system.pdf}
}

@article{albouy_distinct_2020,
  title = {Distinct Sensitivity to Spectrotemporal Modulation Supports Brain Asymmetry for Speech and Melody},
  author = {Albouy, Philippe and Benjamin, Lucas and Morillon, Benjamin and Zatorre, Robert J.},
  year = {2020},
  month = feb,
  journal = {Science},
  volume = {367},
  number = {6481},
  pages = {1043--1047},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaz3468},
  abstract = {Speech versus music in the brain To what extent does the perception of speech and music depend on different mechanisms in the human brain? What is the anatomical basis underlying this specialization? Albouy et al. created a corpus of a cappella songs that contain both speech (semantic) and music (melodic) information and degraded each stimulus selectively in either the temporal or spectral domain. Degradation of temporal information impaired speech recognition but not melody recognition, whereas degradation of spectral information impaired melody recognition but not speech recognition. Brain scanning revealed a right-left asymmetry for speech and music. Classification of speech content occurred exclusively in the left auditory cortex, whereas classification of melodic content occurred only in the right auditory cortex. Science, this issue p. 1043 Does brain asymmetry for speech and music emerge from acoustical cues or from domain-specific neural networks? We selectively filtered temporal or spectral modulations in sung speech stimuli for which verbal and melodic content was crossed and balanced. Perception of speech decreased only with degradation of temporal information, whereas perception of melodies decreased only with spectral degradation. Functional magnetic resonance imaging data showed that the neural decoding of speech and melodies depends on activity patterns in left and right auditory regions, respectively. This asymmetry is supported by specific sensitivity to spectrotemporal modulation rates within each region. Finally, the effects of degradation on perception were paralleled by their effects on neural classification. Our results suggest a match between acoustical properties of communicative signals and neural specializations adapted to that purpose. A human brain imaging study reveals that low-level acoustical cues support hemispheric lateralization of speech and music perception. A human brain imaging study reveals that low-level acoustical cues support hemispheric lateralization of speech and music perception.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {32108113},
  file = {/Users/xzfang/Zotero/storage/32RW5NIV/Albouy et al. - 2020 - Distinct sensitivity to spectrotemporal modulation.pdf;/Users/xzfang/Zotero/storage/MAP9BB8Y/1043.html}
}

@article{alday_eeg_2019,
  title = {M/{{EEG}} Analysis of Naturalistic Stories: A Review from Speech to Language Processing},
  shorttitle = {M/{{EEG}} Analysis of Naturalistic Stories},
  author = {Alday, Phillip M.},
  year = {2019},
  month = apr,
  journal = {Language, Cognition and Neuroscience},
  volume = {34},
  number = {4},
  pages = {457--473},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1546882},
  abstract = {M/EEG research using naturally spoken stories as stimuli has focused largely on speech and not language processing. The temporal resolution of M/EEG is a two-edged sword, allowing for the study of the fine acoustic structure of speech, yet easily overwhelmed by the temporal noise of variation in constituent length. Recent theories on the neural encoding of linguistic structure require the temporal resolution of M/EEG, yet suffer from confounds when studied on traditional, heavily controlled stimuli. Recent methodological advances allow for synthesising naturalistic designs and traditional, controlled designs into effective M/EEG research on naturalistic language. In this review, we highlight common threads throughout the at-times distinct research traditions of speech and language processing. We conclude by examining the tradeoffs and successes of three M/EEG studies on fully naturalistic language paradigms and the future directions they suggest.},
  keywords = {auditory perception,language,M/EEG,naturalistic stimuli,speech},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2018.1546882},
  file = {/Users/xzfang/Zotero/storage/H95NF6ES/Alday - 2019 - MEEG analysis of naturalistic stories a review f.pdf;/Users/xzfang/Zotero/storage/MH7YC9X7/23273798.2018.html}
}

@article{alday_electrophysiology_2017,
  title = {Electrophysiology {{Reveals}} the {{Neural Dynamics}} of {{Naturalistic Auditory Language Processing}}: {{Event-Related Potentials Reflect Continuous Model Updates}}},
  shorttitle = {Electrophysiology {{Reveals}} the {{Neural Dynamics}} of {{Naturalistic Auditory Language Processing}}},
  author = {Alday, Phillip M. and Schlesewsky, Matthias and {Bornkessel-Schlesewsky}, Ina},
  year = {2017},
  month = nov,
  journal = {eNeuro},
  volume = {4},
  number = {6},
  publisher = {{Society for Neuroscience}},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0311-16.2017},
  abstract = {The recent trend away from ANOVA-based analyses places experimental investigations into the neurobiology of cognition in more naturalistic and ecologically valid designs within reach. Using mixed-effects models for epoch-based regression, we demonstrate the feasibility of examining event-related potentials (ERPs), and in particular the N400, to study the neural dynamics of human auditory language processing in a naturalistic setting. Despite the large variability between trials during naturalistic stimulation, we replicated previous findings from the literature: the effects of frequency, animacy, and word order and find previously unexplored interaction effects. This suggests a new perspective on ERPs, namely, as a continuous modulation reflecting continuous stimulation instead of a series of discrete and essentially sequential processes locked to discrete events.},
  chapter = {New Research},
  copyright = {Copyright \textcopyright{} 2017 Alday et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {29379867},
  keywords = {ecological validity,mixed-effects models,naturalistic stimuli,predictive coding},
  file = {/Users/xzfang/Zotero/storage/VFAB334R/Alday et al. - 2017 - Electrophysiology Reveals the Neural Dynamics of N.pdf;/Users/xzfang/Zotero/storage/AQHIR6Y2/ENEURO.0311-16.html}
}

@article{alday_how_2019,
  title = {How Much Baseline Correction Do We Need in {{ERP}} Research? {{Extended GLM}} Model Can Replace Baseline Correction While Lifting Its Limits},
  shorttitle = {How Much Baseline Correction Do We Need in {{ERP}} Research?},
  author = {Alday, Phillip M.},
  year = {2019},
  journal = {Psychophysiology},
  volume = {56},
  number = {12},
  pages = {e13451},
  issn = {1469-8986},
  doi = {10.1111/psyp.13451},
  abstract = {Baseline correction plays an important role in past and current methodological debates in ERP research (e.g., the Tanner vs. Maess debate in the Journal of Neuroscience Methods), serving as a potential alternative to strong high-pass filtering. However, the very assumptions that underlie traditional baseline also undermine it, implying a reduction in the signal-to-noise ratio. In other words, traditional baseline correction is statistically unnecessary and even undesirable. Including the baseline interval as a predictor in a GLM-based statistical approach allows the data to determine how much baseline correction is needed, including both full traditional and no baseline correction as special cases. This reduces the amount of variance in the residual error term and thus has the potential to increase statistical power.},
  langid = {english},
  keywords = {analysis/statistical methods,EEG,ERPs,oscillation/time frequency analyses},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.13451},
  file = {/Users/xzfang/Zotero/storage/BF8CRSLH/Alday - 2019 - How much baseline correction do we need in ERP res.pdf;/Users/xzfang/Zotero/storage/GFG3U9Y5/psyp.html}
}

@article{alderete_cascading_2021,
  title = {Cascading Activation in Phonological Planning and Articulation: {{Evidence}} from Spontaneous Speech Errors},
  shorttitle = {Cascading Activation in Phonological Planning and Articulation},
  author = {Alderete, John and {Baese-Berk}, Melissa and Leung, Keith and Goldrick, Matthew},
  year = {2021},
  month = may,
  journal = {Cognition},
  volume = {210},
  pages = {104577},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104577},
  abstract = {Speaking involves both retrieving the sounds of a word (phonological planning) and realizing these selected sounds in fluid speech (articulation). Recent phonetic research on speech errors has argued that multiple candidate sounds in phonological planning can influence articulation because the pronunciation of mis-selected error sounds is slightly skewed towards unselected target sounds. Yet research to date has only examined these phonetic distortions in experimentally-elicited errors, leaving doubt as to whether they reflect tendencies in spontaneous speech. Here, we analyzed the pronunciation of speech errors of English-speaking adults in natural conversations relative to matched correct words by the same speakers, and found the conjectured phonetic distortions. Comparison of these data with a larger set of experimentally-elicited errors failed to reveal significant differences between the two types of errors. These findings provide ecologically-valid data supporting models that allow for information about multiple planning representations to simultaneously influence speech articulation.},
  langid = {english},
  keywords = {Articulation,Cascading activation,Phonetics,Phonological encoding,Speech errors,Speech production}
}

@article{alderson-day_inner_2015,
  title = {Inner {{Speech}}: {{Development}}, {{Cognitive Functions}}, {{Phenomenology}}, and {{Neurobiology}}},
  shorttitle = {Inner {{Speech}}},
  author = {{Alderson-Day}, Ben and Fernyhough, Charles},
  year = {2015},
  month = sep,
  journal = {Psychological Bulletin},
  volume = {141},
  number = {5},
  pages = {931--965},
  issn = {0033-2909},
  doi = {10.1037/bul0000021},
  abstract = {Inner speech\textemdash also known as covert speech or verbal thinking\textemdash has been implicated in theories of cognitive development, speech monitoring, executive function, and psychopathology. Despite a growing body of knowledge on its phenomenology, development, and function, approaches to the scientific study of inner speech have remained diffuse and largely unintegrated. This review examines prominent theoretical approaches to inner speech and methodological challenges in its study, before reviewing current evidence on inner speech in children and adults from both typical and atypical populations. We conclude by considering prospects for an integrated cognitive science of inner speech, and present a multicomponent model of the phenomenon informed by developmental, cognitive, and psycholinguistic considerations. Despite its variability among individuals and across the life span, inner speech appears to perform significant functions in human cognition, which in some cases reflect its developmental origins and its sharing of resources with other cognitive processes.},
  pmcid = {PMC4538954},
  pmid = {26011789},
  file = {/Users/xzfang/Zotero/storage/K5D4F2WN/Alderson-Day and Fernyhough - 2015 - Inner Speech Development, Cognitive Functions, Ph.pdf}
}

@article{alexander_reading_2008,
  title = {Reading Voices and Hearing Text: {{Talker-specific}} Auditory Imagery in Reading.},
  shorttitle = {Reading Voices and Hearing Text},
  author = {Alexander, Jessica D. and Nygaard, Lynne C.},
  year = {2008},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {34},
  number = {2},
  pages = {446--459},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.34.2.446},
  abstract = {A series of experiments was conducted to determine if linguistic representations accessed during reading include auditory imagery for characteristics of a talker's voice. In 3 experiments, participants were familiarized with two talkers during a brief prerecorded conversation. One talker spoke at a fast speaking rate, and one spoke at a slow speaking rate. Each talker was identified by name. At test, participants were asked to either read aloud (Experiment 1) or silently (Experiments 1, 2, and 3) a passage that they were told was written by either the fast or the slow talker. Reading times, both silent and aloud, were significantly slower when participants thought they were reading a passage written by the slow talker than when reading a passage written by the fast talker. Reading times differed as a function of passage author more for difficult than for easy texts, and individual differences in general auditory imagery ability were related to reading times. These results suggest that readers engage in a type of auditory imagery while reading that preserves the perceptual details of an author's voice.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/AI4TRXQV/Alexander and Nygaard - 2008 - Reading voices and hearing text Talker-specific a.pdf}
}

@article{alexander_specificity_2019,
  title = {Specificity and Generalization in Perceptual Adaptation to Accented Speech},
  author = {Alexander, Jessica E. D. and Nygaard, Lynne C.},
  year = {2019},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {145},
  number = {6},
  pages = {3382--3398},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.5110302},
  abstract = {The present study investigated the degree to which perceptual adaptation to foreign-accented speech is specific to the regularities in pronunciation associated with a particular accent. Across experiments, the conditions under which generalization of learning did or did not occur were evaluated. In Experiment 1, listeners trained on word-length utterances in Korean-accented English and tested with words produced by the same or different set of Korean-accented speakers. Listeners performed better than untrained controls when tested with novel words from the same or different speakers. In Experiment 2, listeners were trained with Spanish-, Korean-, or mixed-accented speech and transcribed novel words produced by unfamiliar Korean- or Spanish-accented speakers at test. The findings revealed relative specificity of learning. Listeners trained and tested on the same variety of accented speech showed better transcription at test than those trained with a different accent or untrained controls. Performance after mixed-accent training was intermediate. Patterns of errors and analysis of acoustic properties for accented vowels suggested perceptual improvement for regularities arising from each accent, with learning dependent on the relative similarity of linguistic form within and across accents.},
  file = {/Users/xzfang/Zotero/storage/UV8BZ38C/Alexander and Nygaard - 2019 - Specificity and generalization in perceptual adapt.pdf;/Users/xzfang/Zotero/storage/F9MHDX86/1.html}
}

@misc{ali_predictive_2021,
  title = {Predictive Coding Is a Consequence of Energy Efficiency in Recurrent Neural Networks},
  author = {Ali, Abdullahi and Ahmad, Nasir and de Groot, Elgar and van Gerven, Marcel A. J. and Kietzmann, Tim C.},
  year = {2021},
  month = nov,
  pages = {2021.02.16.430904},
  institution = {{bioRxiv}},
  doi = {10.1101/2021.02.16.430904},
  abstract = {Predictive coding represents a promising framework for understanding brain function. It postulates that the brain continuously inhibits predictable sensory input, ensuring a preferential processing of surprising elements. A central aspect of this view is its hierarchical connectivity, involving recurrent message passing between excitatory bottom-up signals and inhibitory top-down feedback. Here we use computational modelling to demonstrate that such architectural hard-wiring is not necessary. Rather, predictive coding is shown to emerge as a consequence of energy efficiency. When training recurrent neural networks to minimise their energy consumption while operating in predictive environments, the networks self-organise into prediction and error units with appropriate inhibitory and excitatory interconnections, and learn to inhibit predictable sensory input. Moving beyond the view of purely top-down driven predictions, we furthermore demonstrate, via virtual lesioning experiments, that networks perform predictions on two timescales: fast lateral predictions among sensory units, and slower prediction cycles that integrate evidence over time.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Q6LSQP7M/Ali et al. - 2021 - Predictive coding is a consequence of energy effic.pdf;/Users/xzfang/Zotero/storage/5VQVRWZC/2021.02.16.html}
}

@misc{alib-ms_science_,
  title = {The Science of Word Recognition - {{Typography}}},
  author = {{alib-ms}},
  howpublished = {https://docs.microsoft.com/en-us/typography/develop/word-recognition},
  langid = {american},
  file = {/Users/xzfang/Zotero/storage/XQIUAXVS/word-recognition.html}
}

@article{aliko_naturalistic_2020,
  title = {A Naturalistic Neuroimaging Database for Understanding the Brain Using Ecological Stimuli},
  author = {Aliko, Sarah and Huang, Jiawen and Gheorghiu, Florin and Meliss, Stefanie and Skipper, Jeremy I.},
  year = {2020},
  month = oct,
  journal = {Scientific Data},
  volume = {7},
  number = {1},
  pages = {347},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-00680-2},
  abstract = {Neuroimaging has advanced our understanding of human psychology using reductionist stimuli that often do not resemble information the brain naturally encounters. It has improved our understanding of the network organization of the brain mostly through analyses of `resting-state' data for which the functions of networks cannot be verifiably labelled. We make a `Naturalistic Neuroimaging Database' (NNDb v1.0) publically available to allow for a more complete understanding of the brain under more ecological conditions during which networks can be labelled. Eighty-six participants underwent behavioural testing and watched one of 10 full-length movies while functional magnetic resonance imaging was acquired. Resulting timeseries data are shown to be of high quality, with good signal-to-noise ratio, few outliers and low movement. Data-driven functional analyses provide further evidence of data quality. They also demonstrate accurate timeseries/movie alignment and how movie annotations might be used to label networks. The NNDb can be used to answer questions previously unaddressed with standard neuroimaging approaches, progressing our knowledge of how the brain works in the real world.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/A9MU8WUW/Aliko et al. - 2020 - A naturalistic neuroimaging database for understan.pdf;/Users/xzfang/Zotero/storage/9342TV28/s41597-020-00680-2.html}
}

@article{alin_multicollinearity_2010,
  title = {Multicollinearity},
  author = {Alin, Aylin},
  year = {2010},
  journal = {WIREs Computational Statistics},
  volume = {2},
  number = {3},
  pages = {370--374},
  issn = {1939-0068},
  doi = {10.1002/wics.84},
  abstract = {Multicollinearity refers to the linear relation among two or more variables. It is a data problem which may cause serious difficulty with the reliability of the estimates of the model parameters. In this article, multicollinearity among the explanatory variables in the multiple linear regression model is considered. Its effects on the linear regression model and some multicollinearity diagnostics for this model are presented. Copyright \textcopyright{} 2010 John Wiley \& Sons, Inc. This article is categorized under: Statistical Models {$>$} Linear Models Statistical Models {$>$} Multivariate Models},
  langid = {english},
  keywords = {collinearity,correlation,ill-conditioned data,linear regression,multicollinearity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.84},
  file = {/Users/xzfang/Zotero/storage/2K6JMMWV/Alin - 2010 - Multicollinearity.pdf;/Users/xzfang/Zotero/storage/E8HGS9E2/wics.html}
}

@article{allen_distinguishing_2012,
  title = {Distinguishing Grammatical Constructions with {{fMRI}} Pattern Analysis},
  author = {Allen, Kachina and Pereira, Francisco and Botvinick, Matthew and Goldberg, Adele E.},
  year = {2012},
  month = dec,
  journal = {Brain and Language},
  volume = {123},
  number = {3},
  pages = {174--182},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2012.08.005},
  abstract = {All linguistic and psycholinguistic theories aim to provide psychologically valid analyses of particular grammatical patterns and the relationships that hold among them. Until recently, no tools were available to distinguish neural correlates of particular grammatical constructions that shared the same content words, propositional meaning, and degree of surface complexity, such as the dative (e.g., Sally gave the book to Joe) and the ditransitive (e.g., Sally gave Joe a book). We report the first fMRI data that distinguish such closely related, abstract grammatical patterns. Multi-voxel pattern analysis (MVPA) proved capable of discriminating at above-chance levels between activity patterns arising during reading of dative and ditransitive sentences. Region-of-interest analyses reveal that the union of certain language-relevant areas, anterior and posterior BA22, BA44/45 and BA47, yield classification accuracy above chance and above that of control conditions in the left hemisphere but not in the right. Looking more closely at the LH ROIs, we find that the combination of areas aBA22 and BA47 is sufficient to distinguish the two constructions better than the controls and better than chance. The fact that both of these areas\textemdash particularly BA47\textemdash have been implicated in semantics, lends support to claims that the two constructions are distinguishable semantically. More generally, the ability to distinguish closely related grammatical constructions using MVPA offers the promise of addressing traditional theoretical questions on a neuroscientifically grounded basis.},
  langid = {english},
  keywords = {BA22,BA47,fMRI,Grammatical constructions,MVPA},
  file = {/Users/xzfang/Zotero/storage/5FCSKIQY/Allen et al. - 2012 - Distinguishing grammatical constructions with fMRI.pdf;/Users/xzfang/Zotero/storage/9Y2KPZQF/S0093934X12001599.html}
}

@article{allen_listener_2004,
  title = {Listener Sensitivity to Individual Talker Differences in Voice-Onset-Time},
  author = {Allen, J. Sean and Miller, Joanne L.},
  year = {2004},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {115},
  number = {6},
  pages = {3171--3183},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.1701898},
  file = {/Users/xzfang/Zotero/storage/XI8DW75K/Allen and Miller - 2004 - Listener sensitivity to individual talker differen.pdf;/Users/xzfang/Zotero/storage/UYSJTV6Q/1.html}
}

@article{allen_representations_2017,
  title = {Representations of {{Pitch}} and {{Timbre Variation}} in {{Human Auditory Cortex}}},
  author = {Allen, Emily J. and Burton, Philip C. and Olman, Cheryl A. and Oxenham, Andrew J.},
  year = {2017},
  month = feb,
  journal = {The Journal of Neuroscience},
  volume = {37},
  number = {5},
  pages = {1284--1293},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.2336-16.2016},
  abstract = {Pitch and timbre are two primary dimensions of auditory perception, but how they are represented in the human brain remains a matter of contention. Some animal studies of auditory cortical processing have suggested modular processing, with different brain regions preferentially coding for pitch or timbre, whereas other studies have suggested a distributed code for different attributes across the same population of neurons. This study tested whether variations in pitch and timbre elicit activity in distinct regions of the human temporal lobes. Listeners were presented with sequences of sounds that varied in either fundamental frequency (eliciting changes in pitch) or spectral centroid (eliciting changes in brightness, an important attribute of timbre), with the degree of pitch or timbre variation in each sequence parametrically manipulated. The BOLD responses from auditory cortex increased with increasing sequence variance along each perceptual dimension. The spatial extent, region, and laterality of the cortical regions most responsive to variations in pitch or timbre at the univariate level of analysis were largely overlapping. However, patterns of activation in response to pitch or timbre variations were discriminable in most subjects at an individual level using multivoxel pattern analysis, suggesting a distributed coding of the two dimensions bilaterally in human auditory cortex., SIGNIFICANCE STATEMENT Pitch and timbre are two crucial aspects of auditory perception. Pitch governs our perception of musical melodies and harmonies, and conveys both prosodic and (in tone languages) lexical information in speech. Brightness\textemdash an aspect of timbre or sound quality\textemdash allows us to distinguish different musical instruments and speech sounds. Frequency-mapping studies have revealed tonotopic organization in primary auditory cortex, but the use of pure tones or noise bands has precluded the possibility of dissociating pitch from brightness. Our results suggest a distributed code, with no clear anatomical distinctions between auditory cortical regions responsive to changes in either pitch or timbre, but also reveal a population code that can differentiate between changes in either dimension within the same cortical regions.},
  pmcid = {PMC5296797},
  pmid = {28025255},
  file = {/Users/xzfang/Zotero/storage/7GJ44Q5Y/Allen et al. - 2017 - Representations of Pitch and Timbre Variation in H.pdf}
}

@article{allopenna_tracking_1998,
  title = {Tracking the {{Time Course}} of {{Spoken Word Recognition Using Eye Movements}}: {{Evidence}} for {{Continuous Mapping Models}}},
  shorttitle = {Tracking the {{Time Course}} of {{Spoken Word Recognition Using Eye Movements}}},
  author = {Allopenna, Paul D. and Magnuson, James S. and Tanenhaus, Michael K.},
  year = {1998},
  month = may,
  journal = {Journal of Memory and Language},
  volume = {38},
  number = {4},
  pages = {419--439},
  issn = {0749-596X},
  doi = {10.1006/jmla.1997.2558},
  abstract = {Eye movements to pictures of four objects on a screen were monitored as participants followed a spoken instruction to move one of the objects, e.g., ``Pick up the beaker; now put it below the diamond'' (Experiment 1) or heard progressively larger gates and tried to identify the referent (Experiment 2). The distractor objects included a cohort competitor with a name that began with the same onset and vowel as the name of the target object (e.g.,beetle), a rhyme competitor (e.g.speaker), and an unrelated competitor (e.g.,carriage). In Experiment 1, there was clear evidence for both cohort and rhyme activation as predicted by continuous mapping models such as TRACE (McClelland and Elman, 1986) and Shortlist (Norris, 1994). Additionally, the time course and probabilities of eye movements closely corresponded to response probabilities derived from TRACE simulations using the Luce choice rule (Luce, 1959). In the gating task, which emphasizes word-initial information, there was clear evidence for multiple activation of cohort members, as measured by judgments and eye movements, but no suggestion of rhyme effects. Given that the same sets of pictures were present during the gating task as in Experiment 1, we conclude that the rhyme effects in Experiment 1 were not an artifact of using a small set of visible alternatives.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/THVHZUB6/Allopenna et al. - 1998 - Tracking the Time Course of Spoken Word Recognitio.pdf;/Users/xzfang/Zotero/storage/KVLVZ999/S0749596X97925584.html}
}

@article{alvarez_representing_2011,
  title = {Representing Multiple Objects as an Ensemble Enhances Visual Cognition},
  author = {Alvarez, George A.},
  year = {2011},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {15},
  number = {3},
  pages = {122--131},
  issn = {13646613},
  doi = {10.1016/j.tics.2011.01.003},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5EFMI8V9/Alvarez - 2011 - Representing multiple objects as an ensemble enhan.pdf}
}

@article{aly_learning_,
  title = {Learning {{Naturalistic Temporal Structure}} in the {{Posterior Medial Network}}},
  author = {Aly, Mariam and Chen, Janice and {Turk-Browne}, Nicholas B and Hasson, Uri},
  volume = {30},
  number = {9},
  pages = {21},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RW9WFFTH/Aly et al. - Learning Naturalistic Temporal Structure in the Po.pdf}
}

@article{andermann_neuromagnetic_2017,
  title = {Neuromagnetic Correlates of Voice Pitch, Vowel Type, and Speaker Size in Auditory Cortex},
  author = {Andermann, Martin and Patterson, Roy D. and Vogt, Carolin and Winterstetter, Lisa and Rupp, Andr{\'e}},
  year = {2017},
  month = sep,
  journal = {NeuroImage},
  volume = {158},
  pages = {79--89},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2017.06.065},
  abstract = {Vowel recognition is largely immune to differences in speaker size despite the waveform differences associated with variation in speaker size. This has led to the suggestion that voice pitch and mean formant frequency (MFF) are extracted early in the hierarchy of hearing/speech processing and used to normalize the internal representation of vowel sounds. This paper presents a magnetoencephalographic (MEG) experiment designed to locate and compare neuromagnetic activity associated with voice pitch, MFF and vowel type in human auditory cortex. Sequences of six sustained vowels were used to contrast changes in the three components of vowel perception, and MEG responses to the changes were recorded from 25 participants. A staged procedure was employed to fit the MEG data with a source model having one bilateral pair of dipoles for each component of vowel perception. This dipole model showed that the activity associated with the three perceptual changes was functionally separable; the pitch source was located in Heschl's gyrus (bilaterally), while the vowel-type and formant-frequency sources were located (bilaterally) just behind Heschl's gyrus in planum temporale. The results confirm that vowel normalization begins in auditory cortex at an early point in the hierarchy of speech processing.},
  langid = {english},
  keywords = {Auditory perception,Magnetoencephalography,Speaker size,Vowel type},
  file = {/Users/xzfang/Zotero/storage/9CQRMQBT/S1053811917305360.html}
}

@article{andermann_neuromagnetic_2017a,
  title = {Neuromagnetic Correlates of Voice Pitch, Vowel Type, and Speaker Size in Auditory Cortex},
  author = {Andermann, Martin and Patterson, Roy D. and Vogt, Carolin and Winterstetter, Lisa and Rupp, Andr{\'e}},
  year = {2017},
  month = sep,
  journal = {NeuroImage},
  volume = {158},
  pages = {79--89},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2017.06.065},
  abstract = {Vowel recognition is largely immune to differences in speaker size despite the waveform differences associated with variation in speaker size. This has led to the suggestion that voice pitch and mean formant frequency (MFF) are extracted early in the hierarchy of hearing/speech processing and used to normalize the internal representation of vowel sounds. This paper presents a magnetoencephalographic (MEG) experiment designed to locate and compare neuromagnetic activity associated with voice pitch, MFF and vowel type in human auditory cortex. Sequences of six sustained vowels were used to contrast changes in the three components of vowel perception, and MEG responses to the changes were recorded from 25 participants. A staged procedure was employed to fit the MEG data with a source model having one bilateral pair of dipoles for each component of vowel perception. This dipole model showed that the activity associated with the three perceptual changes was functionally separable; the pitch source was located in Heschl's gyrus (bilaterally), while the vowel-type and formant-frequency sources were located (bilaterally) just behind Heschl's gyrus in planum temporale. The results confirm that vowel normalization begins in auditory cortex at an early point in the hierarchy of speech processing.},
  langid = {english},
  keywords = {Auditory perception,Magnetoencephalography,Speaker size,Vowel type}
}

@misc{andersen_play_2021,
  title = {Play in {{Predictive Minds}}: {{A Cognitive Theory}} of {{Play}}},
  shorttitle = {Play in {{Predictive Minds}}},
  author = {Andersen, Marc Malmdorf and Roepstorff, Andreas},
  year = {2021},
  month = feb,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/u86qy},
  abstract = {Play occurs universally in human beings, and it has been a subject of considerable academic scrutiny for over 100 years. In this article, we propose a cognitive theory of play building on recent advances in cognitive and computational neuroscience that portray the human brain as an advanced prediction machine. Central to the theory is the idea that when an agent is free from the demands of certain competing cognitive systems, it may deliberately seek out and create surprising situations that gravitate towards sweet-spots of relative complexity. We argue that this framework can explain why humans play and why playing is so fun and rewarding.},
  keywords = {Cognitive Neuroscience,Cognitive Psychology,Computational Neuroscience,Developmental Psychology,Emotion,Learning,Neuroscience,Niche Construction,Play,Predictive Processing,Social and Behavioral Sciences,Surprise},
  file = {/Users/xzfang/Zotero/storage/WCBB6LES/Andersen and Roepstorff - 2021 - Play in Predictive Minds A Cognitive Theory of Pl.pdf}
}

@article{anderson_integrated_2019,
  title = {An {{Integrated Neural Decoder}} of {{Linguistic}} and {{Experiential Meaning}}},
  author = {Anderson, Andrew James and Binder, Jeffrey R. and Fernandino, Leonardo and Humphries, Colin J. and Conant, Lisa L. and Raizada, Rajeev D. S. and Lin, Feng and Lalor, Edmund C.},
  year = {2019},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {45},
  pages = {8969--8987},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2575-18.2019},
  abstract = {The brain is thought to combine linguistic knowledge of words and nonlinguistic knowledge of their referents to encode sentence meaning. However, functional neuroimaging studies aiming at decoding language meaning from neural activity have mostly relied on distributional models of word semantics, which are based on patterns of word co-occurrence in text corpora. Here, we present initial evidence that modeling nonlinguistic ``experiential'' knowledge contributes to decoding neural representations of sentence meaning. We model attributes of peoples' sensory, motor, social, emotional, and cognitive experiences with words using behavioral ratings. We demonstrate that fMRI activation elicited in sentence reading is more accurately decoded when this experiential attribute model is integrated with a text-based model than when either model is applied in isolation (participants were 5 males and 9 females). Our decoding approach exploits a representation-similarity-based framework, which benefits from being parameter free, while performing at accuracy levels comparable with those from parameter fitting approaches, such as ridge regression. We find that the text-based model contributes particularly to the decoding of sentences containing linguistically oriented ``abstract'' words and reveal tentative evidence that the experiential model improves decoding of more concrete sentences. Finally, we introduce a cross-participant decoding method to estimate an upper bound on model-based decoding accuracy. We demonstrate that a substantial fraction of neural signal remains unexplained, and leverage this gap to pinpoint characteristics of weakly decoded sentences and hence identify model weaknesses to guide future model development. SIGNIFICANCE STATEMENT Language gives humans the unique ability to communicate about historical events, theoretical concepts, and fiction. Although words are learned through language and defined by their relations to other words in dictionaries, our understanding of word meaning presumably draws heavily on our nonlinguistic sensory, motor, interoceptive, and emotional experiences with words and their referents. Behavioral experiments lend support to the intuition that word meaning integrates aspects of linguistic and nonlinguistic ``experiential'' knowledge. However, behavioral measures do not provide a window on how meaning is represented in the brain and tend to necessitate artificial experimental paradigms. We present a model-based approach that reveals early evidence that experiential and linguistically acquired knowledge can be detected in brain activity elicited in reading natural sentences.},
  copyright = {Copyright \textcopyright{} 2019 the authors},
  langid = {english},
  pmid = {31570538},
  keywords = {concepts,fMRI,lexical semantics,multivoxel pattern analysis,semantic model,sentence comprehension},
  file = {/Users/xzfang/Zotero/storage/TBCPBK2G/8969.html}
}

@article{angele_effect_2014,
  title = {The Effect of High- and Low-Frequency Previews and Sentential Fit on Word Skipping during Reading},
  author = {Angele, Bernhard and Laishley, Abby and Rayner, Keith and Liversedge, Simon P.},
  year = {2014},
  month = jul,
  journal = {Journal of experimental psychology. Learning, memory, and cognition},
  volume = {40},
  number = {4},
  pages = {1181--1203},
  issn = {0278-7393},
  doi = {10.1037/a0036396},
  abstract = {In a previous gaze-contingent boundary experiment, Angele and Rayner (2012) found that readers are likely to skip a word that appears to be the definite article the even when syntactic constraints do not allow for articles to occur in that position. In the present study, we investigated whether the word frequency of the preview of a three-letter target word influences a reader's decision to fixate or skip that word. We found that the word frequency rather than the felicitousness (syntactic fit) of the preview affected how often the upcoming word was skipped. These results indicate that visual information about the upcoming word trumps information from the sentence context when it comes to making a skipping decision. Skipping parafoveal instances of the therefore may simply be an extreme case of skipping high-frequency words.},
  pmcid = {PMC4100595},
  pmid = {24707791},
  file = {/Users/xzfang/Zotero/storage/33Q2DFPZ/Angele et al. - 2014 - The effect of high- and low-frequency previews and.pdf}
}

@article{angele_parafovealfoveal_2013,
  title = {Parafoveal-Foveal {{Overlap Can Facilitate Ongoing Word Identification During Reading}}: {{Evidence}} from {{Eye Movements}}},
  shorttitle = {Parafoveal-Foveal {{Overlap Can Facilitate Ongoing Word Identification During Reading}}},
  author = {Angele, Bernhard and Tran, Randy and Rayner, Keith},
  year = {2013},
  month = apr,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {39},
  number = {2},
  pages = {526--538},
  issn = {0096-1523},
  doi = {10.1037/a0029492},
  abstract = {Readers continuously receive parafoveal information about the upcoming word in addition to the foveal information about the currently fixated word. Previous research () showed that the presence of a parafoveal word which was similar to the foveal word facilitated processing of the foveal word. In three experiments, we used the gaze-contingent boundary paradigm () to manipulate the parafoveal information that subjects received before or while fixating a target word (e.g. news) within a sentence. Specifically a reader's parafovea could contain a repetition of the target (news), a correct preview of the post-target word (once), an unrelated word (warm), random letters (cxmr), a nonword neighbor of the target (niws), a semantically related word (tale), or a nonword neighbor of that word (tule). Target fixation times were significantly lower in the parafoveal repetition condition than in all other conditions, suggesting that foveal processing can be facilitated by parafoveal repetition. We present a simple model framework that can account for these effects.},
  pmcid = {PMC3596446},
  pmid = {22866764},
  file = {/Users/xzfang/Zotero/storage/2882LWJH/Angele et al. - 2013 - Parafoveal-foveal Overlap Can Facilitate Ongoing W.pdf}
}

@article{angele_processing_2013,
  title = {Processing the in the Parafovea: {{Are}} Articles Skipped Automatically?},
  shorttitle = {Processing the in the Parafovea},
  author = {Angele, Bernhard and Rayner, Keith},
  year = {2013},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {39},
  number = {2},
  pages = {649--662},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/a0029294},
  abstract = {One of the words that readers of English skip most often is the definite article the. Most accounts of reading assume that in order for a reader to skip a word, it must have received some lexical processing. The definite article is skipped so regularly, however, that the oculomotor system might have learned to skip the letter string t-h-e automatically. We tested whether skipping of articles in English is sensitive to context information or whether it is truly automatic in the sense that any occurrence of the letter string the will trigger a skip. This was done using the gaze-contingent boundary paradigm (Rayner, 1975) to provide readers with false parafoveal previews of the article the. All experimental sentences contained a short target verb, the preview of which could be correct (i.e., identical to the actual subsequent word in the sentence; e.g., ace), a nonword (tda), or an infelicitous article preview (the). Our results indicated that readers tended to skip the infelicitous the previews frequently, suggesting that, in many cases, they seemed to be unable to detect the syntactic anomaly in the preview and based their skipping decision solely on the orthographic properties of the article. However, there was some evidence that readers sometimes detected the anomaly, as they also showed increased skipping of the pretarget word in the the preview condition. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Eye Movements,Foveal Vision,Psycholinguistics,Reading,Syntax,Words (Phonetic Units)},
  file = {/Users/xzfang/Zotero/storage/B2S85GRA/2012-18644-001.html}
}

@article{antony_cued_2012,
  title = {Cued {{Memory Reactivation During Sleep Influences Skill Learning}}},
  author = {Antony, James W. and Gobel, Eric W. and O'Hare, Justin K. and Reber, Paul J. and Paller, Ken A.},
  year = {2012},
  month = jun,
  journal = {Nature neuroscience},
  volume = {15},
  number = {8},
  pages = {1114--1116},
  issn = {1097-6256},
  doi = {10.1038/nn.3152},
  abstract = {Information acquired during waking can be reactivated during sleep, promoting memory stabilization. After people learned to produce two melodies in time with moving visual symbols, we produced a relative improvement in performance by presenting one melody during an afternoon nap. Electrophysiological signs of memory processing during sleep corroborated the notion that appropriate auditory stimulation that does not disrupt sleep can nevertheless bias memory consolidation in relevant brain circuitry.},
  pmcid = {PMC3498459},
  pmid = {22751035},
  file = {/Users/xzfang/Zotero/storage/GFNNR2CV/Antony et al. - 2012 - Cued Memory Reactivation During Sleep Influences S.pdf}
}

@article{apfelbaum_relative_2015,
  title = {Relative Cue Encoding in the Context of Sophisticated Models of Categorization: {{Separating}} Information from Categorization},
  shorttitle = {Relative Cue Encoding in the Context of Sophisticated Models of Categorization},
  author = {Apfelbaum, Keith S. and McMurray, Bob},
  year = {2015},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {22},
  number = {4},
  pages = {916--943},
  issn = {1531-5320},
  doi = {10.3758/s13423-014-0783-2},
  abstract = {Traditional studies of human categorization often treat the processes of encoding features and cues as peripheral to the question of how stimuli are categorized. However, in domains where the features and cues are less transparent, how information is encoded prior to categorization may constrain our understanding of the architecture of categorization. This is particularly true in speech perception, where acoustic cues to phonological categories are ambiguous and influenced by multiple factors. Here, it is crucial to consider the joint contributions of the information in the input and the categorization architecture. We contrasted accounts that argue for raw acoustic information encoding with accounts that posit that cues are encoded relative to expectations, and investigated how two categorization architectures\textemdash exemplar models and back-propagation parallel distributed processing models\textemdash deal with each kind of information. Relative encoding, akin to predictive coding, is a form of noise reduction, so it can be expected to improve model accuracy; however, like predictive coding, the use of relative encoding in speech perception by humans is controversial, so results are compared to patterns of human performance, rather than on the basis of overall accuracy. We found that, for both classes of models, in the vast majority of parameter settings, relative cues greatly helped the models approximate human performance. This suggests that expectation-relative processing is a crucial precursor step in phoneme categorization, and that understanding the information content is essential to understanding categorization processes.},
  langid = {english},
  keywords = {Categorization,Exemplar models,Expectation-relative processing,PDP models,Predictive coding,Speech perception},
  file = {/Users/xzfang/Zotero/storage/FCQNK5TV/Apfelbaum and McMurray - 2015 - Relative cue encoding in the context of sophistica.pdf}
}

@article{araragi_evidence_2012,
  title = {Evidence for a {{Size Underestimation}} of {{Upright Faces}}},
  author = {Araragi, Yukyu and Aotani, Takehiro and Kitaoka, Akiyoshi},
  year = {2012},
  month = jul,
  journal = {Perception},
  volume = {41},
  number = {7},
  pages = {840--853},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0301-0066},
  doi = {10.1068/p7058},
  abstract = {We quantitatively examined the difference in perceived size between upright and inverted faces using the method of constant stimuli. The stimuli included eight face images modified from two cartoon faces produced by Kitaoka (2007, http://www.psy.ritsumei.ac.jp/{$\sim$}akitaoka/kao-e.html and 2008, Cognitive Psychology5 177\textendash 185) and six photographic faces, including a photographic face used by Thompson (2010, http://illusioncontest.neuralcorrelate.com/2010/the-fat-face-thin-fft-illusion/). Experiment 1 showed that an upright face and outline were perceived to be significantly smaller than an inverted face and outline, respectively. Moreover, the amount of the size underestimation in the face stimulus condition was significantly larger than that in the outline stimulus condition. Experiment 2 showed that an upright face was perceived to be significantly smaller than 90\textdegree{} and 270\textdegree{} rotated faces, whereas an inverted face was not perceived to be significantly larger than a 90\textdegree{} or 270\textdegree{} rotated face. Experiment 3 showed that upright faces were perceived to be significantly smaller than upright and inverted outlines, whereas inverted faces were not perceived to be significantly larger than upright or inverted outlines. Experiments 4 and 5 showed that upright photographic faces were also perceived to be significantly smaller than inverted photographic faces. These results provide quantitative evidence for a size underestimation of upright faces.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QB2AT2RG/Araragi et al. - 2012 - Evidence for a Size Underestimation of Upright Fac.pdf}
}

@article{armeni_frequencyspecific_2019,
  title = {Frequency-Specific Brain Dynamics Related to Prediction during Language Comprehension},
  author = {Armeni, Kristijan and Willems, Roel M. and {van den Bosch}, Antal and Schoffelen, Jan-Mathijs},
  year = {2019},
  month = sep,
  journal = {NeuroImage},
  volume = {198},
  pages = {283--295},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2019.04.083},
  abstract = {The brain's remarkable capacity to process spoken language virtually in real time requires fast and efficient information processing machinery. In this study, we investigated how frequency-specific brain dynamics relate to models of probabilistic language prediction during auditory narrative comprehension. We recorded MEG activity while participants were listening to auditory stories in Dutch. Using trigram statistical language models, we estimated for every word in a story its conditional probability of occurrence. On the basis of word probabilities, we computed how unexpected the current word is given its context (word perplexity) and how (un)predictable the current linguistic context is (word entropy). We then evaluated whether source-reconstructed MEG oscillations at different frequency bands are modulated as a function of these language processing metrics. We show that theta-band source dynamics are increased in high relative to low entropy states, likely reflecting lexical computations. Beta-band dynamics are increased in situations of low word entropy and perplexity possibly reflecting maintenance of ongoing cognitive context. These findings lend support to the idea that the brain engages in the active generation and evaluation of predicted language based on the statistical properties of the input signal.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DLF7G67F/Armeni et al. - 2019 - Frequency-specific brain dynamics related to predi.pdf}
}

@article{armeni_probabilistic_2017,
  title = {Probabilistic Language Models in Cognitive Neuroscience: {{Promises}} and Pitfalls},
  shorttitle = {Probabilistic Language Models in Cognitive Neuroscience},
  author = {Armeni, Kristijan and Willems, Roel M. and Frank, Stefan L.},
  year = {2017},
  month = dec,
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {83},
  pages = {579--588},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2017.09.001},
  abstract = {Cognitive neuroscientists of language comprehension study how neural computations relate to cognitive computations during comprehension. On the cognitive part of the equation, it is important that the computations and processing complexity are explicitly defined. Probabilistic language models can be used to give a computationally explicit account of language complexity during comprehension. Whereas such models have so far predominantly been evaluated against behavioral data, only recently have the models been used to explain neurobiological signals. Measures obtained from these models emphasize the probabilistic, information-processing view of language understanding and provide a set of tools that can be used for testing neural hypotheses about language comprehension. Here, we provide a cursory review of the theoretical foundations and example neuroimaging studies employing probabilistic language models. We highlight the advantages and potential pitfalls of this approach and indicate avenues for future research.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/G46NM7JR/Armeni et al. - 2017 - Probabilistic language models in cognitive neurosc.pdf}
}

@article{arnold_rapid_2000,
  title = {The Rapid Use of Gender Information: Evidence of the Time Course of Pronoun Resolution from Eyetracking},
  shorttitle = {The Rapid Use of Gender Information},
  author = {Arnold, J},
  year = {2000},
  month = jul,
  journal = {Cognition},
  volume = {76},
  number = {1},
  pages = {B13-B26},
  issn = {00100277},
  doi = {10.1016/S0010-0277(00)00073-1},
  abstract = {Eye movements of listeners were monitored to investigate how gender information and accessibility in\textasciimacron uence the initial processes of pronoun interpretation. Previous studies on this issue have produced mixed results, and several studies have concluded that gender cues are not automatically used during the early processes of pronoun interpretation (e.g. Garnham, A., Oakhill, J. \& Cruttenden, H. (1992). The role of implicit causality and gender cue in the interpretation of pronouns. Language and Cognitive Processes, 73 (4), 231{$\pm$}255; Greene, S. B., McKoon, G. \& Ratcliff, R. (1992). Pronoun resolution and discourse models. Journal of Experimental Psychology: Learning, Memory, and Cognition, 182, 266{$\pm$}283). In the two experiments presented here, participants viewed a picture with two familiar cartoon characters of either same or different gender. They listened to a text describing the picture, in which a pronoun referred to either the \textregistered rst, more accessible, character, or the second. (For example, Donald is bringing some mail to \{Mickey/Minnie\} while a violent storm is beginning. He's carrying an umbrella{$\frac{1}{4}$}.) The results of both experiments show rapid use of both gender and accessibility at approximately 200 ms after the pronoun offset. q 2000 Published by Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PRST7AAN/Arnold - 2000 - The rapid use of gender information evidence of t.pdf}
}

@article{aron_reward_2005,
  title = {Reward, {{Motivation}}, and {{Emotion Systems Associated With Early-Stage Intense Romantic Love}}},
  author = {Aron, Arthur and Fisher, Helen and Mashek, Debra J. and Strong, Greg and Li, Haifang and Brown, Lucy L.},
  year = {2005},
  month = jul,
  journal = {Journal of Neurophysiology},
  volume = {94},
  number = {1},
  pages = {327--337},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00838.2004},
  abstract = {Early-stage romantic love can induce euphoria, is a cross-cultural phenomenon, and is possibly a developed form of a mammalian drive to pursue preferred mates. It has an important influence on social behaviors that have reproductive and genetic consequences. To determine which reward and motivation systems may be involved, we used functional magnetic resonance imaging and studied 10 women and 7 men who were intensely ``in love'' from 1 to 17 mo. Participants alternately viewed a photograph of their beloved and a photograph of a familiar individual, interspersed with a distraction-attention task. Group activation specific to the beloved under the two control conditions occurred in dopamine-rich areas associated with mammalian reward and motivation, namely the right ventral tegmental area and the right postero-dorsal body and medial caudate nucleus. Activation in the left ventral tegmental area was correlated with facial attractiveness scores. Activation in the right anteromedial caudate was correlated with questionnaire scores that quantified intensity of romantic passion. In the left insula-putamen-globus pallidus, activation correlated with trait affect intensity. The results suggest that romantic love uses subcortical reward and motivation systems to focus on a specific individual, that limbic cortical regions process individual emotion factors, and that there is localization heterogeneity for reward functions in the human brain.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2XWCMW8P/Aron et al. - 2005 - Reward, Motivation, and Emotion Systems Associated.pdf}
}

@article{ashby_neuropsychological_1998,
  title = {A Neuropsychological Theory of Multiple Systems in Category Learning},
  author = {Ashby, F. Gregory and {Alfonso-reese}, Leola A. and Turken, U. and Waldron, Elliott M.},
  year = {1998},
  journal = {Psychological Review},
  pages = {442--481},
  abstract = {A neuropsychological theory is proposed that assumes category learning is a competition between separate verbal and implicit (i.e., procedural-learning-based) categorization systems. The theory assumes that the caudate nucleus is an important component of the implicit system and that the anterior cingulate and prefrontal cortices are critical to the verbal system. In addition to making predictions for normal human adults, the theory makes specific predictions for children, elderly people, and patients suffering from Parkinson's disease, Huntington's disease, major depression, amnesia, or lesions of the prefrontal cortex. Two separate formal descriptions of the theory are also provided. One describes trial-by-trial learning, and the other describes global dynamics. The theory is tested on published neuropsychological data and on category learning data with normal adults. Humans are remarkably adept at categorizing objects and events in their environment. In fact, it is now well established that humans can learn some extremely complex (i.e., nonlinear)},
  file = {/Users/xzfang/Zotero/storage/S3W4CJVS/Ashby et al. - 1998 - A neuropsychological theory of multiple systems in.pdf;/Users/xzfang/Zotero/storage/FRDA9WSV/summary.html}
}

@article{auerbach-asch_face_2020,
  title = {Face {{Selective Neural Activity}}: {{Comparisons Between Fixed}} and {{Free Viewing}}},
  shorttitle = {Face {{Selective Neural Activity}}},
  author = {{Auerbach-Asch}, Carmel R. and Bein, Oded and Deouell, Leon Y.},
  year = {2020},
  month = may,
  journal = {Brain Topography},
  volume = {33},
  number = {3},
  pages = {336--354},
  issn = {1573-6792},
  doi = {10.1007/s10548-020-00764-7},
  abstract = {Event Related Potentials (ERPs) are widely used to study category-selective EEG responses to visual stimuli, such as the face-selective N170 component. Typically, this is done by flashing stimuli at the point of static gaze fixation. While allowing for good experimental control, these paradigms ignore the dynamic role of eye-movements in natural vision. Fixation-related potentials (FRPs), obtained using simultaneous EEG and eye-tracking, overcome this limitation. Various studies have used FRPs to study processes such as lexical processing, target detection and attention allocation. The goal of this study was to carefully compare face-sensitive activity time-locked to an abrupt stimulus onset at fixation, with that time-locked to a self-generated fixation on a stimulus. Twelve participants participated in three experimental conditions: Free-viewing (FRPs), Cued-viewing (FRPs) and Control (ERPs). We used a multiple regression approach to disentangle overlapping activity components. Our results show that the N170 face-effect is evident for the first fixation on a stimulus, whether it follows a self-generated saccade or stimulus appearance at fixation point. The N170 face-effect has similar topography across viewing conditions, but there were major differences within each stimulus category. We ascribe these differences to an overlap of the fixation-related lambda response and the N170. We tested the plausibility of this account using dipole simulations. Finally, the N170 exhibits category-specific adaptation in free viewing. This study establishes the comparability of the free-viewing N170 face-effect with the classic event-related effect, while highlighting the importance of accounting for eye-movement related effects.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PXX9ZGX4/Auerbach-Asch et al. - 2020 - Face Selective Neural Activity Comparisons Betwee.pdf}
}

@article{auksztulewicz_attentional_2015,
  title = {Attentional {{Enhancement}} of {{Auditory Mismatch Responses}}: A {{DCM}}/{{MEG Study}}},
  shorttitle = {Attentional {{Enhancement}} of {{Auditory Mismatch Responses}}},
  author = {Auksztulewicz, Ryszard and Friston, Karl},
  year = {2015},
  month = nov,
  journal = {Cerebral Cortex},
  volume = {25},
  number = {11},
  pages = {4273--4283},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhu323},
  abstract = {Despite similar behavioral effects, attention and expectation influence evoked responses differently: Attention typically enhances event-related responses, whereas expectation reduces them. This dissociation has been reconciled under predictive coding, where prediction errors are weighted by precision associated with attentional modulation. Here, we tested the predictive coding account of attention and expectation using magnetoencephalography and modeling. Temporal attention and sensory expectation were orthogonally manipulated in an auditory mismatch paradigm, revealing opposing effects on evoked response amplitude. Mismatch negativity (MMN) was enhanced by attention, speaking against its supposedly pre-attentive nature. This interaction effect was modeled in a canonical microcircuit using dynamic causal modeling, comparing models with modulation of extrinsic and intrinsic connectivity at different levels of the auditory hierarchy. While MMN was explained by recursive interplay of sensory predictions and prediction errors, attention was linked to the gain of inhibitory interneurons, consistent with its modulation of sensory precision.},
  file = {/Users/xzfang/Zotero/storage/HXWSCNCS/Auksztulewicz and Friston - 2015 - Attentional Enhancement of Auditory Mismatch Respo.pdf;/Users/xzfang/Zotero/storage/TF4TTAQQ/2366858.html}
}

@article{averbeck_hypothalamic_2020,
  title = {Hypothalamic {{Interactions}} with {{Large-Scale Neural Circuits Underlying Reinforcement Learning}} and {{Motivated Behavior}}},
  author = {Averbeck, Bruno B. and Murray, Elisabeth A.},
  year = {2020},
  month = sep,
  journal = {Trends in Neurosciences},
  volume = {43},
  number = {9},
  pages = {681--694},
  publisher = {{Elsevier}},
  issn = {0166-2236, 1878-108X},
  doi = {10.1016/j.tins.2020.06.006},
  langid = {english},
  pmid = {32762959},
  keywords = {amygdala,basal ganglia,devaluation,frontal-striatal circuits,hypothalamus,motivation,prefrontal cortex,reinforcement learning,striatum},
  file = {/Users/xzfang/Zotero/storage/4IWNQ274/Averbeck and Murray - 2020 - Hypothalamic Interactions with Large-Scale Neural .pdf;/Users/xzfang/Zotero/storage/KCNZ2A6S/S0166-2236(20)30148-X.html}
}

@article{averbeck_neural_2006,
  title = {Neural Correlations, Population Coding and Computation},
  author = {Averbeck, Bruno B. and Latham, Peter E. and Pouget, Alexandre},
  year = {2006},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {7},
  number = {5},
  pages = {358--366},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn1888},
  abstract = {Sensory and motor information in the brain is represented as activity in populations of neurons. But how does correlated noise affect population coding? These authors evaluate empirical and theoretical evidence on the interactions between correlations, population codes and neural computations.},
  copyright = {2006 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/V9CS67VZ/Averbeck et al. - 2006 - Neural correlations, population coding and computa.pdf;/Users/xzfang/Zotero/storage/7W6SYSSY/nrn1888.html}
}

@article{baart_degrading_2014,
  title = {Degrading Phonetic Information Affects Matching of Audiovisual Speech in Adults, but Not in Infants},
  author = {Baart, Martijn and Vroomen, Jean and Shaw, Kathleen and Bortfeld, Heather},
  year = {2014},
  month = jan,
  journal = {Cognition},
  volume = {130},
  number = {1},
  pages = {31--43},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.09.006},
  abstract = {Infants and adults are well able to match auditory and visual speech, but the cues on which they rely (viz. temporal, phonetic and energetic correspondence in the auditory and visual speech streams) may differ. Here we assessed the relative contribution of the different cues using sine-wave speech (SWS). Adults (N=52) and infants (N=34, age ranged in between 5 and 15months) matched 2 trisyllabic speech sounds (`kalisu' and `mufapi'), either natural or SWS, with visual speech information. On each trial, adults saw two articulating faces and matched a sound to one of these, while infants were presented the same stimuli in a preferential looking paradigm. Adults' performance was almost flawless with natural speech, but was significantly less accurate with SWS. In contrast, infants matched the sound to the articulating face equally well for natural speech and SWS. These results suggest that infants rely to a lesser extent on phonetic cues than adults do to match audio to visual speech. This is in line with the notion that the ability to extract phonetic information from the visual signal increases during development, and suggests that phonetic knowledge might not be the basis for early audiovisual correspondence detection in speech.},
  langid = {english},
  keywords = {Adults,Audiovisual speech integration,Infants,Phonetic correspondence,Sine-wave speech},
  file = {/Users/xzfang/Zotero/storage/MZTTSCKB/Baart et al. - 2014 - Degrading phonetic information affects matching of.pdf;/Users/xzfang/Zotero/storage/VVWALENF/S001002771300190X.html}
}

@article{baart_phonetic_2015,
  title = {Phonetic Matching of Auditory and Visual Speech Develops during Childhood: {{Evidence}} from Sine-Wave Speech},
  shorttitle = {Phonetic Matching of Auditory and Visual Speech Develops during Childhood},
  author = {Baart, Martijn and Bortfeld, Heather and Vroomen, Jean},
  year = {2015},
  month = jan,
  journal = {Journal of Experimental Child Psychology},
  volume = {129},
  pages = {157--164},
  issn = {0022-0965},
  doi = {10.1016/j.jecp.2014.08.002},
  abstract = {The correspondence between auditory speech and lip-read information can be detected based on a combination of temporal and phonetic cross-modal cues. Here, we determined the point in developmental time at which children start to effectively use phonetic information to match a speech sound with one of two articulating faces. We presented 4- to 11-year-olds (N=77) with three-syllabic sine-wave speech replicas of two pseudo-words that were perceived as non-speech and asked them to match the sounds with the corresponding lip-read video. At first, children had no phonetic knowledge about the sounds, and matching was thus based on the temporal cues that are fully retained in sine-wave speech. Next, we trained all children to perceive the phonetic identity of the sine-wave speech and repeated the audiovisual (AV) matching task. Only at around 6.5years of age did the benefit of having phonetic knowledge about the stimuli become apparent, thereby indicating that AV matching based on phonetic cues presumably develops more slowly than AV matching based on temporal cues.},
  langid = {english},
  keywords = {Audiovisual speech,Cross-modal correspondence,Development,Phonetic cues,Sine-wave speech,Temporal cues},
  file = {/Users/xzfang/Zotero/storage/M8XPDBKG/Baart et al. - 2015 - Phonetic matching of auditory and visual speech de.pdf;/Users/xzfang/Zotero/storage/C4FDFPU4/S0022096514001520.html}
}

@article{babel_evidence_2012,
  title = {Evidence for Phonetic and Social Selectivity in Spontaneous Phonetic Imitation},
  author = {Babel, Molly},
  year = {2012},
  month = jan,
  journal = {Journal of Phonetics},
  volume = {40},
  number = {1},
  pages = {177--189},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2011.09.001},
  abstract = {Spontaneous phonetic imitation is the process by which a talker comes to be more similar-sounding to a model talker as the result of exposure. The current experiment investigates this phenomenon, examining whether vowel spectra are automatically imitated in a lexical shadowing task and how social liking affects imitation. Participants were assigned to either a Black talker or White talker; within this talker manipulation, participants were either put into a condition with a digital image of their assigned model talker or one without an image. Liking was measured through attractiveness rating. Participants accommodated toward vowels selectively; the low vowels /\ae{} É‘/ showed the strongest effects of imitation compared to the vowels /i o u/, but the degree of this trend varied across conditions. In addition to these findings of phonetic selectivity, the degree to which these vowels were imitated was subtly affected by attractiveness ratings and this also interacted with the experimental condition. The results demonstrate the labile nature of linguistic segments with respect to both their perceptual encoding and their variation in production.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JXCF7745/Babel - 2012 - Evidence for phonetic and social selectivity in sp.pdf;/Users/xzfang/Zotero/storage/AUDD2YC4/S0095447011000763.html}
}

@article{babel_expectations_2015,
  title = {Expectations and Speech Intelligibility},
  author = {Babel, Molly and Russell, Jamie},
  year = {2015},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {137},
  number = {5},
  pages = {2823--2833},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4919317},
  abstract = {Socio-indexical cues and paralinguistic information are often beneficial to speech processing as this information assists listeners in parsing the speech stream. Associations that particular populations speak in a certain speech style can, however, make it such that socio-indexical cues have a cost. In this study, native speakers of Canadian English who identify as Chinese Canadian and White Canadian read sentences that were presented to listeners in noise. Half of the sentences were presented with a visual-prime in the form of a photo of the speaker and half were presented in control trials with fixation crosses. Sentences produced by Chinese Canadians showed an intelligibility cost in the face-prime condition, whereas sentences produced by White Canadians did not. In an accentedness rating task, listeners rated White Canadians as less accented in the face-prime trials, but Chinese Canadians showed no such change in perceived accentedness. These results suggest a misalignment between an expected and an observed speech signal for the face-prime trials, which indicates that social information about a speaker can trigger linguistic associations that come with processing benefits and costs.},
  file = {/Users/xzfang/Zotero/storage/HLACFW74/Babel and Russell - 2015 - Expectations and speech intelligibility.pdf;/Users/xzfang/Zotero/storage/DHLP79N7/1.html}
}

@article{backer_neural_2010,
  title = {Neural {{Time Course}} of {{Echo Suppression}} in {{Humans}}},
  author = {Backer, K. C. and Hill, K. T. and Shahin, A. J. and Miller, L. M.},
  year = {2010},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {5},
  pages = {1905--1913},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4391-09.2010},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TGEEA3YF/Backer et al. - 2010 - Neural Time Course of Echo Suppression in Humans.pdf}
}

@article{backer_novel_2019,
  title = {A Novel {{EEG}} Paradigm to Simultaneously and Rapidly Assess the Functioning of Auditory and Visual Pathways},
  author = {Backer, Kristina C. and Kessler, Andrew S. and Lawyer, Laurel A. and Corina, David P. and Miller, Lee M.},
  year = {2019},
  month = oct,
  journal = {Journal of Neurophysiology},
  volume = {122},
  number = {4},
  pages = {1312--1329},
  issn = {0022-3077},
  doi = {10.1152/jn.00868.2018},
  abstract = {Objective assessment of the sensory pathways is crucial for understanding their development across the life span and how they may be affected by neurodevelopmental disorders (e.g., autism spectrum) and neurological pathologies (e.g., stroke, multiple sclerosis, etc.). Quick and passive measurements, for example, using electroencephalography (EEG), are especially important when working with infants and young children and with patient populations having communication deficits (e.g., aphasia). However, many EEG paradigms are limited to measuring activity from one sensory domain at a time, may be time consuming, and target only a subset of possible responses from that particular sensory domain (e.g., only auditory brainstem responses or only auditory P1-N1-P2 evoked potentials). Thus we developed a new multisensory paradigm that enables simultaneous, robust, and rapid (6\textendash 12 min) measurements of both auditory and visual EEG activity, including auditory brainstem responses, auditory and visual evoked potentials, as well as auditory and visual steady-state responses. This novel method allows us to examine neural activity at various stations along the auditory and visual hierarchies with an ecologically valid continuous speech stimulus, while an unrelated video is playing. Both the speech stimulus and the video can be customized for any population of interest. Furthermore, by using two simultaneous visual steady-state stimulation rates, we demonstrate the ability of this paradigm to track both parafoveal and peripheral visual processing concurrently. We report results from 25 healthy young adults, which validate this new paradigm., NEW \& NOTEWORTHY A novel electroencephalography paradigm enables the rapid, reliable, and noninvasive assessment of neural activity along both auditory and visual pathways concurrently. The paradigm uses an ecologically valid continuous speech stimulus for auditory evaluation and can simultaneously track visual activity to both parafoveal and peripheral visual space. This new methodology may be particularly appealing to researchers and clinicians working with infants and young children and with patient populations with limited communication abilities.},
  pmcid = {PMC6843102},
  pmid = {31268796},
  file = {/Users/xzfang/Zotero/storage/GJQ2YJS3/Backer et al. - 2019 - A novel EEG paradigm to simultaneously and rapidly.pdf}
}

@article{backer_orienting_2020,
  title = {Orienting {{Attention}} to {{Short-Term Memory Representations}} via {{Sensory Modality}} and {{Semantic Category Retro-Cues}}},
  author = {Backer, Kristina C. and Buchsbaum, Bradley R. and Alain, Claude},
  year = {2020},
  month = nov,
  journal = {eNeuro},
  volume = {7},
  number = {6},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0018-20.2020},
  abstract = {There is growing interest in characterizing the neural mechanisms underlying the interactions between attention and memory. Current theories posit that reflective attention to memory representations generally involves a fronto-parietal attentional control network. The present study aimed to test this idea by manipulating how a particular short-term memory (STM) representation is accessed, that is, based on its input sensory modality or semantic category, during functional magnetic resonance imaging (fMRI). Human participants performed a novel variant of the retro-cue paradigm, in which they were presented with both auditory and visual non-verbal stimuli followed by Modality, Semantic, or Uninformative retro-cues. Modality and, to a lesser extent, Semantic retro-cues facilitated response time relative to Uninformative retro-cues. The univariate and multivariate pattern analyses (MVPAs) of fMRI time-series revealed three key findings. First, the posterior parietal cortex (PPC), including portions of the intraparietal sulcus (IPS) and ventral angular gyrus (AG), had activation patterns that spatially overlapped for both modality-based and semantic-based reflective attention. Second, considering both the univariate and multivariate analyses, Semantic retro-cues were associated with a left-lateralized fronto-parietal network. Finally, the experimental design enabled us to examine how dividing attention cross-modally within STM modulates the brain regions involved in reflective attention. This analysis revealed that univariate activation within bilateral portions of the PPC increased when participants simultaneously attended both auditory and visual memory representations. Therefore, prefrontal and parietal regions are flexibly recruited during reflective attention, depending on the representational feature used to selectively access STM representations.},
  pmcid = {PMC7716432},
  pmid = {33139321},
  file = {/Users/xzfang/Zotero/storage/XAIL963D/Backer et al. - 2020 - Orienting Attention to Short-Term Memory Represent.pdf}
}

@article{baese-berk_accentindependent_2013,
  title = {Accent-Independent Adaptation to Foreign Accented Speech},
  author = {{Baese-Berk}, Melissa M. and Bradlow, Ann R. and Wright, Beverly A.},
  year = {2013},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {133},
  number = {3},
  pages = {EL174-EL180},
  issn = {0001-4966},
  doi = {10.1121/1.4789864},
  abstract = {Foreign-accented speech can be difficult to understand but listeners can adapt to novel talkers and accents with appropriate experience. Previous studies have demonstrated talker-independent but accent-dependent learning after training on multiple talkers from a single language background. Here, listeners instead were exposed to talkers from five language backgrounds during training. After training, listeners generalized their learning to novel talkers from language backgrounds both included and not included in the training set. These findings suggest that generalization of foreign-accent adaptation is the result of exposure to systematic variability in accented speech that is similar across talkers from multiple language backgrounds.},
  pmcid = {PMC3579861},
  pmid = {23464125},
  file = {/Users/xzfang/Zotero/storage/7ES88TU7/Baese-Berk et al. - 2013 - Accent-independent adaptation to foreign accented .pdf}
}

@article{baese-berk_perception_2020,
  title = {Perception of Non-Native Speech},
  author = {Baese-Berk, Melissa M. and McLaughlin, Drew J. and McGowan, Kevin B.},
  year = {2020},
  journal = {Language and Linguistics Compass},
  volume = {14},
  number = {7},
  pages = {e12375},
  issn = {1749-818X},
  doi = {10.1111/lnc3.12375},
  abstract = {For nearly 25 years, researchers have recognized the rich and numerous facets of native perception of non-native speech, driving a large, and growing, body of work that has shed light on how native listeners understand non-native speech. The bulk of this work, however, has focused on the talker. That is, most researchers have asked what perception of non-native speech tells us about the non-native speaker, or when interacting with non-native speakers more generally. It is clear that listeners perceive speech not only in terms of the acoustic signal, but also with their own experience and biases driving their perception. It is also clear that native listeners can improve their perception of non-native speech for both familiar and unfamiliar accents. Therefore, it is imperative that research in non-native communication also consider an active role for the listener. To truly understand communication between native and non-native speakers, it is critically important to understand both the properties of non-native speech and how this speech is perceived. In the present review, we describe non-native speech and then review previous research, examining the methodological shift from using native listeners as tools to understand properties of non-native speech to understanding listeners as partners in conversation. We discuss how current models not only limit our understanding of non-native speech, but also limit what types of questions researchers set out to answer. We demonstrate that while non-native speakers capable of shifting their productions to be better understood by listeners, native listeners are also capable of shifting their perception to more accurately perceive non-native speech. We conclude by setting forth a series of recommendations for future research, emphasizing the contributions of native listeners and non-native speakers as equally important for communicative success.},
  copyright = {\textcopyright{} 2020 John Wiley \& Sons Ltd},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12375},
  file = {/Users/xzfang/Zotero/storage/36FT2L9B/Baeseâ€Berk et al. - 2020 - Perception of non-native speech.pdf;/Users/xzfang/Zotero/storage/HFTIQHUJ/lnc3.html}
}

@article{baillet_magnetoencephalography_2017,
  title = {Magnetoencephalography for Brain Electrophysiology and Imaging},
  author = {Baillet, Sylvain},
  year = {2017},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {20},
  number = {3},
  pages = {327--339},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4504},
  abstract = {Magnetoencephalography (MEG) tracks the millisecond electrical activity of the brain noninvasively. This review emphasizes MEG's unique assets, especially in terms of imaging and resolving the mechanisms underlying the apparent complexity of polyrhythmic brain dynamics. It also identifies practical challenges and clarifies misconceptions about the technique.},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QU66S8P8/Baillet - 2017 - Magnetoencephalography for brain electrophysiology.pdf;/Users/xzfang/Zotero/storage/XA2D8NW9/nn.html}
}

@article{baker_visual_2007,
  title = {Visual Word Processing and Experiential Origins of Functional Selectivity in Human Extrastriate Cortex},
  author = {Baker, C. I. and Liu, J. and Wald, L. L. and Kwong, K. K. and Benner, T. and Kanwisher, N.},
  year = {2007},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {21},
  pages = {9087--9092},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0703300104},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/V9SGQ2ZL/Baker et al. - 2007 - Visual word processing and experiential origins of.pdf}
}

@misc{bakhtiari_functional_2021,
  title = {The Functional Specialization of Visual Cortex Emerges from Training Parallel Pathways with Self-Supervised Predictive Learning},
  author = {Bakhtiari, Shahab and Mineault, Patrick and Lillicrap, Tim and Pack, Christopher C. and Richards, Blake A.},
  year = {2021},
  month = oct,
  pages = {2021.06.18.448989},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.18.448989},
  abstract = {The visual system of mammals is comprised of parallel, hierarchical specialized pathways. Different pathways are specialized in so far as they use representations that are more suitable for supporting specific downstream behaviours. In particular, the clearest example is the specialization of the ventral (``what'') and dorsal (``where'') pathways of the visual cortex. These two pathways support behaviours related to visual recognition and movement, respectively. To-date, deep neural networks have mostly been used as models of the ventral, recognition pathway. However, it is unknown whether both pathways can be modelled with a single deep ANN. Here, we ask whether a single model with a single loss function can capture the properties of both the ventral and the dorsal pathways. We explore this question using data from mice, who like other mammals, have specialized pathways that appear to support recognition and movement behaviours. We show that when we train a deep neural network architecture with two parallel pathways using a self-supervised predictive loss function, we can outperform other models in fitting mouse visual cortex. Moreover, we can model both the dorsal and ventral pathways. These results demonstrate that a self-supervised predictive learning approach applied to parallel pathway architectures can account for some of the functional specialization seen in mammalian visual systems.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J58PEBRE/Bakhtiari et al. - 2021 - The functional specialization of visual cortex eme.pdf;/Users/xzfang/Zotero/storage/3VJYDIWA/2021.06.18.html}
}

@article{balasubramaniam_neural_2021,
  title = {Neural {{Encoding}} and {{Representation}} of {{Time}} for {{Sensorimotor Control}} and {{Learning}}},
  author = {Balasubramaniam, Ramesh and Haegens, Saskia and Jazayeri, Mehrdad and Merchant, Hugo and Sternad, Dagmar and Song, Joo-Hyun},
  year = {2021},
  month = feb,
  journal = {The Journal of Neuroscience},
  volume = {41},
  number = {5},
  pages = {866--872},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1652-20.2020},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Q6C2ZMCU/Balasubramaniam et al. - 2021 - Neural Encoding and Representation of Time for Sen.pdf}
}

@article{baldassano_discovering_2017,
  title = {Discovering {{Event Structure}} in {{Continuous Narrative Perception}} and {{Memory}}},
  author = {Baldassano, Christopher and Chen, Janice and Zadbood, Asieh and Pillow, Jonathan W. and Hasson, Uri and Norman, Kenneth A.},
  year = {2017},
  month = aug,
  journal = {Neuron},
  volume = {95},
  number = {3},
  pages = {709-721.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.06.041},
  abstract = {During realistic, continuous perception, humans automatically segment experiences into discrete events. Using a novel model of cortical event dynamics, we investigate how cortical structures generate event representations during narrative perception and how these events are stored to and retrieved from memory.~Our data-driven approach allows us to detect event boundaries as shifts between stable patterns of brain~activity without relying on stimulus annotations and~reveals a nested hierarchy from short events in~sensory regions to long events in high-order~areas~(including angular gyrus and posterior medial~cortex), which represent abstract, multimodal situation models. High-order event boundaries are coupled to increases in hippocampal activity, which predict pattern reinstatement during later free recall. These areas also show evidence of~anticipatory reinstatement as subjects listen to a familiar narrative. Based on these results, we propose that brain activity is naturally structured into nested events, which form the basis of long-term memory representations.},
  langid = {english},
  keywords = {event model,event segmentation,fMRI,Hidden Markov Model,hippocampus,memory,narrative,perception,recall,reinstatement,situation model},
  file = {/Users/xzfang/Zotero/storage/N2ZLLA6M/Baldassano et al. - 2017 - Discovering Event Structure in Continuous Narrativ.pdf;/Users/xzfang/Zotero/storage/IKA5M65Y/S0896627317305937.html}
}

@article{baltzell_hierarchical_2019,
  title = {Hierarchical Organization of Melodic Sequences Is Encoded by Cortical Entrainment},
  author = {Baltzell, Lucas S. and Srinivasan, Ramesh and Richards, Virginia},
  year = {2019},
  month = oct,
  journal = {NeuroImage},
  volume = {200},
  pages = {490--500},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.06.054},
  abstract = {Natural speech is organized according to a hierarchical structure, with individual speech sounds combining to form abstract linguistic units, and abstract linguistic units combining to form higher-order linguistic units. Since the boundaries between these units are not always indicated by acoustic cues, they must often be computed internally. Signatures of this internal computation were reported by Ding et~al. (2016), who presented isochronous sequences of mono-syllabic words that combined to form phrases that combined to form sentences, and showed that cortical responses simultaneously encode boundaries at multiple levels of the linguistic hierarchy. In the present study, we designed melodic sequences that were hierarchically organized according to Western music conventions. Specifically, isochronous sequences of ``sung'' nonsense syllables were constructed such that syllables combined to form triads outlining individual chords, which combined to form harmonic progressions. EEG recordings were made while participants listened to these sequences with the instruction to detect when violations in the sequence structure occurred. We show that cortical responses simultaneously encode boundaries at multiple levels of a melodic hierarchy, suggesting that the encoding of hierarchical structure is not unique to speech. No effect of musical training on cortical encoding was observed.},
  langid = {english},
  keywords = {Cortical,EEG,Entrainment,Hierarchical,Language,Music},
  file = {/Users/xzfang/Zotero/storage/G9FIN2JY/Baltzell et al. - 2019 - Hierarchical organization of melodic sequences is .pdf}
}

@article{bambini_capturing_2021,
  title = {Capturing Language Change through {{EEG}}: {{Weaker P600}} for a Fading Gender Value in a Southern {{Italo-Romance}} Dialect},
  shorttitle = {Capturing Language Change through {{EEG}}},
  author = {Bambini, Valentina and Canal, Paolo and Breimaier, Federica and Meo, Domenico and Pescarini, Diego and Loporcaro, Michele},
  year = {2021},
  month = aug,
  journal = {Journal of Neurolinguistics},
  volume = {59},
  pages = {101004},
  issn = {0911-6044},
  doi = {10.1016/j.jneuroling.2021.101004},
  abstract = {Grammatical gender processing is a classic topic in the neurolinguistic literature. However, most of the studies consider major standard languages, such as English, Dutch, Spanish or Italian. This is a limitation, since these languages display relatively simple systems that are fairly stable and do not allow to address language variation and change. Here we investigated gender agreement in a dialect of Southern Italy spoken in Agnone, using Event-Related Potentials. Compared with Italian and most standard Romance languages, the Agnonese gender system is more complex (with three target genders: feminine, masculine, neuter) and, also under the pressure of standard Italian, undergoing change, with the neuter merging into the masculine. Results showed that all determiner-noun gender mismatches elicited robust P600 and moderate Left Anterior Negativity (LAN) effects. Yet not all violations involving neuter nouns were equally outright. For masculine-neuter, the amplitude of the P600 was less pronounced than for feminine-neuter violations. Moreover, when looking at individual differences, the P600 for masculine-neuter was progressively reduced as participants' competence in the Agnonese conservative variety decreased. Since sociolinguistics shows that individual variation within the speech community may reflect linguistic change, the smaller P600 exhibited by less conservative speakers could be described as a brain signature of ongoing language change in the Agnonese gender system. In other words, the attenuation of ungrammaticality-driven brain responses may provide a measurable indicator of the fading of the neuter and its merging into the masculine. This work paves the way to the neurolinguistic description of non-standard varieties and also breaks ground for a ``diachronic neurolinguistics'', extending investigation into the neurobiology of language beyond the synchronic dimension.},
  langid = {english},
  keywords = {Agreement,Diachronic neurolinguistics,ERP,Language change,Romance dialects},
  file = {/Users/xzfang/Zotero/storage/CPSD8FJY/Bambini et al. - 2021 - Capturing language change through EEG Weaker P600.pdf;/Users/xzfang/Zotero/storage/Z6INY6QW/S0911604421000208.html}
}

@article{banaieboroujeni_interneuronspecific_2021,
  title = {Interneuron-Specific Gamma Synchronization Indexes Cue Uncertainty and Prediction Errors in Lateral Prefrontal and Anterior Cingulate Cortex},
  author = {Banaie Boroujeni, Kianoush and Tiesinga, Paul and Womelsdorf, Thilo},
  editor = {Haegens, Saskia and Frank, Michael J},
  year = {2021},
  month = jun,
  journal = {eLife},
  volume = {10},
  pages = {e69111},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.69111},
  abstract = {Inhibitory interneurons are believed to realize critical gating functions in cortical circuits, but it has been difficult to ascertain the content of gated information for well-characterized interneurons in primate cortex. Here, we address this question by characterizing putative interneurons in primate prefrontal and anterior cingulate cortex while monkeys engaged in attention demanding reversal learning. We find that subclasses of narrow spiking neurons have a relative suppressive effect on the local circuit indicating they are inhibitory interneurons. One of these interneuron subclasses showed prominent firing rate modulations and (35\textendash 45 Hz) gamma synchronous spiking during periods of uncertainty in both, lateral prefrontal cortex (LPFC) and anterior cingulate cortex (ACC). In LPFC, this interneuron subclass activated when the uncertainty of attention cues was resolved during flexible learning, whereas in ACC it fired and gamma-synchronized when outcomes were uncertain and prediction errors were high during learning. Computational modeling of this interneuron-specific gamma band activity in simple circuit motifs suggests it could reflect a soft winner-take-all gating of information having high degree of uncertainty. Together, these findings elucidate an electrophysiologically characterized interneuron subclass in the primate, that forms gamma synchronous networks in two different areas when resolving uncertainty during adaptive goal-directed behavior.},
  keywords = {cell types,cognitive flexibility,feature-based attention,nonhuman primates,reinforcement learning,reversal learning},
  file = {/Users/xzfang/Zotero/storage/ZW8RGVDA/Banaie Boroujeni et al. - 2021 - Interneuron-specific gamma synchronization indexes.pdf}
}

@article{banerjee_it_2012,
  title = {Is {{It Light}} or {{Dark}}? {{Recalling Moral Behavior Changes Perception}} of {{Brightness}}},
  shorttitle = {Is {{It Light}} or {{Dark}}?},
  author = {Banerjee, Pronobesh and Chatterjee, Promothesh and Sinha, Jayati},
  year = {2012},
  month = apr,
  journal = {Psychological Science},
  volume = {23},
  number = {4},
  pages = {407--409},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611432497},
  langid = {english}
}

@article{bao_map_2020,
  title = {A Map of Object Space in Primate Inferotemporal Cortex},
  author = {Bao, Pinglei and She, Liang and McGill, Mason and Tsao, Doris Y.},
  year = {2020},
  month = jul,
  journal = {Nature},
  volume = {583},
  number = {7814},
  pages = {103--108},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2350-5},
  abstract = {The inferotemporal (IT) cortex is responsible for object recognition, but it is unclear how the representation of visual objects is organized in this part of the brain. Areas that are selective for categories such as faces, bodies, and scenes have been found1\textendash 5, but large parts of IT cortex lack any known specialization, raising the question of what general principle governs IT organization. Here we used functional MRI, microstimulation, electrophysiology, and deep networks to investigate the organization of macaque IT cortex. We built a low-dimensional object space to describe general objects using a feedforward deep neural network trained on object classification6. Responses of IT cells to a large set of objects revealed that single IT cells project incoming objects onto specific axes of this space. Anatomically, cells were clustered into four networks according to the first two components of their preferred axes, forming a map of object space. This map was repeated across three hierarchical stages of increasing view invariance, and cells that comprised these maps collectively harboured sufficient coding capacity to approximately reconstruct objects. These results provide a unified picture of IT organization in which category-selective regions are part of a coarse map of object space whose dimensions can be extracted from a deep network.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8IRMBFGC/Bao et al. - 2020 - A map of object space in primate inferotemporal co.pdf;/Users/xzfang/Zotero/storage/EMFHQBH8/s41586-020-2350-5.html}
}

@article{barascud_brain_2016,
  title = {Brain Responses in Humans Reveal Ideal Observer-like Sensitivity to Complex Acoustic Patterns},
  author = {Barascud, Nicolas and Pearce, Marcus T. and Griffiths, Timothy D. and Friston, Karl J. and Chait, Maria},
  year = {2016},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {5},
  pages = {E616-E625},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1508523113},
  abstract = {We use behavioral methods, magnetoencephalography, and functional MRI to investigate how human listeners discover temporal patterns and statistical regularities in complex sound sequences. Sensitivity to patterns is fundamental to sensory processing, in particular in the auditory system, because most auditory signals only have meaning as successions over time. Previous evidence suggests that the brain is tuned to the statistics of sensory stimulation. However, the process through which this arises has been elusive. We demonstrate that listeners are remarkably sensitive to the emergence of complex patterns within rapidly evolving sound sequences, performing on par with an ideal observer model. Brain responses reveal online processes of evidence accumulation\textemdash dynamic changes in tonic activity precisely correlate with the expected precision or predictability of ongoing auditory input\textemdash both in terms of deterministic (first-order) structure and the entropy of random sequences. Source analysis demonstrates an interaction between primary auditory cortex, hippocampus, and inferior frontal gyrus in the process of discovering the regularity within the ongoing sound sequence. The results are consistent with precision based predictive coding accounts of perceptual inference and provide compelling neurophysiological evidence of the brain's capacity to encode high-order temporal structure in sensory signals.},
  chapter = {PNAS Plus},
  copyright = {\textcopyright{}  . http://www.pnas.org/preview\_site/misc/userlicense.xhtml},
  langid = {english},
  pmid = {26787854},
  keywords = {fMRI,MEG,MMN,pattern detection,statistical learning},
  file = {/Users/xzfang/Zotero/storage/YAK38J93/Barascud et al. - 2016 - Brain responses in humans reveal ideal observer-li.pdf;/Users/xzfang/Zotero/storage/6W239IX6/E616.html}
}

@article{barczak_topdown_2018,
  title = {Top-down, Contextual Entrainment of Neuronal Oscillations in the Auditory Thalamocortical Circuit},
  author = {Barczak, Annamaria and O'Connell, Monica Noelle and McGinnis, Tammy and Ross, Deborah and Mowery, Todd and Falchier, Arnaud and Lakatos, Peter},
  year = {2018},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {32},
  pages = {E7605-E7614},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1714684115},
  abstract = {Prior studies have shown that repetitive presentation of acoustic stimuli results in an alignment of ongoing neuronal oscillations to the sequence rhythm via oscillatory entrainment by external cues. Our study aimed to explore the neural correlates of the perceptual parsing and grouping of complex repeating auditory patterns that occur based solely on statistical regularities, or context. Human psychophysical studies suggest that the recognition of novel auditory patterns amid a continuous auditory stimulus sequence occurs automatically halfway through the first repetition. We hypothesized that once repeating patterns were detected by the brain, internal rhythms would become entrained, demarcating the temporal structure of these repetitions despite lacking external cues defining pattern on- or offsets. To examine the neural correlates of pattern perception, neuroelectric activity of primary auditory cortex (A1) and thalamic nuclei was recorded while nonhuman primates passively listened to streams of rapidly presented pure tones and bandpass noise bursts. At arbitrary intervals, random acoustic patterns composed of 11 stimuli were repeated five times without any perturbance of the constant stimulus flow. We found significant delta entrainment by these patterns in the A1, medial geniculate body, and medial pulvinar. In A1 and pulvinar, we observed a statistically significant, pattern structure-aligned modulation of neuronal firing that occurred earliest in the pulvinar, supporting the idea that grouping and detecting complex auditory patterns is a top-down, context-driven process. Besides electrophysiological measures, a pattern-related modulation of pupil diameter verified that, like humans, nonhuman primates consciously detect complex repetitive patterns that lack physical boundaries.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GWVEFCT8/Barczak et al. - 2018 - Top-down, contextual entrainment of neuronal oscil.pdf}
}

@article{bardes_vicreg_2022,
  title = {{{VICReg}}: {{Variance-Invariance-Covariance Regularization}} for {{Self-Supervised Learning}}},
  shorttitle = {{{VICReg}}},
  author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  year = {2022},
  month = jan,
  journal = {arXiv:2105.04906 [cs]},
  eprint = {2105.04906},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/2XDQM8II/Bardes et al. - 2022 - VICReg Variance-Invariance-Covariance Regularizat.pdf}
}

@article{baron-cohen_does_1985,
  title = {Does the Autistic Child Have a ``Theory of Mind'' ?},
  author = {{Baron-Cohen}, Simon and Leslie, Alan M. and Frith, Uta},
  year = {1985},
  month = oct,
  journal = {Cognition},
  volume = {21},
  number = {1},
  pages = {37--46},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(85)90022-8},
  abstract = {We use a new model of metarepresentational development to predict a cognitive deficit which could explain a crucial component of the social impairment in childhood autism. One of the manifestations of a basic metarepresentational capacity is a `theory of mind'. We have reason to believe that autistic children lack such a `theory'. If this were so, then they would be unable to impute beliefs to others and to predict their behaviour. This hypothesis was tested using Wimmer and Perner's puppet play paradigm. Normal children and those with Down's syndrome were used as controls for a group of autistic children. Even though the mental age of the autistic children was higher than that of the controls, they alone failed to impute beliefs to others. Thus the dysfunction we have postulated and demonstrated is independent of mental retardation and specific to autism. R\'esum\'e Les auteurs pr\'esentent un nouveau mod`\'ele de d\'eveloppement m\'eta-cognitif pour pr\'edire le d\'eficit cognitif qui rendrait compte d'un composant essentiel du handicap social de l'enfant autiste. Une des manifestations d'une capacit\'e de base m\'eta-cognitive est une `theorie de l'esprit'. Nous avons des raisons de croire que cette th\'eorie fait defaut chez l'enfant autiste. Celui-ci serait done incapable d'attribuer des croyances aux autres ou de pr\'edire leur comportement. Cette hypoth\`ese a \'et\'e test\'ee avec le paradigme de jeu des marionettes utilis\'e par Wimmer et Perner. Des enfants normaux et des enfants avec trisomie 21 ont servi de groupe contr\^ole. Bien que Page mental des enfants autistes ait \'et\'e plus \'elev\'e que deux du groupe contr\^ole, seuls les enfants autistes Wont pu attribuer aux autres des croyances. Ainsi le dysfonctionnement pr\'evu a pu \^etre d\'emontre, il s'av\`ere ind\'ependant du retard mental et sp\'ecifique a l'autiste.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EVP8SEY5/Baron-Cohen et al. - 1985 - Does the autistic child have a â€œtheory of mindâ€ .pdf;/Users/xzfang/Zotero/storage/68UHKCX8/0010027785900228.html}
}

@article{bates_fitting_2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {\textbf{Lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9ZWY9PN4/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{batterink_functional_2015,
  title = {Functional Differences between Statistical Learning with and without Explicit Training},
  author = {Batterink, Laura J. and Reber, Paul J. and Paller, Ken A.},
  year = {2015},
  month = nov,
  journal = {Learning \& Memory},
  volume = {22},
  number = {11},
  pages = {544--556},
  issn = {1549-5485},
  doi = {10.1101/lm.037986.114},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LWUBKQEP/Batterink et al. - 2015 - Functional differences between statistical learnin.pdf}
}

@article{batterink_implicit_2015,
  title = {Implicit and Explicit Contributions to Statistical Learning},
  author = {Batterink, Laura J. and Reber, Paul J. and Neville, Helen J. and Paller, Ken A.},
  year = {2015},
  month = aug,
  journal = {Journal of memory and language},
  volume = {83},
  pages = {62--78},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.04.004},
  abstract = {Statistical learning allows learners to detect regularities in the environment and appears to emerge automatically as a consequence of experience. Statistical learning paradigms bear many similarities to those of artificial grammar learning and other types of implicit learning. However, whether learning effects in statistical learning tasks are driven by implicit knowledge has not been thoroughly examined. The present study addressed this gap by examining the role of implicit and explicit knowledge within the context of a typical auditory statistical learning paradigm. Learners were exposed to a continuous stream of repeating nonsense words. Learning was tested (a) directly via a forced-choice recognition test combined with a remember/know procedure and (b) indirectly through a novel reaction time (RT) test. Behavior and brain potentials revealed statistical learning effects with both tests. On the recognition test, accurate responses were associated with subjective feelings of stronger recollection, and learned nonsense words relative to nonword foils elicited an enhanced late positive potential indicative of explicit knowledge. On the RT test, both RTs and P300 amplitudes differed as a function of syllable position, reflecting facilitation attributable to statistical learning. Explicit stimulus recognition did not correlate with RT or P300 effects on the RT test. These results provide evidence that explicit knowledge is accrued during statistical learning, while bringing out the possibility that dissociable implicit representations are acquired in parallel. The commonly used recognition measure primarily reflects explicit knowledge, and thus may underestimate the total amount of knowledge produced by statistical learning. Indirect measures may be more sensitive indices of learning, capturing knowledge above and beyond what is reflected by recognition accuracy.},
  pmcid = {PMC4448134},
  pmid = {26034344},
  file = {/Users/xzfang/Zotero/storage/C677ETAK/Batterink et al. - 2015 - Implicit and explicit contributions to statistical.pdf}
}

@article{batterink_online_2017,
  title = {Online Neural Monitoring of Statistical Learning},
  author = {Batterink, Laura J. and Paller, Ken A.},
  year = {2017},
  month = may,
  journal = {Cortex; a journal devoted to the study of the nervous system and behavior},
  volume = {90},
  pages = {31--45},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2017.02.004},
  abstract = {The extraction of patterns in the environment plays a critical role in many types of human learning, from motor skills to language acquisition. This process is known as statistical learning. Here we propose that statistical learning has two dissociable components: (1) perceptual binding of individual stimulus units into integrated composites and (2) storing those integrated representations for later use. Statistical learning is typically assessed using post-learning tasks, such that the two components are conflated. Our goal was to characterize the online perceptual component of statistical learning. Participants were exposed to a structured stream of repeating trisyllabic nonsense words and a random syllable stream. Online learning was indexed by an EEG-based measure that quantified neural entrainment at the frequency of the repeating words relative to that of individual syllables. Statistical learning was subsequently assessed using conventional measures in an explicit rating task and a reaction-time task. In the structured stream, neural entrainment to trisyllabic words was higher than in the random stream, increased as a function of exposure to track the progression of learning, and predicted performance on the RT task. These results demonstrate that monitoring this critical component of learning via rhythmic EEG entrainment reveals a gradual acquisition of knowledge whereby novel stimulus sequences are transformed into familiar composites. This online perceptual transformation is a critical component of learning.},
  pmcid = {PMC5438777},
  pmid = {28324696},
  file = {/Users/xzfang/Zotero/storage/K57JZWQS/Batterink and Paller - 2017 - Online neural monitoring of statistical learning.pdf}
}

@article{batterink_optimizing_2021,
  title = {Optimizing Steady-State Responses to Index Statistical Learning: {{Response}} to {{Benjamin}} and Colleagues},
  shorttitle = {Optimizing Steady-State Responses to Index Statistical Learning},
  author = {Batterink, Laura J. and Choi, Dawoon},
  year = {2021},
  month = sep,
  journal = {Cortex},
  volume = {142},
  pages = {379--388},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2021.06.008},
  abstract = {Neural entrainment refers to the tendency of neural activity to align with an ongoing rhythmic stimulus. Measures of neural entrainment have been increasingly leveraged as a tool to understand how the brain tracks different types of regularities in sensory input. However, the methods used to quantify neural entrainment are varied, with numerous analytic decision points whose consequences have not been well-characterized. In a valuable contribution to this field, Benjamin, Dehaene-Lambertz and Flo (submitted) systematically compare various methodological approaches for studying neural entrainment. They demonstrate that the use of overlapping epochs, in which sliding time windows are extracted and analyzed, results in an artifactual inflation of entrainment estimates at the frequency of overlap. Here, in response to this updated best practice recommendation, we reanalyzed three previously published datasets that had been previously analyzed with overlapping epochs. Although our main results and conclusions are unaltered from those originally reported, we agree with Benjamin and colleagues that overlapping epochs should generally be avoided in classic analyses of steady-state experiments, which aim to quantify overall peaks in phase or power across an entire experimental duration. However, we present a case that overlapping epochs may be beneficial in fine-grained analyses of neural entrainment over time. The use of overlapping epochs in such analyses could improve temporal resolution without complicating interpretability of the results in cases where the question of interest relates to relative changes in neural entrainment over time within a given frequency.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PF29JVBC/S0010945221002227.html}
}

@article{batterink_rapid_2017,
  title = {Rapid {{Statistical Learning Supporting Word Extraction From Continuous Speech}}},
  author = {Batterink, Laura J.},
  year = {2017},
  month = jul,
  journal = {Psychological Science},
  volume = {28},
  number = {7},
  pages = {921--928},
  issn = {0956-7976},
  doi = {10.1177/0956797617698226},
  abstract = {The identification of words in continuous speech, known as speech segmentation, is a critical early step in language acquisition. This process is partially supported by statistical learning, the ability to extract patterns from the environment. Given that speech segmentation represents a potential bottleneck for language acquisition, patterns in speech may be extracted very rapidly, without extensive exposure. This hypothesis was examined by exposing participants to continuous speech streams composed of novel repeating nonsense words. Learning was measured on-line using a reaction time task. After merely one exposure to an embedded novel word, learners demonstrated significant learning effects, as revealed by faster responses to predictable than to unpredictable syllables. These results demonstrate that learners gained sensitivity to the statistical structure of unfamiliar speech on a very rapid timescale. This ability may play an essential role in early stages of language acquisition, allowing learners to rapidly identify word candidates and ``break in'' to an unfamiliar language.},
  pmcid = {PMC5507727},
  pmid = {28493810},
  file = {/Users/xzfang/Zotero/storage/7M4R5G32/Batterink - 2017 - Rapid Statistical Learning Supporting Word Extract.pdf}
}

@article{batterink_sleep_2014,
  title = {Sleep Facilitates Learning a New Linguistic Rule},
  author = {Batterink, Laura J. and Oudiette, Delphine and Reber, Paul J. and Paller, Ken A.},
  year = {2014},
  month = dec,
  journal = {Neuropsychologia},
  volume = {65},
  pages = {169--179},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2014.10.024},
  abstract = {Natural languages contain countless regularities. Extraction of these patterns is an essential component of language acquisition. Here we examined the hypothesis that memory processing during sleep contributes to this learning. We exposed participants to a hidden linguistic rule by presenting a large number of two-word phrases, each including a noun preceded by one of four novel words that functioned as an article (e.g., gi rhino). These novel words (ul, gi, ro and ne) were presented as obeying an explicit rule: two words signified that the noun referent was relatively near, and two that it was relatively far. Undisclosed to participants was the fact that the novel articles also predicted noun animacy, with two of the articles preceding animate referents and the other two preceding inanimate referents. Rule acquisition was tested implicitly using a task in which participants responded to each phrase according to whether the noun was animate or inanimate. Learning of the hidden rule was evident in slower responses to phrases that violated the rule. Responses were delayed regardless of whether rule-knowledge was consciously accessible. Brain potentials provided additional confirmation of implicit and explicit rule-knowledge. An afternoon nap was interposed between two 20-min learning sessions. Participants who obtained greater amounts of both slow-wave and rapid-eye-movement sleep showed increased sensitivity to the hidden linguistic rule in the second session. We conclude that during sleep, reactivation of linguistic information linked with the rule was instrumental for stabilizing learning. The combination of slow-wave and rapid-eye-movement sleep may synergistically facilitate the abstraction of complex patterns in linguistic input.},
  pmcid = {PMC4259849},
  pmid = {25447376},
  file = {/Users/xzfang/Zotero/storage/2S48XZKD/Batterink et al. - 2014 - Sleep facilitates learning a new linguistic rule.pdf}
}

@article{batterink_sleepbased_2017,
  title = {Sleep-{{Based Memory Processing Facilitates Grammatical Generalization}}: {{Evidence}} from {{Targeted Memory Reactivation}}},
  shorttitle = {Sleep-{{Based Memory Processing Facilitates Grammatical Generalization}}},
  author = {Batterink, Laura J. and Paller, Ken A.},
  year = {2017},
  month = apr,
  journal = {Brain and language},
  volume = {167},
  pages = {83--93},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2015.09.003},
  abstract = {Generalization \textemdash{} the ability to abstract regularities from specific examples and apply them to novel instances \textemdash{} is an essential component of language acquisition. Generalization not only depends on exposure to input during wake, but may also improve offline during sleep. Here we examined whether targeted memory reactivation during sleep can influence grammatical generalization. Participants gradually acquired the grammatical rules of an artificial language through an interactive learning procedure. Then, phrases from the language (experimental group) or stimuli from an unrelated task (control group) were covertly presented during an afternoon nap. Compared to control participants, participants re-exposed to the language during sleep showed larger gains in grammatical generalization. Sleep cues produced a bias, not necessarily a pure gain, suggesting that the capacity for memory replay during sleep is limited. We conclude that grammatical generalization was biased by auditory cueing during sleep, and by extension, that sleep likely influences grammatical generalization in general.},
  pmcid = {PMC4819015},
  pmid = {26443322},
  file = {/Users/xzfang/Zotero/storage/A2EFTYNF/Batterink and Paller - 2017 - Sleep-Based Memory Processing Facilitates Grammati.pdf}
}

@article{batterink_statistical_2019,
  title = {Statistical Learning of Speech Regularities Can Occur Outside the Focus of Attention},
  author = {Batterink, Laura J. and Paller, Ken A.},
  year = {2019},
  month = jun,
  journal = {Cortex},
  volume = {115},
  pages = {56--71},
  issn = {00109452},
  doi = {10.1016/j.cortex.2019.01.013},
  abstract = {Statistical learning, the process of extracting regularities from the environment, plays an essential role in many aspects of cognition, including speech segmentation and language acquisition. A key component of statistical learning in a linguistic context is the perceptual binding of adjacent individual units (e.g., syllables) into integrated composites (e.g., multisyllabic words). A second, conceptually dissociable component of statistical learning is the memory storage of these integrated representations. Here we examine whether these two dissociable components of statistical learning are differentially impacted by topdown, voluntary attentional resources. Learners' attention was either focused towards or diverted from a speech stream made up of repeating nonsense words. Building on our previous findings, we quantified the online perceptual binding of individual syllables into component words using an EEG-based neural entrainment measure. Following exposure, statistical learning was assessed using offline tests, sensitive to both perceptual binding and memory storage. Neural measures verified that our manipulation of selective attention successfully reduced limited-capacity resources to the speech stream. Diverting attention away from the speech stream did not alter neural entrainment to the component words or post-exposure familiarity ratings, but did impact performance on an indirect reaction-time based memory test. We conclude that theoretically dissociable components of statistically learning are differentially impacted by attention and top-down processing resources. A reduction in attention to the speech stream may impede memory storage of the component words. In contrast, the moment-by-moment perceptual binding of speech regularities can occur even while learners' attention is focused on a demanding concurrent task, and we found no evidence that selective attention modulates this process. These results suggest that learners can acquire basic statistical properties of language without directly focusing on the speech input, potentially opening up previously overlooked opportunities for language learning, particularly in adult learners.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/D8GM7LNQ/Batterink and Paller - 2019 - Statistical learning of speech regularities can oc.pdf}
}

@article{batterink_syllables_2020,
  title = {Syllables in {{Sync Form}} a {{Link}}: {{Neural Phase-Locking Reflects Word Knowledge During Language Learning}}},
  shorttitle = {Syllables in {{Sync Form}} a {{Link}}},
  author = {Batterink, Laura},
  year = {2020},
  month = sep,
  journal = {Journal of cognitive neuroscience},
  volume = {32},
  number = {9},
  pages = {1735--1748},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01581},
  abstract = {Language is composed of small building blocks, which combine to form larger meaningful structures. To understand language, we must process, track and concatenate these building blocks into larger linguistic units as speech unfolds over time. An influential idea is that phase-locking of neural oscillations across different levels of linguistic structure provides a mechanism for this process. Building on this framework, the goal of the current study was to determine whether neural phase-locking occurs more robustly to novel linguistic items that are successfully learned and encoded into memory, compared to items that are not learned. Participants listened to a continuous speech stream composed of repeating nonsense words while their EEG was recorded, and then performed a recognition test on the component words. Neural phase-locking to individual words during the learning period strongly predicted the strength of subsequent word knowledge, suggesting that neural phase-locking indexes the subjective perception of specific linguistic items during real-time language learning. These findings support neural oscillatory models of language, demonstrating that words that are successfully perceived as functional units are tracked by oscillatory activity at the matching word rate. In contrast, words that are not learned are processed merely as a sequence of unrelated syllables, and thus not tracked by corresponding word-rate oscillations.},
  pmcid = {PMC7395883},
  pmid = {32427066},
  file = {/Users/xzfang/Zotero/storage/J4KZY4AL/Batterink - 2020 - Syllables in Sync Form a Link Neural Phase-Lockin.pdf}
}

@article{baumann_unified_2013,
  title = {A Unified Framework for the Organization of the Primate Auditory Cortex},
  author = {Baumann, Simon and Petkov, Christopher I. and Griffiths, Timothy D.},
  year = {2013},
  journal = {Frontiers in Systems Neuroscience},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2013.00011},
  abstract = {In nonhuman primates a scheme for the organisation of the auditory cortex is frequently used to localise auditory processes. The scheme allows a common basis for comparison of functional organisation across nonhuman primate species. However, although a body of functional and structural data in nonhuman primates supports an accepted scheme of nearly a dozen neighbouring functional areas, can this scheme be directly applied to humans? Attempts to expand the scheme of auditory cortical fields in humans have been severely hampered by a recent controversy about the organisation of tonotopic maps in humans, centred on two different models with radically different organisation. We point out observations that reconcile the previous models and suggest a distinct model in which the human cortical organisation is much more like that of other primates. This unified framework allows a more robust and detailed comparison of auditory cortex organisation across primate species including humans.},
  langid = {english},
  keywords = {Anatomy,Auditory Cortex,Comparative,Humans,Primate,tonotopy},
  file = {/Users/xzfang/Zotero/storage/2HPGINVL/Baumann et al. - 2013 - A unified framework for the organization of the pr.pdf}
}

@article{baumgarten_neural_2021,
  title = {Neural Integration Underlying Naturalistic Prediction Flexibly Adapts to Varying Sensory Input Rate},
  author = {Baumgarten, Thomas J. and Maniscalco, Brian and Lee, Jennifer L. and Flounders, Matthew W. and Abry, Patrice and He, Biyu J.},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2643},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22632-z},
  abstract = {Prediction of future sensory input based on past sensory information is essential for organisms to effectively adapt their behavior in dynamic environments. Humans successfully predict future stimuli in various natural settings. Yet, it remains elusive how the brain achieves effective prediction despite enormous variations in sensory input rate, which directly affect how fast sensory information can accumulate. We presented participants with acoustic sequences capturing temporal statistical regularities prevalent in nature and investigated neural mechanisms underlying predictive computation using MEG. By parametrically manipulating sequence presentation speed, we tested two hypotheses: neural prediction relies on integrating past sensory information over fixed time periods or fixed amounts of information. We demonstrate that across halved and doubled presentation speeds, predictive information in neural activity stems from integration over fixed amounts of information. Our findings reveal the neural mechanisms enabling humans to robustly predict dynamic stimuli in natural environments despite large sensory input rate variations.},
  copyright = {2021 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7HTKN72J/Baumgarten et al. - 2021 - Neural integration underlying naturalistic predict.pdf;/Users/xzfang/Zotero/storage/GF5TC35M/s41467-021-22632-z.html}
}

@article{beach_electrophysiological_2021,
  title = {Electrophysiological Correlates of Perceptual Prediction Error Are Attenuated in Dyslexia},
  author = {Beach, Sara D. and Lim, Sung-Joo and {Cardenas-Iniguez}, Carlos and Eddy, Marianna D. and Gabrieli, John D. E. and Perrachione, Tyler K.},
  year = {2021},
  month = jun,
  journal = {bioRxiv},
  pages = {2021.06.22.449408},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.22.449408},
  abstract = {{$<$}p{$>$}A perceptual adaptation deficit often accompanies reading difficulty in dyslexia, manifesting in poor perceptual learning of consistent stimuli and reduced neurophysiological adaptation to stimulus repetition. However, it is not known how adaptation deficits relate to differences in feedforward or feedback processes in the brain. Here we used electroencephalography (EEG) to interrogate the feedforward and feedback contributions to neural adaptation as adults with and without dyslexia viewed pairs of faces and words in a paradigm that manipulated whether there was a high probability of stimulus repetition versus a high probability of stimulus change. We measured three neural dependent variables: expectation (the difference between prestimulus EEG power with and without the expectation of stimulus repetition), feedforward repetition (the difference between event-related potentials (ERPs) evoked by an expected change and an unexpected repetition), and feedback-mediated prediction error (the difference between ERPs evoked by an unexpected change and an expected repetition). Expectation significantly modulated prestimulus theta- and alpha-band EEG in both groups. Unexpected repetitions of words, but not faces, also led to significant feedforward repetition effects in the ERPs of both groups. However, neural prediction error when an unexpected change occurred instead of an expected repetition was significantly weaker in dyslexia than the control group for both faces and words. These results suggest that the neural and perceptual adaptation deficits observed in dyslexia reflect the failure to effectively integrate perceptual predictions with feedforward sensory processing. In addition to reducing perceptual efficiency, the attenuation of neural prediction error signals would also be deleterious to the wide range of perceptual and procedural learning abilities that are critical for developing accurate and fluent reading skills.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7CMNXJPE/Beach et al. - 2021 - Electrophysiological correlates of perceptual pred.pdf;/Users/xzfang/Zotero/storage/KJZYIJ3M/2021.06.22.449408v1.html}
}

@article{beach_neural_2021,
  title = {Neural {{Decoding Reveals Concurrent Phonemic}} and {{Subphonemic Representations}} of {{Speech Across Tasks}}},
  author = {Beach, Sara D. and {Ozernov-Palchik}, Ola and May, Sidney C. and Centanni, Tracy M. and Gabrieli, John D. E. and Pantazis, Dimitrios},
  year = {2021},
  month = mar,
  journal = {Neurobiology of Language},
  pages = {1--26},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00034},
  abstract = {Robust and efficient speech perception relies on the interpretation of acoustically-variable phoneme realizations, yet prior neuroimaging studies are inconclusive regarding the degree to which subphonemic detail is maintained over time as categorical representations arise. It is also unknown whether this depends on the demands of the listening task. We addressed these questions by using neural decoding to quantify the (dis)similarity of brain response patterns evoked during two different tasks. We recorded magnetoencephalography (MEG) as adult participants heard isolated, randomized tokens from a /ba/-/da/ speech continuum. In the passive task, their attention was diverted. In the active task, they categorized each token as ba or da. We found that linear classifiers successfully decoded ba vs. da perception from the MEG data. Data from the left hemisphere were sufficient to decode the percept early in the trial, while the right hemisphere was necessary but not sufficient for decoding at later time points. We also decoded stimulus representations and found that they were maintained longer in the active task than in the passive task; however, these representations did not pattern more like discrete phonemes when an active categorical response was required. Instead, in both tasks, early phonemic patterns gave way to a representation of stimulus ambiguity that coincided in time with reliable percept decoding. Our results suggest that the categorization process does not require the loss of subphonemic detail, and that the neural representation of isolated speech sounds includes concurrent phonemic and subphonemic information.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NSPCVHFM/Beach et al. - 2021 - Neural Decoding Reveals Concurrent Phonemic and Su.pdf}
}

@article{becker_stage_2008,
  title = {The Stage of Priming: {{Are}} Intertrial Repetition Effects Attentional or Decisional?},
  shorttitle = {The Stage of Priming},
  author = {Becker, Stefanie I.},
  year = {2008},
  month = feb,
  journal = {Vision Research},
  volume = {48},
  number = {5},
  pages = {664--684},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2007.10.025},
  abstract = {In a visual search task, reaction times to a target are shorter when its features are repeated than when they switch. The present study investigated whether these priming effects affect the attentional stage of target selection, as proposed by the priming of pop-out account, or whether they modulate performance at a later, post-selectional stage, as claimed by the episodic retrieval view. Secondly, to test whether priming affects only the target-defining feature, or whether priming can apply to all target-features in a holistic fashion, two presentation conditions were invoked, that either promoted encoding of only the target-defining feature or holistic encoding of all target features. Results from four eye tracking experiments involving a size and colour singleton target showed that, first, priming modulates selectional processes concerned with guiding attention. Second, there were traces of holistic priming effects, which however were not modulated by the displays, but by expectation and task difficulty.},
  langid = {english},
  keywords = {Attention,Episodic retrieval,Intertrial effects,Priming of pop-out,Visual search},
  file = {/Users/xzfang/Zotero/storage/ASECBWLM/Becker - 2008 - The stage of priming Are intertrial repetition ef.pdf;/Users/xzfang/Zotero/storage/TVACCCQ7/S0042698907004725.html}
}

@article{beddor_languagespecific_2002,
  title = {Language-Specific Patterns of Vowel-to-Vowel Coarticulation: Acoustic Structures and Their Perceptual Correlates},
  shorttitle = {Language-Specific Patterns of Vowel-to-Vowel Coarticulation},
  author = {Beddor, Patrice Speeter and Harnsberger, James D. and Lindemann, Stephanie},
  year = {2002},
  month = oct,
  journal = {Journal of Phonetics},
  volume = {30},
  number = {4},
  pages = {591--627},
  issn = {0095-4470},
  doi = {10.1006/jpho.2002.0177},
  abstract = {Three experiments tested the hypothesis that V-to-V coarticulatory organization differs in Shona and English, and that Shona- and English-speaking listeners' sensitivity to V-to-V coarticulatory effects is correspondingly language-specific. An acoustic study of Shona and English CV{${'}$}CVCV trisyllables (Experiment 1) showed that the two languages differ in the carryover vs. anticipatory influences of stressed and unstressed vowels on each other. In 4IAX discrimination tests in which both Shona and English coarticulatory effects were spliced into different coarticulatory contexts (Experiment 2), Shona and English listeners perceptually compensated more (i.e., attributed more of a vowel's acoustic properties to its coarticulatory context in targeted test trials) for effects that were consistent with their linguistic experience. Similarly, when these listeners identified synthetic target vowels embedded into different vowel contexts (Experiment 3), Shona listeners compensated more (i.e., showed larger category boundary shifts) for the vowel contexts that triggered larger acoustic influences in the production study. English listeners' boundary shifts were more complicated but, when these data were combined with those from a follow-up identification study, they showed the perceptual shifts expected on the basis of the English coarticulatory findings. Overall, the relation between the production and perception data suggests that listeners are attuned to native-language coarticulatory patterns.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/D53NF66F/Beddor et al. - 2002 - Language-specific patterns of vowel-to-vowel coart.pdf;/Users/xzfang/Zotero/storage/U8KY2J4B/S0095447002901774.html}
}

@article{bedny_evidence_2017,
  title = {Evidence from {{Blindness}} for a {{Cognitively Pluripotent Cortex}}},
  author = {Bedny, Marina},
  year = {2017},
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {21},
  number = {9},
  pages = {637--648},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2017.06.003},
  abstract = {Cognitive neuroscience seeks to discover how cognitive functions are implemented in neural circuits. Studies of plasticity in blindness suggest that this mind\textendash brain mapping is highly flexible during development. In blindness, `visual' cortices take on higher-cognitive functions, including language and mathematics, becoming sensitive to the grammatical structure of spoken sentences and the difficulty of math equations. Visual cortex activity at rest becomes synchronized with higher-cognitive networks. Such repurposing is striking in light of the cognitive and evolutionary differences between vision, language, and mathematics. We propose that human cortices are cognitively pluripotent, that is, capable of assuming a wide range of cognitive functions. Specialization is driven by input during development, which is itself constrained by connectivity and experience. `The child who methodically adds two numbers from right to left, carrying a digit when necessary, may be using the same algorithm that is implemented by the wires and transistors of the cash register in the neighborhood supermarket\ldots ' {$\blockthreeqtrshaded\blockthreeqtrshaded$}Vision, 1982, David Marr},
  langid = {english},
  keywords = {blindness,development,language,plasticity,visual cortex},
  file = {/Users/xzfang/Zotero/storage/FJ6T4CNM/S1364661317301274.html}
}

@article{bedny_language_2011,
  title = {Language Processing in the Occipital Cortex of Congenitally Blind Adults},
  author = {Bedny, Marina and {Pascual-Leone}, Alvaro and {Dodell-Feder}, David and Fedorenko, Evelina and Saxe, Rebecca},
  year = {2011},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {11},
  pages = {4429--4434},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1014818108},
  file = {/Users/xzfang/Zotero/storage/53HM6LQE/Bedny et al. - 2011 - Language processing in the occipital cortex of con.pdf}
}

@article{bedny_sensitive_2010,
  title = {Sensitive {{Period}} for a {{Multimodal Response}} in {{Human Visual Motion Area MT}}/{{MST}}},
  author = {Bedny, Marina and Konkle, Talia and Pelphrey, Kevin and Saxe, Rebecca and {Pascual-Leone}, Alvaro},
  year = {2010},
  month = nov,
  journal = {Current Biology},
  volume = {20},
  number = {21},
  pages = {1900--1906},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2010.09.044},
  abstract = {The middle temporal complex (MT/MST) is a brain region specialized for the perception of motion in the visual modality [1, 2, 3, 4]. However, this specialization is modified by visual experience: after long-standing blindness, MT/MST responds to sound [5]. Recent evidence also suggests that the auditory response of MT/MST is selective for motion [6, 7]. The developmental time course of this plasticity is not known. To test for a sensitive period in MT/MST development, we used fMRI to compare MT/MST function in congenitally blind, late-blind, and sighted adults. MT/MST responded to sound in congenitally blind adults, but not in late-blind or sighted adults, and not in an individual who lost his vision between ages of 2 and 3 years. All blind adults had reduced functional connectivity between MT/MST and other visual regions. Functional connectivity was increased between MT/MST and lateral prefrontal areas in congenitally blind relative to sighted and late-blind adults. These data suggest that early blindness affects the function of feedback projections from prefrontal cortex to MT/MST. We conclude that there is a sensitive period for visual specialization in MT/MST. During typical development, early visual experience either maintains or creates a vision-dominated response. Once established, this response profile is not altered by long-standing blindness.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CFXGR5UU/Bedny et al. - 2010 - Sensitive Period for a Multimodal Response in Huma.pdf;/Users/xzfang/Zotero/storage/6T3UMV7K/S0960982210011620.html}
}

@article{behera_microwave_2004,
  title = {Microwave Heating and Conventional Roasting of Cumin Seeds ({{Cuminum}} Cyminum {{L}}.) and Effect on Chemical Composition of Volatiles},
  author = {Behera, Sushmita and Nagarajan, S and Jagan Mohan Rao, L},
  year = {2004},
  month = aug,
  journal = {Food Chemistry},
  volume = {87},
  number = {1},
  pages = {25--29},
  issn = {0308-8146},
  doi = {10.1016/j.foodchem.2003.10.012},
  abstract = {Microwave processing and cooking of foods is a recent development, which is gaining momentum in household as well as large-scale food applications. Processing of spices using microwaves is a newer dimension. This alternative methodology is preferred, due to the convenience and ease of handling. In Indian tradition, most of the spices are subjected to roasting before addition to food preparations. In the present study, cumin seeds are subjected to heating by microwaves, using various power levels, and conventional roasting at different temperatures. The conditions were standardized in both methods. The volatile oils distilled from these samples were analysed by GC and GC\textendash MS. The results indicated that the microwave-heated samples showed better retention of characteristic flavour compounds, such as aldehydes, than did the conventionally roasted samples. Comparative data on yield, chemical composition, flavour quality and physicochemical parameters of the volatile oils are presented.},
  keywords = {Chemical composition,Cumin seeds,Cuminum cyminum,Flavour quality,GCâ€“MS,Microwave heating and conventional roasting,Umbelliferae,Volatile compounds},
  file = {/Users/xzfang/Zotero/storage/QGV34F96/Behera et al. - 2004 - Microwave heating and conventional roasting of cum.html}
}

@article{behrens_what_2018,
  title = {What {{Is}} a {{Cognitive Map}}? {{Organizing Knowledge}} for {{Flexible Behavior}}},
  shorttitle = {What {{Is}} a {{Cognitive Map}}?},
  author = {Behrens, Timothy E. J. and Muller, Timothy H. and Whittington, James C. R. and Mark, Shirley and Baram, Alon B. and Stachenfeld, Kimberly L. and {Kurth-Nelson}, Zeb},
  year = {2018},
  month = oct,
  journal = {Neuron},
  volume = {100},
  number = {2},
  pages = {490--509},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.10.002},
  abstract = {It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behavior, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviors has rekindled the notion of a systematic organization of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalizable knowledge and suggest that map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organizing knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behavior and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalizations, abstractions, and inferences that characterize human cognition.},
  langid = {english},
  keywords = {Cognitive Map,Decision Making,Generalization,Hippocampal Formation,Inference,Prefrontal Cortex,Reinforcement Learning,Spatial Cognition,Statistical Learning,Structure Learning},
  file = {/Users/xzfang/Zotero/storage/GV46FSJP/Behrens et al. - 2018 - What Is a Cognitive Map Organizing Knowledge for .pdf;/Users/xzfang/Zotero/storage/RL4CJL85/S0896627318308560.html}
}

@article{bein_mnemonic_2021,
  title = {Mnemonic Prediction Errors Promote Detailed Memories},
  author = {Bein, Oded and Plotkin, Natalie A. and Davachi, Lila},
  year = {2021},
  month = nov,
  journal = {Learning \& Memory},
  volume = {28},
  number = {11},
  pages = {422--434},
  publisher = {{Cold Spring Harbor Lab}},
  issn = {1072-0502, 1549-5485},
  doi = {10.1101/lm.053410.121},
  abstract = {When our experience violates our predictions, it is adaptive to update our knowledge to promote a more accurate representation of the world and facilitate future predictions. Theoretical models propose that these mnemonic prediction errors should be encoded into a distinct memory trace to prevent interference with previous, conflicting memories. We investigated this proposal by repeatedly exposing participants to pairs of sequentially presented objects (A \textrightarrow{} B), thus evoking expectations. Then, we violated participants' expectations by replacing the second object in the pairs with a novel object (A \textrightarrow{} C). The following item memory test required participants to discriminate between identical old items and similar lures, thus testing detailed and distinctive item memory representations. In two experiments, mnemonic prediction errors enhanced item memory: Participants correctly identified more old items as old when those items violated expectations during learning, compared with items that did not violate expectations. This memory enhancement for C items was only observed when participants later showed intact memory for the related A \textrightarrow{} B pairs, suggesting that strong predictions are required to facilitate memory for violations. Following up on this, a third experiment reduced prediction strength prior to violation and subsequently eliminated the memory advantage of violations. Interestingly, mnemonic prediction errors did not increase gist-based mistakes of identifying old items as similar lures or identifying similar lures as old. Enhanced item memory in the absence of gist-based mistakes suggests that violations enhanced memory for items' details, which could be mediated via distinct memory traces. Together, these results advance our knowledge of how mnemonic prediction errors promote memory formation.},
  langid = {english},
  pmid = {34663695},
  file = {/Users/xzfang/Zotero/storage/WRKEQZ6X/Bein et al. - 2021 - Mnemonic prediction errors promote detailed memori.pdf;/Users/xzfang/Zotero/storage/MHL7UFNU/422.html}
}

@article{beis_production_2000,
  title = {Production of Essential Oil from {{Cumin}} Seeds},
  author = {Beis, S. H. and Azcan, N. and Ozek, T. and Kara, M. and Baser, K. H. C.},
  year = {2000},
  month = may,
  journal = {Chemistry of Natural Compounds},
  volume = {36},
  number = {3},
  pages = {265--268},
  issn = {0009-3130, 1573-8388},
  doi = {10.1007/BF02238331},
  abstract = {Steam distillation of cumin (Cuminum cyminum L.) seeds was studied to show the effects of particle size, batch size, and distillation rate on their essential oil recovery. Experiments were carried out both on bench and pilot scale. The composition of the oil was analyzed by GC/MS.},
  langid = {english},
  keywords = {cumin seed,Cuminum cyminum L.,essential oil,GC/MS,Organic Chemistry,Pharmacy,Plant Sciences,steam distillation},
  file = {/Users/xzfang/Zotero/storage/NQNN5NPI/Beis et al. - 2000 - Production of essential oil from Cumin seeds.pdf;/Users/xzfang/Zotero/storage/86UHHSP9/Beis et al. - 2000 - Production of essential oil from Cumin seeds.html}
}

@article{bejjanki_cue_2011,
  title = {Cue {{Integration}} in {{Categorical Tasks}}: {{Insights}} from {{Audio-Visual Speech Perception}}},
  shorttitle = {Cue {{Integration}} in {{Categorical Tasks}}},
  author = {Bejjanki, Vikranth Rao and Clayards, Meghan and Knill, David C. and Aslin, Richard N.},
  year = {2011},
  month = may,
  journal = {PLOS ONE},
  volume = {6},
  number = {5},
  pages = {e19812},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019812},
  abstract = {Previous cue integration studies have examined continuous perceptual dimensions (e.g., size) and have shown that human cue integration is well described by a normative model in which cues are weighted in proportion to their sensory reliability, as estimated from single-cue performance. However, this normative model may not be applicable to categorical perceptual dimensions (e.g., phonemes). In tasks defined over categorical perceptual dimensions, optimal cue weights should depend not only on the sensory variance affecting the perception of each cue but also on the environmental variance inherent in each task-relevant category. Here, we present a computational and experimental investigation of cue integration in a categorical audio-visual (articulatory) speech perception task. Our results show that human performance during audio-visual phonemic labeling is qualitatively consistent with the behavior of a Bayes-optimal observer. Specifically, we show that the participants in our task are sensitive, on a trial-by-trial basis, to the sensory uncertainty associated with the auditory and visual cues, during phonemic categorization. In addition, we show that while sensory uncertainty is a significant factor in determining cue weights, it is not the only one and participants' performance is consistent with an optimal model in which environmental, within category variability also plays a role in determining cue weights. Furthermore, we show that in our task, the sensory variability affecting the visual modality during cue-combination is not well estimated from single-cue performance, but can be estimated from multi-cue performance. The findings and computational principles described here represent a principled first step towards characterizing the mechanisms underlying human cue integration in categorical tasks.},
  langid = {english},
  keywords = {Acoustic signals,Audio signal processing,Phonemes,Sensory cues,Sensory perception,Speech,Speech signal processing,Vision},
  file = {/Users/xzfang/Zotero/storage/3N2VU7Q7/Bejjanki et al. - 2011 - Cue Integration in Categorical Tasks Insights fro.pdf;/Users/xzfang/Zotero/storage/IG33QGRM/article.html}
}

@article{bek_language_2010,
  title = {Language and Spatial Reorientation: {{Evidence}} from Severe Aphasia.},
  shorttitle = {Language and Spatial Reorientation},
  author = {Bek, Judith and Blades, Mark and Siegal, Michael and Varley, Rosemary},
  year = {2010},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {36},
  number = {3},
  pages = {646},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1285},
  doi = {10.1037/a0018281},
  file = {/Users/xzfang/Zotero/storage/R3SK6ZVG/2010-08037-007.html}
}

@misc{bellana_narrative_2021,
  title = {Narrative Thinking Lingers in Spontaneous Thought},
  author = {Bellana, Buddhika and Mahabal, Abhijit and Honey, Christopher J.},
  year = {2021},
  month = aug,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/gxzyj},
  abstract = {What we think about at any moment is shaped by what preceded it. Why do some experiences, such as reading an immersive story, feel as if they linger in mind for longer than others? In this study, we hypothesize that the stream of our thinking is especially affected by "deeper" forms of processing, emphasizing the meaning and implications of a stimulus rather than its immediate physical properties or low-level semantics (e.g., reading a story vs. reading disconnected words). To test this idea, we presented participants with short stories that preserved different levels of coherence (word-level, sentence-level, or intact narrative), and we measured participants' self-reports of lingering and spontaneous word generation. Participants reported that stories lingered in their minds after reading, but this effect was greatly reduced when the same words were read with sentence or word-order randomly shuffled. Furthermore, the words that participants spontaneously generated after reading shared semantic meaning with the story's central themes, particularly when the story was coherent (i.e., intact). Crucially, regardless of the objective coherence of what each participant read, lingering was strongest amongst participants who reported being `transported' into the world of the story while reading. We further generalized this result to a non-narrative stimulus, finding that participants reported lingering after reading a list of words, especially when they had sought an underlying narrative or theme across words. We conclude that recent experiences are most likely to exert a lasting mental context when we seek to extract and represent their deep situation-level meaning.},
  keywords = {Cognitive Psychology,deep processing,memory,Memory,mental context,mind wandering,narratives,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/Z5HXBMRQ/Bellana et al. - 2021 - Narrative thinking lingers in spontaneous thought.pdf}
}

@article{bemis_simple_2011,
  title = {Simple {{Composition}}: {{A Magnetoencephalography Investigation}} into the {{Comprehension}} of {{Minimal Linguistic Phrases}}},
  shorttitle = {Simple {{Composition}}},
  author = {Bemis, Douglas K. and Pylkk{\"a}nen, Liina},
  year = {2011},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {31},
  number = {8},
  pages = {2801--2814},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5003-10.2011},
  abstract = {The expressive power of language lies in its ability to construct an infinite array of ideas out of a finite set of pieces. Surprisingly, few neurolinguistic investigations probe the basic processes that constitute the foundation of this ability, choosing instead to focus on relatively complex combinatorial operations. Contrastingly, in the present work, we investigate the neural circuits underlying simple linguistic composition, such as required by the minimal phrase ``red boat.'' Using magnetoencephalography, we examined activity in humans generated at the visual presentation of target nouns, such as ``boat,'' and varied the combinatorial operations induced by its surrounding context. Nouns in minimal compositional contexts (``red boat'') were compared with those appearing in matched non-compositional contexts, such as after an unpronounceable consonant string (``xkq boat'') or within a list (``cup, boat''). Source analysis did not implicate traditional language areas (inferior frontal gyrus, posterior temporal regions) in such basic composition. Instead, we found increased combinatorial-related activity in the left anterior temporal lobe (LATL) and ventromedial prefrontal cortex (vmPFC). These regions have been linked previously to syntactic (LATL) and semantic (vmPFC) combinatorial processing in more complex linguistic contexts. Thus, we suggest that these regions play a role in basic syntactic and semantic composition, respectively. Importantly, the temporal ordering of the effects, in which LATL activity ({$\sim$}225 ms) precedes vmPFC activity ({$\sim$}400 ms), is consistent with many processing models that posit syntactic composition before semantic composition during the construction of linguistic representations.},
  copyright = {Copyright \textcopyright{} 2011 the authors 0270-6474/11/312801-14\$15.00/0},
  langid = {english},
  pmid = {21414902},
  file = {/Users/xzfang/Zotero/storage/A3H87CPC/Bemis and PylkkÃ¤nen - 2011 - Simple Composition A Magnetoencephalography Inves.pdf;/Users/xzfang/Zotero/storage/4I2ZN2KE/2801.html}
}

@article{benjamin_remarks_2021,
  title = {Remarks on the Analysis of Steady-State Responses: {{Spurious}} Artifacts Introduced by Overlapping Epochs},
  shorttitle = {Remarks on the Analysis of Steady-State Responses},
  author = {Benjamin, Lucas and {Dehaene-Lambertz}, Ghislaine and Fl{\'o}, Ana},
  year = {2021},
  month = sep,
  journal = {Cortex},
  volume = {142},
  pages = {370--378},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2021.05.023},
  abstract = {Periodic and stable sensory input can result in rhythmic and stable neural responses, a phenomenon commonly referred to as neural entrainment. Although the use of neural entrainment to investigate the regularities the brain tracks has increased in recent years, the methods used for its quantification are not well-defined in the literature. Here we argue that some strategies used in previous papers, are inadequate for the study of steady-state response, and lead to methodological artefacts. The aim of this commentary is to discuss these articles and to propose alternative measures of neural entrainment. Specifically, we applied four possible alternatives and two epoching approaches reported in the literature to quantify neural entrainment on simulated datasets. Our results demonstrate that overlapping epochs, as used in the original Batterink and colleagues articles, inevitably lead to a methodological artefact at the frequency corresponding to the overlap. We therefore strongly discourage this approach and encourage the re-analysis of data based on overlapping epochs. Additionally, we argue that the use of time\textendash frequency decomposition to compute phase coherence at low frequencies to reveal neural entrainment is not optimal.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CUFQ7XG6/Benjamin et al. - 2021 - Remarks on the analysis of steady-state responses.pdf;/Users/xzfang/Zotero/storage/AIRLZJ55/S0010945221002215.html}
}

@article{benoit_specifying_2015,
  title = {Specifying the Core Network Supporting Episodic Simulation and Episodic Memory by Activation Likelihood Estimation},
  author = {Benoit, Roland G. and Schacter, Daniel L.},
  year = {2015},
  month = aug,
  journal = {Neuropsychologia},
  volume = {75},
  pages = {450--457},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2015.06.034},
  abstract = {It has been suggested that the simulation of hypothetical episodes and the recollection of past episodes are supported by fundamentally the same set of brain regions. The present article specifies this core network via Activation Likelihood Estimation (ALE). Specifically, a first meta-analysis revealed joint engagement of expected core-network regions during episodic memory and episodic simulation. These include parts of the medial surface, the hippocampus and parahippocampal cortex within the medial temporal lobes, and the temporal and inferior posterior parietal cortices on the lateral surface. Both capacities also jointly recruited additional regions such as parts of the bilateral dorsolateral prefrontal cortex. All of these core regions overlapped with the default network. Moreover, it has further been suggested that episodic simulation may require a stronger engagement of some of the core network's nodes as well as the recruitment of additional brain regions supporting control functions. A second ALE meta-analysis indeed identified such regions that were consistently more strongly engaged during episodic simulation than episodic memory. These comprised the core-network clusters located in the left dorsolateral prefrontal cortex and posterior inferior parietal lobe and other structures distributed broadly across the default and fronto-parietal control networks. Together, the analyses determine the set of brain regions that allow us to experience past and hypothetical episodes, thus providing an important foundation for studying the regions' specialized contributions and interactions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IM6QSQAU/Benoit and Schacter - 2015 - Specifying the core network supporting episodic si.pdf}
}

@article{bentin_erp_1999,
  title = {{{ERP Manifestations}} of {{Processing Printed Words}} at {{Different Psycholinguistic Levels}}: {{Time Course}} and {{Scalp Distribution}}},
  shorttitle = {{{ERP Manifestations}} of {{Processing Printed Words}} at {{Different Psycholinguistic Levels}}},
  author = {Bentin, S. and {Mouchetant-Rostaing}, Y. and Giard, M. H. and Echallier, J. F. and Pernier, J.},
  year = {1999},
  month = may,
  journal = {Journal of Cognitive Neuroscience},
  volume = {11},
  number = {3},
  pages = {235--260},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/089892999563373},
  abstract = {Abstract             The aim of the present study was to examine the time course and scalp distribution of electrophysiological manifestations of the visual word recognition mechanism. Event-related potentials (ERPs) elicited by visually presented lists of words were recorded while subjects were involved in a series of oddball tasks. The distinction between the designated target and nontarget stimuli was manipulated to induce a different level of processing in each session (visual, phonological/phonetic, phonological/lexical, and semantic). The ERPs of main interest in this study were those elicited by nontarget stimuli. In the visual task the targets were twice as big as the nontargets. Words, pseudowords, strings of consonants, strings of alphanumeric symbols, and strings of forms elicited a sharp negative peak at 170 msec (N170); their distribution was limited to the occipito-temporal sites. For the left hemisphere electrode sites, the N170 was larger for orthographic than for nonorthographic stimuli and vice versa for the right hemisphere. The ERPs elicited by all orthographic stimuli formed a clearly distinct cluster that was different from the ERPs elicited by nonorthographic stimuli. In the phonological/phonetic decision task the targets were words and pseudowords rhyming with the French word vitrail, whereas the nontargets were words, pseudowords, and strings of consonants that did not rhyme with vitrail. The most conspicuous potential was a negative peak at 320 msec, which was similarly elicited by pronounceable stimuli but not by nonpronounceable stimuli. The N320 was bilaterally distributed over the middle temporal lobe and was significantly larger over the left than over the right hemisphere. In the phonological/lexical processing task we compared the ERPs elicited by strings of consonants (among which words were selected), pseudowords (among which words were selected), and by words (among which pseudowords were selected). The most conspicuous potential in these tasks was a negative potential peaking at 350 msec (N350) elicited by phonologically legal but not by phonologically illegal stimuli. The distribution of the N350 was similar to that of the N320, but it was broader and including temporo-parietal areas that were not activated in the ``rhyme'' task. Finally, in the semantic task the targets were abstract words, and the nontargets were concrete words, pseudowords, and strings of consonants. The negative potential in this task peaked at 450 msec. Unlike the lexical decision, the negative peak in this task significantly distinguished not only between phonologically legal and illegal words but also between meaningful (words) and meaningless (pseudowords) phonologically legal structures. The distribution of the N450 included the areas activated in the lexical decision task but also areas in the fronto-central regions. The present data corroborated the functional neuro-anatomy of word recognition systems suggested by other neuroimaging methods and described their timecourse, supporting a cascade-type process that involves different but interconnected neural modules, each responsible for a different level of processing word-related information.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NWMLZHWA/Bentin et al. - 1999 - ERP Manifestations of Processing Printed Words at .pdf}
}

@article{bentz_adaptive_2015,
  title = {Adaptive {{Communication}}: {{Languages}} with {{More Non-Native Speakers Tend}} to {{Have Fewer Word Forms}}},
  shorttitle = {Adaptive {{Communication}}},
  author = {Bentz, Christian and Verkerk, Annemarie and Kiela, Douwe and Hill, Felix and Buttery, Paula},
  year = {2015},
  month = jun,
  journal = {PLOS ONE},
  volume = {10},
  number = {6},
  pages = {e0128254},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0128254},
  abstract = {Explaining the diversity of languages across the world is one of the central aims of typological, historical, and evolutionary linguistics. We consider the effect of language contact-the number of non-native speakers a language has-on the way languages change and evolve. By analysing hundreds of languages within and across language families, regions, and text types, we show that languages with greater levels of contact typically employ fewer word forms to encode the same information content (a property we refer to as lexical diversity). Based on three types of statistical analyses, we demonstrate that this variance can in part be explained by the impact of non-native speakers on information encoding strategies. Finally, we argue that languages are information encoding systems shaped by the varying needs of their speakers. Language evolution and change should be modeled as the co-evolution of multiple intertwined adaptive systems: On one hand, the structure of human societies and human learning capabilities, and on the other, the structure of language.},
  langid = {english},
  keywords = {Human learning,Language acquisition,Language families,Languages,Phylogenetic analysis,Phylogenetics,Semantics,Syntax},
  file = {/Users/xzfang/Zotero/storage/CIZX7XTR/Bentz et al. - 2015 - Adaptive Communication Languages with More Non-Na.pdf;/Users/xzfang/Zotero/storage/Y44IZ33V/article.html}
}

@article{berezutskaya_open_2021,
  title = {Open Multimodal {{iEEG-fMRI}} Dataset from Naturalistic Stimulation with a Short Audiovisual Film},
  author = {Berezutskaya, Julia and Vansteensel, Mariska J. and Aarnoutse, Erik J. and Freudenburg, Zachary V. and Piantoni, Giovanni and Branco, Mariana P. and Ramsey, Nick F.},
  year = {2021},
  month = jun,
  journal = {bioRxiv},
  pages = {2021.06.09.447733},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.09.447733},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Intracranial human recordings are a valuable and rare resource that the whole neuroscience community can benefit from. Making such data available to the neuroscience community not only helps tackle the reproducibility issues in science, it also helps make more use of this valuable data. The latter is especially true for data collected using naturalistic tasks. Here, we describe a dataset collected from a large group of human subjects while they watched a short audiovisual film. The dataset is characterized by several unique features. First, it combines a large amount of intracranial data from 51 intracranial electroencephalography (iEEG) participants, who all did the same task. Second, the intracranial data are accompanied by fMRI recordings acquired for the same task in 30 functional magnetic resonance imaging (fMRI) participants. Third, the data were acquired using a rich audiovisual stimulus, for which we provide detailed speech and video annotations. This multimodal dataset can be used to address questions about neural mechanisms of multimodal perception and language comprehension as well as the nature of the neural signal acquired during the same task across brain recording modalities.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GVE4MZTG/Berezutskaya et al. - 2021 - Open multimodal iEEG-fMRI dataset from naturalisti.pdf;/Users/xzfang/Zotero/storage/2DPNHJBF/2021.06.09.html}
}

@misc{berger_distribution_2021,
  title = {Distribution of Multi-Unit Pitch Responses Recorded Intracranially from Human Auditory Cortex},
  author = {Berger, Joel I. and Gander, Phillip E. and Kikuchi, Yukiko and Kumar, Sukhbinder and Kovach, Christopher and Oya, Hiroyuki and Kawasaki, Hiroto and Howard, Matthew A. and Griffiths, Timothy D.},
  year = {2021},
  month = oct,
  pages = {2021.10.22.465330},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.10.22.465330},
  abstract = {The perception of pitch requires the abstraction of stimulus properties related to the spectrotemporal structure of sound. Previous studies utilizing both animal electrophysiology and human imaging have indicated the presence of a center for pitch representation in the auditory cortex. Recent data from our own group - examining local field potentials (LFPs) in humans - indicate more widely distributed pitch-associated responses within the auditory cortex (Gander et al., 2019). To probe this with greater spatial resolution, we examined multi-unit activity related to three different auditory stimuli, in seven epilepsy patients who were implanted with high-impedance electrodes in auditory cortex for the clinical purpose of localizing seizures. The stimuli were regular-interval noise (RIN) with a pitch strength that is related to the temporal regularity, and pitch value determined by repetition rate, and harmonic complexes with missing fundamentals. We demonstrated increases in spiking activity in 69 of 104 (66\%) responsive multiunit activity in auditory cortex due to pitch-associated stimuli. Importantly, these responses were distributed across the entire extent of Heschl's gyrus (HG), in both primary and non-primary areas, rather than isolated to a specific region, and this finding was evident regardless of the stimulus presented. These findings are the first multi-unit pitch responses recorded from humans, and align with a recent study in macaques (Kikuchi et al., 2019) demonstrating that both local field potential and unit responses to pitch-inducing stimuli are distributed throughout auditory cortex. Significance Statement The perception of pitch is a fundamental acoustic attribute that is mediated by the auditory system. Despite its importance, there is still debate as to the precise areas responsible for its encoding, which may be due to differences in the recording measures and choices of stimuli used in previous studies. Here, we present the first study to measure multi-unit pitch responses in the auditory cortices of intracranially-implanted humans. Importantly, we demonstrate reliable responses to three different pitch-inducing paradigms that are distributed throughout Heschl's gyrus, rather than being localized to a particular region. These data provide a bridge across animal and human studies, and aid in our understanding of the processing of a critical attribute of acoustic stimuli.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/U96K7N9Z/Berger et al. - 2021 - Distribution of multi-unit pitch responses recorde.pdf;/Users/xzfang/Zotero/storage/RS4XYKVW/2021.10.22.html}
}

@article{bermudez_conditional_2005,
  title = {Conditional {{Associative Memory}} for {{Musical Stimuli}} in {{Nonmusicians}}: {{Implications}} for {{Absolute Pitch}}},
  shorttitle = {Conditional {{Associative Memory}} for {{Musical Stimuli}} in {{Nonmusicians}}},
  author = {Bermudez, P.},
  year = {2005},
  month = aug,
  journal = {Journal of Neuroscience},
  volume = {25},
  number = {34},
  pages = {7718--7723},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1560-05.2005},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8HCE9V3C/Bermudez - 2005 - Conditional Associative Memory for Musical Stimuli.pdf}
}

@article{berniker_estimating_2008,
  title = {Estimating the Sources of Motor Errors for Adaptation and Generalization},
  author = {Berniker, Max and Kording, Konrad},
  year = {2008},
  month = dec,
  journal = {Nature neuroscience},
  volume = {11},
  number = {12},
  pages = {1454--1461},
  issn = {1097-6256},
  doi = {10.1038/nn.2229},
  abstract = {Motor adaptation is usually defined as the process by which our nervous system produces accurate movements, while the properties of our bodies and our environment continuously change. Numerous experimental and theoretical studies have characterized this process by assuming that the nervous system uses internal models to compensate for motor errors. Here we extend these approaches and construct a probabilistic model that not only compensates for motor errors but estimates the sources of these errors. These estimates dictate how the nervous system should generalize. For example, estimated changes of limb properties will affect movements across the workspace but not movements with the other limb. We provide evidence that many movement generalization phenomena emerge from a strategy by which the nervous system estimates the sources of our motor errors.},
  pmcid = {PMC2707921},
  pmid = {19011624},
  file = {/Users/xzfang/Zotero/storage/HA6JDXJX/Berniker and Kording - 2008 - Estimating the sources of motor errors for adaptat.pdf}
}

@article{bertelson_visual_2003,
  title = {Visual {{Recalibration}} of {{Auditory Speech Identification}}: {{A McGurk Aftereffect}}},
  shorttitle = {Visual {{Recalibration}} of {{Auditory Speech Identification}}},
  author = {Bertelson, Paul and Vroomen, Jean and {de Gelder}, B{\'e}atrice},
  year = {2003},
  month = nov,
  journal = {Psychological Science},
  volume = {14},
  number = {6},
  pages = {592--597},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1046/j.0956-7976.2003.psci_1470.x},
  abstract = {The kinds of aftereffects, indicative of cross-modal recalibration, that are observed after exposure to spatially incongruent inputs from different sensory modalities have not been demonstrated so far for identity incongruence. We show that exposure to incongruent audiovisual speech (producing the well-known McGurk effect) can recalibrate auditory speech identification. In Experiment 1, exposure to an ambiguous sound intermediate between /aba/ and /ada/ dubbed onto a video of a face articulating either /aba/ or /ada/ increased the proportion of /aba/ or /ada/ responses, respectively, during subsequent sound identification trials. Experiment 2 demonstrated the same recalibration effect or the opposite one, fewer /aba/ or /ada/ responses, revealing selective speech adaptation, depending on whether the ambiguous sound or a congruent nonambiguous one was used during exposure. In separate forced-choice identification trials, bimodal stimulus pairs producing these contrasting effects were identically categorized, which makes a role of postperceptual factors in the generation of the effects unlikely.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BZMT2G6J/Bertelson et al. - 2003 - Visual Recalibration of Auditory Speech Identifica.pdf}
}

@article{besson_eventrelated_1987,
  title = {An {{Event-Related Potential Analysis}} of {{Incongruity}} in {{Music}} and {{Other Non-Linguistic Contexts}}},
  author = {Besson, M. and Macar, F.},
  year = {1987},
  journal = {Psychophysiology},
  volume = {24},
  number = {1},
  pages = {14--25},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.1987.tb01853.x},
  abstract = {This study was designed to determine whether the N400 component, described by Kutas and Hillyard as an index of semantic expectancy, would he elicited by deviations involving non-linguistic expectancies. The stimuli within the four experimental conditions included: 1) sentences, 2) geometric patterns of increasing or decreasing size, 3) scale-notes of increasing or decreasing frequency, and 4) well-known French melodies. An N400 appeared only following semantic incongruities within sentences. Non-linguistic deviations were followed by a late positivity whose amplitude varied across conditions. These results are consistent with an interpretation of N400 as an index of the further processing required by linguistic incongruities rather than by violations of arbitrary or over learned rules in general. However, alternative interpretations exist and are discussed.},
  langid = {english},
  keywords = {Event-related potentials,Language,Learned vs. over-learned mechanisms,Music,Non-linguistic incongruities,Semantic},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.1987.tb01853.x},
  file = {/Users/xzfang/Zotero/storage/IZDBSMM2/Besson and Macar - 1987 - An Event-Related Potential Analysis of Incongruity.pdf;/Users/xzfang/Zotero/storage/NKIAYYNZ/j.1469-8986.1987.tb01853.html}
}

@article{besson_eventrelated_1995,
  title = {An Event-Related Potential ({{ERP}}) Study of Musical Expectancy: {{Comparison}} of Musicians with Nonmusicians.},
  shorttitle = {An Event-Related Potential ({{ERP}}) Study of Musical Expectancy},
  author = {Besson, Mireille and Fa{\"i}ta, Fr{\'e}d{\'e}rique},
  year = {1995},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {21},
  number = {6},
  pages = {1278--1296},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.21.6.1278},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZFN9XTAD/Besson and FaÃ¯ta - 1995 - An event-related potential (ERP) study of musical .pdf}
}

@article{best_buildup_2018,
  title = {A ``{{Buildup}}'' of {{Speech Intelligibility}} in {{Listeners With Normal Hearing}} and {{Hearing Loss}}},
  author = {Best, Virginia and Swaminathan, Jayaganesh and Kop{\v c}o, Norbert and Roverud, Elin and {Shinn-Cunningham}, Barbara},
  year = {2018},
  month = jan,
  journal = {Trends in Hearing},
  volume = {22},
  pages = {2331216518807519},
  publisher = {{SAGE Publications Inc}},
  issn = {2331-2165},
  doi = {10.1177/2331216518807519},
  abstract = {The perception of simple auditory mixtures is known to evolve over time. For instance, a common example of this is the ``buildup'' of stream segregation that is observed for sequences of tones alternating in pitch. Yet very little is known about how the perception of more complicated auditory scenes, such as multitalker mixtures, changes over time. Previous data are consistent with the idea that the ability to segregate a target talker from competing sounds improves rapidly when stable cues are available, which leads to improvements in speech intelligibility. This study examined the time course of this buildup in listeners with normal and impaired hearing. Five simultaneous sequences of digits, varying in length from three to six digits, were presented from five locations in the horizontal plane. A synchronized visual cue at one location indicated which sequence was the target on each trial. We observed a buildup in digit identification performance, driven primarily by reductions in confusions between the target and the maskers, that occurred over the course of three to four digits. Performance tended to be poorer in listeners with hearing loss; however, there was only weak evidence that the buildup was diminished or slowed in this group.},
  langid = {english},
  keywords = {cocktail party,competing talkers,segregation},
  file = {/Users/xzfang/Zotero/storage/VLU9X9KT/Best et al. - 2018 - A â€œBuildupâ€ of Speech Intelligibility in Listeners.pdf}
}

@article{best_object_2008,
  title = {Object Continuity Enhances Selective Auditory Attention},
  author = {Best, V. and Ozmeral, E. J. and Kopco, N. and {Shinn-Cunningham}, B. G.},
  year = {2008},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {35},
  pages = {13174--13178},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0803718105},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RNB8GHZU/Best et al. - 2008 - Object continuity enhances selective auditory atte.pdf}
}

@article{bhattasali_alice_2020,
  title = {The {{Alice Datasets}}: {{fMRI}} \& {{EEG Observations}} of {{Natural Language Comprehension}}},
  author = {Bhattasali, Shohini and Brennan, Jonathan and Luh, Wen-Ming and Franzluebbers, Berta and Hale, John},
  year = {2020},
  pages = {6},
  abstract = {The Alice Datasets combine observations from magnetic resonance imaging as well as electrophysiology while human participants listened to the same literary narrative in English. Along with these neural signals and the text of the story, we also provide a variety of word-by-word predictors motivated by research in computational linguistics and cognitive science. These predictors range from prosody to morphology to syntax. These annotated, naturalistic datasets can be used to replicate prior work and test new hypotheses about natural language comprehension in the brain.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CZN6RVGM/Bhattasali et al. - The Alice Datasets fMRI & EEG Observations of Nat.pdf}
}

@article{bhaya-grossman_speech_2022,
  title = {Speech {{Computations}} of the {{Human Superior Temporal Gyrus}}},
  author = {{Bhaya-Grossman}, Ilina and Chang, Edward F.},
  year = {2022},
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {null},
  doi = {10.1146/annurev-psych-022321-035256},
  abstract = {Human speech perception results from neural computations that transform external acoustic speech signals into internal representations of words. The superior temporal gyrus (STG) contains the nonprimary auditory cortex and is a critical locus for phonological processing. Here, we describe how speech sound representation in the STG relies on fundamentally nonlinear and dynamical processes, such as categorization, normalization, contextual restoration, and the extraction of temporal structure. A spatial mosaic of local cortical sites on the STG exhibits complex auditory encoding for distinct acoustic-phonetic and prosodic features. We propose that as a population ensemble, these distributed patterns of neural activity give rise to abstract, higher-order phonemic and syllabic representations that support speech perception. This review presents a multi-scale, recurrent model of phonological processing in the STG, highlighting the critical interface between auditory and language systems. Expected final online publication date for the Annual Review of Psychology, Volume 73 is January 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  pmid = {34672685},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-psych-022321-035256},
  file = {/Users/xzfang/Zotero/storage/MAFH9JIA/Bhaya-Grossman and Chang - 2022 - Speech Computations of the Human Superior Temporal.pdf}
}

@article{bi_dual_2021,
  title = {Dual Coding of Knowledge in the Human Brain},
  author = {Bi, Yanchao},
  year = {2021},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {25},
  number = {10},
  pages = {883--895},
  issn = {13646613},
  doi = {10.1016/j.tics.2021.07.006},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QTFPVV9S/Bi - 2021 - Dual coding of knowledge in the human brain.pdf}
}

@article{bian_novel_2020,
  title = {Novel Stress Phonotactics Are Learnable by {{English}} Speakers: {{Novel}} Tone Phonotactics Are Not},
  shorttitle = {Novel Stress Phonotactics Are Learnable by {{English}} Speakers},
  author = {Bian, Yuan and Dell, Gary S.},
  year = {2020},
  journal = {Memory \& Cognition},
  volume = {48},
  number = {2},
  pages = {176--187},
  publisher = {{Springer}},
  address = {{Germany}},
  issn = {1532-5946(Electronic),0090-502X(Print)},
  doi = {10.3758/s13421-019-01000-9},
  abstract = {Speech errors are sensitive to newly learned phonotactic constraints. For example, if speakers produce strings of syllables in which /f/ is an onset if the vowel is /\ae/, but a coda if the vowel is /I/, their slips will respect that constraint after a period of sleep. Constraints in which the contextual factor is nonlinguistic, however, do not appear to be learnable by this method\textemdash for example, /f/ is an onset if the speech rate is fast, but /f/ is a coda if the speech rate is slow. The present study demonstrated that adult English speakers can learn (after a sleep period) constraints based on stress (e.g., /f/ is an onset if the syllable is stressed, but /f/ is a coda if the syllable is unstressed), but cannot learn analogous constraints based on tone (e.g., /f/ is an onset if the tone is rising, but /f/ is a coda if the tone is falling). The results are consistent with the fact that, in English, stress is a relevant lexical phonological property (e.g., ``INsight'' and ``inCITE'' are different words), but tone is not (e.g., ``yes!'' and ``yes?'' are the same word, despite their different pragmatic functions). The results provide useful constraints on how consolidation effects in learning may interact with early learning experiences. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Implicit Learning,Oral Communication,Phonetics,Sleep,Speech Rate,Stress,Syllables,Vowels},
  file = {/Users/xzfang/Zotero/storage/C3NSVZ89/Bian and Dell - 2020 - Novel stress phonotactics are learnable by English.pdf;/Users/xzfang/Zotero/storage/A28P22C9/2019-81585-001.html}
}

@article{bichot_parallel_2005,
  title = {Parallel and {{Serial Neural Mechanisms}} for {{Visual Search}} in {{Macaque Area V4}}},
  author = {Bichot, Narcisse P. and Rossi, Andrew F. and Desimone, Robert},
  year = {2005},
  month = apr,
  journal = {Science},
  volume = {308},
  number = {5721},
  pages = {529--534},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1109676},
  abstract = {To find a target object in a crowded scene, a face in a crowd for example, the visual system might turn the neural representation of each object on and off in a serial fashion, testing each representation against a template of the target item. Alternatively, it might allow the processing of all objects in parallel but bias activity in favor of those neurons that represent critical features of the target, until the target emerges from the background. To test these possibilities, we recorded neurons in area V4 of monkeys freely scanning a complex array to find a target defined by color, shape, or both. Throughout the period of searching, neurons gave enhanced responses and synchronized their activity in the gamma range whenever a preferred stimulus in their receptive field matched a feature of the target, as predicted by parallel models. Neurons also gave enhanced responses to candidate targets that were selected for saccades, or foveation, reflecting a serial component of visual search. Thus, serial and parallel mechanisms of response enhancement and neural synchrony work together to identify objects in a scene. To find a target object in a crowded scene, a face in a crowd for example, the visual system might turn the neural representation of each object on and off in a serial fashion, testing each representation against a template of the target item. Alternatively, it might allow the processing of all objects in parallel but bias activity in favor of those neurons that represent critical features of the target, until the target emerges from the background. To test these possibilities, we recorded neurons in area V4 of monkeys freely scanning a complex array to find a target defined by color, shape, or both. Throughout the period of searching, neurons gave enhanced responses and synchronized their activity in the gamma range whenever a preferred stimulus in their receptive field matched a feature of the target, as predicted by parallel models. Neurons also gave enhanced responses to candidate targets that were selected for saccades, or foveation, reflecting a serial component of visual search. Thus, serial and parallel mechanisms of response enhancement and neural synchrony work together to identify objects in a scene. Monkeys look for a particular object in a scene by searching items one by one while simultaneously sensing diagnostic features of objects peripheral to their direct gaze. Monkeys look for a particular object in a scene by searching items one by one while simultaneously sensing diagnostic features of objects peripheral to their direct gaze.},
  chapter = {Research Article},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {15845848},
  file = {/Users/xzfang/Zotero/storage/AQ5WLE82/Bichot et al. - 2005 - Parallel and Serial Neural Mechanisms for Visual S.pdf;/Users/xzfang/Zotero/storage/82LIPLS3/529.html}
}

@article{bichot_role_2019,
  title = {The Role of Prefrontal Cortex in the Control of Feature Attention in Area {{V4}}},
  author = {Bichot, Narcisse P. and Xu, Rui and Ghadooshahy, Azriel and Williams, Michael L. and Desimone, Robert},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {5727},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13761-7},
  abstract = {When searching for an object in a cluttered scene, we can use our memory of the target object features to guide our search, and the responses of neurons in multiple cortical visual areas are enhanced when their receptive field contains a stimulus sharing target object features. Here we tested the role of the ventral prearcuate region (VPA) of prefrontal cortex in the control of feature attention in cortical visual area V4. VPA was unilaterally inactivated in monkeys performing a free-viewing visual search for a target stimulus in an array of stimuli, impairing monkeys' ability to find the target in the array in the affected hemifield, but leaving intact their ability to make saccades to targets presented alone. Simultaneous recordings in V4 revealed that the effects of feature attention on V4 responses were eliminated or greatly reduced while leaving the effects of spatial attention on responses intact. Altogether, the results suggest that feedback from VPA modulates processing in visual cortex during attention to object features.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XTWNDCVD/Bichot et al. - 2019 - The role of prefrontal cortex in the control of fe.pdf;/Users/xzfang/Zotero/storage/YCRMT6I3/s41467-019-13761-7.html}
}

@article{bichot_source_2015,
  title = {A Source for Feature Based Attention in the Prefrontal Cortex},
  author = {Bichot, Narcisse P. and Heard, Matthew T. and DeGennaro, Ellen M. and Desimone, Robert},
  year = {2015},
  month = nov,
  journal = {Neuron},
  volume = {88},
  number = {4},
  pages = {832--844},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2015.10.001},
  abstract = {In cluttered scenes, we can use feature-based attention to quickly locate a target object. To understand how feature attention is used to find and select objects for action, we focused on the ventral pre-arcuate (VPA) region of prefrontal cortex. In a visual search task, VPA cells responded selectively to search cues, maintained their feature selectivity throughout the delay and subsequent saccades, and discriminated the search target in their receptive fields with a timecourse earlier than in FEF or IT cortex. Inactivation of VPA impaired the animals' ability to find targets, and simultaneous recordings in FEF revealed that the effects of feature attention were eliminated while leaving the effects of spatial attention in FEF intact. Altogether, the results suggest that VPA neurons compute the locations of objects with the features sought and send this information to FEF to guide eye movements to those relevant stimuli.},
  pmcid = {PMC4655197},
  pmid = {26526392},
  file = {/Users/xzfang/Zotero/storage/8CMR5DUB/Bichot et al. - 2015 - A source for feature based attention in the prefro.pdf}
}

@article{bicknell_now_2016,
  title = {Now or \ldots{} Later: {{Perceptual}} Data Are Not Immediately Forgotten during Language Processing},
  shorttitle = {Now or \ldots{} Later},
  author = {Bicknell, Klinton and Jaeger, T. Florian and Tanenhaus, Michael K.},
  year = {2016/ed},
  journal = {Behavioral and Brain Sciences},
  volume = {39},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X15000734},
  abstract = {Christiansen \& Chater (C\&C) propose that language comprehenders must immediately compress perceptual data by ``chunking'' them into higher-level categories. Effective language understanding, however, requires maintaining perceptual information long enough to integrate it with downstream cues. Indeed, recent results suggest comprehenders do this. Although cognitive systems are undoubtedly limited, frameworks that do not take into account the tasks that these systems evolved to solve risk missing important insights.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RWW64XRW/BA9A9EB84E2A944630F5F2F4077DB95A.html}
}

@article{biederman_size_,
  title = {Size {{Invariance}} in {{Visual Object Priming}}},
  author = {Biederman, Irving and Cooper, Eric E},
  pages = {13},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4RFV2Q65/Biederman and Cooper - Size Invariance in Visual Object Priming.pdf}
}

@article{bimbard_behavioral_2021,
  title = {Behavioral Origin of Sound-Evoked Activity in Visual Cortex},
  author = {Bimbard, C{\'e}lian and Sit, Timothy PH and Lebedeva, Anna and Harris, Kenneth D. and Carandini, Matteo},
  year = {2021},
  month = jul,
  journal = {bioRxiv},
  pages = {2021.07.01.450721},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.07.01.450721},
  abstract = {{$<$}p{$>$}Sensory cortices are increasingly thought to encode multisensory information. For instance, primary visual cortex (V1) appears to be influenced by sounds. Here we show that sound-evoked responses in mouse V1 are low-dimensional, similar across neurons and across brains, and can be explained by highly stereotyped uninstructed movements of eyes and body. Thus, neural activity previously interpreted as being sensory or multisensory may have a behavioral origin.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZBZD4RHA/Bimbard et al. - 2021 - Behavioral origin of sound-evoked activity in visu.pdf;/Users/xzfang/Zotero/storage/B6K55QVJ/2021.07.01.450721v1.html}
}

@article{binder_brainbased_2016,
  title = {Toward a Brain-Based Componential Semantic Representation},
  author = {Binder, Jeffrey R. and Conant, Lisa L. and Humphries, Colin J. and Fernandino, Leonardo and Simons, Stephen B. and Aguilar, Mario and Desai, Rutvik H.},
  year = {2016},
  month = may,
  journal = {Cognitive Neuropsychology},
  volume = {33},
  number = {3-4},
  pages = {130--174},
  issn = {0264-3294, 1464-0627},
  doi = {10.1080/02643294.2016.1147426},
  abstract = {Componential theories of lexical semantics assume that concepts can be represented by sets of features or attributes that are in some sense primitive or basic components of meaning. The binary features used in classical category and prototype theories are problematic in that these features are themselves complex concepts, leaving open the question of what constitutes a primitive feature. The present availability of brain imaging tools has enhanced interest in how concepts are represented in brains, and accumulating evidence supports the claim that these representations are at least partly ``embodied'' in the perception, action, and other modal neural systems through which concepts are experienced. In this study we explore the possibility of devising a componential model of semantic representation based entirely on such functional divisions in the human brain. We propose a basic set of approximately 65 experiential attributes based on neurobiological considerations, comprising sensory, motor, spatial, temporal, affective, social, and cognitive experiences. We provide normative data on the salience of each attribute for a large set of English nouns, verbs, and adjectives, and show how these attribute vectors distinguish a priori conceptual categories and capture semantic similarity. Robust quantitative differences between concrete object categories were observed across a large number of attribute dimensions. A within- versus between-category similarity metric showed much greater separation between categories than representations derived from distributional (latent semantic) analysis of text. Cluster analyses were used to explore the similarity structure in the data independent of a priori labels, revealing several novel category distinctions. We discuss how such a representation might deal with various longstanding problems in semantic theory, such as feature selection and weighting, representation of abstract concepts, effects of context on semantic retrieval, and conceptual combination. In contrast to componential models based on verbal features, the proposed representation systematically relates semantic content to largescale brain networks and biologically plausible accounts of concept acquisition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3H4YMN2G/Binder et al. - 2016 - Toward a brain-based componential semantic represe.pdf}
}

@article{blanco-elorrieta_adaptation_2021,
  title = {Adaptation to Mis-Pronounced Speech: Evidence for a Prefrontal-Cortex Repair Mechanism},
  shorttitle = {Adaptation to Mis-Pronounced Speech},
  author = {{Blanco-Elorrieta}, Esti and Gwilliams, Laura and Marantz, Alec and Pylkk{\"a}nen, Liina},
  year = {2021},
  month = dec,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {97},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-79640-0},
  abstract = {Abstract             Speech is a complex and ambiguous acoustic signal that varies significantly within and across speakers. Despite the processing challenge that such variability poses, humans adapt to systematic variations in pronunciation rapidly. The goal of this study is to uncover the neurobiological bases of the attunement process that enables such fluent comprehension. Twenty-four native English participants listened to words spoken by a ``canonical'' American speaker and two non-canonical speakers, and performed a word-picture matching task, while magnetoencephalography was recorded. Non-canonical speech was created by including systematic phonological substitutions within the word (e.g. [s]\,\textrightarrow\,[sh]). Activity in the auditory cortex (superior temporal gyrus) was greater in response to substituted phonemes, and, critically, this was not attenuated by exposure. By contrast, prefrontal regions showed an interaction between the presence of a substitution and the amount of exposure: activity decreased for canonical speech over time, whereas responses to non-canonical speech remained consistently elevated. Grainger causality analyses further revealed that prefrontal responses serve to modulate activity in auditory regions, suggesting the recruitment of top-down processing to decode non-canonical pronunciations. In sum, our results suggest that the behavioural deficit in processing mispronounced phonemes may be due to a disruption to the typical exchange of information between the prefrontal and auditory cortices as observed for canonical speech.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RHE2L6C6/Blanco-Elorrieta et al. - 2021 - Adaptation to mis-pronounced speech evidence for .pdf}
}

@article{blanco-elorrieta_common_2021,
  title = {A Common Selection Mechanism at Each Linguistic Level in Bilingual and Monolingual Language Production},
  author = {{Blanco-Elorrieta}, Esti and Caramazza, Alfonso},
  year = {2021},
  month = aug,
  journal = {Cognition},
  series = {Special {{Issue}} in {{Honour}} of {{Jacques Mehler}}, {{Cognition}}'s Founding Editor},
  volume = {213},
  pages = {104625},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104625},
  abstract = {The primary goal of research on the functional and neural architecture of bilingualism is to elucidate how bilingual individuals' language architecture is organized such that they can both speak in a single language without accidental insertions of the other, but also flexibly switch between their two languages if the context allows/demands them to. Here we review the principles under which any proposed architecture could operate, and present a framework where the selection mechanism for individual elements strictly operates on the basis of the highest level of activation and does not require suppressing representations in the non-target language. We specify the conjunction of parameters and factors that jointly determine these levels of activation and develop a theory of bilingual language organization that extends beyond the lexical level to other levels of representation (i.e., semantics, morphology, syntax and phonology). The proposed architecture assumes a common selection principle at each linguistic level to account for attested features of bilingual speech in, but crucially also out, of experimental settings.},
  langid = {english},
  keywords = {Bilingual communication,Bilingual language organization,Bilingual language production,Language selection,Lexical access,Selection-by-activation},
  file = {/Users/xzfang/Zotero/storage/ZGZF479D/S0010027721000445.html}
}

@techreport{blanco-elorrieta_neural_2019,
  type = {Preprint},
  title = {Neural Adaptation to Accented Speech: Prefrontal Cortex Aids Attunement in Auditory Cortices},
  shorttitle = {Neural Adaptation to Accented Speech},
  author = {{Blanco-Elorrieta}, Esti and Gwilliams, Laura and Marantz, Alec and Pylkk{\"a}nen, Liina},
  year = {2019},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/852616},
  abstract = {Speech is a complex and ambiguous acoustic signal that varies significantly within and across speakers. A prevalent and ubiquitous example of such variation is accented speech, to which humans adapt extremely rapidly. The goal of this study is to uncover the neurobiological bases of the attunement process that enables such fluent comprehension. Twenty-four native English participants listened to words spoken by an unaccented ``canonical'' American talker and two ``accented'' talkers, and performed a word-picture matching task, while magnetoencephalography (MEG) was recorded. Accented speech was created by including systematic phonological substitutions within the word (e.g. [s] \textrightarrow{} [sh]). Activity in the auditory cortex (superior temporal gyrus) was greater for accented speech, but, critically, this was not attenuated by exposure. By contrast, prefrontal regions showed an interaction between the presence of an accent and amount of exposure: while activity decreased for canonical speech over time, responses to accented speech remained consistently elevated. Grainger causality analyses further revealed that prefrontal responses serve to modulate activity in auditory regions, suggesting the recruitment of top-down processing to decode accented signal. In sum, our results show that accented speech does not elicit the same prefrontal reduction in amplitude over time that unaccented speech does, and points to a dynamic exchange of information between the prefrontal and auditory cortices in order to recalculate phonetic classification and subsequent identification of lexical items.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/S9J95JBP/Blanco-Elorrieta et al. - 2019 - Neural adaptation to accented speech prefrontal c.pdf}
}

@article{blank_syntactic_2016,
  title = {Syntactic Processing Is Distributed across the Language System},
  author = {Blank, Idan and Balewski, Zuzanna and Mahowald, Kyle and Fedorenko, Evelina},
  year = {2016},
  month = feb,
  journal = {NeuroImage},
  volume = {127},
  pages = {307--323},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2015.11.069},
  abstract = {Language comprehension recruits an extended set of regions in the human brain. Is syntactic processing localized to a particular region or regions within this system, or is it distributed across the entire ensemble of brain regions that support high-level linguistic processing? Evidence from aphasic patients is more consistent with the latter possibility: damage to many different language regions and to white-matter tracts connecting them has been shown to lead to similar syntactic comprehension deficits. However, brain imaging investigations of syntactic processing continue to focus on particular regions within the language system, often parts of Broca's area and regions in the posterior temporal cortex. We hypothesized that, whereas the entire language system is in fact sensitive to syntactic complexity, the effects in some regions may be difficult to detect because of the overall lower response to language stimuli. Using an individual-subjects approach to localizing the language system, shown in prior work to be more sensitive than traditional group analyses, we indeed find responses to syntactic complexity throughout this system, consistent with the findings from the neuropsychological patient literature. We speculate that such distributed nature of syntactic processing could perhaps imply that syntax is inseparable from other aspects of language comprehension (e.g., lexico-semantic processing), in line with current linguistic and psycholinguistic theories and evidence. Neuroimaging investigations of syntactic processing thus need to expand their scope to include the entire system of high-level language processing regions in order to fully understand how syntax is instantiated in the human brain.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/R6NSQKVF/Blank et al. - 2016 - Syntactic processing is distributed across the lan.pdf}
}

@article{blei_latent_,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M},
  pages = {30},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UNPEF4M8/Blei - Latent Dirichlet Allocation.pdf}
}

@article{bleichner_concealed_2017,
  title = {Concealed, {{Unobtrusive Ear-Centered EEG Acquisition}}: {{cEEGrids}} for {{Transparent EEG}}},
  shorttitle = {Concealed, {{Unobtrusive Ear-Centered EEG Acquisition}}},
  author = {Bleichner, Martin G. and Debener, Stefan},
  year = {2017},
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00163},
  abstract = {Electroencephalography (EEG) is an important clinical tool and frequently used to study the brain-behavior relationship in humans noninvasively. Traditionally, EEG signals are recorded by distributing electrodes on the scalp and keeping them in place with glue, rubber bands, or elastic caps. This setup provides good coverage of the head, but is impractical for EEG acquisition in natural daily-life situations. Here we propose the transparent EEG concept. Transparent EEG aims for motion tolerant, highly portable, unobtrusive and near invisible data acquisition with minimum disturbance of a user's daily activities. In recent years several ear-centered EEG solutions that are compatible with the transparent EEG concept have been presented. We discuss work showing that miniature electrodes placed in and around the human ear are a feasible solution, as they are sensitive to pick up electrical signals stemming from various brain and non-brain sources. We also describe the cEEGrid flex-printed sensor array, which enables unobtrusive multi-channel EEG acquisition from around the ear. In a number of validation studies we found that the cEEGrid enables the recording of meaningful continuous EEG, event-related potentials and neural oscillations. Here we explain the rationale underlying the cEEGrid ear-EEG solution, present possible use cases and identify and open issues that need to be solved on the way towards transparent EEG.},
  langid = {english},
  keywords = {Ear EEG,ear-centered EEG,mobile EEG,transparent EEG,Wearable EEG},
  file = {/Users/xzfang/Zotero/storage/TE8E92JH/Bleichner and Debener - 2017 - Concealed, Unobtrusive Ear-Centered EEG Acquisitio.pdf}
}

@article{blom_predictions_2020,
  title = {Predictions Drive Neural Representations of Visual Events Ahead of Incoming Sensory Information},
  author = {Blom, Tessel and Feuerriegel, Daniel and Johnson, Philippa and Bode, Stefan and Hogendoorn, Hinze},
  year = {2020},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {13},
  pages = {7510--7515},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1917777117},
  abstract = {The transmission of sensory information through the visual system takes time. As a result of these delays, the visual information available to the brain always lags behind the timing of events in the present moment. Compensating for these delays is crucial for functioning within dynamic environments, since interacting with a moving object (e.g., catching a ball) requires real-time localization of the object. One way the brain might achieve this is via prediction of anticipated events. Using time-resolved decoding of electroencephalographic (EEG) data, we demonstrate that the visual system represents the anticipated future position of a moving object, showing that predictive mechanisms activate the same neural representations as afferent sensory input. Importantly, this activation is evident before sensory input corresponding to the stimulus position is able to arrive. Finally, we demonstrate that, when predicted events do not eventuate, sensory information arrives too late to prevent the visual system from representing what was expected but never presented. Taken together, we demonstrate how the visual system can implement predictive mechanisms to preactivate sensory representations, and argue that this might allow it to compensate for its own temporal constraints, allowing us to interact with dynamic visual environments in real time.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/D5VF3C3G/Blom et al. - 2020 - Predictions drive neural representations of visual.pdf}
}

@article{blundon_sequential_2017,
  title = {Sequential Search Asymmetry: {{Behavioral}} and Psychophysiological Evidence from a Dual Oddball Task},
  shorttitle = {Sequential Search Asymmetry},
  author = {Blundon, Elizabeth G. and Rumak, Samuel P. and Ward, Lawrence M.},
  year = {2017},
  month = mar,
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0173237},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0173237},
  abstract = {We conducted five experiments in order to explore the generalizability of a new type of search asymmetry, which we have termed sequential search asymmetry, across sensory modalities, and to better understand its origin. In all five experiments rare oddballs occurred randomly within longer sequences of more frequent standards. Oddballs and standards all consisted of rapidly-presented runs of five pure tones (Experiments 1 and 5) or five colored annuli (Experiments 2 through 4) somewhat analogous to simultaneously-presented feature-present and feature-absent stimuli in typical visual search tasks. In easy tasks feature-present reaction times and P300 latencies were shorter than feature-absent ones, similar to findings in search tasks with simultaneously-presented stimuli. Moreover the P3a subcomponent of the P300 ERP was strongly apparent only in the feature-present condition. In more difficult tasks requiring focused attention, however, RT and P300 latency differences disappeared but the P300 amplitude difference was significant. Importantly in all five experiments d' for feature-present targets was larger than that for feature-absent targets. These results imply that sequential search asymmetry arises from discriminability differences between feature-present and feature-absent targets. Response time and P300 latency differences can be attributed to the use of different attention strategies in search for feature-present and feature-absent targets, indexed by the presence of a dominant P3a subcomponent in the feature-present target-evoked P300s that is lacking in the P300s to the feature-absent targets.},
  langid = {english},
  keywords = {Attention,Behavior,Electroencephalography,Electrophysiology,Event-related potentials,Reaction time,Scalp,Target detection},
  file = {/Users/xzfang/Zotero/storage/RRYB8NKV/Blundon et al. - 2017 - Sequential search asymmetry Behavioral and psycho.pdf;/Users/xzfang/Zotero/storage/GMVN3BE5/article.html}
}

@article{boersma_praat_2001,
  title = {Praat, a System for Doing Phonetics by Computer},
  author = {BOERSMA, P.},
  year = {2001},
  journal = {Glot. Int.},
  volume = {5},
  number = {9},
  pages = {341--345},
  file = {/Users/xzfang/Zotero/storage/LMYKBSQW/10026090047.html}
}

@article{bogaerts_there_2022,
  title = {Is There Such a Thing as a `Good Statistical Learner'?},
  author = {Bogaerts, Louisa and Siegelman, Noam and Christiansen, Morten H. and Frost, Ram},
  year = {2022},
  month = jan,
  journal = {Trends in Cognitive Sciences},
  volume = {26},
  number = {1},
  pages = {25--37},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2021.10.012},
  abstract = {A growing body of research investigates individual differences in the learning of statistical structure, tying them to variability in cognitive (dis)abilities. This approach views statistical learning (SL) as a general individual ability that underlies performance across a range of cognitive domains. But is there a general SL capacity that can sort individuals from `bad' to `good' statistical learners? Explicating the suppositions underlying this approach, we suggest that current evidence supporting it is meager. We outline an alternative perspective that considers the variability of statistical environments within different cognitive domains. Once we focus on learning that is tuned to the statistics of real-world sensory inputs, an alternative view of SL computations emerges with a radically different outlook for SL research.},
  langid = {english},
  keywords = {cognitive abilities,individual differences,statistical learning},
  file = {/Users/xzfang/Zotero/storage/AHPB92QR/S1364661321002825.html}
}

@article{bogers_automated_,
  title = {Automated and {{Partner-Specific Factors Influencing Lexical Entrainment}}},
  author = {Bogers, Rens and Bouwens, Tommy},
  pages = {7},
  abstract = {Both automated priming (Pickering \& Garrod, 2004) and partner-specific adaptation (Brennan \& Hanna, 2009) have been proposed to underlie lexical entrainment (the repetition of words across interlocutors). Since activation levels of infrequently used words are relatively low, the effect of automated priming is predicted to be weaker in L2- than in L1-conversations, leaving more room for deliberate partnerspecificity (Costa, Pickering, \& Sorace, 2008). We tested this prediction by means of a production experiment, in which we varied whether participants interacted in their L1 or L2, and whether they addressed the confederate who had introduced a certain reference or another addressee. We found that in their L2, participants repeated references more frequently when addressing the person who had introduced the reference. Yet we did not find this effect of partner-specificity in the L1 conditions. Therefore, our results support the proposed combination of the two accounts.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XJ4KEKRQ/Bogers and Bouwens - Automated and Partner-Specific Factors Influencing.pdf}
}

@misc{bohn_longitudinal_2021,
  title = {A {{Longitudinal Study}} of {{Great Ape Cognition}}: {{Stability}}, {{Reliability}} and the {{Influence}} of {{Individual Characteristics}}},
  shorttitle = {A {{Longitudinal Study}} of {{Great Ape Cognition}}},
  author = {Bohn, Manuel and Eckert, Johanna and Hanus, Daniel and Haun, Daniel B. M.},
  year = {2021},
  month = apr,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/pdt5w},
  abstract = {Primate cognition research allows us to reconstruct the evolution of human cognition. However, temporal and contextual factors that induce variation in cognitive studies with great apes are poorly understood. Here we report on a longitudinal study where we repeatedly tested a comparatively large sample of great apes (N = 40) with the same set of cognitive measures. We investigated the stability of group-level results, the reliability of individual differences, and the relation between cognitive performance and individual-level characteristics. We found results to be relatively stable on a group level. Some, but not all, tasks showed acceptable levels of reliability. Cognitive performance across tasks was not systematically related to any particular individual-level predictor. This study highlights the importance of methodological considerations \textendash{} especially when studying individual differences \textendash{} on the route to building a more robust science of primate cognitive evolution.},
  keywords = {Animal Learning and Behavior,Causal Reasoning,Cognitive Psychology,Individual Differences,Inference,Meta-science,Primate Cognition,Reasoning,Reliability,Social and Behavioral Sciences,Social Cognition,Stability},
  file = {/Users/xzfang/Zotero/storage/R6RMR3QW/Bohn et al. - 2021 - A Longitudinal Study of Great Ape Cognition Stabi.pdf}
}

@article{boland_zoom_2021,
  title = {Zoom Disrupts the Rhythm of Conversation},
  author = {Boland, Julie E. and Fonseca, Pedro and Mermelstein, Ilana and Williamson, Myles},
  year = {2021},
  journal = {Journal of Experimental Psychology: General},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/xge0001150},
  abstract = {Small, variable transmission delays over Zoom disrupt the typical rhythm of conversation, leading to delays in turn initiation. This study compared local and remote (Zoom) turn transition times using both a tightly controlled yes/no Question and Answer (Q\&A) paradigm (Corps et al., 2018) and unscripted conversation. In the Q\&A paradigm (Experiment 1), participants responded yes/no as quickly as possible to prerecorded questions. Half of the questions were played over Zoom and half were played locally from their own computer. Local responses had an average latency of 297 ms, whereas remote responses averaged 976 ms. These large increases in transition times over Zoom are far greater than the estimated 30\textendash 70 ms of audio transmission delay, suggesting disruption of automated mechanisms that normally guide the timing of turn initiation in conversation. In face\textendash to\textendash face conversations (Experiment 2), turn transition times averaged 135 ms, but transition times for the same dyads over Zoom averaged 487 ms. We consider the possibility that electronic transmission delays disrupt neural oscillators that normally synchronize on syllable rate, at around, 150\textendash 300 ms per cycle (Wilson \& Wilson, 2005), and enable interlocutors to effortlessly and precisely time the initiation of their turns. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Computers,Conversation,Responses,Rhythm,Teleconferencing},
  file = {/Users/xzfang/Zotero/storage/8E6VTULL/Boland et al. - 2021 - Zoom disrupts the rhythm of conversation.pdf;/Users/xzfang/Zotero/storage/TB9C4BBK/2022-02247-001.html}
}

@article{bones_congenital_2017,
  title = {Congenital Amusics Use a Secondary Pitch Mechanism to Identify Lexical Tones},
  author = {Bones, Oliver and Wong, Patrick C. M.},
  year = {2017},
  month = sep,
  journal = {Neuropsychologia},
  volume = {104},
  pages = {48--53},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2017.08.004},
  abstract = {Amusia is a pitch perception disorder associated with deficits in processing and production of both musical and lexical tones, which previous reports have suggested may be constrained to fine-grained pitch judgements. In the present study speakers of tone-languages, in which lexical tones are used to convey meaning, identified words present in chimera stimuli containing conflicting pitch-cues in the temporal fine-structure and temporal envelope, and which therefore conveyed two distinct utterances. Amusics were found to be more likely than controls to judge the word according to the envelope pitch-cues. This demonstrates that amusia is not associated with fine-grained pitch judgements alone, and is consistent with there being two distinct pitch mechanisms and with amusics having an atypical reliance on a secondary mechanism based upon envelope cues.},
  langid = {english},
  keywords = {Amusia,Pitch mechanism,Pitch perception,Temporal envelope,Temporal fine-structure,Tone-language},
  file = {/Users/xzfang/Zotero/storage/FGCUTW3U/Bones and Wong - 2017 - Congenital amusics use a secondary pitch mechanism.pdf;/Users/xzfang/Zotero/storage/JPHLPUWE/S0028393217302932.html}
}

@article{bonner_coding_2017,
  title = {Coding of Navigational Affordances in the Human Visual System},
  author = {Bonner, Michael F. and Epstein, Russell A.},
  year = {2017},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {18},
  pages = {4793--4798},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1618228114},
  abstract = {A central component of spatial navigation is determining where one can and cannot go in the immediate environment. We used fMRI to test the hypothesis that the human visual system solves this problem by automatically identifying the navigational affordances of the local scene. Multivoxel pattern analyses showed that a scene-selective region of dorsal occipitoparietal cortex, known as the occipital place area, represents pathways for movement in scenes in a manner that is tolerant to variability in other visual features. These effects were found in two experiments: One using tightly controlled artificial environments as stimuli, the other using a diverse set of complex, natural scenes. A reconstruction analysis demonstrated that the population codes of the occipital place area could be used to predict the affordances of novel scenes. Taken together, these results reveal a previously unknown mechanism for perceiving the affordance structure of navigable space.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SM9QLXCZ/Bonner and Epstein - 2017 - Coding of navigational affordances in the human vi.pdf}
}

@article{bonte_dynamic_2009,
  title = {Dynamic and {{Task-Dependent Encoding}} of {{Speech}} and {{Voice}} by {{Phase Reorganization}} of {{Cortical Oscillations}}},
  author = {Bonte, Milene and Valente, Giancarlo and Formisano, Elia},
  year = {2009},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {29},
  number = {6},
  pages = {1699--1706},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3694-08.2009},
  abstract = {Speech and vocal sounds are at the core of human communication. Cortical processing of these sounds critically depends on behavioral demands. However, the neurocomputational mechanisms enabling this adaptive processing remain elusive. Here we examine the task-dependent reorganization of electroencephalographic responses to natural speech sounds (vowels /a/, /i/, /u/) spoken by three speakers (two female, one male) while listeners perform a one-back task on either vowel or speaker identity. We show that dynamic changes of sound-evoked responses and phase patterns of cortical oscillations in the alpha band (8\textendash 12 Hz) closely reflect the abstraction and analysis of the sounds along the task-relevant dimension. Vowel categorization leads to a significant temporal realignment of responses to the same vowel, e.g., /a/, independent of who pronounced this vowel, whereas speaker categorization leads to a significant temporal realignment of responses to the same speaker, e.g., speaker 1, independent of which vowel she/he pronounced. This transient and goal-dependent realignment of neuronal responses to physically different external events provides a robust cortical coding mechanism for forming and processing abstract representations of auditory (speech) input.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2009 Society for Neuroscience 0270-6474/09/291699-08\$15.00/0},
  langid = {english},
  pmid = {19211877},
  keywords = {alpha,auditory cortex,EEG,language,speech,synchrony},
  file = {/Users/xzfang/Zotero/storage/VCETSJW5/Bonte et al. - 2009 - Dynamic and Task-Dependent Encoding of Speech and .pdf;/Users/xzfang/Zotero/storage/F7YUEAFD/1699.html}
}

@article{bonte_taskdependent_2014,
  title = {Task-{{Dependent Decoding}} of {{Speaker}} and {{Vowel Identity}} from {{Auditory Cortical Response Patterns}}},
  author = {Bonte, Milene and Hausfeld, Lars and Scharke, Wolfgang and Valente, Giancarlo and Formisano, Elia},
  year = {2014},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {34},
  number = {13},
  pages = {4548--4557},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4339-13.2014},
  abstract = {Selective attention to relevant sound properties is essential for everyday listening situations. It enables the formation of different perceptual representations of the same acoustic input and is at the basis of flexible and goal-dependent behavior. Here, we investigated the role of the human auditory cortex in forming behavior-dependent representations of sounds. We used single-trial fMRI and analyzed cortical responses collected while subjects listened to the same speech sounds (vowels /a/, /i/, and /u/) spoken by different speakers (boy, girl, male) and performed a delayed-match-to-sample task on either speech sound or speaker identity. Univariate analyses showed a task-specific activation increase in the right superior temporal gyrus/sulcus (STG/STS) during speaker categorization and in the right posterior temporal cortex during vowel categorization. Beyond regional differences in activation levels, multivariate classification of single trial responses demonstrated that the success with which single speakers and vowels can be decoded from auditory cortical activation patterns depends on task demands and subject's behavioral performance. Speaker/vowel classification relied on distinct but overlapping regions across the (right) mid-anterior STG/STS (speakers) and bilateral mid-posterior STG/STS (vowels), as well as the superior temporal plane including Heschl's gyrus/sulcus. The task dependency of speaker/vowel classification demonstrates that the informative fMRI response patterns reflect the top-down enhancement of behaviorally relevant sound representations. Furthermore, our findings suggest that successful selection, processing, and retention of task-relevant sound properties relies on the joint encoding of information across early and higher-order regions of the auditory cortex.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2014 the authors 0270-6474/14/344548-10\$15.00/0},
  langid = {english},
  pmid = {24672000},
  keywords = {auditory cortex,fMRI decoding,speech,voice,vowels},
  file = {/Users/xzfang/Zotero/storage/A2SKS45I/Bonte et al. - 2014 - Task-Dependent Decoding of Speaker and Vowel Ident.pdf;/Users/xzfang/Zotero/storage/DJ8LNT2X/4548.html}
}

@article{borghi_stable_2015,
  title = {Stable and Variable Affordances Are Both Automatic and Flexible},
  author = {Borghi, Anna M. and Riggio, Lucia},
  year = {2015},
  journal = {Frontiers in Human Neuroscience},
  volume = {9},
  pages = {351},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2015.00351},
  abstract = {The mere observation of pictures or words referring to manipulable objects is sufficient to evoke their affordances since objects and their nouns elicit components of appropriate motor programs associated with object interaction. While nobody doubts that objects actually evoke motor information, the degree of automaticity of this activation has been recently disputed. Recent evidence has indeed revealed that affordances activation is flexibly modulated by the task and by the physical and social context. It is therefore crucial to understand whether these results challenge previous evidence showing that motor information is activated independently from the task. The context and the task can indeed act as an early or late filter. We will review recent data consistent with the notion that objects automatically elicit multiple affordances and that top-down processes select among them probably inhibiting motor information that is not consistent with behavior goals. We will therefore argue that automaticity and flexibility of affordances are not in conflict. We will also discuss how language can incorporate affordances showing similarities, but also differences, between the motor information elicited by vision and language. Finally we will show how the distinction between stable and variable affordances can accommodate all these effects.},
  file = {/Users/xzfang/Zotero/storage/V7YM9I22/Borghi and Riggio - 2015 - Stable and variable affordances are both automatic.pdf}
}

@article{boroditsky_metaphoric_2000,
  title = {Metaphoric Structuring: Understanding Time through Spatial Metaphors},
  shorttitle = {Metaphoric Structuring},
  author = {Boroditsky, Lera},
  year = {2000},
  month = apr,
  journal = {Cognition},
  volume = {75},
  number = {1},
  pages = {1--28},
  issn = {00100277},
  doi = {10.1016/S0010-0277(99)00073-6},
  abstract = {The present paper evaluates the claim that abstract conceptual domains are structured through metaphorical mappings from domains grounded directly in experience. In particular, the paper asks whether the abstract domain of time gets its relational structure from the more concrete domain of space. Relational similarities between space and time are outlined along with several explanations of how these similarities may have arisen. Three experiments designed to distinguish between these explanations are described. The results indicate that (1) the domains of space and time do share conceptual structure, (2) spatial relational information is just as useful for thinking about time as temporal information, and (3) with frequent use, mappings between space and time come to be stored in the domain of time and so thinking about time does not necessarily require access to spatial schemas. These \textregistered ndings provide some of the \textregistered rst empirical evidence for Metaphoric Structuring. It appears that abstract domains such as time are indeed shaped by metaphorical mappings from more concrete and experiential domains such as space. q 2000 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IMXTENLZ/Boroditsky - 2000 - Metaphoric structuring understanding time through.pdf}
}

@article{boroditsky_remembrances_2010,
  title = {Remembrances of {{Times East}}: {{Absolute Spatial Representations}} of {{Time}} in an {{Australian Aboriginal Community}}},
  shorttitle = {Remembrances of {{Times East}}},
  author = {Boroditsky, Lera and Gaby, Alice},
  year = {2010},
  month = nov,
  journal = {Psychological Science},
  volume = {21},
  number = {11},
  pages = {1635--1639},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797610386621},
  abstract = {How do people think about time? Here we describe representations of time in Pormpuraaw, a remote Australian Aboriginal community. Pormpuraawans' representations of time differ strikingly from all others documented to date. Previously, people have been shown to represent time spatially from left to right or right to left, or from front to back or back to front. All of these representations are with respect to the body. Pormpuraawans instead arrange time according to cardinal directions: east to west. That is, time flows from left to right when one is facing south, from right to left when one is facing north, toward the body when one is facing east, and away from the body when one is facing west. These findings reveal a qualitatively different set of representations of time, with time organized in a coordinate frame that is independent from others reported previously. The results demonstrate that conceptions of even such fundamental domains as time can differ dramatically across cultures.},
  langid = {english},
  keywords = {Australian Aboriginal,cross-cultural differences,frames of reference,space,space-time mapping,time},
  file = {/Users/xzfang/Zotero/storage/KCZ6DUKH/Boroditsky and Gaby - 2010 - Remembrances of Times East Absolute Spatial Repre.pdf}
}

@article{borovsky_once_2012,
  title = {Once Is {{Enough}}: {{N400 Indexes Semantic Integration}} of {{Novel Word Meanings}} from a {{Single Exposure}} in {{Context}}},
  shorttitle = {Once Is {{Enough}}},
  author = {Borovsky, Arielle and Elman, Jeffrey L. and Kutas, Marta},
  year = {2012},
  journal = {Language learning and development : the official journal of the Society for Language Development},
  volume = {8},
  number = {3},
  pages = {278--302},
  issn = {1547-5441},
  doi = {10.1080/15475441.2011.614893},
  abstract = {We investigated the impact of contextual constraint on the integration of novel word meanings into semantic memory. Adults read strongly or weakly constraining sentences ending in known or unknown (novel) words as scalp-recorded electrical brain activity was recorded. Word knowledge was assessed via a lexical decision task in which recently seen known and unknown word sentence endings served as primes for semantically related, unrelated, and synonym/identical target words. As expected, N400 amplitudes to target words preceded by known word primes were reduced by prime-target relatedness. Critically, N400 amplitudes to targets preceded by novel primes also varied with prime-target relatedness, but only when they had initially appeared in highly constraining sentences. This demonstrates for the first time that fast-mapped word representations can develop strong associations with semantically related word meanings and reveals a rapid neural process that can integrate information about word meanings into the mental lexicon of young adults.},
  pmcid = {PMC3484686},
  pmid = {23125559},
  file = {/Users/xzfang/Zotero/storage/SU322F2M/Borovsky et al. - 2012 - Once is Enough N400 Indexes Semantic Integration .pdf}
}

@article{borst_cost_2015,
  title = {The Cost of Blocking the Mirror Generalization Process in Reading: Evidence for the Role of Inhibitory Control in Discriminating Letters with Lateral Mirror-Image Counterparts},
  shorttitle = {The Cost of Blocking the Mirror Generalization Process in Reading},
  author = {Borst, Gr{\'e}goire and Ahr, Emmanuel and Roell, Margot and Houd{\'e}, Olivier},
  year = {2015},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {22},
  number = {1},
  pages = {228--234},
  issn = {1531-5320},
  doi = {10.3758/s13423-014-0663-9},
  abstract = {Mirror generalization is detrimental for identifying letters with lateral mirror-image counterparts (`b/d'). In the present study, we investigated whether the discrimination of this type of letters in expert readers might be rooted in the ability to inhibit the mirror-generalization process. In our negative priming paradigm, participants judged whether two letters were identical on the prime and two animals (or buildings) were identical on the probe. In Experiment 1, participants required more time when determining that two animals (but not two buildings) were mirror images of each other when preceded by letters with mirror-image counterparts than without mirror-image counterparts (`a/h'). In Experiment 2, we replicated the results with different letters without mirror-image counterparts and with the type of probe stimuli (animal or building) manipulated as a within-subject factors. Our results suggest that expert readers never completely ``unlearn'' the mirror-generalization process and still need to inhibit this heuristic to overcome mirror errors.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/347RN2RT/Borst et al. - 2015 - The cost of blocking the mirror generalization pro.pdf}
}

@article{borst_information_1999,
  title = {Information Theory and Neural Coding},
  author = {Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {1999},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {2},
  number = {11},
  pages = {947--957},
  issn = {1546-1726},
  doi = {10.1038/14731},
  abstract = {Information theory quantifies how much information a neural response carries about the stimulus. This can be compared to the information transferred in particular models of the stimulus\textendash response function and to maximum possible information transfer. Such comparisons are crucial because they validate assumptions present in any neurophysiological analysis. Here we review information-theory basics before demonstrating its use in neural coding. We show how to use information theory to validate simple stimulus\textendash response models of neural coding of dynamic stimuli. Because these models require specification of spike timing precision, they can reveal which time scales contain information in neural coding. This approach shows that dynamic stimuli can be encoded efficiently by single neurons and that each spike contributes to information transmission. We argue, however, that the data obtained so far do not suggest a temporal code, in which the placement of spikes relative to each other yields additional information.},
  copyright = {1999 Nature America Inc.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4PHFDQ9X/Borst and Theunissen - 1999 - Information theory and neural coding.pdf;/Users/xzfang/Zotero/storage/58T5QKXI/nn1199_947.html}
}

@article{bosch_reinstatement_2014,
  title = {Reinstatement of {{Associative Memories}} in {{Early Visual Cortex Is Signaled}} by the {{Hippocampus}}},
  author = {Bosch, Sander E. and Jehee, Janneke F. M. and Fern{\'a}ndez, Guill{\'e}n and Doeller, Christian F.},
  year = {2014},
  month = may,
  journal = {Journal of Neuroscience},
  volume = {34},
  number = {22},
  pages = {7493--7500},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0805-14.2014},
  abstract = {The cortical reinstatement hypothesis of memory retrieval posits that content-specific cortical activity at encoding is reinstated at retrieval. Evidence for cortical reinstatement was found in higher-order sensory regions, reflecting reactivation of complex object-based information. However, it remains unclear whether the same detailed sensory, feature-based information perceived during encoding is subsequently reinstated in early sensory cortex and what the role of the hippocampus is in this process. In this study, we used a combination of visual psychophysics, functional neuroimaging, multivoxel pattern analysis, and a well controlled cued recall paradigm to address this issue. We found that the visual information human participants were retrieving could be predicted by the activation patterns in early visual cortex. Importantly, this reinstatement resembled the neural pattern elicited when participants viewed the visual stimuli passively, indicating shared representations between stimulus-driven activity and memory. Furthermore, hippocampal activity covaried with the strength of stimulus-specific cortical reinstatement on a trial-by-trial level during cued recall. These findings provide evidence for reinstatement of unique associative memories in early visual cortex and suggest that the hippocampus modulates the mnemonic strength of this reinstatement.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2014 the authors 0270-6474/14/347493-08\$15.00/0},
  langid = {english},
  pmid = {24872554},
  keywords = {cortical reinstatement,hippocampus,multivariate analysis,visual cortex},
  file = {/Users/xzfang/Zotero/storage/JKA5Z2S9/Bosch et al. - 2014 - Reinstatement of Associative Memories in Early Vis.pdf;/Users/xzfang/Zotero/storage/7SYV9YG3/7493.html}
}

@article{bosker_counting_2019,
  title = {Counting `Uhm's: {{How}} Tracking the Distribution of Native and Non-Native Disfluencies Influences Online Language Comprehension},
  shorttitle = {Counting `Uhm's},
  author = {Bosker, Hans Rutger and {van Os}, Marjolein and Does, Rik and {van Bergen}, Geertje},
  year = {2019},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {106},
  pages = {189--202},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2019.02.006},
  abstract = {Disfluencies, like uh, have been shown to help listeners anticipate reference to low-frequency words. The associative account of this `disfluency bias' proposes that listeners learn to associate disfluency with low-frequency referents based on prior exposure to non-arbitrary disfluency distributions (i.e., greater probability of low-frequency words after disfluencies). However, there is limited evidence for listeners actually tracking disfluency distributions online. The present experiments are the first to show that adult listeners, exposed to a typical or more atypical disfluency distribution (i.e., hearing a talker unexpectedly say uh before high-frequency words), flexibly adjust their predictive strategies to the disfluency distribution at hand (e.g., learn to predict high-frequency referents after disfluency). However, when listeners were presented with the same atypical disfluency distribution but produced by a non-native speaker, no adjustment was observed. This suggests pragmatic inferences can modulate distributional learning, revealing the flexibility of, and constraints on, distributional learning in incremental language comprehension.},
  langid = {english},
  keywords = {Disfluencies,Distributional learning,Eye-tracking,Non-native speech,Pragmatic inferences,Prediction},
  file = {/Users/xzfang/Zotero/storage/6TAZSU94/Bosker et al. - 2019 - Counting â€˜uhmâ€™s How tracking the distribution of .pdf;/Users/xzfang/Zotero/storage/QCYC7QPM/S0749596X19300208.html}
}

@article{bosker_temporal_2020,
  title = {Temporal Contrast Effects in Human Speech Perception Are Immune to Selective Attention},
  author = {Bosker, Hans Rutger and Sjerps, Matthias J. and Reinisch, Eva},
  year = {2020},
  month = mar,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {5607},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-62613-8},
  abstract = {Two fundamental properties of perception are selective attention and perceptual contrast, but how these two processes interact remains unknown. Does an attended stimulus history exert a larger contrastive influence on the perception of a following target than unattended stimuli? Dutch listeners categorized target sounds with a reduced prefix ``ge-'' marking tense (e.g., ambiguous between gegaan-gaan ``gone-go''). In `single talker' Experiments 1\textendash 2, participants perceived the reduced syllable (reporting gegaan) when the target was heard after a fast sentence, but not after a slow sentence (reporting gaan). In `selective attention' Experiments 3\textendash 5, participants listened to two simultaneous sentences from two different talkers, followed by the same target sounds, with instructions to attend only one of the two talkers. Critically, the speech rates of attended and unattended talkers were found to equally influence target perception \textendash{} even when participants could watch the attended talker speak. In fact, participants' target perception in `selective attention' Experiments 3\textendash 5 did not differ from participants who were explicitly instructed to divide their attention equally across the two talkers (Experiment 6). This suggests that contrast effects of speech rate are immune to selective attention, largely operating prior to attentional stream segregation in the auditory processing hierarchy.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PDTZQ94A/Bosker et al. - 2020 - Temporal contrast effects in human speech percepti.pdf;/Users/xzfang/Zotero/storage/WN94VYP3/s41598-020-62613-8.html}
}

@article{bowers_spoken_2016,
  title = {Spoken Word Identification Involves Accessing Position Invariant Phoneme Representations},
  author = {Bowers, Jeffrey S. and Kazanina, Nina and Andermane, Nora},
  year = {2016},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {87},
  pages = {71--83},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.11.002},
  abstract = {In two adaptation experiments we investigated the role of phonemes in speech perception. Participants repeatedly categorized an ambiguous test word that started with a blended /f/-/s/ fricative (?ail can be perceived as /fail/ or /sail/) or a blended /d/-/b/ stop (?ump can be perceived as /bump/ or /dump/) after exposure to a set of adaptor words. The adaptors all included unambiguous /f/ or /s/ fricatives, or alternatively, /d/ or /b/ stops. In Experiment 1 we manipulated the position of the adaptor phonemes so that they occurred at the start of the word (e.g., farm), at the start of the second syllable (e.g., tofu), or the end of the word (e.g., leaf). We found that adaptation effects occurred across positions: Participants were less likely to categorize the ambiguous test stimulus as if it contained the adapted phoneme. For example, after exposure to the adaptors leaf, golf... etc., participants were more likely to categorize the ambiguous test word ?ail as `sail'. In Experiment 2 we also varied the voice of the speaker: Words with unambiguous final phoneme adaptors were spoken by a female while the ambiguous initial test phonemes were spoken by a male. Again robust adaptation effects occurred. Critically, in both experiments, similar adaptation effects were obtained for the fricatives and stops despite the fact that the acoustics of stops vary more as a function of position. We take these findings to support the claim that position independent phonemes play a role in spoken word identification.},
  langid = {english},
  keywords = {Lexical access code,Phoneme,Position specificity,Spoken word identification},
  file = {/Users/xzfang/Zotero/storage/QTBN2QJH/Bowers et al. - 2016 - Spoken word identification involves accessing posi.pdf}
}

@article{bradlow_perceptual_2008,
  title = {Perceptual {{Adaptation}} to {{Non-Native Speech}}},
  author = {Bradlow, Ann R and Bent, Tessa},
  year = {2008},
  pages = {22},
  abstract = {This study investigated talker-dependent and talker-independent perceptual adaptation to foreignaccent English. Experiment 1 investigated talker-dependent adaptation by comparing native English listeners' recognition accuracy for Chinese-accented English across single and multiple talker presentation conditions. Results showed that the native listeners adapted to the foreign-accented speech over the course of the single talker presentation condition with some variation in the rate and extent of this adaptation depending on the baseline sentence intelligibility of the foreign-accented talker. Experiment 2 investigated talker-independent perceptual adaptation to Chinese-accented English by exposing native English listeners to Chinese-accented English and then testing their perception of English produced by a novel Chinese-accented talker. Results showed that, if exposed to multiple talkers of Chinese-accented English during training, native English listeners could achieve talker-independent adaptation to Chinese-accented English. Taken together, these findings provide evidence for highly flexible speech perception processes that can adapt to speech that deviates substantially from the pronunciation norms in the native talker community along multiple acousticphonetic dimensions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7ED8F4P4/Bradlow and Bent - 2009 - Perceptual Adaptation to Non-Native Speech.pdf}
}

@article{brady_role_2019,
  title = {The {{Role}} of {{Meaning}} in {{Visual Memory}}: {{Face-Selective Brain Activity Predicts Memory}} for {{Ambiguous Face Stimuli}}},
  shorttitle = {The {{Role}} of {{Meaning}} in {{Visual Memory}}},
  author = {Brady, Timothy F. and Alvarez, George A. and St{\"o}rmer, Viola S.},
  year = {2019},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {6},
  pages = {1100--1108},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1693-18.2018},
  abstract = {How people process images is known to affect memory for those images, but these effects have typically been studied using explicit task instructions to vary encoding. Here, we investigate the effects of intrinsic variation in processing on subsequent memory, testing whether recognizing an ambiguous stimulus as meaningful (as a face vs as shape blobs) predicts subsequent visual memory even when matching the perceptual features and the encoding strategy between subsequently remembered and subsequently forgotten items. We show in adult humans of either sex that single trial EEG activity can predict whether participants will subsequently remember an ambiguous Mooney face image (e.g., an image that will sometimes be seen as a face and sometimes not be seen as a face). In addition, we show that a classifier trained only to discriminate between whether participants perceive a face versus non-face can generalize to predict whether an ambiguous image is subsequently remembered. Furthermore, when we examine the N170, an event-related potential index of face processing, we find that images that elicit larger N170s are more likely to be remembered than those that elicit smaller N170s, even when the exact same image elicited larger or smaller N170s across participants. Thus, images processed as meaningful, in this case as a face, during encoding are better remembered than identical images that are not processed as a face. This provides strong evidence that understanding the meaning of a stimulus during encoding plays a critical role in visual memory. SIGNIFICANCE STATEMENT Is visual memory inherently visual or does meaning and other conceptual information necessarily play a role even in memory for detailed visual information? Here we show that it is easier to remember an image when it is processed in a meaningful way, as indexed by the amount of category-specific brain activity it elicits. In particular, we use single-trial EEG activity to predict whether an image will be subsequently remembered, and show that the main driver of this prediction ability is whether or not an image is seen as meaningful or non-meaningful. This shows that the extent to which an image is processed as meaningful can be used to predict subsequent memory even when controlling for perceptual factors and encoding strategies that typically differ across images.},
  copyright = {Copyright \textcopyright{} 2019 the authors 0270-6474/19/391100-09\$15.00/0},
  langid = {english},
  pmid = {30541914},
  keywords = {EEG,face memory,memory,N170,visual memory},
  file = {/Users/xzfang/Zotero/storage/GAW2SES5/Brady et al. - 2019 - The Role of Meaning in Visual Memory Face-Selecti.pdf;/Users/xzfang/Zotero/storage/VGAVWZTL/1100.html}
}

@techreport{braga_situating_2019,
  type = {Preprint},
  title = {Situating the {{Left-Lateralized Language Network}} in the {{Broader Organization}} of {{Multiple Specialized Large-Scale Distributed Networks}}},
  author = {Braga, Rodrigo M. and DiNicola, Lauren M. and Buckner, Randy L.},
  year = {2019},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/2019.12.11.873174},
  abstract = {Using procedures optimized to explore network organization within the individual, the topography of a candidate language network was characterized and situated within the broader context of adjacent networks. The candidate network was first identified using functional connectivity and replicated across individuals, datasets, acquisition tasks, and analytic methods. In addition to classical language regions near to perisylvian cortex and temporal pole, additional regions were observed in dorsal posterior cingulate, midcingulate, anterior superior frontal and inferior temporal cortex. The candidate network was selectively activated when processing meaningful (as contrast to non-word) sentences, while spatially adjacent networks showed minimal or even decreased activity. Examined in relation to adjacent networks, the topography of the language network was found to parallel the motif of other association networks including the transmodal association networks linked to theory of mind and episodic remembering (often collectively called the default network). The several networks contained juxtaposed regions in multiple association zones. Outside of these juxtaposed higher-order networks, we further noted a distinct frontotemporal network situated between language regions and a frontal orofacial motor region and a temporal auditory region. A possibility is that these functionally-related sensorimotor regions might anchor specialization of neighboring association regions that develop into the language network. What is most striking is that the canonical language network appears to be just one of multiple similarly organized, differentially specialized distributed networks that populate the evolutionarily expanded zones of human association cortex.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BPJ2TGBS/Braga et al. - 2019 - Situating the Left-Lateralized Language Network in.pdf}
}

@article{branigan_syntactic_2007,
  title = {Syntactic Alignment and Participant Role in Dialogue},
  author = {Branigan, Holly P. and Pickering, Martin J. and McLean, Janet F. and Cleland, Alexandra A.},
  year = {2007},
  month = aug,
  journal = {Cognition},
  volume = {104},
  number = {2},
  pages = {163--197},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2006.05.006},
  abstract = {We report three experiments that investigated whether the linguistic behavior of participants in a dialogue is affected by their role within that interaction. All experiments were concerned with the way in which speakers choose between syntactic forms with very similar meanings. Theories of dialogue assume that speakers address their contributions directly to their addressees, but also indirectly to side participants. In Experiments 1 and 2, speakers produced picture descriptions that had the same syntactic structure as a previous speaker's descriptions which had been addressed to a third person. This indicated that syntactic alignment is not limited to speaker-addressee dyads. However, the prior participant role of the current speaker affected alignment: prior addressees aligned more than prior side-participants. In contrast, Experiments 2 and 3 demonstrated that alignment was unaffected by the prior participant role of the current addressee. We interpret these findings in terms of depth of processing during encoding.},
  langid = {english},
  keywords = {Alignment,Dialogue,Participant role,Syntactic priming,Syntax}
}

@article{brattico_musical_2006,
  title = {Musical Scale Properties Are Automatically Processed in the Human Auditory Cortex},
  author = {Brattico, Elvira and Tervaniemi, Mari and N{\"a}{\"a}t{\"a}nen, Risto and Peretz, Isabelle},
  year = {2006},
  month = oct,
  journal = {Brain Research},
  volume = {1117},
  number = {1},
  pages = {162--174},
  issn = {00068993},
  doi = {10.1016/j.brainres.2006.08.023},
  abstract = {While listening to music, we immediately detect `wrong' tones that do not match our expectations based on the prior context. This study aimed to determine whether such expectations can occur preattentively, as indexed by event-related potentials (ERPs), and whether these are modulated by attentional processes. To this end, we recorded ERPs in nonmusicians while they were presented with unfamiliar melodies, containing either a pitch deviating from the equal-tempered chromatic scale (out-of-tune) or a pitch deviating from the diatonic scale (out-of-key). ERPs were recorded in a passive experiment in which subjects were distracted from the sounds and in an active experiment in which they were judging how incongruous each melody was. In both the experiments, pitch incongruities elicited an early frontal negativity that was not modulated by attentional focus. This early negativity, closely corresponding to the mismatch negativity (MMN) of the ERPs, was mainly originated in the auditory cortex and occurred in response to both pitch violations but with larger amplitude for the more salient out-of-tune pitch than the less salient out-of-key pitch. Attentional processes leading to the conscious access of musical scale information were indexed by the late parietal positivity (resembling the P600 of the ERPs) elicited in response to both incongruous pitches in the active experiment only. Our results indicate that the relational properties of the musical scale are quickly and automatically extracted by the auditory cortex even before the intervention of focused attention.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WRMGTYQH/Brattico et al. - 2006 - Musical scale properties are automatically process.pdf}
}

@article{bravo_role_1992,
  title = {The Role of Attention in Different Visual-Search Tasks},
  author = {Bravo, Mary J. and Nakayama, Ken},
  year = {1992},
  month = sep,
  journal = {Perception \& Psychophysics},
  volume = {51},
  number = {5},
  pages = {465--472},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03211642},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DXPP284F/Bravo and Nakayama - 1992 - The role of attention in different visual-search t.pdf}
}

@article{brayanov_bayesian_2010,
  title = {Bayesian and ``{{Anti-Bayesian}}'' {{Biases}} in {{Sensory Integration}} for {{Action}} and {{Perception}} in the {{Size}}\textendash{{Weight Illusion}}},
  author = {Brayanov, Jordan B. and Smith, Maurice A.},
  year = {2010},
  month = mar,
  journal = {Journal of Neurophysiology},
  volume = {103},
  number = {3},
  pages = {1518--1531},
  issn = {0022-3077},
  doi = {10.1152/jn.00814.2009},
  abstract = {Which is heavier: a pound of lead or a pound of feathers? This classic trick question belies a simple but surprising truth: when lifted, the pound of lead feels heavier\textemdash a phenomenon known as the size\textendash weight illusion. To estimate the weight of an object, our CNS combines two imperfect sources of information: a prior expectation, based on the object's appearance, and direct sensory information from lifting it. Bayes' theorem (or Bayes' law) defines the statistically optimal way to combine multiple information sources for maximally accurate estimation. Here we asked whether the mechanisms for combining these information sources produce statistically optimal weight estimates for both perceptions and actions. We first studied the ability of subjects to hold one hand steady when the other removed an object from it, under conditions in which sensory information about the object's weight sometimes conflicted with prior expectations based on its size. Since the ability to steady the supporting hand depends on the generation of a motor command that accounts for lift timing and object weight, hand motion can be used to gauge biases in weight estimation by the motor system. We found that these motor system weight estimates reflected the integration of prior expectations with real-time proprioceptive information in a Bayesian, statistically optimal fashion that discounted unexpected sensory information. This produces a motor size\textendash weight illusion that consistently biases weight estimates toward prior expectations. In contrast, when subjects compared the weights of two objects, their perceptions defied Bayes' law, exaggerating the value of unexpected sensory information. This produces a perceptual size\textendash weight illusion that biases weight perceptions away from prior expectations. We term this effect ``anti-Bayesian'' because the bias is opposite that seen in Bayesian integration. Our findings suggest that two fundamentally different strategies for the integration of prior expectations with sensory information coexist in the nervous system for weight estimation.},
  pmcid = {PMC4422348},
  pmid = {20089821},
  file = {/Users/xzfang/Zotero/storage/7BWEWBK7/Brayanov and Smith - 2010 - Bayesian and â€œAnti-Bayesianâ€ Biases in Sensory Int.pdf}
}

@article{bree_sustained_2021,
  title = {Sustained Neural Rhythms Reveal Endogenous Oscillations Supporting Speech Perception},
  author = {van Bree, Sander and Sohoglu, Ediz and Davis, Matthew H. and Zoefel, Benedikt},
  year = {2021},
  month = feb,
  journal = {PLOS Biology},
  volume = {19},
  number = {2},
  pages = {e3001142},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3001142},
  abstract = {Rhythmic sensory or electrical stimulation will produce rhythmic brain responses. These rhythmic responses are often interpreted as endogenous neural oscillations aligned (or ``entrained'') to the stimulus rhythm. However, stimulus-aligned brain responses can also be explained as a sequence of evoked responses, which only appear regular due to the rhythmicity of the stimulus, without necessarily involving underlying neural oscillations. To distinguish evoked responses from true oscillatory activity, we tested whether rhythmic stimulation produces oscillatory responses which continue after the end of the stimulus. Such sustained effects provide evidence for true involvement of neural oscillations. In Experiment 1, we found that rhythmic intelligible, but not unintelligible speech produces oscillatory responses in magnetoencephalography (MEG) which outlast the stimulus at parietal sensors. In Experiment 2, we found that transcranial alternating current stimulation (tACS) leads to rhythmic fluctuations in speech perception outcomes after the end of electrical stimulation. We further report that the phase relation between electroencephalography (EEG) responses and rhythmic intelligible speech can predict the tACS phase that leads to most accurate speech perception. Together, we provide fundamental results for several lines of research\textemdash including neural entrainment and tACS\textemdash and reveal endogenous neural oscillations as a key underlying principle for speech perception.},
  langid = {english},
  keywords = {Behavior,Electroencephalography,Functional electrical stimulation,Magnetoencephalography,Sensory perception,Speech,Statistical distributions,Transcranial alternating current stimulation},
  file = {/Users/xzfang/Zotero/storage/2U5YSRS5/Bree et al. - 2021 - Sustained neural rhythms reveal endogenous oscilla.pdf;/Users/xzfang/Zotero/storage/XMQRJPHC/article.html}
}

@article{breen_acoustic_2010,
  title = {Acoustic Correlates of Information Structure},
  author = {Breen, Mara and Fedorenko, Evelina and Wagner, Michael and Gibson, Edward},
  year = {2010},
  month = sep,
  journal = {Language and Cognitive Processes},
  volume = {25},
  number = {7-9},
  pages = {1044--1098},
  issn = {0169-0965},
  doi = {10.1080/01690965.2010.504378},
  abstract = {This paper reports three studies aimed at addressing three questions about the acoustic correlates of information structure in English: (1) do speakers mark information structure prosodically, and, to the extent they do; (2) what are the acoustic features associated with different aspects of information structure; and (3) how well can listeners retrieve this information from the signal? The information structure of subject\textendash verb\textendash object sentences was manipulated via the questions preceding those sentences: elements in the target sentences were either focused (i.e., the answer to a wh-question) or given (i.e., mentioned in prior discourse); furthermore, focused elements had either an implicit or an explicit contrast set in the discourse; finally, either only the object was focused (narrow object focus) or the entire event was focused (wide focus). The results across all three experiments demonstrated that people reliably mark (1) focus location (subject, verb, or object) using greater intensity, longer duration, and higher mean and maximum F0, and (2) focus breadth, such that narrow object focus is marked with greater intensity, longer duration, and higher mean and maximum F0 on the object than wide focus. Furthermore, when participants are made aware of prosodic ambiguity present across different information structures, they reliably mark focus type, so that contrastively focused elements are produced with greater intensity, longer duration, and lower mean and maximum F0 than noncontrastively focused elements. In addition to having important theoretical consequences for accounts of semantics and prosody, these experiments demonstrate that linear residualisation successfully removes individual differences in people's productions thereby revealing cross-speaker generalisations. Furthermore, discriminant modelling allows us to objectively determine the acoustic features that underlie meaning differences.},
  keywords = {Information structure,Prosody},
  file = {/Users/xzfang/Zotero/storage/7539X2F8/Breen et al. - 2010 - Acoustic correlates of information structure.pdf;/Users/xzfang/Zotero/storage/KHB9JIU9/01690965.2010.html}
}

@article{breen_acoustic_2010a,
  title = {Acoustic Correlates of Information Structure},
  author = {Breen, Mara and Fedorenko, Evelina and Wagner, Michael and Gibson, Edward},
  year = {2010},
  month = sep,
  journal = {Language and Cognitive Processes},
  volume = {25},
  number = {7-9},
  pages = {1044--1098},
  issn = {0169-0965},
  doi = {10.1080/01690965.2010.504378},
  abstract = {This paper reports three studies aimed at addressing three questions about the acoustic correlates of information structure in English: (1) do speakers mark information structure prosodically, and, to the extent they do; (2) what are the acoustic features associated with different aspects of information structure; and (3) how well can listeners retrieve this information from the signal? The information structure of subject\textendash verb\textendash object sentences was manipulated via the questions preceding those sentences: elements in the target sentences were either focused (i.e., the answer to a wh-question) or given (i.e., mentioned in prior discourse); furthermore, focused elements had either an implicit or an explicit contrast set in the discourse; finally, either only the object was focused (narrow object focus) or the entire event was focused (wide focus). The results across all three experiments demonstrated that people reliably mark (1) focus location (subject, verb, or object) using greater intensity, longer duration, and higher mean and maximum F0, and (2) focus breadth, such that narrow object focus is marked with greater intensity, longer duration, and higher mean and maximum F0 on the object than wide focus. Furthermore, when participants are made aware of prosodic ambiguity present across different information structures, they reliably mark focus type, so that contrastively focused elements are produced with greater intensity, longer duration, and lower mean and maximum F0 than noncontrastively focused elements. In addition to having important theoretical consequences for accounts of semantics and prosody, these experiments demonstrate that linear residualisation successfully removes individual differences in people's productions thereby revealing cross-speaker generalisations. Furthermore, discriminant modelling allows us to objectively determine the acoustic features that underlie meaning differences.},
  keywords = {Information structure,Prosody},
  file = {/Users/xzfang/Zotero/storage/EZUTW9UL/Breen et al. - 2010 - Acoustic correlates of information structure.pdf;/Users/xzfang/Zotero/storage/Y3Y83IH9/01690965.2010.html}
}

@book{bregman_auditory_1990,
  title = {Auditory {{Scene Analysis}}: {{The Perceptual Organization}} of {{Sound}}},
  shorttitle = {Auditory {{Scene Analysis}}},
  author = {Bregman, Albert S.},
  year = {1990},
  month = may,
  doi = {10.7551/mitpress/1486.001.0001},
  abstract = {Auditory Scene Analysis addresses the problem of hearing complex auditory environments, using a series of creative analogies to describe the process required of},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2KMDWYBS/Auditory-Scene-AnalysisThe-Perceptual-Organization.html}
}

@article{brehm_probabilistic_2021,
  title = {Probabilistic Online Processing of Sentence Anomalies},
  author = {Brehm, Laurel and Jackson, Carrie N. and Miller, Karen L.},
  year = {2021},
  month = oct,
  journal = {Language, Cognition and Neuroscience},
  volume = {36},
  number = {8},
  pages = {959--983},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2021.1900579},
  abstract = {Listeners can successfully interpret the intended meaning of an utterance even when it contains errors or other unexpected anomalies. The present work combines an online measure of attention to sentence referents (visual world eye-tracking) with offline judgments of sentence meaning to disclose how the interpretation of anomalous sentences unfolds over time in order to explore mechanisms of non-literal processing. We use a metalinguistic judgment in Experiment 1 and an elicited imitation task in Experiment 2. In both experiments, we focus on one morphosyntactic anomaly (Subject-verb agreement; The key to the cabinets literally *were \ldots{} ) and one semantic anomaly (Without; Lulu went to the gym without her hat ?off) and show that non-literal referents to each are considered upon hearing the anomalous region of the sentence. This shows that listeners understand anomalies by overwriting or adding to an initial interpretation and that this occurs incrementally and adaptively as the sentence unfolds.},
  keywords = {good-enough processing,implicit negation,noisy channel,Sentence processing,subject-verb agreement,visual-world paradigm},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2021.1900579},
  file = {/Users/xzfang/Zotero/storage/SZFQ7GZY/Brehm et al. - 2021 - Probabilistic online processing of sentence anomal.pdf;/Users/xzfang/Zotero/storage/8E7V3A77/23273798.2021.html}
}

@article{brennan_conceptual_1996,
  title = {Conceptual Pacts and Lexical Choice in Conversation},
  author = {Brennan, S. E. and Clark, H. H.},
  year = {1996},
  month = nov,
  journal = {Journal of Experimental Psychology. Learning, Memory, and Cognition},
  volume = {22},
  number = {6},
  pages = {1482--1493},
  issn = {0278-7393},
  doi = {10.1037//0278-7393.22.6.1482},
  abstract = {When people in conversation refer repeatedly to the same object, they come to use the same terms. This phenomenon, called lexical entrainment, has several possible explanations. Ahistorical accounts appeal only to the informativeness and availability of terms and to the current salience of the object's features. Historical accounts appeal in addition to the recency and frequency of past references and to partner-specific conceptualizations of the object that people achieve interactively. Evidence from 3 experiments favors a historical account and suggests that when speakers refer to an object, they are proposing a conceptualization of it, a proposal their addresses may or may not agree to. Once they do establish a shared conceptualization, a conceptual pact, they appeal to it in later references even when they could use simpler references. Over time, speakers simplify conceptual pacts and, when necessary, abandon them for new conceptualizations.},
  langid = {english},
  pmid = {8921603},
  keywords = {Choice Behavior,Female,Humans,Male,Verbal Behavior,Vocabulary}
}

@article{brennan_hierarchical_2019,
  title = {Hierarchical Structure Guides Rapid Linguistic Predictions during Naturalistic Listening},
  author = {Brennan, Jonathan R. and Hale, John T.},
  year = {2019},
  month = jan,
  journal = {PLOS ONE},
  volume = {14},
  number = {1},
  pages = {e0207741},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0207741},
  abstract = {The grammar, or syntax, of human language is typically understood in terms of abstract hierarchical structures. However, theories of language processing that emphasize sequential information, not hierarchy, successfully model diverse phenomena. Recent work probing brain signals has shown mixed evidence for hierarchical information in some tasks. We ask whether sequential or hierarchical information guides the expectations that a human listener forms about a word's part-of-speech when simply listening to every-day language. We compare the predictions of three computational models against electroencephalography signals recorded from human participants who listen passively to an audiobook story. We find that predictions based on hierarchical structure correlate with the human brain response above-and-beyond predictions based only on sequential information. This establishes a link between hierarchical linguistic structure and neural signals that generalizes across the range of syntactic structures found in every-day language.},
  langid = {english},
  keywords = {Acoustic signals,Electroencephalography,Event-related potentials,Grammar,Language,Neurolinguistics,Semantics,Syntax},
  file = {/Users/xzfang/Zotero/storage/MCUGC77R/Brennan and Hale - 2019 - Hierarchical structure guides rapid linguistic pre.pdf;/Users/xzfang/Zotero/storage/K32TNACB/article.html}
}

@article{brenner_adaptive_2000,
  title = {Adaptive {{Rescaling Maximizes Information Transmission}}},
  author = {Brenner, Naama and Bialek, William and van Steveninck, Rob de Ruyter},
  year = {2000},
  month = jun,
  journal = {Neuron},
  volume = {26},
  number = {3},
  pages = {695--702},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(00)81205-2},
  langid = {english},
  pmid = {10896164},
  file = {/Users/xzfang/Zotero/storage/CL556576/Brenner et al. - 2000 - Adaptive Rescaling Maximizes Information Transmiss.pdf;/Users/xzfang/Zotero/storage/HPI84PEM/S0896-6273(00)81205-2.html}
}

@article{bressler_bottomup_2014,
  title = {Bottom-up Influences of Voice Continuity in Focusing Selective Auditory Attention},
  author = {Bressler, Scott and Masud, Salwa and Bharadwaj, Hari and {Shinn-Cunningham}, Barbara},
  year = {2014},
  journal = {Psychological research},
  volume = {78},
  number = {3},
  pages = {349--360},
  issn = {0340-0727},
  doi = {10.1007/s00426-014-0555-7},
  abstract = {Selective auditory attention causes a relative enhancement of the neural representation of important information and suppression of the neural representation of distracting sound, which enables a listener to analyze and interpret information of interest. Some studies suggest that in both vision and in audition, the ``unit'' on which attention operates is an object: an estimate of the information coming from a particular external source out in the world. In this view, which object ends up in the attentional foreground depends on the interplay of top-down, volitional attention and stimulus-driven, involuntary attention. Here, we test the idea that auditory attention is object based by exploring whether continuity of a non-spatial feature (talker identity, a feature that helps acoustic elements bind into one perceptual object) also influences selective attention performance. In Experiment 1, we show that perceptual continuity of target talker voice helps listeners report a sequence of spoken target digits embedded in competing reversed digits spoken by different talkers. In Experiment 2, we provide evidence that this benefit of voice continuity is obligatory and automatic, as if voice continuity biases listeners by making it easier to focus on a subsequent target digit when it is perceptually linked to what was already in the attentional foreground. Our results support the idea that feature continuity enhances streaming automatically, thereby influencing the dynamic processes that allow listeners to successfully attend to objects through time in the cacophony that assails our ears in many everyday settings.},
  pmcid = {PMC4648263},
  pmid = {24633644},
  file = {/Users/xzfang/Zotero/storage/IFIW64JU/Bressler et al. - 2014 - Bottom-up influences of voice continuity in focusi.pdf}
}

@article{bridges_timing_2020,
  title = {The Timing Mega-Study: Comparing a Range of Experiment Generators, Both Lab-Based and Online},
  shorttitle = {The Timing Mega-Study},
  author = {Bridges, David and Pitiot, Alain and MacAskill, Michael R. and Peirce, Jonathan W.},
  year = {2020},
  month = jul,
  journal = {PeerJ},
  volume = {8},
  pages = {e9414},
  publisher = {{PeerJ Inc.}},
  issn = {2167-8359},
  doi = {10.7717/peerj.9414},
  abstract = {Many researchers in the behavioral sciences depend on research software that presents stimuli, and records response times, with sub-millisecond precision. There are a large number of software packages with which to conduct these behavioral experiments and measure response times and performance of participants. Very little information is available, however, on what timing performance they achieve in practice. Here we report a wide-ranging study looking at the precision and accuracy of visual and auditory stimulus timing and response times, measured with a Black Box Toolkit. We compared a range of popular packages: PsychoPy, E-Prime\textregistered, NBS Presentation\textregistered, Psychophysics Toolbox, OpenSesame, Expyriment, Gorilla, jsPsych, Lab.js and Testable. Where possible, the packages were tested on Windows, macOS, and Ubuntu, and in a range of browsers for the online studies, to try to identify common patterns in performance. Among the lab-based experiments, Psychtoolbox, PsychoPy, Presentation and E-Prime provided the best timing, all with mean precision under 1 millisecond across the visual, audio and response measures. OpenSesame had slightly less precision across the board, but most notably in audio stimuli and Expyriment had rather poor precision. Across operating systems, the pattern was that precision was generally very slightly better under Ubuntu than Windows, and that macOS was the worst, at least for visual stimuli, for all packages. Online studies did not deliver the same level of precision as lab-based systems, with slightly more variability in all measurements. That said, PsychoPy and Gorilla, broadly the best performers, were achieving very close to millisecond precision on several browser/operating system combinations. For response times (measured using a high-performance button box), most of the packages achieved precision at least under 10 ms in all browsers, with PsychoPy achieving a precision under 3.5 ms in all. There was considerable variability between OS/browser combinations, especially in audio-visual synchrony which is the least precise aspect of the browser-based experiments. Nonetheless, the data indicate that online methods can be suitable for a wide range of studies, with due thought about the sources of variability that result. The results, from over 110,000 trials, highlight the wide range of timing qualities that can occur even in these dedicated software packages for the task. We stress the importance of scientists making their own timing validation measurements for their own stimuli and computer configuration.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YMTD4ZPM/Bridges et al. - 2020 - The timing mega-study comparing a range of experi.pdf}
}

@article{briesemeister_pseudohomophone_2009,
  title = {The Pseudohomophone Effect: {{Evidence}} for an Orthography\textendash Phonology-Conflict},
  shorttitle = {The Pseudohomophone Effect},
  author = {Briesemeister, Benny B. and Hofmann, Markus J. and Tamm, Sascha and Kuchinke, Lars and Braun, Mario and Jacobs, Arthur M.},
  year = {2009},
  month = may,
  journal = {Neuroscience Letters},
  volume = {455},
  number = {2},
  pages = {124--128},
  issn = {0304-3940},
  doi = {10.1016/j.neulet.2009.03.010},
  abstract = {The standard pseudohomophone effect in the lexical decision task, i.e. longer response times and higher error rates for pseudohomophones compared with spelling controls, is commonly explained by an orthography\textendash phonology-conflict. This study tested this conflict account, using a multi-method approach including participant's behavioral responses, confidence ratings, pupillary responses and event-related potentials (ERPs). The classic pseudohomophone effect was replicated using relatively long, multi-syllabic stimuli. Pseudohomophones were rated less confidently as being nonwords than spelling controls, and they affected the pupillary response by increasing the peak pupil diameter. Both findings are interpreted in terms of increased conflict and higher cognitive demands leading to uncertainty while solving the task. The ERP revealed an N400 component for spelling controls, showing a graded effect: word},
  langid = {english},
  keywords = {Conflict,Lexical decision task,MROM-p,N400,Pseudohomophones,Pupillary responses}
}

@inproceedings{brill_improved_2000,
  title = {An Improved Error Model for Noisy Channel Spelling Correction},
  booktitle = {Proceedings of the 38th {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}  - {{ACL}} '00},
  author = {Brill, Eric and Moore, Robert C.},
  year = {2000},
  pages = {286--293},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong}},
  doi = {10.3115/1075218.1075255},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/C28UCXN7/Brill and Moore - 2000 - An improved error model for noisy channel spelling.pdf}
}

@misc{brodbeck_cortical_2021,
  title = {Cortical Tracking of Voice Pitch in the Presence of Multiple Speakers Depends on Selective Attention},
  author = {Brodbeck, Christian and Simon, Jonathan Z.},
  year = {2021},
  month = dec,
  pages = {2021.12.03.471122},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.12.03.471122},
  abstract = {Voice pitch carries linguistic as well as non-linguistic information. Previous studies have described cortical tracking of voice pitch in clean speech, with responses reflecting both pitch strength and pitch value. However, pitch is also a powerful cue for auditory stream segregation, especially when competing streams have pitch differing in fundamental frequency, as is the case when multiple speakers talk simultaneously. We therefore investigated how cortical speech pitch tracking is affected in the presence of a second, task-irrelevant speaker. We analyzed human magnetoencephalography (MEG) responses to continuous narrative speech, presented either as a single talker in a quiet background, or as a two-talker mixture of a male and a female speaker. In clean speech, voice pitch was associated with a right-dominant response, peaking at a latency of around 100 ms, consistent with previous EEG and ECoG results. The response tracked both the presence of pitch as well as the relative value of the speaker's fundamental frequency. In the two-talker mixture, pitch of the attended speaker was tracked bilaterally, regardless of whether or not there was simultaneously present pitch in the speech of the irrelevant speaker. Pitch tracking for the irrelevant speaker was reduced: only the right hemisphere still significantly tracked pitch of the unattended speaker, and only during intervals in which no pitch was present in the attended talker's speech. Taken together, these results suggest that pitch-based segregation of multiple speakers, at least as measured by macroscopic cortical tracking, is not entirely automatic but strongly dependent on selective attention.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/M2C2H5P8/Brodbeck and Simon - 2021 - Cortical tracking of voice pitch in the presence o.pdf}
}

@article{brodbeck_parallel_2022,
  title = {Parallel Processing in Speech Perception with Local and Global Representations of Linguistic Context},
  author = {Brodbeck, Christian and Bhattasali, Shohini and Cruz Heredia, Aura AL and Resnik, Philip and Simon, Jonathan Z and Lau, Ellen},
  editor = {{van Wassenhove}, Virginie and {Shinn-Cunningham}, Barbara G and Brennan, Jonathan},
  year = {2022},
  month = jan,
  journal = {eLife},
  volume = {11},
  pages = {e72056},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.72056},
  abstract = {Speech processing is highly incremental. It is widely accepted that human listeners continuously use the linguistic context to anticipate upcoming concepts, words, and phonemes. However, previous evidence supports two seemingly contradictory models of how a predictive context is integrated with the bottom-up sensory input: Classic psycholinguistic paradigms suggest a two-stage process, in which acoustic input initially leads to local, context-independent representations, which are then quickly integrated with contextual constraints. This contrasts with the view that the brain constructs a single coherent, unified interpretation of the input, which fully integrates available information across representational hierarchies, and thus uses contextual constraints to modulate even the earliest sensory representations. To distinguish these hypotheses, we tested magnetoencephalography responses to continuous narrative speech for signatures of local and unified predictive models. Results provide evidence that listeners employ both types of models in parallel. Two local context models uniquely predict some part of early neural responses, one based on sublexical phoneme sequences, and one based on the phonemes in the current word alone; at the same time, even early responses to phonemes also reflect a unified model that incorporates sentence-level constraints to predict upcoming phonemes. Neural source localization places the anatomical origins of the different predictive models in nonidentical parts of the superior temporal lobes bilaterally, with the right hemisphere showing a relative preference for more local models. These results suggest that speech processing recruits both local and unified predictive models in parallel, reconciling previous disparate findings. Parallel models might make the perceptual system more robust, facilitate processing of unexpected inputs, and serve a function in language acquisition.},
  keywords = {entropy,MEG,surprisal,temporal response functions},
  file = {/Users/xzfang/Zotero/storage/XT6SHI8A/Brodbeck et al. - 2022 - Parallel processing in speech perception with loca.pdf}
}

@article{brodbeck_rapid_2018,
  title = {Rapid {{Transformation}} from {{Auditory}} to {{Linguistic Representations}} of {{Continuous Speech}}},
  author = {Brodbeck, Christian and Hong, L. Elliot and Simon, Jonathan Z.},
  year = {2018},
  month = dec,
  journal = {Current Biology},
  volume = {28},
  number = {24},
  pages = {3976-3983.e5},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2018.10.042},
  abstract = {During speech perception, a central task of the auditory cortex is to analyze complex acoustic patterns to allow detection of the words that encode a linguistic message [1]. It is generally thought that this process includes at least one intermediate, phonetic, level of representations [2, 3, 4, 5, 6], localized bilaterally in the superior temporal lobe [7, 8, 9]. Phonetic representations reflect a transition from acoustic to linguistic information, classifying acoustic patterns into linguistically meaningful units, which can serve as input to mechanisms that access abstract word representations [10, 11]. While recent research has identified neural signals arising from successful recognition of individual words in continuous speech [12, 13, 14, 15], no explicit neurophysiological signal has been found demonstrating the transition from acoustic and/or phonetic to symbolic, lexical representations. Here, we report a response reflecting the incremental integration of phonetic information for word identification, dominantly localized to the left temporal lobe. The short response latency, approximately 114~ms relative to phoneme onset, suggests that phonetic information is used for lexical processing as soon as it becomes available. Responses also tracked word boundaries, confirming previous reports of immediate lexical segmentation [16, 17]. These new results were further investigated using a cocktail-party paradigm [18, 19] in which participants listened to a mix of~two talkers, attending to one and ignoring the other. Analysis indicates neural lexical processing of only the attended, but not the unattended, speech stream. Thus, while responses to acoustic features reflect attention through selective amplification of attended speech, responses consistent with a lexical processing model reveal categorically selective processing.},
  langid = {english},
  keywords = {cohort entropy,cohort model,magentoencephalography,phoneme surprisal,temporal response function},
  file = {/Users/xzfang/Zotero/storage/Y2XRDNQF/Brodbeck et al. - 2018 - Rapid Transformation from Auditory to Linguistic R.pdf;/Users/xzfang/Zotero/storage/G4GLFLIQ/S096098221831409X.html}
}

@article{broderick_dissociable_2021,
  title = {Dissociable Electrophysiological Measures of Natural Language Processing Reveal Differences in Speech Comprehension Strategy in Healthy Ageing},
  author = {Broderick, Michael P. and Di Liberto, Giovanni M. and Anderson, Andrew J. and Rofes, Adri{\`a} and Lalor, Edmund C.},
  year = {2021},
  month = mar,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {4963},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-84597-9},
  abstract = {Healthy ageing leads to changes in the brain that impact upon sensory and cognitive processing. It is not fully clear how these changes affect the processing of everyday spoken language. Prediction is thought to play an important role in language comprehension, where information about upcoming words is pre-activated across multiple representational levels. However, evidence from electrophysiology suggests differences in how older and younger adults use context-based predictions, particularly at the level of semantic representation. We investigate these differences during natural speech comprehension by presenting older and younger subjects with continuous, narrative speech while recording their electroencephalogram. We use time-lagged linear regression to test how distinct computational measures of (1) semantic dissimilarity and (2) lexical surprisal are processed in the brains of both groups. Our results reveal dissociable neural correlates of these two measures that suggest differences in how younger and older adults successfully comprehend speech. Specifically, our results suggest that, while younger and older subjects both employ context-based lexical predictions, older subjects are significantly less likely to pre-activate the semantic features relating to upcoming words. Furthermore, across our group of older adults, we show that the weaker the neural signature of this semantic pre-activation mechanism, the lower a subject's semantic verbal fluency score. We interpret these findings as prediction playing a generally reduced role at a semantic level in the brains of older listeners during speech comprehension and that these changes may be part of an overall strategy to successfully comprehend speech with reduced cognitive resources.},
  copyright = {2021 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DA86LE2Z/Broderick et al. - 2021 - Dissociable electrophysiological measures of natur.pdf;/Users/xzfang/Zotero/storage/YKAVSJG9/s41598-021-84597-9.html}
}

@article{broderick_electrophysiological_2018,
  title = {Electrophysiological {{Correlates}} of {{Semantic Dissimilarity Reflect}} the {{Comprehension}} of {{Natural}}, {{Narrative Speech}}},
  author = {Broderick, Michael P. and Anderson, Andrew J. and Liberto, Giovanni M. Di and Crosse, Michael J. and Lalor, Edmund C.},
  year = {2018},
  month = mar,
  journal = {Current Biology},
  volume = {28},
  number = {5},
  pages = {803-809.e3},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2018.01.080},
  langid = {english},
  pmid = {29478856},
  keywords = {cocktail party,computational linguistics,cortical entrainment,EEG,multisensory integration,selective attention,semantic processing},
  file = {/Users/xzfang/Zotero/storage/6NF6Y465/Broderick et al. - 2018 - Electrophysiological Correlates of Semantic Dissim.pdf;/Users/xzfang/Zotero/storage/FXC3G9C3/S0960-9822(18)30146-5.html}
}

@article{broderick_more_2020,
  title = {More than {{Words}}: {{Neurophysiological Correlates}} of {{Semantic Dissimilarity Depend}} on {{Comprehension}} of the {{Speech Narrative}}},
  shorttitle = {More than {{Words}}},
  author = {Broderick, Michael P. and Zuk, Nathaniel J. and Anderson, Andrew J. and Lalor, Edmund C.},
  year = {2020},
  month = dec,
  journal = {bioRxiv},
  pages = {2020.12.14.422789},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.14.422789},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Speech comprehension relies on the ability to understand the meaning of words within a coherent context. Recent studies have attempted to obtain electrophysiological indices of this process by modelling how brain activity is affected by a word's semantic dissimilarity to preceding words. While the resulting indices appear robust and are strongly modulated by attention, it remains possible that, rather than capturing the contextual understanding of words, they may actually reflect word-to-word changes in semantic content without the need for a narrative-level understanding on the part of the listener. To test this possibility, we recorded EEG from subjects who listened to speech presented in either its original, narrative form, or after scrambling the word order by varying amounts. This manipulation affected the ability of subjects to comprehend the narrative content of the speech, but not the ability to recognize the individual words. Neural indices of semantic understanding and low-level acoustic processing were derived for each scrambling condition using the temporal response function (TRF) approach. Signatures of semantic processing were observed for conditions where speech was unscrambled or minimally scrambled and subjects were able to understand the speech. The same markers were absent for higher levels of scrambling when speech comprehension dropped below chance. In contrast, word recognition remained high and neural measures related to envelope tracking did not vary significantly across the different scrambling conditions. This supports the previous claim that electrophysiological indices based on the semantic dissimilarity of words to their context reflect a listener's understanding of those words relative to that context. It also highlights the relative insensitivity of neural measures of low-level speech processing to speech comprehension.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WYBSN7A4/Broderick et al. - 2020 - More than Words Neurophysiological Correlates of .pdf;/Users/xzfang/Zotero/storage/QYU5GXKJ/2020.12.14.html}
}

@article{broderick_semantic_2019,
  title = {Semantic {{Context Enhances}} the {{Early Auditory Encoding}} of {{Natural Speech}}},
  author = {Broderick, Michael P. and Anderson, Andrew J. and Lalor, Edmund C.},
  year = {2019},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {38},
  pages = {7564--7575},
  file = {/Users/xzfang/Zotero/storage/BNMATQ4D/7564.html}
}

@misc{brohl_visual_2022,
  title = {Visual and Auditory Cortices Represent Acoustic Speech-Related Information during Silent Lip Reading},
  author = {Br{\"o}hl, Felix and Keitel, Anne and Kayser, Christoph},
  year = {2022},
  month = feb,
  pages = {2022.02.21.481292},
  institution = {{bioRxiv}},
  doi = {10.1101/2022.02.21.481292},
  abstract = {Speech is an intrinsically multisensory signal and seeing the speaker's lips forms a cornerstone of communication in acoustically impoverished environments. Still, it remains unclear how the brain exploits visual speech for comprehension and previous work debated whether lip signals are mainly processed along the auditory pathways or whether the visual system directly implements speech-related processes. To probe this question, we systematically characterized dynamic representations of multiple acoustic and visual speech-derived features in source localized MEG recordings that were obtained while participants listened to speech or viewed silent speech. Using a mutual-information framework we provide a comprehensive assessment of how well temporal and occipital cortices reflect the physically presented signals and speech-related features that were physically absent but may still be critical for comprehension. Our results demonstrate that both cortices are capable of a functionally specific form of multisensory restoration: during lip reading both reflect unheard acoustic features, with occipital regions emphasizing spectral information and temporal regions emphasizing the speech envelope. Importantly, the degree of envelope restoration was predictive of lip reading performance. These findings suggest that when seeing the speaker's lips the brain engages both visual and auditory pathways to support comprehension by exploiting multisensory correspondences between lip movements and spectro-temporal acoustic cues. HighlightsVisual and auditory cortex represent unheard acoustic information during lip readingAuditory cortex emphasizes the acoustic envelopeVisual cortex emphasizes a pitch signatureTracking of unheard features in auditory cortex is associated with behavior},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2D5LVXTC/BrÃ¶hl et al. - 2022 - Visual and auditory cortices represent acoustic sp.pdf;/Users/xzfang/Zotero/storage/ADVW7QX4/2022.02.21.481292v1.html}
}

@article{brothers_anticipating_2016,
  title = {Anticipating Syntax during Reading: {{Evidence}} from the Boundary Change Paradigm},
  shorttitle = {Anticipating Syntax during Reading},
  author = {Brothers, Trevor and Traxler, Matthew J.},
  year = {2016},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {42},
  number = {12},
  pages = {1894--1906},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/xlm0000257},
  abstract = {Previous evidence suggests that grammatical constraints have a rapid influence during language comprehension, particularly at the level of word categories (noun, verb, preposition). These findings are in conflict with a recent study from Angele, Laishley, Rayner, and Liversedge (2014), in which sentential fit had no early influence on word skipping rates during reading. In the present study, we used a gaze-contingent boundary change paradigm to manipulate the syntactic congruity of an upcoming noun or verb outside of participants' awareness. Across 3 experiments (total N = 148), we observed higher skipping rates for syntactically valid previews (The admiral would not confess . . .), when compared with violation previews (The admiral would not surgeon . . .). Readers were less likely to skip an ungrammatical continuation, even when that word was repeated within the same sentence (The admiral would not admiral . . .), suggesting that word-class constraints can take precedence over lexical repetition effects. To our knowledge, these results provide the first evidence for an influence of syntactic context during parafoveal word recognition. On the basis of the early time-course of this effect, we argue that readers can use grammatical constraints to generate syntactic expectations for upcoming words. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Expectations,Nouns,Prediction,Syntax,Verbs,Visual Tracking},
  file = {/Users/xzfang/Zotero/storage/8ZFHMIJG/Brothers and Traxler - 2016 - Anticipating syntax during reading Evidence from .pdf;/Users/xzfang/Zotero/storage/E7CIU4TH/2016-20642-001.html}
}

@article{brothers_effects_2015,
  title = {Effects of Prediction and Contextual Support on Lexical Processing: {{Prediction}} Takes Precedence},
  shorttitle = {Effects of Prediction and Contextual Support on Lexical Processing},
  author = {Brothers, Trevor and Swaab, Tamara Y. and Traxler, Matthew J.},
  year = {2015},
  month = mar,
  journal = {Cognition},
  volume = {136},
  pages = {135--149},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2014.10.017},
  abstract = {Readers may use contextual information to anticipate and pre-activate specific lexical items during reading. However, prior studies have not clearly dissociated the effects of accurate lexical prediction from other forms of contextual facilitation such as plausibility or semantic priming. In this study, we measured electrophysiological responses to predicted and unpredicted target words in passages providing varying levels of contextual support. This method was used to isolate the neural effects of prediction from other potential contextual influences on lexical processing. While both prediction and discourse context influenced ERP amplitudes within the time range of the N400, the effects of prediction occurred much more rapidly, preceding contextual facilitation by approximately 100ms. In addition, a frontal, post-N400 positivity (PNP) was modulated by both prediction accuracy and the overall plausibility of the preceding passage. These results suggest a unique temporal primacy for prediction in facilitating lexical access. They also suggest that the frontal PNP may index the costs of revising discourse representations following an incorrect lexical prediction.},
  langid = {english},
  keywords = {Event-related potentials,N250,N400,Prediction,Sentence processing},
  file = {/Users/xzfang/Zotero/storage/UT9Q3897/Brothers et al. - 2015 - Effects of prediction and contextual support on le.pdf;/Users/xzfang/Zotero/storage/LTMDX7EN/S0010027714002182.html}
}

@article{brothers_flexible_2019,
  title = {Flexible Predictions during Listening Comprehension: {{Speaker}} Reliability Affects Anticipatory Processes},
  shorttitle = {Flexible Predictions during Listening Comprehension},
  author = {Brothers, Trevor and Dave, Shruti and Hoversten, Liv J. and Traxler, Matthew J. and Swaab, Tamara Y.},
  year = {2019},
  month = dec,
  journal = {Neuropsychologia},
  volume = {135},
  pages = {107225},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2019.107225},
  abstract = {During listening comprehension, the identification of individual words can be strongly influenced by properties of the preceding context. While sentence context can facilitate both behavioral and neural responses, it is unclear whether these effects can be attributed to the pre-activation of lexico-semantic features or the facilitated integration of contextually congruent words. Moreover, little is known about how statistics of the broader language environment, or information about the current speaker, might shape these facilitation effects. In the present study, we measured neural responses to predictable and unpredictable words as participants listened to sentences for comprehension. Critically, we manipulated the reliability of each speaker's utterances, such that individual speakers either tended to complete sentences with words that were highly predictable (reliable speaker) or with words that were unpredictable but still plausible (unreliable speaker). As expected, the amplitude of the N400 was reduced for locally predictable words, but, critically, these context effects were also modulated by speaker identity. Sentences from a reliable speaker showed larger facilitation effects with an earlier onset, suggesting that listeners engaged in enhanced anticipatory processing when a speaker's behavior was more predictable. This finding suggests that listeners can implicitly track the reliability of predictive cues in their environment and use these statistics to adaptively regulate predictive processing.},
  langid = {english},
  keywords = {Adaptation,Comprehension,N400,Prediction,Speaker identity},
  file = {/Users/xzfang/Zotero/storage/RH2FTJWK/Brothers et al. - 2019 - Flexible predictions during listening comprehensio.pdf;/Users/xzfang/Zotero/storage/TIF7LHKH/S0028393219302684.html}
}

@article{brouwer_speech_2012,
  title = {Speech Reductions Change the Dynamics of Competition during Spoken Word Recognition},
  author = {Brouwer, Susanne and Mitterer, Holger and Huettig, Falk},
  year = {2012},
  month = may,
  journal = {Language and Cognitive Processes},
  volume = {27},
  number = {4},
  pages = {539--571},
  publisher = {{Routledge}},
  issn = {0169-0965},
  doi = {10.1080/01690965.2011.555268},
  abstract = {Three eye-tracking experiments investigated how phonological reductions (e.g., ``puter'' for ``computer'') modulate phonological competition. Participants listened to sentences extracted from a spontaneous speech corpus and saw four printed words: a target (e.g., ``computer''), a competitor similar to the canonical form (e.g., ``companion''), one similar to the reduced form (e.g., ``pupil''), and an unrelated distractor. In Experiment 1, we presented canonical and reduced forms in a syllabic and in a sentence context. Listeners directed their attention to a similar degree to both competitors independent of the target's spoken form. In Experiment 2, we excluded reduced forms and presented canonical forms only. In such a listening situation, participants showed a clear preference for the ``canonical form'' competitor. In Experiment 3, we presented canonical forms intermixed with reduced forms in a sentence context and replicated the competition pattern of Experiment 1. These data suggest that listeners penalize acoustic mismatches less strongly when listening to reduced speech than when listening to fully articulated speech. We conclude that flexibility to adjust to speech-intrinsic factors is a key feature of the spoken word recognition system.},
  keywords = {Dutch,Eye tracking,Reduced forms,Spoken word recognition},
  annotation = {\_eprint: https://doi.org/10.1080/01690965.2011.555268},
  file = {/Users/xzfang/Zotero/storage/P2U34PYZ/Brouwer et al. - 2012 - Speech reductions change the dynamics of competiti.pdf;/Users/xzfang/Zotero/storage/D3MMTRXQ/01690965.2011.html}
}

@article{brouwer_temporal_2016,
  title = {The Temporal Dynamics of Spoken Word Recognition in Adverse Listening Conditions},
  author = {Brouwer, Susanne and Bradlow, Ann R.},
  year = {2016},
  month = oct,
  journal = {Journal of psycholinguistic research},
  volume = {45},
  number = {5},
  pages = {1151--1160},
  issn = {0090-6905},
  doi = {10.1007/s10936-015-9396-9},
  abstract = {This study examined the temporal dynamics of spoken word recognition in noise and background speech. In two visual-world experiments, English participants listened to target words while looking at four pictures on the screen: a target (e.g. candle), an onset competitor (e.g. candy), a rhyme competitor (e.g. sandal), and an unrelated distractor (e.g. lemon). Target words were presented in quiet, mixed with broadband noise, or mixed with background speech. Results showed that lexical competition changes as a function of what is presented in the background. These findings suggest that stream segregation and lexical competition interact during spoken word recognition.},
  pmcid = {PMC5664918},
  pmid = {26420754},
  file = {/Users/xzfang/Zotero/storage/PLTQIM4C/Brouwer and Bradlow - 2016 - The temporal dynamics of spoken word recognition i.pdf}
}

@article{brown-schmidt_gradient_2017,
  title = {Gradient Acoustic Information Induces Long-Lasting Referential Uncertainty in Short Discourses},
  author = {{Brown-Schmidt}, Sarah and Toscano, Joseph C.},
  year = {2017},
  month = nov,
  journal = {Language, Cognition and Neuroscience},
  volume = {32},
  number = {10},
  pages = {1211--1228},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2017.1325508},
  abstract = {Three experiments examined the influence of gradient acoustic information on referential interpretation during spoken language processing and how this influence persists over time. Acoustic continua varying between the pronouns ``he'' and ``she'' were created and validated in two offline experiments. A third experiment examined whether these acoustic differences influence online pronoun interpretation, and whether this influence persists across words in a discourse. Measures of eye gaze showed immediate sensitivity to graded acoustic information. Moreover, acoustically induced uncertainty persisted across a five-word delay: When listeners encountered a word that disambiguated the referent of the pronoun differently than it had originally been interpreted, the amount of time they took to recover from an initial misinterpretation was directly related to distance along the acoustic continuum between the pronoun and the endpoint corresponding to the correct referent. These findings show that finegrained acoustic detail induces referential uncertainty that is maintained over extended periods of time.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2D7STGUW/Brown-Schmidt and Toscano - 2017 - Gradient acoustic information induces long-lasting.pdf}
}

@misc{brown-schmidt_journal_2005,
  title = {Journal of {{Memory}} and {{Language}} 54 (2006) 592\textendash 609 {{Journal}} of {{Memory}} And},
  author = {{Brown-schmidt}, Sarah and Tanenhaus, Michael K.},
  year = {2005},
  abstract = {Language www.elsevier.com/locate/jml Watching the eyes when talking about size: An investigation of message formulation and utterance planning q},
  file = {/Users/xzfang/Zotero/storage/PKGTW3KF/Brown-schmidt and Tanenhaus - 2005 - Journal of Memory and Language 54 (2006) 592â€“609 J.pdf;/Users/xzfang/Zotero/storage/RG9L373W/download.html}
}

@article{brown-schmidt_partnerspecific_2009,
  title = {Partner-Specific Interpretation of Maintained Referential Precedents during Interactive Dialog},
  author = {{Brown-Schmidt}, Sarah},
  year = {2009},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {61},
  number = {2},
  pages = {171--190},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2009.04.003},
  abstract = {In dialog settings, conversational partners converge on similar names for referents. These lexically entrained terms [Garrod, S., \& Anderson, A. (1987). Saying what you mean in dialog: A study in conceptual and semantic co-ordination. Cognition, 27, 181\textendash 218] are part of the common ground between the particular individuals who established the entrained term [Brennan, S. E., \& Clark, H. H. (1996). Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology: Learning, Memory, and Cognition, 22, 1482\textendash 1493], and are thought to be encoded in memory with a partner-specific cue. Thus far, analyses of the time-course of interpretation suggest that partner-specific information may not constrain the initial interpretation of referring expressions [Barr, D. J., \& Keysar, B. (2002). Anchoring comprehension in linguistic precedents. Journal of Memory and Language, 46, 391\textendash 418; Kronm\"uller, E., \& Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56, 436\textendash 455]. However, these studies used non-interactive paradigms, which may limit the use of partner-specific representations. This article presents the results of three eye-tracking experiments. Experiment 1a used an interactive conversation methodology in which the experimenter and participant jointly established entrained terms for various images. On critical trials, the same experimenter, or a new experimenter described a critical image using an entrained term, or a new term. The results demonstrated an early, on-line partner-specific effect for interpretation of entrained terms, as well as preliminary evidence for an early, partner-specific effect for new terms. Experiment 1b used a non-interactive paradigm in which participants completed the same task by listening to image descriptions recorded during Experiment 1a; the results showed that partner-specific effects were eliminated. Experiment 2 replicated the partner-specific findings of Experiment 1a with an interactive paradigm and scenes that contained previously unmentioned images. The results suggest that partner-specific interpretation is most likely to occur in interactive dialog settings; the number of critical trials and stimulus characteristics may also play a role. The results are consistent with a large body of work demonstrating that the language processing system uses a rich source of contextual and pragmatic representations to guide on-line processing decisions.},
  langid = {english},
  keywords = {Common ground,Conceptual pact,Conversation,Entrainment,Eye-tracking,Perspective,Precedent,Reference},
  file = {/Users/xzfang/Zotero/storage/728X2LEI/Brown-Schmidt - 2009 - Partner-specific interpretation of maintained refe.pdf}
}

@article{brunelliere_speakers_2013,
  title = {The Speakers' Accent Shapes the Listeners' Phonological Predictions during Speech Perception},
  author = {Brunelli{\`e}re, Ang{\`e}le and {Soto-Faraco}, Salvador},
  year = {2013},
  month = apr,
  journal = {Brain and Language},
  volume = {125},
  number = {1},
  pages = {82--93},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2013.01.007},
  abstract = {This study investigates the specificity of predictive coding in spoken word comprehension using event-related potentials (ERPs). We measured word-evoked ERPs in Catalan speakers listening to semantically constraining sentences produced in their native regional accent (Experiment 1) or in a non-native accent (Experiment 2). Semantically anomalous words produced long-lasting negative shift (N400) starting as early as 250ms, thus reflecting phonological as well as semantic mismatch. Semantically expected but phonologically unexpected (non-native forms embedded in a native context) produced only an early ({$\sim$}250ms) negative difference. In contrast, this phonological expectancy effect failed for native albeit phonologically unexpected target words embedded in a non-native context. These results suggest phonologically precise expectations when operating over native input, whilst phonologically less specified expectations in a non-native context. Our findings shed light on contextual influence during word recognition, suggesting that word form prediction based on context is sensitive and adaptive to phonological variability.},
  langid = {english},
  keywords = {Event-related potentials,Phonological variability,Predictive mechanisms,Spoken-word comprehension},
  file = {/Users/xzfang/Zotero/storage/CXC9KIBI/BrunelliÃ¨re and Soto-Faraco - 2013 - The speakersâ€™ accent shapes the listenersâ€™ phonolo.pdf;/Users/xzfang/Zotero/storage/YYCYLGDU/S0093934X13000230.html}
}

@article{brungart_informational_2001,
  title = {Informational and Energetic Masking Effects in the Perception of Multiple Simultaneous Talkers},
  author = {Brungart, Douglas S. and Simpson, Brian D. and Ericson, Mark A. and Scott, Kimberly R.},
  year = {2001},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {110},
  number = {5},
  pages = {2527--2538},
  issn = {0001-4966},
  doi = {10.1121/1.1408946},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FFVJEUYL/Brungart et al. - 2001 - Informational and energetic masking effects in the.pdf}
}

@article{buchweitz_identifying_2012,
  title = {Identifying Bilingual Semantic Neural Representations across Languages},
  author = {Buchweitz, Augusto and Shinkareva, Svetlana V. and Mason, Robert A. and Mitchell, Tom M. and Just, Marcel Adam},
  year = {2012},
  month = mar,
  journal = {Brain and language},
  volume = {120},
  number = {3},
  pages = {282--289},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2011.09.003},
  abstract = {The goal of the study was to identify the neural representation of a noun's meaning in one language based on the neural representation of that same noun in another language. Machine learning methods were used to train classifiers to identify which individual noun bilingual participants were thinking about in one language based solely on their brain activation in the other language. The study shows reliable (p {$<$} .05) pattern-based classification accuracies for the classification of brain activity for nouns across languages. It also shows that the stable voxels used to classify the brain activation were located in areas associated with encoding information about semantic dimensions of the words in the study. The identification of the semantic trace of individual nouns from the pattern of cortical activity demonstrates the existence of a multi-voxel pattern of activation across the cortex for a single noun common to both languages in bilinguals.},
  pmcid = {PMC4620940},
  pmid = {21978845},
  file = {/Users/xzfang/Zotero/storage/VWBQDUXD/Buchweitz et al. - 2012 - Identifying bilingual semantic neural representati.pdf}
}

@article{bulkes_semantic_2020,
  title = {Semantic Constraint, Reading Control, and the Granularity of Form-Based Expectations during Semantic Processing: {{Evidence}} from {{ERPs}}},
  shorttitle = {Semantic Constraint, Reading Control, and the Granularity of Form-Based Expectations during Semantic Processing},
  author = {Bulkes, Nyssa Z. and Christianson, Kiel and Tanner, Darren},
  year = {2020},
  month = feb,
  journal = {Neuropsychologia},
  volume = {137},
  pages = {107294},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2019.107294},
  abstract = {We investigated the role that semantic constraint and participant control over stimulus presentation have on early stages of visual word recognition. Namely, we tested how the presence of a highly constraining sentential context influences the expectations that readers have during incremental sentence processing. Further, we tested whether allowing participants to self-pace the experiment affected early sensory perceptions of written stimuli. Event-related potentials (ERPs) were recorded in three experiments. Participants read sentences containing a target word from one of four conditions: 1) the target, spelled as expected; 2) the target with two internal characters transposed; 3) a nonword one vowel different from a target; or 4) an illegal consonant string. In Experiment 1, sentences were minimally constraining up to the target word (average cloze at target word: 0.01); in Experiments 2 and 3, sentences were highly constraining (average cloze at target word: 0.93). In both Experiments 1 and 2, sentences were presented using rapid-serial-visual presentation (RSVP). In Experiment 3, participants saw the same sentences used in Experiment 2 but were allowed to self-pace the presentation of each word in every trial. In Experiments 1 and 2, results showed early neural sensitivity to nonsensical consonant strings only, and only when they appeared within high constraint. In Experiment 3, results showed graded N170 effects to all target words containing unexpected visual information. P600 modulations were observed in all three experiments, indexing the difficulty of processing unexpected orthography, particularly in downstream, integrative processing. Results support a nuanced view of early visual processing, namely one arguing that visual processing is more fine-grained the more control participants have over how they read.},
  langid = {english},
  keywords = {ERP,Language comprehension,N170,Prediction,Visual word recognition},
  file = {/Users/xzfang/Zotero/storage/NFF9DHNH/Bulkes et al. - 2020 - Semantic constraint, reading control, and the gran.pdf;/Users/xzfang/Zotero/storage/Y39WQRDU/S0028393219303355.html}
}

@article{buonocore_postsaccadic_2020,
  title = {Post-{{Saccadic Face Processing Is Modulated}} by {{Pre-Saccadic Preview}}: {{Evidence}} from {{Fixation-Related Potentials}}},
  shorttitle = {Post-{{Saccadic Face Processing Is Modulated}} by {{Pre-Saccadic Preview}}},
  author = {Buonocore, Antimo and Dimigen, Olaf and Melcher, David},
  year = {2020},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {40},
  number = {11},
  pages = {2305--2313},
  file = {/Users/xzfang/Zotero/storage/QMZY38YA/2305.html}
}

@article{burchill_maintaining_2018,
  title = {Maintaining Information about Speech Input during Accent Adaptation},
  author = {Burchill, Zachary and Liu, Linda and Jaeger, T. Florian},
  year = {2018},
  month = aug,
  journal = {PLOS ONE},
  volume = {13},
  number = {8},
  pages = {e0199358},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0199358},
  abstract = {Speech understanding can be thought of as inferring progressively more abstract representations from a rapidly unfolding signal. One common view of this process holds that lower-level information is discarded as soon as higher-level units have been inferred. However, there is evidence that subcategorical information about speech percepts is not immediately discarded, but is maintained past word boundaries and integrated with subsequent input. Previous evidence for such subcategorical information maintenance has come from paradigms that lack many of the demands typical to everyday language use. We ask whether information maintenance is also possible under more typical constraints, and in particular whether it can facilitate accent adaptation. In a web-based paradigm, participants listened to isolated foreign-accented words in one of three conditions: subtitles were displayed concurrently with the speech, after speech offset, or not displayed at all. The delays between speech offset and subtitle presentation were manipulated. In a subsequent test phase, participants then transcribed novel words in the same accent without the aid of subtitles. We find that subtitles facilitate accent adaptation, even when displayed with a 6 second delay. Listeners thus maintained subcategorical information for sufficiently long to allow it to benefit adaptation. We close by discussing what type of information listeners maintain\textemdash subcategorical phonetic information, or just uncertainty about speech categories.},
  langid = {english},
  keywords = {Audio equipment,Language,Phonemes,Phonology,Speech,Speech signal processing,Syllables,Vowels},
  file = {/Users/xzfang/Zotero/storage/3Y59NJ2V/Burchill et al. - 2018 - Maintaining information about speech input during .pdf;/Users/xzfang/Zotero/storage/EQCWKJEQ/article.html}
}

@article{burkner_brms_2017,
  title = {{\textbf{Brms}} : {{An}} {{{\emph{R}}}} {{Package}} for {{Bayesian Multilevel Models Using}} {{{\emph{Stan}}}}},
  shorttitle = {{\textbf{Brms}}},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {80},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit \textendash{} among others \textendash{} linear, robust linear, binomial, Poisson, survival, response times, ordinal, quantile, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as metaanalytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared using posterior-predictive checks and leave-one-out crossvalidation. If you use brms, please cite this article as published in the Journal of Statistical Software (Bu\textasciidieresis rkner 2017).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NVZVFGM6/BÃ¼rkner - 2017 - brms  An R Package for Bayesian Mul.pdf}
}

@article{burr_personal_2002,
  title = {Personal Epistemology and Theory of Mind: Deciphering Young Children's Beliefs about Knowledge and Knowing},
  shorttitle = {Personal Epistemology and Theory of Mind},
  author = {Burr, Jean E and Hofer, Barbara K},
  year = {2002},
  month = aug,
  journal = {New Ideas in Psychology},
  series = {Folk {{Epistemology}}},
  volume = {20},
  number = {2},
  pages = {199--224},
  issn = {0732-118X},
  doi = {10.1016/S0732-118X(02)00010-7},
  abstract = {The beliefs that individuals hold about knowledge and knowing have been the focus of a growing body of work on ``personal epistemology.'' There has been general agreement among researchers about a developmental trajectory of epistemological understanding that takes place in adolescence and adulthood. Rarely has this research included children, however, and we know little about the origins of epistemological awareness or its early development. A separate group of researchers have investigated children's ``theory of mind,'' or the ability to understand others' beliefs, actions, and desires, with primary attention to the onset of this cognitive achievement between the ages of 3 and 5. This article reviews the theoretical foundation for a proposed relation between these constructs, and reports on an exploratory investigation with 3\textendash 5 year olds, in which epistemological level was significantly related to theory of mind ability. Results are discussed in relation to a general timeline depicting the development of children's beliefs about knowledge and knowing, a process that involves an ongoing tension between objective and subjective perspectives. We propose that the trajectory of epistemological development be expanded to include an initial period of egocentric subjectivity that characterizes epistemological thinking prior to the achievement of theory of mind.},
  langid = {english}
}

@article{burr_visual_2008,
  title = {A {{Visual Sense}} of {{Number}}},
  author = {Burr, David and Ross, John},
  year = {2008},
  month = mar,
  journal = {Current Biology},
  volume = {18},
  number = {6},
  pages = {425--428},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2008.02.052},
  abstract = {Evidence exists for a nonverbal capacity for the apprehension of number, in humans [1] (including infants 2, 3) and in other primates 4, 5, 6. Here, we show that perceived numerosity is susceptible to adaptation, like primary visual properties of a scene, such as color, contrast, size, and speed. Apparent numerosity was decreased by adaptation to large numbers of dots and increased by adaptation to small numbers, the effect depending entirely on the numerosity of the adaptor, not on contrast, size, orientation, or pixel density, and occurring with very low adaptor contrasts. We suggest that the visual system has the capacity to estimate numerosity and that it is an independent primary visual property, not reducible to others like spatial frequency or density of texture [7].},
  langid = {english},
  keywords = {SYSNEURO}
}

@article{bush_using_2015,
  title = {Using {{Grid Cells}} for {{Navigation}}},
  author = {Bush, Daniel and Barry, Caswell and Manson, Daniel and Burgess, Neil},
  year = {2015},
  month = aug,
  journal = {Neuron},
  volume = {87},
  number = {3},
  pages = {507--520},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2015.07.006},
  abstract = {Mammals are able to navigate to hidden goal locations by direct routes that may traverse previously unvisited terrain. Empirical evidence suggests that this ``vector navigation'' relies on an internal representation of space provided by the hippocampal formation. The periodic spatial firing patterns of grid cells in the hippocampal formation offer a compact combinatorial code for location within large-scale space. Here, we consider the computational problem of how to determine the vector between start and goal locations encoded by the firing of grid cells when this vector may be much longer than the largest grid scale. First, we present an algorithmic solution to the problem, inspired by the Fourier shift theorem. Second, we describe several potential neural network implementations of this solution that combine efficiency of search and biological plausibility. Finally, we discuss the empirical predictions of these implementations and their relationship to the anatomy and electrophysiology of the hippocampal formation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/B37MJWXR/Bush et al. - 2015 - Using Grid Cells for Navigation.pdf;/Users/xzfang/Zotero/storage/52RJHMG9/S0896627315006285.html}
}

@article{bushong_maintenance_2017,
  title = {Maintenance of Perceptual Information in Speech Perception},
  author = {Bushong, W. and Jaeger, T. Florian},
  year = {2017},
  journal = {In Proceedings of the 39th Annual Conference of the Cognitive Science Society (CogSci17),},
  pages = {pp. 1129--1134},
  file = {/Users/xzfang/Zotero/storage/J7FD9DBZ/paper0047.pdf}
}

@article{buxo-lugo_evidence_2016,
  title = {Evidence for the {{Influence}} of {{Syntax}} on {{Prosodic Parsing}}},
  author = {{Bux{\'o}-Lugo}, Andr{\'e}s and Watson, Duane G.},
  year = {2016},
  month = oct,
  journal = {Journal of memory and language},
  volume = {90},
  pages = {1--13},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2016.03.001},
  abstract = {We investigate whether expectations based on syntactic position influence the processing of intonational boundaries. In a boundary detection task, we manipulated a) the strength of cues to the presence of a boundary and b) whether or not a location in the sentence was a plausible location for an intonational boundary to occur given the syntactic structure. Listeners consistently reported hearing more boundaries at syntactically licensed locations than at syntactically unlicensed locations, even when the acoustic evidence for an intonational boundary was controlled. This suggests that the processing of an intonational boundary is a product of both acoustic cues and listener expectations.},
  pmcid = {PMC6407892},
  pmid = {30853752},
  file = {/Users/xzfang/Zotero/storage/5PVRD3RL/BuxÃ³-Lugo and Watson - 2016 - Evidence for the Influence of Syntax on Prosodic P.pdf}
}

@article{buxo-lugo_minor_2021,
  title = {Do {{Minor Thirds Characterize}} the {{Prosody}} of {{Sad Speech}}?},
  author = {{Bux{\'o}-Lugo}, Andr{\'e}s and Slevc, L. Robert},
  year = {2021},
  month = may,
  journal = {Auditory Perception \& Cognition},
  pages = {1--12},
  issn = {2574-2442, 2574-2450},
  doi = {10.1080/25742442.2021.1930465},
  abstract = {Pitch can convey information about emotion in both spoken language and in music. Given this, do people use pitch to communicate emotion in similar ways across both domains? To investigate this question we look at intervals between the fundamental frequency (f0) of adjacent syllables in emotional speech produced by actors. We first investigate whether descending minor third intervals are more prevalent in sad speech compared to other types of emotional speech, as has been reported previously. In these data, we see no evidence for descending minor thirds being characteristic of sad speech. In fact, we find little evidence for any specific musical intervals being associated with spe\- cific emotions in these longer sentences. We suggest that speakers might borrow emotional cues from music only when other prosodic options are infeasible.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JKZH2ZQK/BuxÃ³-Lugo and Slevc - 2021 - Do Minor Thirds Characterize the Prosody of Sad Sp.pdf}
}

@article{caffarra_not_2019,
  title = {Not All Errors Are the Same: {{ERP}} Sensitivity to Error Typicality in Foreign Accented Speech Perception},
  shorttitle = {Not All Errors Are the Same},
  author = {Caffarra, Sendy and Martin, Clara D.},
  year = {2019},
  month = jul,
  journal = {Cortex},
  series = {Structure in Words: The Present and Future of Morphological Processing in a Multidisciplinary Perspective},
  volume = {116},
  pages = {308--320},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.03.007},
  abstract = {Intercultural communication has become more and more frequent in the recent globalized society. When native listeners try to understand non-native speakers, they have to deal with different types of grammatical errors, some being frequently encountered and others being less common. The present Event-Related Potential (ERP) study investigated how native listeners process different types of morphosyntactic errors in foreign accented speech and whether they are sensitive to error typicality. Spanish natives listened to Spanish sentences in native and foreign (English) accent. ERPs were recorded in response to morphosyntactic violations that were commonly (gender errors) encountered in English accented Spanish or not (number errors). Although sentence comprehension accuracy did not differ across accents, the ERP responses changed as a function of accent and error type. In line with previous studies, gender and number violations in native accented speech elicited LAN-P600 responses. When speech was uttered by foreign speakers, number violations (uncommon errors) showed a P600 effect, while gender violations (common errors) did not elicit late repair processes (reflected by the P600) but an N400 effect. The present results provide evidence that the neural time course of parsing depends not only on speaker's accent, but also on input error typicality.},
  langid = {english},
  keywords = {ERP,Foreign accent,Morphosyntax,Sentence comprehension},
  file = {/Users/xzfang/Zotero/storage/L5WS3527/Caffarra and Martin - 2019 - Not all errors are the same ERP sensitivity to er.pdf;/Users/xzfang/Zotero/storage/WC6PATX3/S0010945218300881.html}
}

@article{calder-travis_explaining_2020,
  title = {Explaining the Effects of Distractor Statistics in Visual Search},
  author = {{Calder-Travis}, Joshua and Ma, Wei Ji},
  year = {2020},
  month = dec,
  journal = {Journal of Vision},
  volume = {20},
  number = {13},
  pages = {11--11},
  publisher = {{The Association for Research in Vision and Ophthalmology}},
  issn = {1534-7362},
  doi = {10.1167/jov.20.13.11},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HD3ZJQLS/Calder-Travis and Ma - 2020 - Explaining the effects of distractor statistics in.pdf;/Users/xzfang/Zotero/storage/3JCNLI7B/article.html}
}

@article{calma-roddin_music_2020,
  title = {Music, {{Language}}, and {{The N400}}: {{ERP Interference Patterns Across Cognitive Domains}}},
  shorttitle = {Music, {{Language}}, and {{The N400}}},
  author = {{Calma-Roddin}, Nicole and Drury, John E.},
  year = {2020},
  month = jul,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {11222},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-66732-0},
  abstract = {Studies of the relationship of language and music have suggested these two systems may share processing resources involved in the computation/maintenance of abstract hierarchical structure (syntax). One type of evidence comes from ERP interference studies involving concurrent language/music processing showing interaction effects when both processing streams are simultaneously perturbed by violations (e.g., syntactically incorrect words paired with incongruent completion of a chord progression). Here, we employ this interference methodology to target the mechanisms supporting long term memory (LTM) access/retrieval in language and music. We used melody stimuli from previous work showing out-of-key or unexpected notes may elicit a musical analogue of language N400 effects, but only for familiar melodies, and not for unfamiliar ones. Target notes in these melodies were time-locked to visually presented target words in sentence contexts manipulating lexical/conceptual semantic congruity. Our study succeeded in eliciting expected N400 responses from each cognitive domain independently. Among several new findings we argue to be of interest, these data demonstrate that: (i) language N400 effects are delayed in onset by concurrent music processing only when melodies are familiar, and (ii) double violations with familiar melodies (but not with unfamiliar ones) yield a sub-additive N400 response. In addition: (iii) early negativities (RAN effects), which previous work has connected to musical syntax, along with the music N400, were together delayed in onset for familiar melodies relative to the timing of these effects reported in the previous music-only study using these same stimuli, and (iv) double violation cases involving unfamiliar/novel melodies also delayed the RAN effect onset. These patterns constitute the first demonstration of N400 interference effects across these domains and together contribute previously undocumented types of interactions to the available pool of findings relevant to understanding whether language and music may rely on shared underlying mechanisms.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Attention,Cortex,Language,Long-term memory},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Attention;Cortex;Language;Long-term memory Subject\_term\_id: attention;cortex;language;long-term-memory},
  file = {/Users/xzfang/Zotero/storage/8NZFU46T/Calma-Roddin and Drury - 2020 - Music, Language, and The N400 ERP Interference Pa.pdf;/Users/xzfang/Zotero/storage/XP95KBE9/s41598-020-66732-0.html}
}

@article{camos_discontinuity_2008,
  title = {Discontinuity in the Enumeration of Sequentially Presented Auditory and Visual Stimuli},
  author = {Camos, Val{\'e}rie and Tillmann, Barbara},
  year = {2008},
  month = jun,
  journal = {Cognition},
  volume = {107},
  number = {3},
  pages = {1135--1143},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.11.002},
  abstract = {The seeking of discontinuity in enumeration was recently renewed because Cowan [Cowan, N. (2001). The magical number 4 in short-term memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences, 24, 87\textendash 185; Cowan, N. (2005). Working memory capacity. Hove: Psychology Press] suggested that it allows evaluating the limit of the focus of attention, currently estimated at four items. A strong argument in favour of a general constraint of the cognitive system is that similar discontinuities should be observed in modalities different from the classic simultaneous presentation of visual objects. Recently, data were provided on tactile stimuli, but the authors diverged in their conclusion about the existence of such discontinuity [Gallace, A., Tan, H. Z., \& Spence, C. (2006). Numerosity judgments for tactile stimuli distributed over the body surface. Perception, 35(2), 247\textendash 266; Riggs, K. J., Ferrand, L., Lancelin, D., Fryziel, L., Dumur, G., \& Simpson, A. (2006). Subitizing in tactile perception. Psychological Science, 17(4), 271\textendash 272]. Following a similar rationale, our study aimed at evaluating discontinuity in the enumeration of auditory and visual stimuli presented sequentially. The clear and similar discontinuity observed in error rates, response times and given responses for both modalities favours the general capacity limit view, but also questions the size of this capacity, because the discontinuity occurred here at size 2. However, the masking of stimuli in sensory memory could not be entirely discarded.},
  langid = {english},
  keywords = {Auditory stimuli,Counting,Focus of attention,Sequential presentation,Subitizing},
  file = {/Users/xzfang/Zotero/storage/P3XJJNJC/Camos and Tillmann - 2008 - Discontinuity in the enumeration of sequentially p.pdf;/Users/xzfang/Zotero/storage/QK6YBJZ5/S0010027707002739.html}
}

@article{caplan_now_,
  title = {Now {{You Hear Me}}, {{Later You Don}}'t: {{The Immediacy}} of {{Linguistic Computation}} and the {{Representation}} of {{Speech}}},
  author = {Caplan, Spencer and Hafri, Alon and Trueswell, John C},
  pages = {14},
  abstract = {What happens to an acoustic signal after it enters the mind of a listener? Previous work has demonstrated that listeners maintain intermediate representations over time. However, the internal structure of such representations\textemdash be they the acoustic-phonetic signal or more general information about the probability of possible categories\textemdash remains underspecified. We present two experiments using a novel speaker-adaptation paradigm aimed at uncovering the format of speech representations. We exposed adult listeners (N = 297) to a speaker whose utterances contained acoustically ambiguous information concerning phones (and thus words), and we manipulated the temporal availability of disambiguating cues via visually presented text (presented before or after each utterance). Results from a traditional phoneme-categorization task showed that listeners adapted to a modified acoustic distribution when disambiguating text was provided before but not after the audio. These results support the position that speech representations consist of activation over categories and are inconsistent with direct maintenance of the acoustic-phonetic signal.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FVC2TQ9E/Caplan et al. - Now You Hear Me, Later You Donâ€™t The Immediacy of.pdf}
}

@article{caplan_now_2021,
  title = {Now {{You Hear Me}}, {{Later You Don}}'t: {{The Immediacy}} of {{Linguistic Computation}} and the {{Representation}} of {{Speech}}},
  shorttitle = {Now {{You Hear Me}}, {{Later You Don}}'t},
  author = {Caplan, Spencer and Hafri, Alon and Trueswell, John C.},
  year = {2021},
  month = mar,
  journal = {Psychological Science},
  volume = {32},
  number = {3},
  pages = {410--423},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797620968787},
  abstract = {What happens to an acoustic signal after it enters the mind of a listener? Previous work has demonstrated that listeners maintain intermediate representations over time. However, the internal structure of such representations\textemdash be they the acoustic-phonetic signal or more general information about the probability of possible categories\textemdash remains underspecified. We present two experiments using a novel speaker-adaptation paradigm aimed at uncovering the format of speech representations. We exposed adult listeners (N = 297) to a speaker whose utterances contained acoustically ambiguous information concerning phones (and thus words), and we manipulated the temporal availability of disambiguating cues via visually presented text (presented before or after each utterance). Results from a traditional phoneme-categorization task showed that listeners adapted to a modified acoustic distribution when disambiguating text was provided before but not after the audio. These results support the position that speech representations consist of activation over categories and are inconsistent with direct maintenance of the acoustic-phonetic signal.},
  langid = {english},
  keywords = {acoustic maintenance,immediacy of computation,language,mental representation,open data,open materials,preregistered,speech processing},
  file = {/Users/xzfang/Zotero/storage/QPLEFMZT/Caplan et al. - 2021 - Now You Hear Me, Later You Donâ€™t The Immediacy of.pdf}
}

@article{carandini_normalization_2011,
  title = {Normalization as a Canonical Neural Computation},
  author = {Carandini, Matteo and Heeger, David J.},
  year = {2011},
  month = nov,
  journal = {Nature reviews. Neuroscience},
  volume = {13},
  number = {1},
  pages = {51--62},
  issn = {1471-003X},
  doi = {10.1038/nrn3136},
  abstract = {There is increasing evidence that the brain relies on a set of canonical neural computations, repeating them across brain regions and modalities to apply similar operations to different problems. A promising candidate for such a computation is normalization, in which the responses of neurons are divided by a common factor that typically includes the summed activity of a pool of neurons. Normalization was developed to explain responses in the primary visual cortex and is now thought to operate throughout the visual system, and in many other sensory modalities and brain regions. Normalization may underlie operations such as the representation of odours, the modulatory effects of visual attention, the encoding of value and the integration of multisensory information. Its presence in such a diversity of neural systems in multiple species, from invertebrates to mammals, suggests that it serves as a canonical neural computation.},
  pmcid = {PMC3273486},
  pmid = {22108672},
  file = {/Users/xzfang/Zotero/storage/XGWLXZJM/Carandini and Heeger - 2011 - Normalization as a canonical neural computation.pdf}
}

@article{carey_neuroscience_1998,
  title = {{{NEUROSCIENCE}}:{{Knowledge}} of {{Number}}: {{Its Evolution}} and {{Ontogeny}}},
  shorttitle = {{{NEUROSCIENCE}}},
  author = {Carey, S.},
  year = {1998},
  month = oct,
  journal = {Science},
  volume = {282},
  number = {5389},
  pages = {641--642},
  doi = {10.1126/science.282.5389.641},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZEFVRL9L/Carey - 1998 - NEUROSCIENCEKnowledge of Number Its Evolution an.pdf}
}

@article{cargnelutti_language_2019,
  title = {Language {{Brain Representation}} in {{Bilinguals With Different Age}} of {{Appropriation}} and {{Proficiency}} of the {{Second Language}}: {{A Meta-Analysis}} of {{Functional Imaging Studies}}},
  shorttitle = {Language {{Brain Representation}} in {{Bilinguals With Different Age}} of {{Appropriation}} and {{Proficiency}} of the {{Second Language}}},
  author = {Cargnelutti, Elisa and Tomasino, Barbara and Fabbro, Franco},
  year = {2019},
  journal = {Frontiers in Human Neuroscience},
  volume = {13},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2019.00154},
  abstract = {Language representation in the bilingual brain is the result of many factors, of which age of appropriation (AoA) and proficiency of the second language (L2) are probably the most studied. Many studies indeed compared early and late bilinguals, although it is not clear the role of the so-called critical period for L2 learning. In this study, we carried out coordinate-based meta-analysis to address this issue and to inspect the role of proficiency besides that of AoA. After the preliminary inspection of the early (also very early) and late bilinguals' language networks, we explored the specific activations associated with each language and compared them within and between the groups. Results confirmed that the language network associated with L2 was wider than that associated with L1 irrespective to AoA, although differences were more relevant in the late bilinguals' group. In particular, L2 entailed a greater enrollment of the brain areas devoted to executive functions, and this was observed also in proficient bilinguals.. Also the early bilinguals showed a wide language network and as well activated areas involved in cognitive control. Interestingly, these regions activated even in L1 of both early and late bilinguals groups, although less consistently. Overall, these findings suggest that bilinguals in general are constantly subjected to cognitive effort to monitor and regulate the language use, although early AoA and high proficiency are likely to reduce it.},
  langid = {english},
  keywords = {age of appropriation (AoA),bilingualism,first language (L1),Meta-analysis,proficiency,second language (L2)},
  file = {/Users/xzfang/Zotero/storage/NJZZHZDZ/Cargnelutti et al. - 2019 - Language Brain Representation in Bilinguals With D.pdf}
}

@article{carlisle_attentional_2011,
  title = {Attentional {{Templates}} in {{Visual Working Memory}}},
  author = {Carlisle, N. B. and Arita, J. T. and Pardo, D. and Woodman, G. F.},
  year = {2011},
  month = jun,
  journal = {Journal of Neuroscience},
  volume = {31},
  number = {25},
  pages = {9315--9322},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1097-11.2011},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PMKKE4K8/Carlisle et al. - 2011 - Attentional Templates in Visual Working Memory.pdf}
}

@article{carney_speech_2015,
  title = {Speech {{Coding}} in the {{Brain}}: {{Representation}} of {{Vowel Formants}} by {{Midbrain Neurons Tuned}} to {{Sound Fluctuations}},,},
  shorttitle = {Speech {{Coding}} in the {{Brain}}},
  author = {Carney, Laurel H. and Li, Tianhao and McDonough, Joyce M.},
  year = {2015},
  month = jul,
  journal = {eNeuro},
  volume = {2},
  number = {4},
  pages = {ENEURO.0004-15.2015},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0004-15.2015},
  abstract = {Current models for neural coding of vowels are typically based on linear descriptions of the auditory periphery, and fail at high sound levels and in background noise. These models rely on either auditory nerve discharge rates or phase locking to temporal fine structure. However, both discharge rates and phase locking saturate at moderate to high sound levels, and phase locking is degraded in the CNS at middle to high frequencies. The fact that speech intelligibility is robust over a wide range of sound levels is problematic for codes that deteriorate as the sound level increases. Additionally, a successful neural code must function for speech in background noise at levels that are tolerated by listeners. The model presented here resolves these problems, and incorporates several key response properties of the nonlinear auditory periphery, including saturation, synchrony capture, and phase locking to both fine structure and envelope temporal features. The model also includes the properties of the auditory midbrain, where discharge rates are tuned to amplitude fluctuation rates. The nonlinear peripheral response features create contrasts in the amplitudes of low-frequency neural rate fluctuations across the population. These patterns of fluctuations result in a response profile in the midbrain that encodes vowel formants over a wide range of levels and in background noise. The hypothesized code is supported by electrophysiological recordings from the inferior colliculus of awake rabbits. This model provides information for understanding the structure of cross-linguistic vowel spaces, and suggests strategies for automatic formant detection and speech enhancement for listeners with hearing loss.},
  pmcid = {PMC4596011},
  pmid = {26464993},
  file = {/Users/xzfang/Zotero/storage/XQQFNIXM/Carney et al. - 2015 - Speech Coding in the Brain Representation of Vowe.pdf}
}

@article{carpenter_stan_2017,
  title = {Stan: {{A Probabilistic Programming Language}}},
  shorttitle = {Stan},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  month = jan,
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  copyright = {Copyright (c) 2017 Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, Allen Riddell},
  langid = {english},
  keywords = {algorithmic differentiation,Bayesian inference,probabilistic programming,Stan},
  file = {/Users/xzfang/Zotero/storage/VFWH6IKK/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf;/Users/xzfang/Zotero/storage/4CLB9UXK/v076i01.html;/Users/xzfang/Zotero/storage/JTHRIAFT/v076i01.html}
}

@article{carr_algorithms_2021,
  title = {Algorithms for the Automated Correction of Vertical Drift in Eye-Tracking Data},
  author = {Carr, Jon W. and Pescuma, Valentina N. and Furlan, Michele and Ktori, Maria and Crepaldi, Davide},
  year = {2021},
  month = jun,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01554-0},
  abstract = {A common problem in eye-tracking research is vertical drift\textemdash the progressive displacement of fixation registrations on the vertical axis that results from a gradual loss of eye-tracker calibration over time. This is particularly problematic in experiments that involve the reading of multiline passages, where it is critical that fixations on one line are not erroneously recorded on an adjacent line. Correction is often performed manually by the researcher, but this process is tedious, time-consuming, and prone to error and inconsistency. Various methods have previously been proposed for the automated, post hoc correction of vertical drift in reading data, but these methods vary greatly, not just in terms of the algorithmic principles on which they are based, but also in terms of their availability, documentation, implementation languages, and so forth. Furthermore, these methods have largely been developed in isolation with little attempt to systematically evaluate them, meaning that drift correction techniques are moving forward blindly. We document ten major algorithms, including two that are novel to this paper, and evaluate them using both simulated and natural eye-tracking data. Our results suggest that a method based on dynamic time warping offers great promise, but we also find that some algorithms are better suited than others to particular types of drift phenomena and reading behavior, allowing us to offer evidence-based advice on algorithm selection.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6EDLNJQC/Carr et al. - 2021 - Algorithms for the automated correction of vertica.pdf}
}

@article{carrasco_eccentricity_1995,
  title = {The Eccentricity Effect: {{Target}} Eccentricity Affects Performance on Conjunction Searches},
  shorttitle = {The Eccentricity Effect},
  author = {Carrasco, Marisa and Evert, Denise L. and Chang, Irene and Katz, Svetlana M.},
  year = {1995},
  month = nov,
  journal = {Perception \& Psychophysics},
  volume = {57},
  number = {8},
  pages = {1241--1261},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03208380},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5VNJ9J79/Carrasco et al. - 1995 - The eccentricity effect Target eccentricity affec.pdf}
}

@article{carreiras_what_2014,
  title = {The What, When, Where, and How of Visual Word Recognition},
  author = {Carreiras, Manuel and Armstrong, Blair C. and Perea, Manuel and Frost, Ram},
  year = {2014},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {18},
  number = {2},
  pages = {90--98},
  issn = {13646613},
  doi = {10.1016/j.tics.2013.11.005},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WWLCGMJQ/Carreiras et al. - 2014 - The what, when, where, and how of visual word reco.pdf}
}

@misc{casalnuovo_dataset_2020,
  title = {Dataset, {{Survey}}, and {{R Notebooks}} for "{{Does Surprisal Predict Code Comprehension Difficulty}}?"},
  shorttitle = {Dataset, {{Survey}}, and {{R Notebooks}} for "{{Does Surprisal Predict Code Comprehension Difficulty}}?},
  author = {Casalnuovo, Casey and Devanbu, Prem and Morgan, Emily},
  year = {2020},
  month = may,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.3626129},
  abstract = {(Version 1.1 Update) - Removed anonymization after reviewing period ended and paper was accepted at Cogsci 2020. - Added Qualtrics Survey in exported form Dataset and R analysis scripts for the paper "Does Surprisal Predict Code Comprehension Difficulty?". For more details, see "ComprehensionREADME.md" in the included zip file.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {english},
  keywords = {Code Comprehension,Human Subject Study,Meaning Preserving Transformations,Transformer Language Model},
  file = {/Users/xzfang/Zotero/storage/AVI25L5R/Casalnuovo et al. - 2020 - Dataset, Survey, and R Notebooks for Does Surpris.pdf}
}

@article{casalnuovo_programmers_2020,
  title = {Do {{Programmers Prefer Predictable Expressions}} in {{Code}}?},
  author = {Casalnuovo, Casey and Lee, Kevin and Wang, Hulin and Devanbu, Prem and Morgan, Emily},
  year = {2020},
  month = dec,
  journal = {Cognitive Science},
  volume = {44},
  number = {12},
  issn = {0364-0213, 1551-6709},
  doi = {10.1111/cogs.12921},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6HB23LHB/Casalnuovo et al. - 2020 - Do Programmers Prefer Predictable Expressions in C.pdf}
}

@inproceedings{casey_population_2012,
  title = {Population {{Codes Representing Musical Timbre}} for {{High-Level fMRI Categorization}} of {{Music Genres}}},
  booktitle = {Machine {{Learning}} and {{Interpretation}} in {{Neuroimaging}}},
  author = {Casey, Michael and Thompson, Jessica and Kang, Olivia and Raizada, Rajeev and Wheatley, Thalia},
  editor = {Langs, Georg and Rish, Irina and {Grosse-Wentrup}, Moritz and Murphy, Brian},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {34--41},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-34713-9_5},
  abstract = {We present experimental evidence in support of distributed neural codes for timbre that are implicated in discrimination of musical styles. We used functional magnetic resonance imaging (fMRI) in humans and multivariate pattern analysis (MVPA) to identify activation patterns that encode the perception of rich music audio stimuli from five different musical styles. We show that musical styles can be automatically classified from population codes in bilateral superior temporal sulcus (STS). To investigate the possible link between the acoustic features of the auditory stimuli and neural population codes in STS, we conducted a representational similarity analysis and a multivariate regression-retrieval task. We found that the similarity structure of timbral features of our stimuli resembled the similarity structure of the STS more than any other type of acoustic feature. We also found that a regression model trained on timbral features outperformed models trained on other types of audio features. Our results show that human brain responses to complex, natural music can be differentiated by timbral audio features, emphasizing the importance of timbre in auditory perception.},
  isbn = {978-3-642-34713-9},
  langid = {english},
  keywords = {cepstrum,multivariate analysis,music,STS,timbre code}
}

@article{castellucci_speech_2022,
  title = {A Speech Planning Network for Interactive Language Use},
  author = {Castellucci, Gregg A. and Kovach, Christopher K. and Howard, Matthew A. and Greenlee, Jeremy D. W. and Long, Michael A.},
  year = {2022},
  month = feb,
  journal = {Nature},
  volume = {602},
  number = {7895},
  pages = {117--122},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04270-z},
  abstract = {During conversation, people take turns speaking by rapidly responding to their partners while simultaneously avoiding interruption1,2. Such interactions display a remarkable degree of coordination, as gaps between turns are typically about 200\,milliseconds3\textemdash approximately the duration of an eyeblink4. These latencies are considerably shorter than those observed in simple word-production tasks, which indicates that speakers often plan their responses while listening to their partners2. Although a distributed network of brain regions has been implicated in speech planning5\textendash 9, the neural dynamics underlying the specific preparatory processes that enable rapid turn-taking are poorly understood. Here we use intracranial electrocorticography to precisely measure neural activity as participants perform interactive tasks, and we observe a functionally and anatomically distinct class of planning-related cortical dynamics. We localize these responses to a frontotemporal circuit centred on the language-critical caudal inferior frontal cortex10 (Broca's region) and the caudal middle frontal gyrus\textemdash a region not normally implicated in speech planning11\textendash 13. Using a series of motor tasks, we then show that this planning network is more active when preparing speech as opposed to non-linguistic actions. Finally, we delineate planning-related circuitry during natural conversation that is nearly identical to the network mapped with our interactive tasks, and we find this circuit to be most active before participant speech during unconstrained turn-taking. Therefore, we have identified a speech planning network that is central to natural language generation during social interaction.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Cognitive control,Cooperation},
  file = {/Users/xzfang/Zotero/storage/7TUM5WAL/Castellucci et al. - 2022 - A speech planning network for interactive language.pdf;/Users/xzfang/Zotero/storage/SFDE6XWX/s41586-021-04270-z.html}
}

@article{castles_ending_2018,
  title = {Ending the {{Reading Wars}}: {{Reading Acquisition From Novice}} to {{Expert}}},
  shorttitle = {Ending the {{Reading Wars}}},
  author = {Castles, Anne and Rastle, Kathleen and Nation, Kate},
  year = {2018},
  month = jun,
  journal = {Psychological Science in the Public Interest},
  volume = {19},
  number = {1},
  pages = {5--51},
  publisher = {{SAGE Publications Inc}},
  issn = {1529-1006},
  doi = {10.1177/1529100618772271},
  abstract = {There is intense public interest in questions surrounding how children learn to read and how they can best be taught. Research in psychological science has provided answers to many of these questions but, somewhat surprisingly, this research has been slow to make inroads into educational policy and practice. Instead, the field has been plagued by decades of ``reading wars.'' Even now, there remains a wide gap between the state of research knowledge about learning to read and the state of public understanding. The aim of this article is to fill this gap. We present a comprehensive tutorial review of the science of learning to read, spanning from children's earliest alphabetic skills through to the fluent word recognition and skilled text comprehension characteristic of expert readers. We explain why phonics instruction is so central to learning in a writing system such as English. But we also move beyond phonics, reviewing research on what else children need to learn to become expert readers and considering how this might be translated into effective classroom practice. We call for an end to the reading wars and recommend an agenda for instruction and research in reading acquisition that is balanced, developmentally informed, and based on a deep understanding of how language and writing systems work.},
  langid = {english},
  keywords = {language,phonics,reading,reading acquisition,text comprehension},
  file = {/Users/xzfang/Zotero/storage/EZWRNM5Z/Castles et al. - 2018 - Ending the Reading Wars Reading Acquisition From .pdf}
}

@inproceedings{caucheteux_disentangling_2021,
  title = {Disentangling Syntax and Semantics in the Brain with Deep Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
  year = {2021},
  month = jul,
  pages = {1336--1348},
  publisher = {{PMLR}},
  issn = {2640-3498},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PWJDMXEN/Caucheteux et al. - 2021 - Disentangling syntax and semantics in the brain wi.pdf;/Users/xzfang/Zotero/storage/XZ9XLWQR/Caucheteux et al. - 2021 - Disentangling syntax and semantics in the brain wi.pdf}
}

@article{caucheteux_gpt2_2021,
  title = {{{GPT-2}}'s Activations Predict the Degree of Semantic Comprehension in the Human Brain},
  author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  year = {2021},
  month = jun,
  journal = {bioRxiv},
  pages = {2021.04.20.440622},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.04.20.440622},
  abstract = {{$<$}p{$>$}Language transformers, like GPT-2, have demonstrated remarkable abilities to process text, and now constitute the backbone of deep translation, summarization and dialogue algorithms. However, whether these models actually understand language is highly controversial. Here, we show that the representations of GPT-2 not only map onto the brain responses to spoken stories, but also predict the extent to which subjects understand the narratives. To this end, we analyze 101 subjects recorded with functional Magnetic Resonance Imaging while listening to 70 min of short stories. We then fit a linear model to predict brain activity from GPT-2 activations, and correlate this mapping with subjects9 comprehension scores as assessed for each story. The results show that GPT-29s brain predictions significantly correlate with semantic comprehension. These effects are bilaterally distributed in the language network and peak with a correlation above 30\% in the infero-frontal and medio-temporal gyri as well as in the superior frontal cortex, the planum temporale and the precuneus. Overall, this study provides an empirical framework to probe and dissect semantic comprehension in brains and deep learning algorithms.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/I7ACGX8Q/Caucheteux et al. - 2021 - GPT-2â€™s activations predict the degree of semantic.pdf;/Users/xzfang/Zotero/storage/43CN7X6T/2021.04.20.440622v2.html}
}

@article{caucheteux_language_2020,
  title = {Language Processing in Brains and Deep Neural Networks: Computational Convergence and Its Limits},
  shorttitle = {Language Processing in Brains and Deep Neural Networks},
  author = {Caucheteux, Charlotte and King, Jean-R{\'e}mi},
  year = {2020},
  month = jul,
  journal = {bioRxiv},
  pages = {2020.07.03.186288},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.07.03.186288},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Deep learning has recently allowed substantial progress in language tasks such as translation and completion. Do such models process language similarly to humans, and is this similarity driven by systematic structural, functional and learning principles? To address these issues, we tested whether the activations of 7,400 artificial neural networks trained on image, word and sentence processing linearly map onto the hierarchy of human brain responses elicited during a reading task, using source-localized magneto-encephalography (MEG) recordings of one hundred and four subjects. Our results confirm that visual, word and language models sequentially correlate with distinct areas of the left-lateralized cortical hierarchy of reading. However, only specific subsets of these models converge towards brain-like representations during their training. Specifically, when the algorithms are trained on language modeling, their middle layers become increasingly similar to the late responses of the language network in the brain. By contrast, input and output word embedding layers often diverge away from brain activity during training. These differences are primarily rooted in the sustained and bilateral responses of the temporal and frontal cortices. Together, these results suggest that the compositional - but not the lexical - representations of modern language models converge to a brain-like solution.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TVU6BWVS/Caucheteux and King - 2020 - Language processing in brains and deep neural netw.pdf;/Users/xzfang/Zotero/storage/UU4MYS8H/2020.07.03.186288v1.html}
}

@article{chadwick_semantic_2016,
  title = {Semantic Representations in the Temporal Pole Predict False Memories},
  author = {Chadwick, Martin J. and Anjum, Raeesa S. and Kumaran, Dharshan and Schacter, Daniel L. and Spiers, Hugo J. and Hassabis, Demis},
  year = {2016},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {36},
  pages = {10180--10185},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1610686113},
  abstract = {Recent advances in neuroscience have given us unprecedented insight into the neural mechanisms of false memory, showing that artificial memories can be inserted into the memory cells of the hippocampus in a way that is indistinguishable from true memories. However, this alone is not enough to explain how false memories can arise naturally in the course of our daily lives. Cognitive psychology has demonstrated that many instances of false memory, both in the laboratory and the real world, can be attributed to semantic interference. Whereas previous studies have found that a diverse set of regions show some involvement in semantic false memory, none have revealed the nature of the semantic representations underpinning the phenomenon. Here we use fMRI with representational similarity analysis to search for a neural code consistent with semantic false memory. We find clear evidence that false memories emerge from a similarity-based neural code in the temporal pole, a region that has been called the ``semantic hub'' of the brain. We further show that each individual has a partially unique semantic code within the temporal pole, and this unique code can predict idiosyncratic patterns of memory errors. Finally, we show that the same neural code can also predict variation in true-memory performance, consistent with an adaptive perspective on false memory. Taken together, our findings reveal the underlying structure of neural representations of semantic knowledge, and how this semantic structure can both enhance and distort our memories.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GTBMNPIR/Chadwick et al. - 2016 - Semantic representations in the temporal pole pred.pdf}
}

@article{chan_decoding_2011,
  title = {Decoding Word and Category-Specific Spatiotemporal Representations from {{MEG}} and {{EEG}}},
  author = {Chan, Alexander M. and Halgren, Eric and Marinkovic, Ksenija and Cash, Sydney S.},
  year = {2011},
  month = feb,
  journal = {NeuroImage},
  volume = {54},
  number = {4},
  pages = {3028--3039},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2010.10.073},
  abstract = {The organization and localization of lexico-semantic information in the brain has been debated for many years. Specifically, lesion and imaging studies have attempted to map the brain areas representing living versus nonliving objects, however, results remain variable. This may be due, in part, to the fact that the univariate statistical mapping analyses used to detect these brain areas are typically insensitive to subtle, but widespread, effects. Decoding techniques, on the other hand, allow for a powerful multivariate analysis of multichannel neural data. In this study, we utilize machine-learning algorithms to first demonstrate that semantic category, as well as individual words, can be decoded from EEG and MEG recordings of subjects performing a language task. Mean accuracies of 76\% (chance=50\%) and 83\% (chance=20\%) were obtained for the decoding of living vs. nonliving category or individual words respectively. Furthermore, we utilize this decoding analysis to demonstrate that the representations of words and semantic category are highly distributed both spatially and temporally. In particular, bilateral anterior temporal, bilateral inferior frontal, and left inferior temporal-occipital sensors are most important for discrimination. Successful intersubject and intermodality decoding shows that semantic representations between stimulus modalities and individuals are reasonably consistent. These results suggest that both word and category-specific information are present in extracranially recorded neural activity and that these representations may be more distributed, both spatially and temporally, than previous studies suggest.},
  langid = {english},
  keywords = {Decoding,EEG,Language,Machine learning,MEG,Semantic category},
  file = {/Users/xzfang/Zotero/storage/7HTKZ2ZL/Chan et al. - 2011 - Decoding word and category-specific spatiotemporal.pdf;/Users/xzfang/Zotero/storage/C8CRSFCP/S1053811910013819.html}
}

@article{chang_brain_2009,
  title = {Brain Activation Abnormalities during Speech and Non-Speech in Stuttering Speakers},
  author = {Chang, Soo-Eun and Kenney, Mary Kay and Loucks, Torrey M. J. and Ludlow, Christy L.},
  year = {2009},
  month = may,
  journal = {NeuroImage},
  volume = {46},
  number = {1},
  pages = {201--212},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2009.01.066},
  abstract = {Although stuttering is regarded as a speech-specific disorder, there is a growing body of evidence suggesting that subtle abnormalities in the motor planning and execution of non-speech gestures exist in stuttering individuals. We hypothesized that people who stutter (PWS) would differ from fluent controls in their neural responses during motor planning and execution of both speech and non-speech gestures that had auditory targets. Using fMRI with sparse sampling, separate BOLD responses were measured for perception, planning, and fluent production of speech and non-speech vocal tract gestures. During both speech and non-speech perception and planning, PWS had less activation in the frontal and temporoparietal regions relative to controls. During speech and non-speech production, PWS had less activation than the controls in the left superior temporal gyrus (STG) and the left pre-motor areas (BA 6) but greater activation in the right STG, bilateral Heschl's gyrus (HG), insula, putamen, and precentral motor regions (BA 4). Differences in brain activation patterns between PWS and controls were greatest in females and less apparent in males. In conclusion, similar differences in PWS from the controls were found during speech and non-speech; during perception and planning they had reduced activation while during production they had increased activity in the auditory area on the right and decreased activation in the left sensorimotor regions. These results demonstrated that neural activation differences in PWS are not speech-specific.},
  langid = {english},
  keywords = {Auditoryâ€“motor interaction,Forward model,Functional magnetic resonance imaging (fMRI),Non-speech,Planning,Production,Speech perception,Stuttering},
  file = {/Users/xzfang/Zotero/storage/7XG7V8SK/Chang et al. - 2009 - Brain activation abnormalities during speech and n.pdf;/Users/xzfang/Zotero/storage/YKDZJM2S/S1053811909001372.html}
}

@article{chang_endogenous_2021,
  title = {Endogenous Variation in Ventromedial Prefrontal Cortex State Dynamics during Naturalistic Viewing Reflects Affective Experience},
  author = {Chang, Luke J. and Jolly, Eshin and Cheong, Jin Hyun and Rapuano, Kristina M. and Greenstein, Nathan and Chen, Pin-Hao A. and Manning, Jeremy R.},
  year = {2021},
  month = apr,
  journal = {Science Advances},
  volume = {7},
  number = {17},
  pages = {eabf7129},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abf7129},
  abstract = {How we process ongoing experiences is shaped by our personal history, current needs, and future goals. Consequently, ventromedial prefrontal cortex (vmPFC) activity involved in processing these subjective appraisals appears to be highly idiosyncratic across individuals. To elucidate the role of the vmPFC in processing our ongoing experiences, we developed a computational framework and analysis pipeline to characterize the spatiotemporal dynamics of individual vmPFC responses as participants viewed a 45-minute television drama. Through a combination of functional magnetic resonance imaging, facial expression tracking, and self-reported emotional experiences across four studies, our data suggest that the vmPFC slowly transitions through a series of discretized states that broadly map onto affective experiences. Although these transitions typically occur at idiosyncratic times across people, participants exhibited a marked increase in state alignment during high affectively valenced events in the show. Our work suggests that the vmPFC ascribes affective meaning to our ongoing experiences. The vmPFC ascribes affective meaning to experiences as they unfold and exhibits unique idiographic spatiotemporal dynamics. The vmPFC ascribes affective meaning to experiences as they unfold and exhibits unique idiographic spatiotemporal dynamics.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License 4.0 (CC BY).. https://creativecommons.org/licenses/by/4.0/This is an open-access article distributed under the terms of the Creative Commons Attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
  langid = {english},
  pmid = {33893106},
  file = {/Users/xzfang/Zotero/storage/FGAK39VQ/Chang et al. - 2021 - Endogenous variation in ventromedial prefrontal co.pdf;/Users/xzfang/Zotero/storage/YLL9V7ZM/eabf7129.html}
}

@article{chang_relating_2021,
  title = {Relating the {{Past}} with the {{Present}}: {{Information Integration}} and {{Segregation}} during {{Ongoing Narrative Processing}}},
  shorttitle = {Relating the {{Past}} with the {{Present}}},
  author = {Chang, Claire H. C. and Lazaridi, Christina and Yeshurun, Yaara and Norman, Kenneth A. and Hasson, Uri},
  year = {2021},
  month = may,
  journal = {Journal of Cognitive Neuroscience},
  volume = {33},
  number = {6},
  pages = {1106--1128},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01707},
  abstract = {This study examined how the brain dynamically updates event representations by integrating new information over multiple minutes while segregating irrelevant input. A professional writer custom-designed a narrative with two independent storylines, interleaving across minute-long segments (ABAB). In the last (C) part, characters from the two storylines meet and their shared history is revealed. Part C is designed to induce the spontaneous recall of past events, upon the recurrence of narrative motifs from A/B, and to shed new light on them. Our fMRI results showed storyline-specific neural patterns, which were reinstated (i.e., became more active) during storyline transitions. This effect increased along the processing timescale hierarchy, peaking in the default mode network. Similarly, the neural reinstatement of motifs was found during Part C. Furthermore, participants showing stronger motif reinstatement performed better in integrating A/B and C events, demonstrating the role of memory reactivation in information integration over intervening irrelevant events.},
  file = {/Users/xzfang/Zotero/storage/DHVLLS88/Chang et al. - 2021 - Relating the Past with the Present Information In.pdf}
}

@article{chang_unconsciously_2016,
  title = {Unconsciously Elicited Perceptual Prior},
  author = {Chang, Raymond and Baria, Alexis T. and Flounders, Matthew W. and He, Biyu J.},
  year = {2016},
  month = jan,
  journal = {Neuroscience of Consciousness},
  volume = {2016},
  number = {niw008},
  issn = {2057-2107},
  doi = {10.1093/nc/niw008},
  abstract = {Increasing evidence over the past decade suggests that vision is not simply a passive, feed-forward process in which cortical areas relay progressively more abstract information to those higher up in the visual hierarchy, but rather an inferential process with top-down processes actively guiding and shaping perception. However, one major question that persists is whether such processes can be influenced by unconsciously perceived stimuli. Recent psychophysics and neuroimaging studies have revealed that while consciously perceived stimuli elicit stronger responses in higher visual and frontoparietal areas than those that fail to reach conscious awareness, the latter can still drive high-level brain and behavioral responses. We investigated whether unconscious processing of a masked natural image could facilitate subsequent conscious recognition of its degraded counterpart (a black-and-white ``Mooney'' image) presented many seconds later. We found that this is indeed the case, suggesting that conscious vision may be influenced by priors established by unconscious processing of a fleeting image.},
  file = {/Users/xzfang/Zotero/storage/NRRP7WI6/Chang et al. - 2016 - Unconsciously elicited perceptual prior.pdf;/Users/xzfang/Zotero/storage/UPS28WFQ/2757128.html}
}

@article{chao_speakertargeted_2019,
  title = {Speaker-{{Targeted Audio-Visual Models}} for {{Speech Recognition}} in {{Cocktail-Party Environments}}},
  author = {Chao, Guan-Lin and Chan, William and Lane, Ian},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.05962 [cs, eess]},
  eprint = {1906.05962},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Speech recognition in cocktail-party environments remains a significant challenge for state-of-the-art speech recognition systems, as it is extremely difficult to extract an acoustic signal of an individual speaker from a background of overlapping speech with similar frequency and temporal characteristics. We propose the use of speaker-targeted acoustic and audio-visual models for this task. We complement the acoustic features in a hybrid DNN-HMM model with information of the target speaker's identity as well as visual features from the mouth region of the target speaker. Experimentation was performed using simulated cocktail-party data generated from the GRID audio-visual corpus by overlapping two speakers's speech on a single acoustic channel. Our audio-only baseline achieved a WER of 26.3\%. The audio-visual model improved the WER to 4.4\%. Introducing speaker identity information had an even more pronounced effect, improving the WER to 3.6\%. Combining both approaches, however, did not significantly improve performance further. Our work demonstrates that speaker-targeted models can significantly improve the speech recognition in cocktailparty environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/xzfang/Zotero/storage/L285BRW2/Chao et al. - 2019 - Speaker-Targeted Audio-Visual Models for Speech Re.pdf}
}

@article{chartier_encoding_2018,
  title = {Encoding of {{Articulatory Kinematic Trajectories}} in {{Human Speech Sensorimotor Cortex}}},
  author = {Chartier, Josh and Anumanchipalli, Gopala K. and Johnson, Keith and Chang, Edward F.},
  year = {2018},
  month = jun,
  journal = {Neuron},
  volume = {98},
  number = {5},
  pages = {1042-1054.e4},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.04.031},
  abstract = {When speaking, we dynamically coordinate movements of our jaw, tongue, lips, and larynx. To investigate the neural mechanisms underlying articulation, we used direct cortical recordings from human sensorimotor cortex while participants spoke natural sentences that included sounds spanning the entire English phonetic inventory. We used deep neural networks to infer speakers' articulator movements from produced speech acoustics. Individual electrodes encoded a diversity of articulatory kinematic trajectories (AKTs), each revealing coordinated articulator movements toward specific vocal tract shapes. AKTs captured a wide range of movement types, yet they could be differentiated by the place of vocal tract constriction. Additionally, AKTs manifested out-and-back trajectories with harmonic oscillator dynamics. While AKTs were functionally stereotyped across different sentences, context-dependent encoding of preceding and following movements during production of the same phoneme demonstrated the cortical representation of coarticulation. Articulatory movements encoded in sensorimotor cortex give rise to the complex kinematics underlying continuous speech production. Video Abstract},
  langid = {english},
  keywords = {articulation,coordination,decoding,electrocorticography,encoding,movement,sensorimotor cortex,speech production,trajectory},
  file = {/Users/xzfang/Zotero/storage/LWQP92CZ/Chartier et al. - 2018 - Encoding of Articulatory Kinematic Trajectories in.pdf;/Users/xzfang/Zotero/storage/UENTGRBJ/S0896627318303398.html}
}

@article{chehab_deep_2021,
  title = {Deep {{Recurrent Encoder}}: {{A}} Scalable End-to-End Network to Model Brain Signals},
  shorttitle = {Deep {{Recurrent Encoder}}},
  author = {Chehab, Omar and Defossez, Alexandre and Loiseau, Jean-Christophe and Gramfort, Alexandre and King, Jean-Remi},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.02339 [cs, q-bio]},
  eprint = {2103.02339},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Understanding how the brain responds to sensory inputs is challenging: brain recordings are partial, noisy, and high dimensional; they vary across sessions and subjects and they capture highly nonlinear dynamics. These challenges have led the community to develop a variety of preprocessing and analytical (almost exclusively linear) methods, each designed to tackle one of these issues. Instead, we propose to address these challenges through a specific end-to-end deep learning architecture, trained to predict the brain responses of multiple subjects at once. We successfully test this approach on a large cohort of magnetoencephalography (MEG) recordings acquired during a one-hour reading task. Our Deep Recurrent Encoding (DRE) architecture reliably predicts MEG responses to words with a three-fold improvement over classic linear methods. To overcome the notorious issue of interpretability of deep learning, we describe a simple variable importance analysis. When applied to DRE, this method recovers the expected evoked responses to word length and word frequency. The quantitative improvement of the present deep learning approach paves the way to better understand the nonlinear dynamics of brain activity from large datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/xzfang/Zotero/storage/N3UEVS86/Chehab et al. - 2021 - Deep Recurrent Encoder A scalable end-to-end netw.pdf;/Users/xzfang/Zotero/storage/54VFR6DN/2103.html}
}

@article{chen_human_2021,
  title = {The Human Language System Does Not Support Music Processing},
  author = {Chen, Xuanyi and Affourtit, Josef and Ryskin, Rachel and Regev, Tamar I. and {Norman-Haignere}, Samuel and Jouravlev, Olessia and {Malik-Moraleda}, Saima and Kean, Hope and Varley, Rosemary and Fedorenko, Evelina},
  year = {2021},
  month = jun,
  journal = {bioRxiv},
  pages = {2021.06.01.446439},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.01.446439},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Language and music are two human-unique capacities whose relationship remains debated. Some argue for overlap in processing mechanisms, especially for structure processing, but others fail to find overlap. Using fMRI, we examined the responses of language brain regions to diverse music stimuli, and also probed the musical abilities of individuals with severe aphasia. Across four experiments, we obtained a clear answer: music does not recruit nor requires the language system. The language regions' responses to music are generally low and never exceed responses elicited by non-music auditory conditions, like animal sounds. Further, the language regions are not sensitive to music structure: they show low responses to both intact and scrambled music, and to melodies with vs. without structural violations. Finally, individuals with aphasia who cannot judge sentence grammaticality perform well on melody well-formedness judgments. Thus the mechanisms that process structure in language do not appear to support music processing.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KP4W2D5K/Chen et al. - 2021 - The human language system does not support music p.pdf;/Users/xzfang/Zotero/storage/K3BIAW2E/2021.06.01.html}
}

@article{chen_shared_2017,
  title = {Shared Memories Reveal Shared Structure in Neural Activity across Individuals},
  author = {Chen, Janice and Leong, Yuan Chang and Honey, Christopher J. and Yong, Chung H. and Norman, Kenneth A. and Hasson, Uri},
  year = {2017},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {20},
  number = {1},
  pages = {115--125},
  issn = {1546-1726},
  doi = {10.1038/nn.4450},
  abstract = {The authors demonstrate that activity patterns in the default network during unguided spoken recollection of real-world events were similar between individuals recalling the same specific events. Patterns were altered between perception and recall in a systematic manner across brains. These results reveal a common spatial organization for memory representations.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LQSF2N49/Chen et al. - 2017 - Shared memories reveal shared structure in neural .pdf;/Users/xzfang/Zotero/storage/Z76FGA5X/nn.html}
}

@article{cheng_how_2021,
  title = {How Do You Feel the Rhythm: {{Dynamic}} Motor-Auditory Interactions Are Involved in the Imagination of Hierarchical Timing},
  shorttitle = {How Do You Feel the Rhythm},
  author = {Cheng, Tzu-Han Zoe and Creel, Sarah C. and Iversen, John R.},
  year = {2021},
  month = nov,
  journal = {Journal of Neuroscience},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1121-21.2021},
  abstract = {Predicting and organizing patterns of events is important for humans to survive in a dynamically changing world. The motor system has been proposed to be actively, and necessarily, engaged in not only the production but the perception of rhythm by organizing hierarchical timing that influences auditory responses. It is not yet well understood how the motor system interacts with the auditory system to perceive and maintain hierarchical structure in time. This study investigated the dynamic interaction between auditory and motor functional sources during the perception and imagination of musical meters. We pursued this using a novel method combining high-density EEG, EMG and motion capture with independent component analysis (ICA) to separate motor and auditory activity during meter imagery while robustly controlling against covert movement. We demonstrated that endogenous brain activity in both auditory and motor functional sources reflects the imagination of binary and ternary meters in the absence of corresponding acoustic cues or overt movement at the meter rate. We found clear evidence for hypothesized motor-to-auditory information flow at the beat rate in all conditions, suggesting a role for top-down influence of the motor system on auditory processing of beat-based rhythms, and reflecting an auditory-motor system with tight reciprocal informational coupling. These findings align with and further extend a set of motor hypotheses from beat perception to hierarchical meter imagination, adding supporting evidence to active engagement of the motor system in auditory processing, which may more broadly speak to the neural mechanisms of temporal processing in other human cognitive functions. Significance Statement Humans live in a world full of hierarchically structured temporal information, the accurate perception of which is essential for understanding speech and music. Music provides a window into the brain mechanisms of time perception, enabling us to examine how the brain groups musical beats into, for example a march or waltz. Using a novel paradigm combining measurement of electrical brain activity with data-driven analysis, this study directly investigates motor-auditory connectivity during meter imagination. Findings highlight the importance of the motor system in the active imagination of meter. This study sheds new light on a fundamental form of perception by demonstrating how auditory-motor interaction may support hierarchical timing processing, which may have clinical implications for speech and motor rehabilitation.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2021 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {34848500},
  file = {/Users/xzfang/Zotero/storage/UHTFCYF2/JNEUROSCI.1121-21.html}
}

@article{cheung_uncertainty_2019,
  title = {Uncertainty and {{Surprise Jointly Predict Musical Pleasure}} and {{Amygdala}}, {{Hippocampus}}, and {{Auditory Cortex Activity}}},
  author = {Cheung, Vincent K.M. and Harrison, Peter M.C. and Meyer, Lars and Pearce, Marcus T. and Haynes, John-Dylan and Koelsch, Stefan},
  year = {2019},
  month = dec,
  journal = {Current Biology},
  volume = {29},
  number = {23},
  pages = {4084-4092.e4},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.09.067},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IURJWFKV/Cheung et al. - 2019 - Uncertainty and Surprise Jointly Predict Musical P.pdf}
}

@article{cheyette_primarily_2019,
  title = {A Primarily Serial, Foveal Accumulator Underlies Approximate Numerical Estimation},
  author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
  year = {2019},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {36},
  pages = {17729--17734},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1819956116},
  abstract = {The approximate number system (ANS) has attracted broad interest due to its potential importance in early mathematical development and the fact that it is conserved across species. Models of the ANS and behavioral measures of ANS acuity both assume that quantity estimation is computed rapidly and in parallel across an entire view of the visual scene. We present evidence instead that ANS estimates are largely the product of a serial accumulation mechanism operating across visual fixations. We used an eye-tracker to collect data on participants' visual fixations while they performed quantity-estimation and -discrimination tasks. We were able to predict participants' numerical estimates using their visual fixation data: As the number of dots fixated increased, mean estimates also increased, and estimation error decreased. A detailed model-based analysis shows that fixated dots contribute twice as much as peripheral dots to estimated quantities; people do not ``double count'' multiply fixated dots; and they do not adjust for the proportion of area in the scene that they have fixated. The accumulation mechanism we propose explains reported effects of display time on estimation and earlier findings of a bias to underestimate quantities.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J7I3AD67/Cheyette and Piantadosi - 2019 - A primarily serial, foveal accumulator underlies a.pdf}
}

@techreport{cheyette_psychophysics_2021,
  type = {Preprint},
  title = {The Psychophysics of Number Arise from Resource-Limited Spatial Memory},
  author = {Cheyette, Samuel and Wu, Shengyi and Piantadosi, Steven T.},
  year = {2021},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/zyp2t},
  abstract = {People can identify the number of objects in small sets rapidly and without error but become increasingly noisy for larger sets. However, the cognitive mechanisms underlying these ubiquitous psychophysics are poorly understood. We present a model of a limitedcapacity visual system optimized to individuate and remember the location of objects in a scene which gives rise to all key aspects of number psychophysics, including error-free small number perception and scalar variability for larger numbers. We therefore propose that number psychophysics can be understood as an emergent property of primitive perceptual mechanisms \textemdash{} namely, the process of identifying and representing individual objects in a scene. To test our theory, we ran two experiments: a change-localization task to measure participants' memory for the locations of objects (Experiment 1) and a numerical estimation task (Experiment 2). Our model accounts well for participants' performance in both experiments, despite only being optimized to efficiently encode where objects are present in a scene. Our results demonstrate that the key psychophysical features of numerical cognition do not arise from separate modules or capacities specific to number, but rather from lower-level constraints on perception which are manifested even in non-numerical tasks.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GP5G7VQ8/Cheyette et al. - 2021 - The psychophysics of number arise from resource-li.pdf}
}

@article{cheyette_unified_2020,
  title = {A Unified Account of Numerosity Perception},
  author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
  year = {2020},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {12},
  pages = {1265--1272},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00946-0},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2CC5FFWY/Cheyette and Piantadosi - 2020 - A unified account of numerosity perception.pdf}
}

@article{chi_multiresolution_2005,
  title = {Multiresolution Spectrotemporal Analysis of Complex Sounds},
  author = {Chi, Taishih and Ru, Powen and Shamma, Shihab A.},
  year = {2005},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {118},
  number = {2},
  pages = {887--906},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.1945807},
  abstract = {A computational model of auditory analysis is described that is inspired by psychoacoustical and neurophysiological findings in early and central stages of the auditory system. The model provides a unified multiresolution representation of the spectral and temporal features likely critical in the perception of sound. Simplified, more specifically tailored versions of this model have already been validated by successful application in the assessment of speech intelligibility [Elhilali et al., Speech Commun. 41(2-3), 331\textendash 348 (2003); Chi et al., J. Acoust. Soc. Am. 106, 2719\textendash 2732 (1999)] and in explaining the perception of monaural phase sensitivity [R. Carlyon and S. Shamma, J. Acoust. Soc. Am. 114, 333\textendash 348 (2003)]. Here we provide a more complete mathematical formulation of the model, illustrating how complex signals are transformed through various stages of the model, and relating it to comparable existing models of auditory processing. Furthermore, we outline several reconstruction algorithms to resynthesize the sound from the model output so as to evaluate the fidelity of the representation and contribution of different features and cues to the sound percept.},
  file = {/Users/xzfang/Zotero/storage/5X33NDIU/Chi et al. - 2005 - Multiresolution spectrotemporal analysis of comple.pdf}
}

@article{chiandetti_there_2008,
  title = {Is There an Innate Geometric Module? {{Effects}} of Experience with Angular Geometric Cues on Spatial Re-Orientation Based on the Shape of the Environment},
  shorttitle = {Is There an Innate Geometric Module?},
  author = {Chiandetti, Cinzia and Vallortigara, Giorgio},
  year = {2008},
  month = jan,
  journal = {Animal Cognition},
  volume = {11},
  number = {1},
  pages = {139--146},
  issn = {1435-9456},
  doi = {10.1007/s10071-007-0099-y},
  abstract = {Non-human animals and human children can make use of the geometric shape of an environment for spatial reorientation and in some circumstances reliance on purely geometric information (metric properties of surfaces and sense) can overcome the use of local featural cues. Little is known as to whether the use of geometric information is in some way reliant on past experience or, as would likely be argued by advocates of the notion of a geometric module, it is innate. We tested the navigational abilities of newborn domestic chicks reared in either rectangular or circular cages. Chicks were trained in a rectangular-shaped enclosure with panels placed at the corners to provide salient featural cues. Rectangular-reared and circular-reared chicks proved equally able to learn the task. When tested after removal of the featural cues, both rectangular- and circular-reared chicks showed evidence that they had spontaneously encoded geometric information. Moreover, when trained in a rectangular-shaped enclosure without any featural cues, chicks reared in rectangular-, circular-, or c-shaped cages proved to be equally able to learn and perform the task using geometric information. These results suggest that effective use of geometric information for spatial reorientation does not require experience in environments with right angles and metrically distinct surfaces, thus supporting the hypothesis of a predisposed geometric module in the animal brain.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8BFHWIBB/Chiandetti and Vallortigara - 2008 - Is there an innate geometric module Effects of ex.pdf}
}

@article{chien_constructing_2019,
  title = {Constructing and {{Forgetting Temporal Context}} in the {{Human Cerebral Cortex}}},
  author = {Chien, Hsiang-Yun Sherry and Honey, Christopher J.},
  year = {2019},
  month = sep,
  journal = {bioRxiv},
  pages = {761593},
  doi = {10.1101/761593},
  abstract = {{$<$}h3{$>$}Summary{$<$}/h3{$>$} {$<$}p{$>$}How does information from seconds earlier affect neocortical responses to new input? Here, we used empirical measurements and computational modeling to study the integration and forgetting of prior information. We found that when two groups of participants heard the same sentence in a narrative, preceded by different contexts, the neural responses of each group were initially different, but gradually fell into alignment. We observed a hierarchical gradient: sensory cortices aligned most quickly, followed by mid-level regions, while higher-order cortical regions aligned last. In some higher order regions, responses to the same sentence took more than 10 seconds to align. What kinds of computations can explain this hierarchical organization of contextual alignment? Passive linear integration models predict that regions which are slower to integrate new information should also be slower to forget old information. However, we found that higher order regions could rapidly forget prior context. The data were better captured by a model composed of hierarchical autoencoders in time (HAT). In HAT, cortical regions maintain a temporal context representation which is actively integrated with input at each moment, and this integration is gated by prediction error. These data and models suggest that sequences of information are combined throughout the cortical hierarchy using an active and gated integration process.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TQQBYA5T/Chien and Honey - 2019 - Constructing and Forgetting Temporal Context in th.pdf}
}

@article{chien_priming_2016,
  title = {Priming the Representation of {{Mandarin}} Tone 3 Sandhi Words},
  author = {Chien, Yu-Fu and Sereno, Joan A. and Zhang, Jie},
  year = {2016},
  month = feb,
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {2},
  pages = {179--189},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2015.1064976},
  abstract = {Phonological alternation poses problems for spoken word recognition. In Mandarin Tone 3 sandhi, a Tone 3 syllable changes to a Tone 2 syllable when followed by another Tone 3 syllable. A traditional phonological account assumes that the initial syllable of Mandarin disyllabic sandhi words is Tone 3 (T3) underlyingly, but becomes Tone 2 (T2) on the surface. In an auditory\textendash auditory priming lexical decision experiment, each disyllabic tone sandhi target word (e.g., chu3-li3) was preceded by one of three monosyllabic primes: a T2 prime (Surface-Tone overlap) (chu2), a T3 prime (Underlying-Tone overlap) (chu3), or a control prime (Baseline condition) (chu1). Results showed that Tone 3 primes (Underlying-Tone) elicited significantly stronger facilitation effects for the sandhi targets than Tone 2 primes (Surface-Tone), with little effect of target frequency. The data are examined in terms of the contribution of underlying representations for models of spoken word recognition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6A2FBCHA/Chien et al. - 2016 - Priming the representation of Mandarin tone 3 sand.pdf}
}

@article{chini_developmental_2021,
  title = {Developmental Increase of Inhibition Drives Decorrelation of Neural Activity},
  author = {Chini, Mattia and Pfeffer, Thomas and {Hanganu-Opatz}, Ileana L.},
  year = {2021},
  month = jul,
  journal = {bioRxiv},
  pages = {2021.07.06.451299},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.07.06.451299},
  abstract = {{$<$}p{$>$}Throughout development, the brain transits from early highly synchronous activity patterns to a mature state with sparse and decorrelated neural activity, yet the mechanisms underlying this process are unknown. The developmental transition has important functional consequences, as the latter state allows for more efficient storage, retrieval and processing of information. Here, we show that, in the mouse medial prefrontal cortex (mPFC), neural activity during the first two postnatal weeks decorrelates following specific spatial patterns. This process is accompanied by a concomitant tilting of excitation/inhibition (E-I) ratio towards inhibition. Using optogenetic manipulations and neural network modeling, we show that the two phenomena are mechanistically linked, and that a relative increase of inhibition drives the decorrelation of neural activity. Accordingly, in two mouse models of neurodevelopmental disorders, subtle alterations in E-I ratio are associated with specific impairments in the correlational structure of spike trains. Finally, capitalizing on EEG data from newborn babies, we show that an analogous developmental transition takes place also in the human brain. Thus, changes in E-I ratio control the (de)correlation of neural activity and, by these means, its developmental imbalance might contribute to the pathogenesis of neurodevelopmental disorders.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YS2S4RVK/Chini et al. - 2021 - Developmental increase of inhibition drives decorr.pdf;/Users/xzfang/Zotero/storage/DKIWFB3R/2021.07.06.451299v1.html}
}

@article{chodroff_acoustic_2020,
  title = {Acoustic\textendash Phonetic and Auditory Mechanisms of Adaptation in the Perception of Sibilant Fricatives},
  author = {Chodroff, Eleanor and Wilson, Colin},
  year = {2020},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {4},
  pages = {2027--2048},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01894-2},
  abstract = {Listeners are highly proficient at adapting to contextual variation when perceiving speech. In the present study, we examined the effects of brief speech and nonspeech contexts on the perception of sibilant fricatives. We explored three theoretically motivated accounts of contextual adaptation, based on phonetic cue calibration, phonetic covariation, and auditory contrast. Under the cue calibration account, listeners adapt by estimating a talker-specific average for each phonetic cue or dimension; under the cue covariation account, listeners adapt by exploiting consistencies in how the realization of speech sounds varies across talkers; under the auditory contrast account, adaptation results from (partial) masking of spectral components that are shared by adjacent stimuli. The spectral center of gravity, a phonetic cue to fricative identity, was manipulated for several types of context sound: /z/-initial syllables, /v/-initial syllables, and white noise matched in long-term average spectrum (LTAS) to the /z/-initial stimuli. Listeners' perception of the /s/\textendash/{$\Elzesh$}/ contrast was significantly influenced by /z/-initial syllables and LTAS-matched white noise stimuli, but not by /v/-initial syllables. No significant difference in adaptation was observed between exposure to /z/-initial syllables and matched white noise stimuli, and speech did not have a considerable advantage over noise when the two were presented consecutively within a context. The pattern of findings is most consistent with the auditory contrast account of short-term perceptual adaptation. The cue covariation account makes accurate predictions for speech contexts, but not for nonspeech contexts or for the absence of a speech-versus-nonspeech difference.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/I7Q7BS8U/Chodroff and Wilson - 2020 - Acousticâ€“phonetic and auditory mechanisms of adapt.pdf}
}

@article{choi_noninvasive_2019,
  title = {Noninvasive Neurostimulation of Left Temporal Lobe Disrupts Rapid Talker Adaptation in Speech Processing},
  author = {Choi, Ja Young and Perrachione, Tyler K.},
  year = {2019},
  month = sep,
  journal = {Brain and Language},
  volume = {196},
  pages = {104655},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2019.104655},
  abstract = {Talker adaptation improves speech processing efficiency by reducing possible mappings between talkers' speech acoustics and listeners' phonemic representations. We investigated the functional neuroanatomy of talker adaptation by applying noninvasive neurostimulation (high-definition transcranial direct current stimulation; HD-tDCS) to left superior temporal lobe while participants performed an auditory word identification task. We factorially manipulated talker variability (single vs. mixed talkers) and speech context (isolated words vs. connected speech), measuring listeners' speech processing efficiency under anodal, cathodal, or sham stimulation. Speech processing was faster for single talkers than mixed talkers, and connected speech reduced the additional processing costs associated with mixed-talker speech. However, the beneficial effect of connected speech in the mixed-talker condition was significantly attenuated under both anodal and cathodal stimulation versus sham. Stimulation of left superior temporal lobe disrupts the brain's ability to use local phonetic context to rapidly adapt to a talker, revealing this region's causal role in talker adaptation.},
  langid = {english},
  keywords = {Adaptation,Phonetic variability,Speech perception,Superior temporal cortex,tDCS},
  file = {/Users/xzfang/Zotero/storage/SCQP8Z9B/Choi and Perrachione - 2019 - Noninvasive neurostimulation of left temporal lobe.pdf;/Users/xzfang/Zotero/storage/HR3M2UJG/S0093934X19301051.html}
}

@article{choi_time_2019,
  title = {Time and Information in Perceptual Adaptation to Speech},
  author = {Choi, Ja Young and Perrachione, Tyler K.},
  year = {2019},
  month = nov,
  journal = {Cognition},
  volume = {192},
  pages = {103982},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.05.019},
  abstract = {Perceptual adaptation to a talker enables listeners to efficiently resolve the many-to-many mapping between variable speech acoustics and abstract linguistic representations. However, models of speech perception have not delved into the variety or the quantity of information necessary for successful adaptation, nor how adaptation unfolds over time. In three experiments using speeded classification of spoken words, we explored how the quantity (duration), quality (phonetic detail), and temporal continuity of talker-specific context contribute to facilitating perceptual adaptation to speech. In single- and mixed-talker conditions, listeners identified phonetically-confusable target words in isolation or preceded by carrier phrases of varying lengths and phonetic content, spoken by the same talker as the target word. Word identification was always slower in mixed-talker conditions than single-talker ones. However, interference from talker variability decreased as the duration of preceding speech increased but was not affected by the amount of preceding talker-specific phonetic information. Furthermore, efficiency gains from adaptation depended on temporal continuity between preceding speech and the target word. These results suggest that perceptual adaptation to speech may be understood via models of auditory streaming, where perceptual continuity of an auditory object (e.g., a talker) facilitates allocation of attentional resources, resulting in more efficient perceptual processing.},
  langid = {english},
  keywords = {Adaptation,Categorization,Phonetic variability,Speech perception,Talker normalization},
  file = {/Users/xzfang/Zotero/storage/R73MWNWY/Choi and Perrachione - 2019 - Time and information in perceptual adaptation to s.pdf;/Users/xzfang/Zotero/storage/CESQHDZJ/S0010027719301490.html}
}

@article{choi_varying_2018,
  title = {Varying Acoustic-Phonemic Ambiguity Reveals That Talker Normalization Is Obligatory in Speech Processing},
  author = {Choi, Ja Young and Hu, Elly R. and Perrachione, Tyler K.},
  year = {2018},
  month = apr,
  journal = {Attention, perception \& psychophysics},
  volume = {80},
  number = {3},
  pages = {784--797},
  issn = {1943-3921},
  doi = {10.3758/s13414-017-1395-5},
  abstract = {The nondeterministic relationship between speech acoustics and abstract phonemic representations imposes a challenge for listeners to maintain perceptual constancy despite the highly variable acoustic realization of speech. Talker normalization facilitates speech processing by reducing the degrees of freedom for mapping between encountered speech and phonemic representations. While this process has been proposed to facilitate the perception of ambiguous speech sounds, it is currently unknown whether talker normalization is affected by the degree of potential ambiguity in acoustic-phonemic mapping. We explored the effects of talker normalization on speech processing in a series of speeded classification paradigms, parametrically manipulating the potential for inconsistent acoustic-phonemic relationships across talkers for both consonants and vowels. Listeners identified words with varying potential acoustic-phonemic ambiguity across talkers (e.g., beet/boat vs. boot/boat) spoken by single or mixed talkers. Auditory categorization of words was always slower when listening to mixed talkers compared to a single talker, even when there was no potential acoustic ambiguity between target sounds. Moreover, the processing cost imposed by mixed talkers was greatest when words had the most potential acoustic-phonemic overlap across talkers. Models of acoustic dissimilarity between target speech sounds did not account for the pattern of results. These results suggest (i) that talker normalization incurs the greatest processing cost when disambiguating highly-confusable sounds and (ii) that talker normalization appears to be an obligatory component of speech perception, taking place even when the acoustic-phonemic relationships across sounds are unambiguous.},
  pmcid = {PMC5840042},
  pmid = {29417449},
  file = {/Users/xzfang/Zotero/storage/9Z5IKHCC/Choi et al. - 2018 - Varying acoustic-phonemic ambiguity reveals that t.pdf}
}

@article{chomsky_three_1956,
  title = {Three Models for the Description of Language},
  author = {Chomsky, N.},
  year = {1956},
  month = sep,
  journal = {IRE Transactions on Information Theory},
  volume = {2},
  number = {3},
  pages = {113--124},
  issn = {2168-2712},
  doi = {10.1109/TIT.1956.1056813},
  abstract = {We investigate several conceptions of linguistic structure to determine whether or not they can provide simple and "revealing" grammars that generate all of the sentences of English and only these. We find that no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar. Furthermore, the particular subclass of such processes that producen-order statistical approximations to English do not come closer, with increasingn, to matching the output of an English grammar. We formalize-the notions of "phrase structure" and show that this gives us a method for describing language which is essentially more powerful, though still representable as a rather elementary type of finite-state process. Nevertheless, it is successful only when limited to a small subset of simple sentences. We study the formal properties of a set of grammatical transformations that carry sentences with phrase structure into new sentences with derived phrase structure, showing that transformational grammars are processes of the same elementary type as phrase-structure grammars; that the grammar of English is materially simplified if phrase structure description is limited to a kernel of simple sentences from which all other sentences are constructed by repeated transformations; and that this view of linguistic structure gives a certain insight into the use and understanding of language.},
  keywords = {Impedance matching,Kernel,Laboratories,Markov processes,Natural languages,Research and development,Testing},
  file = {/Users/xzfang/Zotero/storage/3ZV5PKGN/Chomsky - 1956 - Three models for the description of language.pdf;/Users/xzfang/Zotero/storage/2Y4V9NSL/1056813.html}
}

@article{chopin_predictive_2012,
  title = {Predictive {{Properties}} of {{Visual Adaptation}}},
  author = {Chopin, Adrien and Mamassian, Pascal},
  year = {2012},
  month = apr,
  journal = {Current Biology},
  volume = {22},
  number = {7},
  pages = {622--626},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2012.02.021},
  langid = {english},
  pmid = {22386314},
  file = {/Users/xzfang/Zotero/storage/KT8I5H9R/Chopin and Mamassian - 2012 - Predictive Properties of Visual Adaptation.pdf;/Users/xzfang/Zotero/storage/R8AJ25Q3/S0960-9822(12)00170-4.html}
}

@article{chow_bagofarguments_2016,
  title = {A ``Bag-of-Arguments'' Mechanism for Initial Verb Predictions},
  author = {Chow, Wing-Yee and Smith, Cybelle and Lau, Ellen and Phillips, Colin},
  year = {2016},
  month = may,
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {5},
  pages = {577--596},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2015.1066832},
  abstract = {Previous studies have shown that comprehenders use rich contextual information to anticipate upcoming input on the fly, but less is known about how comprehenders integrate different sources of information to generate predictions in real time. The current study examines the time course with which the lexical meaning and structural roles of preverbal arguments impact comprehenders' lexical semantic predictions about an upcoming verb in two event-related potential (ERP) experiments that use the N400 amplitude as a measure of online predictability. Experiment 1 showed that the N400 was sensitive to predictability when the verb's cloze probability was reduced by substituting one of the arguments (e.g. ``The superintendent overheard which tenant/realtor the landlord had evicted \ldots{} ''), but not when the verb's cloze probability was reduced by simply swapping the roles of the arguments (e.g. ``The restaurant owner forgot which customer/waitress the waitress/customer had served \ldots{} ''). Experiment 2 showed that argument substitution elicited an N400 effect even when the substituted argument appeared elsewhere in the sentence, indicating that verb predictions are specifically driven by the arguments in the same clause as the verb, rather than by a simple ``bag-of-words'' mechanism. We propose that verb predictions initially rely on a ``bag-of-arguments'' mechanism, which specifically relies on the lexical meaning, but not the structural roles, of the arguments in a clause.},
  keywords = {event-related potentials,Language comprehension,N400,prediction,thematic relations},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2015.1066832},
  file = {/Users/xzfang/Zotero/storage/KD6V7DF3/Chow et al. - 2016 - A â€œbag-of-argumentsâ€ mechanism for initial verb pr.pdf;/Users/xzfang/Zotero/storage/4GQCU4DR/23273798.2015.html}
}

@article{christiansen_nowornever_2016,
  title = {The {{Now-or-Never}} Bottleneck: {{A}} Fundamental Constraint on Language},
  shorttitle = {The {{Now-or-Never}} Bottleneck},
  author = {Christiansen, Morten H. and Chater, Nick},
  year = {2016},
  journal = {Behavioral and Brain Sciences},
  volume = {39},
  pages = {e62},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1500031X},
  abstract = {Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this ``Now-or-Never'' bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This observation has strong implications for the nature of language processing: (1) the language system must ``eagerly'' recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with ``Right-First-Time''; once the original input is lost, there is no way for the language system to recover. This is ``Chunk-and-Pass'' processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning. This approach promises to create a direct relationship between psycholinguistics and linguistic theory. More generally, we outline a framework within which to integrate often disconnected inquiries into language processing, language acquisition, and language change and evolution.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YFNRPWMD/Christiansen and Chater - 2016 - The Now-or-Never bottleneck A fundamental constra.pdf}
}

@article{christiansen_nowornever_2016a,
  title = {The {{Now-or-Never}} Bottleneck: {{A}} Fundamental Constraint on Language},
  shorttitle = {The {{Now-or-Never}} Bottleneck},
  author = {Christiansen, Morten H. and Chater, Nick},
  year = {2016/ed},
  journal = {Behavioral and Brain Sciences},
  volume = {39},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1500031X},
  abstract = {Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this ``Now-or-Never'' bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This observation has strong implications for the nature of language processing: (1) the language system must ``eagerly'' recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with ``Right-First-Time''; once the original input is lost, there is no way for the language system to recover. This is ``Chunk-and-Pass'' processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning. This approach promises to create a direct relationship between psycholinguistics and linguistic theory. More generally, we outline a framework within which to integrate often disconnected inquiries into language processing, language acquisition, and language change and evolution.},
  langid = {english},
  keywords = {chunking,grammaticalization,incremental interpretation,language acquisition,language evolution,language processing,online learning,prediction,processing bottleneck,psycholinguistics},
  file = {/Users/xzfang/Zotero/storage/ZYJ8PNSL/938D54E80A2A90A1C5990F4915B5E8D8.html}
}

@article{christiansen_similar_2012,
  title = {Similar Neural Correlates for Language and Sequential Learning: {{Evidence}} from Event-Related Brain Potentials},
  shorttitle = {Similar Neural Correlates for Language and Sequential Learning},
  author = {Christiansen, Morten H. and Conway, Christopher M. and Onnis, Luca},
  year = {2012},
  month = feb,
  journal = {Language and Cognitive Processes},
  volume = {27},
  number = {2},
  pages = {231--256},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690965.2011.606666},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DZ9GWUM6/Christiansen et al. - 2012 - Similar neural correlates for language and sequent.pdf}
}

@article{christianson_thematic_2001,
  title = {Thematic {{Roles Assigned}} along the {{Garden Path Linger}}},
  author = {Christianson, Kiel and Hollingworth, Andrew and Halliwell, John F. and Ferreira, Fernanda},
  year = {2001},
  month = jun,
  journal = {Cognitive Psychology},
  volume = {42},
  number = {4},
  pages = {368--407},
  issn = {0010-0285},
  doi = {10.1006/cogp.2001.0752},
  abstract = {In the literature dealing with the reanalysis of garden path sentences such as While the man hunted the deer ran into the woods, it is generally assumed either that people completely repair their initial incorrect syntactic representations to yield a final interpretation whose syntactic structure is fully consistent with the input string or that the parse fails. In a series of five experiments, we explored the possibility that partial reanalyses take place. Specifically, we examined the conditions under which part of the initial incorrect analysis persists at the same time that part of the correct final analysis is constructed. In Experiments 1a and 1b, we found that both the length of the ambiguous region and the plausibility of the ultimate interpretation affected the likelihood that such sentences would be fully reanalyzed. In Experiment 2, we compared garden path sentences with non-garden path sentences and compared performance on two different types of comprehension questions. In Experiments 3a and 3b, we constructed garden path sentences using a small class of syntactically unique verbs to provide converging evidence against the position that people employ some sort of ``general reasoning'' or pragmatic inference when faced with syntactically difficult garden paths. The results from these experiments indicate that reanalysis of such sentences is not always complete, so that comprehenders often derive an interpretation for the full sentence in which part of the initial misanalysis persists. We conclude that the goal of language processing is not always to create an idealized structure, but rather to create a representation that is ``good enough'' to satisfy the comprehender that an appropriate interpretation has been obtained.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/M6QUTSZG/Christianson et al. - 2001 - Thematic Roles Assigned along the Garden Path Ling.pdf;/Users/xzfang/Zotero/storage/ZW2DETQY/S0010028501907522.html}
}

@article{christophel_parietal_2015,
  title = {Parietal and Early Visual Cortices Encode Working Memory Content across Mental Transformations},
  author = {Christophel, Thomas B. and Cichy, Radoslaw M. and Hebart, Martin N. and Haynes, John-Dylan},
  year = {2015},
  month = feb,
  journal = {NeuroImage},
  volume = {106},
  pages = {198--206},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.11.018},
  abstract = {Active and flexible manipulations of memory contents ``in the mind's eye'' are believed to occur in a dedicated neural workspace, frequently referred to as visual working memory. Such a neural workspace should have two important properties: The ability to store sensory information across delay periods and the ability to flexibly transform sensory information. Here we used a combination of functional MRI and multivariate decoding to indentify such neural representations. Subjects were required to memorize a complex artificial pattern for an extended delay, then rotate the mental image as instructed by a cue and memorize this transformed pattern. We found that patterns of brain activity already in early visual areas and posterior parietal cortex encode not only the initially remembered image, but also the transformed contents after mental rotation. Our results thus suggest that the flexible and general neural workspace supporting visual working memory can be realized within posterior brain regions.},
  langid = {english},
  keywords = {fMRI,Mental rotation,Multivariate analyses,Short-term memory,Working memory},
  file = {/Users/xzfang/Zotero/storage/93YX4PIW/Christophel et al. - 2015 - Parietal and early visual cortices encode working .pdf;/Users/xzfang/Zotero/storage/JFKJ6Y6J/S1053811914009355.html}
}

@article{chun_taxonomy_2011,
  title = {A {{Taxonomy}} of {{External}} and {{Internal Attention}}},
  author = {Chun, Marvin M. and Golomb, Julie D. and {Turk-Browne}, Nicholas B.},
  year = {2011},
  journal = {Annual Review of Psychology},
  volume = {62},
  number = {1},
  pages = {73--101},
  doi = {10.1146/annurev.psych.093008.100427},
  abstract = {Attention is a core property of all perceptual and cognitive operations. Given limited capacity to process competing options, attentional mechanisms select, modulate, and sustain focus on information most relevant for behavior. A significant problem, however, is that attention is so ubiquitous that it is unwieldy to study. We propose a taxonomy based on the types of information that attention operates over\textemdash the targets of attention. At the broadest level, the taxonomy distinguishes between external attention and internal attention. External attention refers to the selection and modulation of sensory information. External attention selects locations in space, points in time, or modality-specific input. Such perceptual attention can also select features defined across any of these dimensions, or object representations that integrate over space, time, and modality. Internal attention refers to the selection, modulation, and maintenance of internally generated information, such as task rules, responses, long-term memory, or working memory. Working memory, in particular, lies closest to the intersection between external and internal attention. The taxonomy provides an organizing framework that recasts classic debates, raises new issues, and frames understanding of neural mechanisms.},
  pmid = {19575619},
  annotation = {\_eprint: https://doi.org/10.1146/annurev.psych.093008.100427},
  file = {/Users/xzfang/Zotero/storage/B9UMA7H2/Chun et al. - 2011 - A Taxonomy of External and Internal Attention.pdf}
}

@article{cichy_multivariate_2017,
  title = {Multivariate Pattern Analysis of {{MEG}} and {{EEG}}: {{A}} Comparison of Representational Structure in Time and Space},
  shorttitle = {Multivariate Pattern Analysis of {{MEG}} and {{EEG}}},
  author = {Cichy, Radoslaw Martin and Pantazis, Dimitrios},
  year = {2017},
  month = sep,
  journal = {NeuroImage},
  volume = {158},
  pages = {441--454},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2017.07.023},
  abstract = {Multivariate pattern analysis of magnetoencephalography (MEG) and electroencephalography (EEG) data can reveal the rapid neural dynamics underlying cognition. However, MEG and EEG have systematic differences in sampling neural activity. This poses the question to which degree such measurement differences consistently bias the results of multivariate analysis applied to MEG and EEG activation patterns. To investigate, we conducted a concurrent MEG/EEG study while participants viewed images of everyday objects. We applied multivariate classification analyses to MEG and EEG data, and compared the resulting time courses to each other, and to fMRI data for an independent evaluation in space. We found that both MEG and EEG revealed the millisecond spatio-temporal dynamics of visual processing with largely equivalent results. Beyond yielding convergent results, we found that MEG and EEG also captured partly unique aspects of visual representations. Those unique components emerged earlier in time for MEG than for EEG. Identifying the sources of those unique components with fMRI, we found the locus for both MEG and EEG in high-level visual cortex, and in addition for MEG in low-level visual cortex. Together, our results show that multivariate analyses of MEG and EEG data offer a convergent and complimentary view on neural processing, and motivate the wider adoption of these methods in both MEG and EEG research.},
  langid = {english},
  keywords = {EEG,MEG,Multivariate analysis,Object recognition,Pattern classification,Representational similarity analysis},
  file = {/Users/xzfang/Zotero/storage/PR3BCFMI/Cichy and Pantazis - 2017 - Multivariate pattern analysis of MEG and EEG A co.pdf;/Users/xzfang/Zotero/storage/YNIY9BY8/S1053811917305918.html}
}

@article{cichy_resolving_2014,
  title = {Resolving Human Object Recognition in Space and Time},
  author = {Cichy, Radoslaw Martin and Pantazis, Dimitrios and Oliva, Aude},
  year = {2014},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {17},
  number = {3},
  pages = {455--462},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3635},
  abstract = {Using representational similarity analysis to link human MEG with human fMRI and monkey electrophysiological data, the authors provide an integrated temporal and spatial account of object categorization. Early, low-level processing corresponded to activity in primary visual cortex, while later object processing related to inferior temporal activity in a category-specific manner.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6RCF7WMN/Cichy et al. - 2014 - Resolving human object recognition in space and ti.pdf;/Users/xzfang/Zotero/storage/FCYSXMU3/nn.html}
}

@article{clark_languageasfixedeffect_1973,
  title = {The Language-as-Fixed-Effect Fallacy: {{A}} Critique of Language Statistics in Psychological Research},
  shorttitle = {The Language-as-Fixed-Effect Fallacy},
  author = {Clark, Herbert H.},
  year = {1973},
  month = aug,
  journal = {Journal of Verbal Learning and Verbal Behavior},
  volume = {12},
  number = {4},
  pages = {335--359},
  issn = {00225371},
  doi = {10.1016/S0022-5371(73)80014-3},
  langid = {english}
}

@article{clark_referring_1986,
  title = {Referring as a Collaborative Process},
  author = {Clark, Herbert H. and {Wilkes-Gibbs}, Deanna},
  year = {1986},
  month = feb,
  journal = {Cognition},
  volume = {22},
  number = {1},
  pages = {1--39},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(86)90010-7},
  abstract = {In conversation, speakers and addressees work together in the making of a definite reference. In the model we propose, the speaker initiates the process by presenting or inviting a noun phrase. Before going on to the next contribution, the participants, if necessary, repair, expand on, or replace the noun phrase in an iterative process until they reach a version they mutually accept. In doing so they try to minimize their joint effort. The preferred procedure is for the speaker to present a simple noun phrase and for the addressee to accept it by allowing the next contribution to begin. We describe a communication task in which pairs of people conversed about arranging complex figures and show how the proposed model accounts for many features of the references they produced. The model follows, we suggest, from the mutual responsibility that participants in conversation bear toward the understanding of each utterance. R\'esum\'e Au cours d' une conversation, les interlocuteurs travaillent ensemble pour construire une r\'ef\'erence d\'efinie. Dans le mod\`ele propos\`e, le locuteur initie le processus en pr\'esentant un syntagme nominal. Avant de passer \`a la contribution suivante, les participants, si cela est n\'ecessaire, corrigent, d\'eveloppent ou remplacent ce syntagme nominal au cours d'un processus it\'eratif jusqu'a ce que soit atteinte une version que tout deux acceptent. En faisant cela ils essaient de minimiser l'effort conjoint. La procedure pr\'ef\'er\'ee consiste pour le locuteur \`a pr\'esenter un syntagme nominal simple et pour l'allocuteur d'accepter ce syntagme en donnant le feu vert pour l'\'echange suivant. Nous d\'ecrivons une tache de communication an cours de laquelle deux personnes discutent l'agencement de figures complexes et nous montrons comment le modele propos\'e rend compte de nombreux traits des r\'ef\'erences produites. Le mod\'ele d\'ecoule, selon notre suggestion, de la responsabilit\'e mutuelle que les participants prennent pour que soit compris chaque \'enonc\'e durant la conversation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WCBB87G2/Clark and Wilkes-Gibbs - 1986 - Referring as a collaborative process.pdf;/Users/xzfang/Zotero/storage/5AJJT928/0010027786900107.html}
}

@misc{clarke_contextual_2021,
  title = {Contextual Expectations Shape Cortical Reinstatement of Sensory Representations},
  author = {Clarke, Alex and {Crivelli-Decker}, Jordan and Ranganath, Charan},
  year = {2021},
  month = aug,
  pages = {2021.08.05.455275},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.08.05.455275},
  abstract = {When making a turn at a familiar intersection, we know what items and landmarks will come into view. These perceptual expectations, or predictions, come from our knowledge of the context, however it's unclear how memory and perceptual systems interact to support the prediction and reactivation of sensory details in cortex. To address this, human participants learned the spatial layout of animals positioned in a cross maze. During fMRI, participants navigated between animals to reach a target, and in the process saw a predictable sequence of five animal images. Critically, to isolate activity patterns related to item predictions, rather than bottom-up inputs, one quarter of trials ended early, with a blank screen presented instead. Using multivariate pattern similarity analysis, we reveal that activity patterns in early visual cortex, posterior medial regions, and the posterior hippocampus showed greater similarity when seeing the same item compared to different items. Further, item effects in posterior hippocampus were specific to the sequence context. Critically, activity patterns associated with seeing an item in visual cortex and posterior medial cortex, were also related to activity patterns when an item was expected, but omitted, suggesting sequence predictions were reinstated in these regions. Finally, multivariate connectivity showed that patterns in the posterior hippocampus at one position in the sequence were related to patterns in early visual cortex and posterior medial cortex at a later position. Together, our results support the idea that hippocampal representations facilitate sensory processing by modulating visual cortical activity in anticipation of expected items.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5XZJIQIX/Clarke et al. - 2021 - Contextual expectations shape cortical reinstateme.pdf;/Users/xzfang/Zotero/storage/R4R9UT8L/2021.08.05.html}
}

@article{clarke_objectspecific_2014,
  title = {Object-{{Specific Semantic Coding}} in {{Human Perirhinal Cortex}}},
  author = {Clarke, Alex and Tyler, Lorraine K.},
  year = {2014},
  month = apr,
  journal = {The Journal of Neuroscience},
  volume = {34},
  number = {14},
  pages = {4766--4775},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2828-13.2014},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4FINPFWP/Clarke and Tyler - 2014 - Object-Specific Semantic Coding in Human Perirhina.pdf}
}

@article{clarke_rapid_2004,
  title = {Rapid Adaptation to Foreign-Accented {{English}}},
  author = {Clarke, Constance M. and Garrett, Merrill F.},
  year = {2004},
  month = dec,
  journal = {The Journal of the Acoustical Society of America},
  volume = {116},
  number = {6},
  pages = {3647--3658},
  issn = {0001-4966},
  doi = {10.1121/1.1815131},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/B8CPNJUJ/Clarke and Garrett - 2004 - Rapid adaptation to foreign-accented English.pdf}
}

@article{clayards_perception_2008,
  title = {Perception of Speech Reflects Optimal Use of Probabilistic Speech Cues},
  author = {Clayards, Meghan and Tanenhaus, Michael K. and Aslin, Richard N. and Jacobs, Robert A.},
  year = {2008},
  month = sep,
  journal = {Cognition},
  volume = {108},
  number = {3},
  pages = {804--809},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.04.004},
  abstract = {Listeners are exquisitely sensitive to fine-grained acoustic detail within phonetic categories for sounds and words. Here we show that this sensitivity is optimal given the probabilistic nature of speech cues. We manipulated the probability distribution of one probabilistic cue, Voice Onset Time (VOT), which differentiates word-initial labial stops in English (e.g., ``beach'' and ``peach''). Participants categorized words from distributions of VOT with wide or narrow variances. Uncertainty about word identity was measured by four-alternative forced-choice judgments and by the probability of looks to pictures. Both measures closely reflected the posterior probability of the word given the likelihood distributions of VOT, suggesting that listeners are sensitive to these distributions.},
  pmcid = {PMC2582186},
  pmid = {18582855},
  file = {/Users/xzfang/Zotero/storage/WVBKIFPF/Clayards et al. - 2008 - Perception of speech reflects optimal use of proba.pdf}
}

@article{cochrane_reexamining_2020,
  title = {Re-Examining {{Maljkovic}} and {{Nakayama}} (1994): {{Conscious}} Expectancy Does Affect the {{Priming}} of {{Pop-out}} Effect},
  shorttitle = {Re-Examining {{Maljkovic}} and {{Nakayama}} (1994)},
  author = {Cochrane, Brett A. and Pratt, Jay},
  year = {2020},
  month = jul,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {5},
  pages = {2693--2702},
  issn = {1943-393X},
  doi = {10.3758/s13414-020-02034-x},
  abstract = {Maljkovic and Nakayama (Memory \& Cognition, 22(6), 657-672, 1994) observed that color singleton search performance was faster when the target and distractor colors repeated rather than switched across trials - an effect termed Priming of Pop-out (PoP). Two of the key results of this seminal study revealed that the PoP effect was not influenced by the knowledge of the probability of a target color change (Experiment 2), nor was it influenced by anticipating the upcoming target color by subvocalizing it (Experiment 4). Based on these findings they concluded that the PoP effect reflected the automatic priming due to the persistence of the target and distractor colors of the previous trial. Based on recent findings indicating that conscious expectancy may influence the PoP effect, as well as several bygone experimental practices in the original study (i.e., experimenter participants, no inferential statistics, etc.), we felt it worthwhile to evaluate whether their findings were observed when replicated in an empirically rigorous manner. Though the present study revealed that the PoP effect was robust, it was profoundly impacted by the knowledge of the probability of a target color switch (Experiment 1) and vocally anticipating the upcoming target color (Experiment 2). Overall, the results suggest that we should abandon the notion that the PoP effect only reflects the automatic priming of the previous target and distractor colors independent of conscious expectancy.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FZJ4D5JW/Cochrane and Pratt - 2020 - Re-examining Maljkovic and Nakayama (1994) Consci.pdf}
}

@article{cogan_manipulating_2017,
  title = {Manipulating Stored Phonological Input during Verbal Working Memory},
  author = {Cogan, Gregory B. and Iyer, Asha and Melloni, Lucia and Thesen, Thomas and Friedman, Daniel and Doyle, Werner and Devinsky, Orrin and Pesaran, Bijan},
  year = {2017},
  month = feb,
  journal = {Nature Neuroscience},
  volume = {20},
  number = {2},
  pages = {279--286},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4459},
  abstract = {Cognitive tasks require storing and manipulating information for short periods of time. Verbal working memory involves storing and manipulating speech information, but the underlying brain mechanisms remain unknown. The authors identify storage systems for sensory and motor representations and two distinct manipulation systems, demonstrating that multiple subsystems comprise verbal working memory.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Cognitive control,Psychology,Working memory},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cognitive control;Psychology;Working memory Subject\_term\_id: cognitive-control;psychology;working-memory},
  file = {/Users/xzfang/Zotero/storage/KEHV87BW/Cogan et al. - 2017 - Manipulating stored phonological input during verb.pdf;/Users/xzfang/Zotero/storage/QUAIJTMH/nn.html}
}

@article{cogan_sensory_2014,
  title = {Sensory\textendash Motor Transformations for Speech Occur Bilaterally},
  author = {Cogan, Gregory B. and Thesen, Thomas and Carlson, Chad and Doyle, Werner and Devinsky, Orrin and Pesaran, Bijan},
  year = {2014},
  month = mar,
  journal = {Nature},
  volume = {507},
  number = {7490},
  pages = {94--98},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature12935},
  abstract = {Direct neural recordings from electrodes over bilateral cortices show that sensory\textendash motor transformations for speech occur bilaterally; neural responses are robust during both perception and production in an overt word-repetition task, and bilateral sensory\textendash motor responses can perform transformations between speech-perception and speech-production representations during a non-word transformation task.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cognitive neuroscience;Language;Neuroscience;Sensorimotor processing Subject\_term\_id: cognitive-neuroscience;language;neuroscience;sensorimotor-processing},
  file = {/Users/xzfang/Zotero/storage/YUZLL9FZ/Cogan et al. - 2014 - Sensoryâ€“motor transformations for speech occur bil.pdf;/Users/xzfang/Zotero/storage/2FQ89LN4/nature12935.html}
}

@article{cohen_language_2002,
  title = {Language-specific Tuning of Visual Cortex? {{Functional}} Properties of the {{Visual Word Form Area}}},
  shorttitle = {Language-specific Tuning of Visual Cortex?},
  author = {Cohen, Laurent and Leh{\'e}ricy, St{\'e}phane and Chochon, Florence and Lemer, Cathy and Rivaud, Sophie and Dehaene, Stanislas},
  year = {2002},
  month = may,
  journal = {Brain},
  volume = {125},
  number = {5},
  pages = {1054--1069},
  issn = {0006-8950},
  doi = {10.1093/brain/awf094},
  abstract = {The first steps in the process of reading a printed word belong to the domain of visual object perception. They culminate in a representation of letter strings as an ordered set of abstract letter identities, a representation known as the Visual Word Form (VWF). Brain lesions in patients with pure alexia and functional imaging data suggest that the VWF is subtended by a restricted patch of left-hemispheric fusiform cortex, which is reproducibly activated during reading. In order to determine whether the operation of this Visual Word Form Area (VWFA) depends exclusively on the visual features of stimuli, or is influenced by language-dependent parameters, brain activations induced by words, consonant strings and chequerboards were compared in normal subjects using functional MRI (fMRI). Stimuli were presented in the left or right visual hemifield. The VWFA was identified in both a blocked-design experiment and an event-related experiment as a left-hemispheric inferotemporal area showing a stronger activation to alphabetic strings than to chequerboards, and invariant for the spatial location of stimuli. In both experiments, stronger activations of the VWFA to words than to strings of consonants were observed. Considering that the VWFA is equally activated by real words and by readable pseudowords, this result demonstrates that the VWFA is initially plastic and becomes attuned to the orthographic regularities that constrain letter combination during the acquisition of literacy. Additionally, the use of split-field stimulation shed some light on the cerebral bases of the classical right visual field (RVF) advantage in reading. A left occipital extrastriate area was found to be activated by RVF letter strings more than by chequerboards, while no symmetrical region was observed in the right hemisphere. Moreover, activations in the precuneus and the left thalamus were observed when subjects were reading RVF versus left visual field (LVF) words, and are likely to reflect the attentional component of the RVF advantage.},
  file = {/Users/xzfang/Zotero/storage/6RU8694T/Cohen et al. - 2002 - Languageâ€specific tuning of visual cortex Functio.pdf;/Users/xzfang/Zotero/storage/9WTY76TS/328099.html}
}

@article{cohen_visual_2000,
  title = {The Visual Word Form Area: {{Spatial}} and Temporal Characterization of an Initial Stage of Reading in Normal Subjects and Posterior Split-Brain Patients},
  shorttitle = {The Visual Word Form Area},
  author = {Cohen, Laurent and Dehaene, Stanislas and Naccache, Lionel and Leh{\'e}ricy, St{\'e}phane and {Dehaene-Lambertz}, Ghislaine and H{\'e}naff, Marie-Anne and Michel, Fran{\c c}ois},
  year = {2000},
  month = feb,
  journal = {Brain},
  volume = {123},
  number = {2},
  pages = {291--307},
  issn = {0006-8950},
  doi = {10.1093/brain/123.2.291},
  abstract = {A standard model of word reading postulates that visual information is initially processed by occipitotemporal areas contralateral to the stimulated hemifield, from whence it is subsequently transferred to the visual word form (VWF) system, a left inferior temporal region specifically devoted to the processing of letter strings. For stimuli displayed in the left visual field, this transfer proceeds from the right to the left hemisphere through the posterior portion of the corpus callosum. In order to characterize the spatial and temporal organization of these processes, reading tasks with split-field presentation were performed by five control subjects and by two patients suffering from left hemialexia following posterior callosal lesions. The subjects' responses were studied using behavioural measures and functional brain imaging techniques, providing both high spatial resolution (functional MRI, fMRI) and high temporal resolution (high-density event-related potentials, ERPs). Early visual processing was revealed as activations contralateral to stimulation, located by fMRI in the inferior occipitotemporal region and presumably coincident with area V4. A negative wave occurring 150\textendash 160 ms post-stimulus, also strictly contralateral to stimulation, was recorded over posterior electrodes. In contrast with these hemifield-dependent effects, the VWF system was revealed as a strictly left-hemispheric activation which, in control subjects, was identical for stimuli presented in the left or in the right hemifield and was located in the middle portion of the left fusiform gyrus. The electrical signature of the VWF system consisted of a unilateral sharp negativity, recorded 180\textendash 200 ms post-stimulus over left inferior temporal electrodes. In callosal patients, due to the inability of visual information to pass across the posterior part of the corpus callosum, the VWF system was activated only by stimuli presented in the right visual field. Similarly, a significant influence of the word/non-word status on ERPs recorded over the left hemisphere was discernible for either hemifield in controls, while it affected only right-hemifield stimuli in callosal patients. These findings provide direct support for the main components of the classical model of reading and help specify their timing and cerebral substrates.},
  file = {/Users/xzfang/Zotero/storage/3JHFZR9M/Cohen et al. - 2000 - The visual word form area Spatial and temporal ch.pdf;/Users/xzfang/Zotero/storage/F395PA8T/346042.html}
}

@article{cohn-sheehy_hippocampus_2021,
  title = {The Hippocampus Constructs Narrative Memories across Distant Events},
  author = {{Cohn-Sheehy}, Brendan I. and Delarazan, Angelique I. and Reagh, Zachariah M. and {Crivelli-Decker}, Jordan E. and Kim, Kamin and Barnett, Alexander J. and Zacks, Jeffrey M. and Ranganath, Charan},
  year = {2021},
  month = nov,
  journal = {Current Biology},
  volume = {31},
  number = {22},
  pages = {4935-4945.e7},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2021.09.013},
  abstract = {Life's events are scattered throughout time, yet we often recall different events in the context of an integrated narrative. Prior research suggests that the hippocampus, which supports memory for past events, can support the integration of overlapping associations or separate events in memory. However, the conditions that lead to hippocampus-dependent memory integration are unclear. We used functional brain imaging to test whether the opportunity to form a larger narrative (narrative coherence) drives hippocampal memory integration. During encoding of fictional stories, patterns of hippocampal activity, including activity at boundaries between events, were more similar between distant events that formed one coherent narrative, compared with overlapping events taken from unrelated narratives. One day later, the hippocampus preferentially supported detailed recall of coherent narrative events, through reinstatement of hippocampal activity patterns from encoding. These findings demonstrate a key function of the hippocampus: the integration of events into a narrative structure for memory.},
  langid = {english},
  keywords = {episodic memory,event cognition,fMRI,hippocampus,narratives,naturalistic stimuli,pattern similarity},
  file = {/Users/xzfang/Zotero/storage/57QQCW5M/Cohn-Sheehy et al. - 2021 - The hippocampus constructs narrative memories acro.pdf}
}

@article{collins_reasoning_2012,
  title = {Reasoning, {{Learning}}, and {{Creativity}}: {{Frontal Lobe Function}} and {{Human Decision-Making}}},
  shorttitle = {Reasoning, {{Learning}}, and {{Creativity}}},
  author = {Collins, Anne and Koechlin, Etienne},
  year = {2012},
  month = mar,
  journal = {PLOS Biology},
  volume = {10},
  number = {3},
  pages = {e1001293},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1001293},
  abstract = {Computational modeling and behavioral experimentation suggest that human frontal lobe function is capable of monitoring three or four concurrent behavioral strategies in order to select the most suitable one during decision-making.},
  langid = {english},
  keywords = {Decision making,Entropy,Human learning,Human performance,Learning,Long term memory,Reliability,Sensory cues},
  file = {/Users/xzfang/Zotero/storage/CQS89GRY/Collins and Koechlin - 2012 - Reasoning, Learning, and Creativity Frontal Lobe .pdf;/Users/xzfang/Zotero/storage/KASFEAEI/article.html}
}

@article{connell_interoception_2018,
  title = {Interoception: The Forgotten Modality in Perceptual Grounding of Abstract and Concrete Concepts},
  shorttitle = {Interoception},
  author = {Connell, Louise and Lynott, Dermot and Banks, Briony},
  year = {2018},
  month = aug,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {373},
  number = {1752},
  pages = {20170143},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2017.0143},
  abstract = {Conceptual representations are perceptually grounded, but when investigating which perceptual modalities are involved, researchers have typically restricted their consideration to vision, touch, hearing, taste and smell. However, there is another major modality of perceptual information that is distinct from these traditional five senses; that is, interoception, or sensations inside the body. In this paper, we use megastudy data (modality-specific ratings of perceptual strength for over 32 000 words) to explore how interoceptive information contributes to the perceptual grounding of abstract and concrete concepts. We report how interoceptive strength captures a distinct form of perceptual experience across the abstract\textendash concrete spectrum, but is markedly more important to abstract concepts (e.g. hungry, serenity) than to concrete concepts (e.g. capacity, rainy). In particular, interoception dominates emotion concepts, especially negative emotions relating to fear and sadness, moreso than other concepts of equivalent abstractness and valence. Finally, we examine whether interoceptive strength represents valuable information in conceptual content by investigating its role in concreteness effects in word recognition, and find that it enhances semantic facilitation over and above the traditional five sensory modalities. Overall, these findings suggest that interoception has comparable status to other modalities in contributing to the perceptual grounding of abstract and concrete concepts. This article is part of the theme issue `Varieties of abstract concepts: development, use and representation in the brain'.},
  keywords = {concepts,grounding,interoception,perceptual simulation,perceptual strength},
  file = {/Users/xzfang/Zotero/storage/HIGPIUBL/Connell et al. - 2018 - Interoception the forgotten modality in perceptua.pdf}
}

@article{connine_effects_1991,
  title = {Effects of {{Subsequent Sentence Context}} in {{Auditory Word Recognition}}: {{Temporal}} and {{Linguistic Constraints}}},
  shorttitle = {Effects of {{Subsequent Sentence Context}} in {{Auditory Word Recognition}}},
  author = {Connine, Cynthia M. and Blasko, Dawn G. and Hall, Michael},
  year = {1991},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {30},
  number = {1},
  pages = {234--250},
  publisher = {{Academic Press}},
  address = {{New York, United States}},
  issn = {0749-596X},
  langid = {english},
  keywords = {Education,Linguistics},
  file = {/Users/xzfang/Zotero/storage/FXX8JDTP/Connine et al. - 1991 - Effects of Subsequent Sentence Context in Auditory.pdf}
}

@article{constantinescu_organizing_2016,
  title = {Organizing Conceptual Knowledge in Humans with a Gridlike Code},
  author = {Constantinescu, Alexandra O. and O'Reilly, Jill X. and Behrens, Timothy E. J.},
  year = {2016},
  month = jun,
  journal = {Science},
  volume = {352},
  number = {6292},
  pages = {1464--1468},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf0941},
  abstract = {Coding abstract concepts in the brain Grid cells are thought to provide the neuronal code that underlies spatial knowledge in the brain. Grid cells have mostly been studied in the context of path integration. However, recent theoretical studies have suggested that they may have a broader role in the organization of general knowledge. Constantinescu et al. investigated whether the neural representation of concepts follows a structure similar to the representation of space in the entorhinal cortex. Several brain regions, including the entorhinal cortex and the ventromedial prefrontal cortex, showed gridlike neural representation of conceptual space. Science, this issue p. 1464 It has been hypothesized that the brain organizes concepts into a mental map, allowing conceptual relationships to be navigated in a manner similar to that of space. Grid cells use a hexagonally symmetric code to organize spatial representations and are the likely source of a precise hexagonal symmetry in the functional magnetic resonance imaging signal. Humans navigating conceptual two-dimensional knowledge showed the same hexagonal signal in a set of brain regions markedly similar to those activated during spatial navigation. This gridlike signal is consistent across sessions acquired within an hour and more than a week apart. Our findings suggest that global relational codes may be used to organize nonspatial conceptual representations and that these codes may have a hexagonal gridlike pattern when conceptual knowledge is laid out in two continuous dimensions. Grid cells in the brain can also represent nonspatial knowledge. Grid cells in the brain can also represent nonspatial knowledge.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  langid = {english},
  pmid = {27313047},
  file = {/Users/xzfang/Zotero/storage/SQT9MANE/Constantinescu et al. - 2016 - Organizing conceptual knowledge in humans with a g.pdf;/Users/xzfang/Zotero/storage/QFFDHYFD/1464.html}
}

@article{conway_communication_2020,
  title = {Communication Efficiency of Color Naming across Languages Provides a New Framework for the Evolution of Color Terms},
  author = {Conway, Bevil R. and Ratnasingam, Sivalogeswaran and {Jara-Ettinger}, Julian and Futrell, Richard and Gibson, Edward},
  year = {2020},
  month = feb,
  journal = {Cognition},
  volume = {195},
  pages = {104086},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.104086},
  abstract = {Languages vary in their number of color terms. A widely accepted theory proposes that languages evolve, acquiring color terms in a stereotyped sequence. This theory, by Berlin and Kay (BK), is supported by analyzing best exemplars (``focal colors'') of basic color terms in the World Color Survey (WCS) of 110 languages. But the instructions of the WCS were complex and the color chips confounded hue and saturation, which likely impacted focal-color selection. In addition, it is now known that even so-called early-stage languages nonetheless have a complete representation of color distributed across the population. These facts undermine the BK theory. Here we revisit the evolution of color terms using original color-naming data obtained with simple instructions in Tsimane', an Amazonian culture that has limited contact with industrialized society. We also collected data in Bolivian-Spanish speakers and English speakers. We discovered that information theory analysis of color-naming data was not influenced by color-chip saturation, which motivated a new analysis of the WCS data. Embedded within a universal pattern in which warm colors (reds, oranges) are always communicated more efficiently than cool colors (blues, greens), as languages increase in overall communicative efficiency about color, some colors undergo greater increases in communication efficiency compared to others. Communication efficiency increases first for yellow, then brown, then purple. The present analyses and results provide a new framework for understanding the evolution of color terms: what varies among cultures is not whether colors are seen differently, but the extent to which color is useful.},
  langid = {english},
  keywords = {Color categories,Communication efficiency,Cross-cultural,Information theory,Universal},
  file = {/Users/xzfang/Zotero/storage/T8UZGF8P/Conway et al. - 2020 - Communication efficiency of color naming across la.pdf;/Users/xzfang/Zotero/storage/U8AF9XLR/S0010027719302604.html}
}

@article{cooke_foreign_2008,
  title = {The Foreign Language Cocktail Party Problem: {{Energetic}} and Informational Masking Effects in Non-Native Speech Perception},
  shorttitle = {The Foreign Language Cocktail Party Problem},
  author = {Cooke, Martin and Garcia Lecumberri, M. L. and Barker, Jon},
  year = {2008},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {123},
  number = {1},
  pages = {414--427},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2804952},
  file = {/Users/xzfang/Zotero/storage/PR6RCKGS/Cooke et al. - 2008 - The foreign language cocktail party problem Energ.pdf}
}

@article{coopmans_hierarchy_2021,
  title = {Hierarchy in Language Interpretation: Evidence from Behavioural Experiments and Computational Modelling},
  shorttitle = {Hierarchy in Language Interpretation},
  author = {Coopmans, Cas W. and {de Hoop}, Helen and Kaushik, Karthikeya and Hagoort, Peter and Martin, Andrea E.},
  year = {2021},
  month = sep,
  journal = {Language, Cognition and Neuroscience},
  volume = {0},
  number = {0},
  pages = {1--20},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2021.1980595},
  abstract = {It has long been recognised that phrases and sentences are organised hierarchically, but many computational models of language treat them as sequences of words without computing constituent structure. Against this background, we conducted two experiments which showed that participants interpret ambiguous noun phrases, such as second blue ball, in terms of their abstract hierarchical structure rather than their linear surface order. When a neural network model was tested on this task, it could simulate such ``hierarchical'' behaviour. However, when we changed the training data such that they were not entirely unambiguous anymore, the model stopped generalising in a human-like way. It did not systematically generalise to novel items, and when it was trained on ambiguous trials, it strongly favoured the linear interpretation. We argue that these models should be endowed with a bias to make generalisations over hierarchical structure in order to be cognitively adequate models of human language.},
  keywords = {constituency,human-like generalisation,LSTM,meaning,Syntax},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2021.1980595},
  file = {/Users/xzfang/Zotero/storage/3CKC7JB8/Coopmans et al. - 2021 - Hierarchy in language interpretation evidence fro.pdf;/Users/xzfang/Zotero/storage/BHA6K5SG/23273798.2021.html}
}

@inproceedings{corkery_are_2019,
  title = {Are We There yet? {{Encoder-decoder}} Neural Networks as Cognitive Models of {{English}} Past Tense Inflection},
  shorttitle = {Are We There Yet?},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Corkery, Maria and Matusevych, Yevgen and Goldwater, Sharon},
  year = {2019},
  month = jul,
  pages = {3868--3877},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1376},
  abstract = {The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their model in this task. We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong\textemdash worse than an older rule-based model. These findings hold up through several alternative training regimes and evaluation measures. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task.},
  file = {/Users/xzfang/Zotero/storage/JBXEGKKZ/Corkery et al. - 2019 - Are we there yet Encoder-decoder neural networks .pdf}
}

@article{correia_decoding_2015,
  title = {Decoding {{Articulatory Features}} from {{fMRI Responses}} in {{Dorsal Speech Regions}}},
  author = {Correia, J. M. and Jansma, B. M. B. and Bonte, M.},
  year = {2015},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {35},
  number = {45},
  pages = {15015--15025},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0977-15.2015},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J25X2L26/Correia et al. - 2015 - Decoding Articulatory Features from fMRI Responses.pdf}
}

@article{correia_eeg_2015,
  title = {{{EEG}} Decoding of Spoken Words in Bilingual Listeners: From Words to Language Invariant Semantic-Conceptual Representations},
  shorttitle = {{{EEG}} Decoding of Spoken Words in Bilingual Listeners},
  author = {Correia, Jo{\~a}o M. and Jansma, Bernadette and Hausfeld, Lars and Kikkert, Sanne and Bonte, Milene},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00071},
  abstract = {Spoken word recognition and production require fast transformations between acoustic, phonological and conceptual neural representations. Bilinguals perform these transformations in native and non-native languages, deriving unified semantic concepts from equivalent, but acoustically different words. Here we exploit this capacity of bilinguals to investigate input invariant semantic representations in the brain. We acquired EEG data while Dutch subjects, highly proficient in English listened to four monosyllabic and acoustically distinct animal words in both languages (e.g. `paard'-`horse'). Multivariate pattern analysis (MVPA) was applied to identify EEG response patterns that discriminate between individual words within one language (within-language discrimination) and generalize meaning across two languages (across-language generalization). Furthermore, employing two EEG feature selection approaches, we assessed the contribution of temporal and oscillatory EEG features to our classification results. MVPA revealed that within-language discrimination was possible in a broad time-window (\textasciitilde 50-620 ms) after word onset probably reflecting acoustic-phonetic and semantic-conceptual differences between the words. Most interestingly, significant across-language generalization was possible around 550-600 ms, suggesting the activation of common semantic-conceptual representations from the Dutch and English nouns. Both types of classification, showed a strong contribution of oscillations below 12 Hz, indicating the importance of low frequency oscillations in the neural representation of individual words and concepts. This study demonstrates the feasibility of MVPA to decode individual spoken words from EEG responses and to assess the spectro-temporal dynamics of their language invariant semantic-conceptual representations. We discuss how this method and results could be relevant to track the neural mechanisms underlying conceptual encoding in comprehension and production.},
  langid = {english},
  keywords = {Bilinguals,Conceptual representation,EEG decoding,EEG Oscillations,Semantic representations,Speech Perception,spoken word recognition},
  file = {/Users/xzfang/Zotero/storage/EEYWVLNY/Correia et al. - 2015 - EEG decoding of spoken words in bilingual listener.pdf}
}

@article{correia_eeg_2015a,
  title = {{{EEG}} Decoding of Spoken Words in Bilingual Listeners: From Words to Language Invariant Semantic-Conceptual Representations},
  shorttitle = {{{EEG}} Decoding of Spoken Words in Bilingual Listeners},
  author = {Correia, Jo{\~a}o M. and Jansma, Bernadette and Hausfeld, Lars and Kikkert, Sanne and Bonte, Milene},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00071},
  abstract = {Spoken word recognition and production require fast transformations between acoustic, phonological and conceptual neural representations. Bilinguals perform these transformations in native and non-native languages, deriving unified semantic concepts from equivalent, but acoustically different words. Here we exploit this capacity of bilinguals to investigate input invariant semantic representations in the brain. We acquired EEG data while Dutch subjects, highly proficient in English listened to four monosyllabic and acoustically distinct animal words in both languages (e.g. `paard'-`horse'). Multivariate pattern analysis (MVPA) was applied to identify EEG response patterns that discriminate between individual words within one language (within-language discrimination) and generalize meaning across two languages (across-language generalization). Furthermore, employing two EEG feature selection approaches, we assessed the contribution of temporal and oscillatory EEG features to our classification results. MVPA revealed that within-language discrimination was possible in a broad time-window (\textasciitilde 50-620 ms) after word onset probably reflecting acoustic-phonetic and semantic-conceptual differences between the words. Most interestingly, significant across-language generalization was possible around 550-600 ms, suggesting the activation of common semantic-conceptual representations from the Dutch and English nouns. Both types of classification, showed a strong contribution of oscillations below 12 Hz, indicating the importance of low frequency oscillations in the neural representation of individual words and concepts. This study demonstrates the feasibility of MVPA to decode individual spoken words from EEG responses and to assess the spectro-temporal dynamics of their language invariant semantic-conceptual representations. We discuss how this method and results could be relevant to track the neural mechanisms underlying conceptual encoding in comprehension and production.},
  langid = {english},
  keywords = {Bilinguals,Conceptual representation,EEG decoding,EEG Oscillations,Semantic representations,Speech Perception,spoken word recognition},
  file = {/Users/xzfang/Zotero/storage/ABZYSCT9/Correia et al. - 2015 - EEG decoding of spoken words in bilingual listener.pdf}
}

@article{cosgrove_quantifying_2021,
  title = {Quantifying Flexibility in Thought: {{The}} Resiliency of Semantic Networks Differs across the Lifespan},
  shorttitle = {Quantifying Flexibility in Thought},
  author = {Cosgrove, Abigail L. and Kenett, Yoed N. and Beaty, Roger E. and Diaz, Michele T.},
  year = {2021},
  month = jun,
  journal = {Cognition},
  volume = {211},
  pages = {104631},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104631},
  abstract = {Older adults tend to have a broader vocabulary compared to younger adults \textendash{} indicating a richer storage of semantic knowledge \textendash{} but their retrieval abilities decline with age. Recent advances in quantitative methods based on network science have investigated the effect of aging on semantic memory structure. However, it is yet to be determined how this aging effect on semantic memory structure relates to its overall flexibility. Percolation analysis provides a quantitative measure of the flexibility of a semantic network, by examining how a semantic memory network is resistant to ``attacks'' or breaking apart. In this study, we incorporated percolation analyses to examine how semantic networks of younger and older adults break apart to investigate potential age-related differences in language production. We applied the percolation analysis to 3 independent sets of data (total N~=~78 younger, 78 older adults) from which we generated semantic networks based on verbal fluency performance. Across all 3 datasets, the percolation integrals of the younger adults were larger than older adults, indicating that older adults' semantic networks were less flexible and broke down faster than the younger adults'. Our findings provide quantitative evidence for diminished flexibility in older adults' semantic networks, despite the stability of semantic knowledge across the lifespan. This may be one contributing factor to age-related differences in language production.},
  langid = {english},
  keywords = {Aging,Cognition,Percolation,Semantic networks,Verbal fluency},
  file = {/Users/xzfang/Zotero/storage/3CW8CP3M/Cosgrove et al. - 2021 - Quantifying flexibility in thought The resiliency.pdf;/Users/xzfang/Zotero/storage/WTKRXD38/S0010027721000500.html}
}

@article{cowen_mapping_2019,
  title = {Mapping 24 Emotions Conveyed by Brief Human Vocalization.},
  author = {Cowen, Alan S. and Elfenbein, Hillary Anger and Laukka, Petri and Keltner, Dacher},
  year = {2019},
  month = sep,
  journal = {American Psychologist},
  volume = {74},
  number = {6},
  pages = {698--712},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/amp0000399},
  abstract = {Emotional vocalizations are central to human social life. Recent studies have documented that people recognize at least 13 emotions in brief vocalizations. This capacity emerges early in development, is preserved in some form across cultures, and informs how people respond emotionally to music. What is poorly understood is how emotion recognition from vocalization is structured within what we call a semantic space, the study of which addresses questions critical to the field: How many distinct kinds of emotions can be expressed? Do expressions convey emotion categories or affective appraisals (e.g., valence, arousal)? Is the recognition of emotion expressions discrete or continuous? Guided by a new theoretical approach to emotion taxonomies, we apply large-scale data collection and analysis techniques to judgments of 2,032 emotional vocal bursts produced in laboratory settings (Study 1) and 48 found in the real world (Study 2) by U.S. English speakers (N Ï­ 1,105). We find that vocal bursts convey at least 24 distinct kinds of emotion. Emotion categories (sympathy, awe), more so than affective appraisals (including valence and arousal), organize emotion recognition. In contrast to discrete emotion theories, the emotion categories conveyed by vocal bursts are bridged by smooth gradients with continuously varying meaning. We visualize the complex, highdimensional space of emotion conveyed by brief human vocalization within an online interactive map.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Y9C6AJ38/Cowen et al. - 2019 - Mapping 24 emotions conveyed by brief human vocali.pdf}
}

@article{creel_distant_2004,
  title = {Distant {{Melodies}}: {{Statistical Learning}} of {{Nonadjacent Dependencies}} in {{Tone Sequences}}.},
  shorttitle = {Distant {{Melodies}}},
  author = {Creel, Sarah C. and Newport, Elissa L. and Aslin, Richard N.},
  year = {2004},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {30},
  number = {5},
  pages = {1119--1130},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/0278-7393.30.5.1119},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZLECQAE6/Creel et al. - 2004 - Distant Melodies Statistical Learning of Nonadjac.pdf}
}

@article{creel_heeding_2008,
  title = {Heeding the Voice of Experience: {{The}} Role of Talker Variation in Lexical Access},
  shorttitle = {Heeding the Voice of Experience},
  author = {Creel, Sarah C. and Aslin, Richard N. and Tanenhaus, Michael K.},
  year = {2008},
  month = feb,
  journal = {Cognition},
  volume = {106},
  number = {2},
  pages = {633--664},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.03.013},
  abstract = {Two experiments used the head-mounted eye-tracking methodology to examine the time course of lexical activation in the face of a non-phonemic cue, talker variation. We found that lexical competition was attenuated by consistent talker differences between words that would otherwise be lexical competitors. In Experiment 1, some English cohort word-pairs were consistently spoken by a single talker (male couch, male cows), while other word-pairs were spoken by different talkers (male sheep, female sheet). After repeated instances of talker-word pairings, words from different-talker pairs showed smaller proportions of competitor fixations than words from same-talker pairs. In Experiment 2, participants learned to identify black-and-white shapes from novel labels spoken by one of two talkers. All of the 16 novel labels were VCVCV word-forms atypical of, but not phonologically illegal in, English. Again, a word was consistently spoken by one talker, and its cohort or rhyme competitor was consistently spoken either by that same talker (same-talker competitor) or the other talker (different-talker competitor). Targets with different-talker cohorts received greater fixation proportions than targets with same-talker cohorts, while the reverse was true for fixations to cohort competitors; there were fewer erroneous selections of competitor referents for different-talker competitors than same-talker competitors. Overall, these results support a view of the lexicon in which entries contain extra-phonemic information. Extensions of the artificial lexicon paradigm and developmental implications are discussed.},
  langid = {english},
  keywords = {Artificial lexicon,Cohort,Eye-tracking,Lexical access,Talker variation,Word recognition},
  file = {/Users/xzfang/Zotero/storage/A2PKMYKA/Creel et al. - 2008 - Heeding the voice of experience The role of talke.pdf;/Users/xzfang/Zotero/storage/HU5GM5H4/S0010027707000935.html}
}

@article{creel_online_2011,
  title = {On-Line Acoustic and Semantic Interpretation of Talker Information},
  author = {Creel, Sarah C. and Tumlin, Melanie A.},
  year = {2011},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {65},
  number = {3},
  pages = {264--285},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2011.06.005},
  abstract = {Recent work demonstrates that listeners utilize talker-specific information in the speech signal to inform real-time language processing. However, there are multiple representational levels at which this may take place. Listeners might use acoustic cues in the speech signal to access the talker's identity and information about what they tend to talk about, which then immediately constrains processing. Alternatively, or simultaneously, listeners might compare the signal to acoustically-detailed representations of words, without awareness of the talker's identity. In a series of eye-tracked comprehension experiments, we explore the circumstances under which listeners utilize talker-specific information. Experiments 1 and 2 demonstrate talker-specific recognition benefits for newly-learned words both in isolation (Experiment 1) and with preceding context (Experiment 2), but suggest that listeners do not strongly semantically associate talkers with referents. Experiment 3 demonstrates that listeners can recognize talkers rapidly, almost as soon as acoustic information is available, and can associate talkers with multiple arbitrary referents. Experiment 4 demonstrates that if talker identity is highly diagnostic on each trial, listeners readily associate talkers with specific referents, but do not seem to make such associations when diagnostic value is low. Implications for speech processing, talker processing, and learning are discussed.},
  langid = {english},
  keywords = {Eye tracking,Representational specificity,Spoken language comprehension,Talker characteristic,Talker variability,Word learning},
  file = {/Users/xzfang/Zotero/storage/PBYZ68TG/Creel and Tumlin - 2011 - On-line acoustic and semantic interpretation of ta.pdf;/Users/xzfang/Zotero/storage/DXIMU2RJ/S0749596X11000647.html}
}

@article{crosse_multivariate_2016,
  title = {The {{Multivariate Temporal Response Function}} ({{mTRF}}) {{Toolbox}}: {{A MATLAB Toolbox}} for {{Relating Neural Signals}} to {{Continuous Stimuli}}},
  shorttitle = {The {{Multivariate Temporal Response Function}} ({{mTRF}}) {{Toolbox}}},
  author = {Crosse, Michael J. and Di Liberto, Giovanni M. and Bednar, Adam and Lalor, Edmund C.},
  year = {2016},
  journal = {Frontiers in Human Neuroscience},
  volume = {10},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2016.00604},
  abstract = {Understanding how brains process sensory signals in natural environments is one of the key goals of 21st century neuroscience. While brain imaging and invasive electrophysiology will play key roles in this endeavor, there is also an important role to be played by noninvasive, macroscopic techniques with high temporal resolution such as electro- and magnetoencephalography. But challenges exist in determining how best to analyze such complex, time-varying neural responses to complex, time-varying and multivariate natural sensory stimuli. There has been a long history of applying system identification techniques to relate the firing activity of neurons to complex sensory stimuli and such techniques are now seeing increased application to EEG and MEG data. One particular example involves fitting a filter \textendash{} often referred to as a temporal response function \textendash{} that describes a mapping between some feature(s) of a sensory stimulus and the neural response. Here, we first briefly review the history of these system identification approaches and describe a specific technique for deriving temporal response functions known as regularized linear regression. We then introduce a new open-source toolbox for performing this analysis. We describe how it can be used to derive (multivariate) temporal response functions describing a mapping between stimulus and response in both directions. We also explain the importance of regularizing the analysis and how this regularization can be optimized for a particular dataset. We then outline specifically how the toolbox implements these analyses and provide several examples of the types of results that the toolbox can produce. Finally, we consider some of the limitations of the toolbox and opportunities for future development and application.},
  langid = {english},
  keywords = {EEG/MEG,reverse correlation,sensory processing,stimulus reconstruction,system identification},
  file = {/Users/xzfang/Zotero/storage/BJGUNRA8/Crosse et al. - 2016 - The Multivariate Temporal Response Function (mTRF).pdf}
}

@article{cusack_effects_2004,
  title = {Effects of {{Location}}, {{Frequency Region}}, and {{Time Course}} of {{Selective Attention}} on {{Auditory Scene Analysis}}.},
  author = {Cusack, Rhodri and Decks, John and Aikman, Genevieve and Carlyon, Robert P.},
  year = {2004},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {30},
  number = {4},
  pages = {643--656},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.30.4.643},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5EQIWTQ5/Cusack et al. - 2004 - Effects of Location, Frequency Region, and Time Co.pdf}
}

@article{cwiek_bouba_2022,
  title = {The Bouba/Kiki Effect Is Robust across Cultures and Writing Systems},
  author = {{\'C}wiek, Aleksandra and Fuchs, Susanne and Draxler, Christoph and Asu, Eva Liina and Dediu, Dan and Hiovain, Katri and Kawahara, Shigeto and Koutalidis, Sofia and Krifka, Manfred and Lippus, P{\"a}rtel and Lupyan, Gary and Oh, Grace E. and Paul, Jing and Petrone, Caterina and Ridouane, Rachid and Reiter, Sabine and Sch{\"u}mchen, Nathalie and Szalontai, {\'A}d{\'a}m and {\"U}nal-Logacev, {\"O}zlem and Zeller, Jochen and Perlman, Marcus and Winter, Bodo},
  year = {2022},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {377},
  number = {1841},
  pages = {20200390},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2020.0390},
  abstract = {The bouba/kiki effect\textemdash the association of the nonce word bouba with a round shape and kiki with a spiky shape\textemdash is a type of correspondence between speech sounds and visual properties with potentially deep implications for the evolution of spoken language. However, there is debate over the robustness of the effect across cultures and the influence of orthography. We report an online experiment that tested the bouba/kiki effect across speakers of 25 languages representing nine language families and 10 writing systems. Overall, we found strong evidence for the effect across languages, with bouba eliciting more congruent responses than kiki. Participants who spoke languages with Roman scripts were only marginally more likely to show the effect, and analysis of the orthographic shape of the words in different scripts showed that the effect was no stronger for scripts that use rounder forms for bouba and spikier forms for kiki. These results confirm that the bouba/kiki phenomenon is rooted in crossmodal correspondence between aspects of the voice and visual shape, largely independent of orthography. They provide the strongest demonstration to date that the bouba/kiki effect is robust across cultures and writing systems. This article is part of the theme issue `Voice modulation: from origin and mechanism to social impact (Part II)'.},
  keywords = {crossmodal association,iconicity,perception,sound symbolism,universals},
  file = {/Users/xzfang/Zotero/storage/KR8HF8AX/Ä†wiek et al. - 2022 - The boubakiki effect is robust across cultures an.pdf}
}

@article{dahan_contextconditioned_2010,
  title = {Context-Conditioned Generalization in Adaptation to Distorted Speech.},
  author = {Dahan, Delphine and Mead, Rebecca L.},
  year = {2010},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {36},
  number = {3},
  pages = {704--728},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/a0017449},
  abstract = {People were trained to decode noise-vocoded speech by hearing monosyllabic stimuli in distorted and unaltered forms. When later presented with different stimuli, listeners were able to successfully generalize their experience. However, generalization was modulated by the degree to which testing stimuli resembled training stimuli: Testing stimuli's consonants were easier to recognize when they had occurred in the same position at training, or flanked by the same vowel, than when they did not. Furthermore, greater generalization occurred when listeners had been trained on existing words than on nonsense strings. We propose that the process by which adult listeners learn to interpret distorted speech is akin to building phonological categories in one's native language, a process where categories and structure emerge from the words in the ambient language without completely abstracting from them.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/68BF34TB/Dahan and Mead - 2010 - Context-conditioned generalization in adaptation t.pdf}
}

@article{dahan_subcategorical_2001,
  title = {Subcategorical Mismatches and the Time Course of Lexical Access: {{Evidence}} for Lexical Competition},
  shorttitle = {Subcategorical Mismatches and the Time Course of Lexical Access},
  author = {Dahan, Delphine and Magnuson, James S. and Tanenhaus, Michael K. and Hogan, Ellen M.},
  year = {2001},
  month = oct,
  journal = {Language and Cognitive Processes},
  volume = {16},
  number = {5-6},
  pages = {507--534},
  publisher = {{Routledge}},
  issn = {0169-0965},
  doi = {10.1080/01690960143000074},
  abstract = {Participants' eye movements were monitored as they followed spoken instructions to click on a pictured object with a computer mouse (e.g., ''click on the net''). Participants were slower to fixate the target picture when the onset of the target word came from a competitor word (e.g., ne(ck)t) than from a nonword (e.g., ne(p)t), as predicted by models of spoken-word recognition that incorporate lexical competition. This was found whether the picture of the competitor word (e.g., the picture of a neck) was present on the display or not. Simulations with the TRACE model captured the major trends of fixations to the target and its competitor over time. We argue that eye movements provide a fine-grained measure of lexical activation over time, and thus reveal effects of lexical competition that are masked by response measures such as lexical decisions.},
  annotation = {\_eprint: https://doi.org/10.1080/01690960143000074},
  file = {/Users/xzfang/Zotero/storage/YG5NMEMB/Dahan et al. - 2001 - Subcategorical mismatches and the time course of l.pdf;/Users/xzfang/Zotero/storage/5I9SWJPA/01690960143000074.html}
}

@article{dahan_talker_2008,
  title = {Talker Adaptation in Speech Perception: {{Adjusting}} the Signal or the Representations?},
  shorttitle = {Talker Adaptation in Speech Perception},
  author = {Dahan, Delphine and Drucker, Sarah J. and Scarborough, Rebecca A.},
  year = {2008},
  month = sep,
  journal = {Cognition},
  volume = {108},
  number = {3},
  pages = {710},
  publisher = {{NIH Public Access}},
  doi = {10.1016/j.cognition.2008.06.003},
  abstract = {Past research has established that listeners can accommodate a wide range of talkers in understanding language. How this adjustment operates, however, is a matter of debate. Here, listeners were exposed to spoken words from a speaker of an American English ...},
  langid = {english},
  pmid = {18653175},
  file = {/Users/xzfang/Zotero/storage/4V8U872L/Dahan et al. - 2008 - Talker adaptation in speech perception Adjusting .pdf;/Users/xzfang/Zotero/storage/ICHKEXP7/PMC2614823.html}
}

@article{dahan_time_2001,
  title = {Time {{Course}} of {{Frequency Effects}} in {{Spoken-Word Recognition}}: {{Evidence}} from {{Eye Movements}}},
  shorttitle = {Time {{Course}} of {{Frequency Effects}} in {{Spoken-Word Recognition}}},
  author = {Dahan, Delphine and Magnuson, James S. and Tanenhaus, Michael K.},
  year = {2001},
  month = jun,
  journal = {Cognitive Psychology},
  volume = {42},
  number = {4},
  pages = {317--367},
  issn = {0010-0285},
  doi = {10.1006/cogp.2001.0750},
  abstract = {In two experiments, eye movements were monitored as participants followed spoken instructions to click on and move pictures with a computer mouse. In Experiment 1, a referent picture (e.g., the picture of a bench) was presented along with three pictures, two of which had names that shared the same initial phonemes as the name of the referent (e.g., bed and bell). Participants were more likely to fixate the picture with the higher frequency name (bed) than the picture with the lower frequency name (bell). In Experiment 2, referent pictures were presented with three unrelated distractors. Fixation latencies to referents with high-frequency names were shorter than those to referents with low-frequency names. The proportion of fixations to the referents and distractors were analyzed in 33-ms time slices to provide fine-grained information about the time course of frequency effects. These analyses established that frequency affects the earliest moments of lexical access and rule out a late-acting, decision-bias locus for frequency. Simulations using models in which frequency operates on resting-activation levels, on connection strengths, and as a postactivation decision bias provided further constraints on the locus of frequency effects.},
  langid = {english},
  keywords = {eye tracking,Key Words: lexical frequency,spoken-word recognition},
  file = {/Users/xzfang/Zotero/storage/2TWZQDAG/Dahan et al. - 2001 - Time Course of Frequency Effects in Spoken-Word Re.pdf;/Users/xzfang/Zotero/storage/ZBUCI5W4/S0010028501907509.html}
}

@inproceedings{dai_deformable_2017,
  title = {Deformable {{Convolutional Networks}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  year = {2017},
  pages = {764--773},
  file = {/Users/xzfang/Zotero/storage/S9PSF8KU/Dai et al. - 2017 - Deformable Convolutional Networks.pdf;/Users/xzfang/Zotero/storage/QB7YM8ZT/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.html}
}

@article{dai_neural_2018,
  title = {Neural Mechanisms for Selectively Tuning in to the Target Speaker in a Naturalistic Noisy Situation},
  author = {Dai, Bohan and Chen, Chuansheng and Long, Yuhang and Zheng, Lifen and Zhao, Hui and Bai, Xialu and Liu, Wenda and Zhang, Yuxuan and Liu, Li and Guo, Taomei and Ding, Guosheng and Lu, Chunming},
  year = {2018},
  month = jun,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {2405},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04819-z},
  abstract = {The neural mechanism for selectively tuning in~to a target speaker while tuning out the~others in a multi-speaker situation (i.e., the cocktail-party effect) remains elusive. Here we addressed this issue by measuring brain activity simultaneously from a listener and from multiple speakers while they were involved in naturalistic conversations. Results consistently show selectively enhanced interpersonal neural synchronization (INS) between the listener and the attended speaker at left temporal\textendash parietal junction, compared with that between the listener and the unattended speaker across different multi-speaker situations. Moreover, INS increases significantly prior to the occurrence of verbal responses, and even when the listener's brain activity precedes that of the speaker. The INS increase is independent of brain-to-speech synchronization in both the anatomical location and frequency range. These findings suggest that INS underlies the selective process in a multi-speaker situation through neural predictions at the content level~but not the sensory level of speech.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Attention,Cortex,Psychology,Sensory processing},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Attention;Cortex;Psychology;Sensory processing Subject\_term\_id: attention;cortex;psychology;sensory-processing},
  file = {/Users/xzfang/Zotero/storage/6MZG4VEI/Dai et al. - 2018 - Neural mechanisms for selectively tuning in to the.pdf;/Users/xzfang/Zotero/storage/ZQDBW5XW/s41467-018-04819-z.html}
}

@article{daltrozzo_conceptual_2008,
  title = {Conceptual {{Processing}} in {{Music}} as {{Revealed}} by {{N400 Effects}} on {{Words}} and {{Musical Targets}}},
  author = {Daltrozzo, J{\'e}r{\^o}me and Sch{\"o}n, Daniele},
  year = {2008},
  month = oct,
  journal = {Journal of cognitive neuroscience},
  volume = {21},
  pages = {1882--92},
  doi = {10.1162/jocn.2009.21113},
  abstract = {The cognitive processing of concepts, that is, abstract general ideas, has been mostly studied with language. However, other domains, such as music, can also convey concepts. Koelsch et al. [Koelsch, S., Kasper, E., Sammler, D., Schulze, K., Gunter, T., \& Friederici, A. D. Music, language and meaning: Brain signatures of semantic processing. Nature Neuroscience, 7, 302-307, 2004] showed that 10 sec of music can influence the semantic processing of words. However, the length of the musical excerpts did not allow the authors to study the effect of words on musical targets. In this study, we decided to replicate Koelsch et al. findings using 1-sec musical excerpts (Experiment 1). This allowed us to study the reverse influence, namely, of a linguistic context on conceptual processing of musical excerpts (Experiment 2). In both experiments, we recorded behavioral and electrophysiological responses while participants were presented 50 related and 50 unrelated pairs (context/target). Experiments 1 and 2 showed a larger N400 component of the event-related brain potentials to targets following a conceptually unrelated compared to a related context. The presence of an N400 effect with musical targets suggests that music may convey concepts. The relevance of these results for the comprehension of music as a structured set of conceptual units and for the domain specificity of the mechanisms underlying N400 effects are discussed.}
}

@article{dare_serial_2013,
  title = {Serial and Parallel Processing in Reading: {{Investigating}} the Effects of Parafoveal Orthographic Information on Nonisolated Word Recognition},
  shorttitle = {Serial and Parallel Processing in Reading},
  author = {Dare, Natasha and Shillcock, Richard},
  year = {2013},
  month = mar,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  number = {3},
  pages = {487--504},
  publisher = {{SAGE Publications}},
  issn = {1747-0218},
  doi = {10.1080/17470218.2012.703212},
  abstract = {We present a novel lexical decision task and three boundary paradigm eye-tracking experiments that clarify the picture of parallel processing in word recognition in context. First, we show that lexical decision is facilitated by associated letter information to the left and right of the word, with no apparent hemispheric specificity. Second, we show that parafoveal preview of a repeat of word n at word n + 1 facilitates reading of word n relative to a control condition with an unrelated word at word n + 1. Third, using a version of the boundary paradigm that allowed for a regressive eye movement, we show no parafoveal ``postview'' effect on reading word n of repeating word n at word n \textendash{} 1. Fourth, we repeat the second experiment but compare the effects of parafoveal previews consisting of a repeated word n with a transposed central bigram (e.g., caot for coat) and a substituted central bigram (e.g., ceit for coat), showing the latter to have a deleterious effect on processing word n, thereby demonstrating that the parafoveal preview effect is at least orthographic and not purely visual.},
  langid = {english},
  keywords = {Eye tracking,Parafoveal preview,Parallel processing,Reading,Serial processing,Word recognition},
  file = {/Users/xzfang/Zotero/storage/TFK68F2S/Dare and Shillcock - 2013 - Serial and parallel processing in reading Investi.pdf}
}

@article{daube_simple_2019,
  title = {Simple {{Acoustic Features Can Explain Phoneme-Based Predictions}} of {{Cortical Responses}} to {{Speech}}},
  author = {Daube, Christoph and Ince, Robin A. A. and Gross, Joachim},
  year = {2019},
  month = jun,
  journal = {Current Biology},
  volume = {29},
  number = {12},
  pages = {1924-1937.e9},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2019.04.067},
  abstract = {When we listen to speech, we have to make sense of a waveform of sound pressure. Hierarchical models of speech perception assume that, to extract semantic meaning, the signal is transformed into unknown, intermediate neuronal representations. Traditionally, studies of such intermediate representations are guided by linguistically defined concepts, such as phonemes. Here, we argue that in order to arrive at~an unbiased understanding of the neuronal responses to speech, we should focus instead on representations obtained directly from the stimulus. We illustrate our view with a data-driven, information theoretic analysis of a dataset of 24 young, healthy humans who listened to a 1~h narrative while their magnetoencephalogram (MEG) was recorded. We find that two recent results, the improved performance of an encoding model in which annotated linguistic and acoustic features were combined and the decoding of phoneme subgroups from phoneme-locked responses, can be explained by an encoding model that is based entirely on acoustic features. These acoustic features capitalize on acoustic edges and outperform Gabor-filtered spectrograms, which can explicitly describe the spectrotemporal characteristics of individual phonemes. By replicating our results in publicly available electroencephalography (EEG) data, we conclude that models of brain responses based on linguistic features can serve as excellent benchmarks. However, we believe that in order to further our understanding of human cortical responses to speech, we should also explore low-level and parsimonious explanations for apparent high-level phenomena.},
  langid = {english},
  keywords = {EEG,encoding,hyper-parameter,information theory,MEG,mid-level,mTRF,partial information decomposition,phonemes,speech},
  file = {/Users/xzfang/Zotero/storage/8M6QQL3S/Daube et al. - 2019 - Simple Acoustic Features Can Explain Phoneme-Based.pdf;/Users/xzfang/Zotero/storage/UN2L5AXH/S0960982219304968.html}
}

@misc{davidson_examining_2021,
  title = {Examining {{Infant Relation Categorization Through Deep Neural Networks}}},
  author = {Davidson, Guy and Lake, Brenden M.},
  year = {2021},
  month = mar,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/esvuw},
  abstract = {Categorizing spatial relations is central to the development of visual understanding and spatial cognition, with roots in the first few months of life. Quinn (2003) reviews two findings in infant relation categorization: categorizing one object as above/below another precedes categorizing an object as between other objects, and categorizing relations over specific objects predates abstract relations over varying objects. We model these phenomena with deep neural networks, including contemporary architectures specialized for relational learning and vision models pretrained on baby headcam footage (Sullivan et al., 2020). Across two computational experiments, we can account for most of the developmental findings, suggesting these models are useful for studying the computational mechanisms of infant categorization.},
  keywords = {Cognitive Development,Cognitive Psychology,Computational Modeling,developmental computational modeling,Developmental Psychology,infant relation learning,Learning,neural networks,Quantitative Methods,Social and Behavioral Sciences,spatial categorization},
  file = {/Users/xzfang/Zotero/storage/62FSKQ38/Davidson and Lake - 2021 - Examining Infant Relation Categorization Through D.pdf}
}

@article{davis_hearing_2007,
  title = {Hearing Speech Sounds: {{Top-down}} Influences on the Interface between Audition and Speech Perception},
  shorttitle = {Hearing Speech Sounds},
  author = {Davis, Matthew H. and Johnsrude, Ingrid S.},
  year = {2007},
  month = jul,
  journal = {Hearing Research},
  series = {Auditory {{Cortex}} 2006 - {{The Listening Brain}}},
  volume = {229},
  number = {1},
  pages = {132--147},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2007.01.014},
  abstract = {This paper focuses on the cognitive and neural mechanisms of speech perception: the rapid, and highly automatic processes by which complex time-varying speech signals are perceived as sequences of meaningful linguistic units. We will review four processes that contribute to the perception of speech: perceptual grouping, lexical segmentation, perceptual learning and categorical perception, in each case presenting perceptual evidence to support highly interactive processes with top-down information flow driving and constraining interpretations of spoken input. The cognitive and neural underpinnings of these interactive processes appear to depend on two distinct representations of heard speech: an auditory, echoic representation of incoming speech, and a motoric/somatotopic representation of speech as it would be produced. We review the neuroanatomical system supporting these two key properties of speech perception and discuss how this system incorporates interactive processes and two parallel echoic and somato-motoric representations, drawing on evidence from functional neuroimaging studies in humans and from comparative anatomical studies. We propose that top-down interactive mechanisms within auditory networks play an important role in explaining the perception of spoken language.},
  langid = {english},
  keywords = {Auditory cortex,Categorical perception,Feedback,fMRI,Frontal lobe,Lexical segmentation,Perceptual grouping,Perceptual learning,Speech perception,Temporal lobe},
  file = {/Users/xzfang/Zotero/storage/Y8DSMIKW/Davis and Johnsrude - 2007 - Hearing speech sounds Top-down influences on the .pdf}
}

@article{davis_lexical_2005,
  title = {Lexical {{Information Drives Perceptual Learning}} of {{Distorted Speech}}: {{Evidence From}} the {{Comprehension}} of {{Noise-Vocoded Sentences}}.},
  shorttitle = {Lexical {{Information Drives Perceptual Learning}} of {{Distorted Speech}}},
  author = {Davis, Matthew H. and Johnsrude, Ingrid S. and {Hervais-Adelman}, Alexis and Taylor, Karen and McGettigan, Carolyn},
  year = {2005},
  journal = {Journal of Experimental Psychology: General},
  volume = {134},
  number = {2},
  pages = {222--241},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/0096-3445.134.2.222},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/A4B4MD53/Davis et al. - 2005 - Lexical Information Drives Perceptual Learning of .pdf}
}

@article{davis_re_2009,
  title = {Re(de)Fining the Orthographic Neighborhood: {{The}} Role of Addition and Deletion Neighbors in Lexical Decision and Reading.},
  shorttitle = {Re(de)Fining the Orthographic Neighborhood},
  author = {Davis, Colin J. and Perea, Manuel and Acha, Joana},
  year = {2009},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {35},
  number = {5},
  pages = {1550--1570},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/a0014253},
  abstract = {The influence of addition and deletion neighbors on visual word identification was investigated in four experiments. Experiments 1 and 2 used Spanish stimuli. In Experiment 1, lexical decision latencies were slower and less accurate for words and nonwords with higher-frequency deletion neighbors (e.g., jugar in juzgar), relative to control stimuli. Experiment 2 showed a similar interference effect for words and nonwords with higher-frequency addition neighbors (e.g., conejo, which has the addition neighbor consejo), relative to control stimuli. Experiment 3 replicated this addition neighbor interference effect in a lexical decision experiment with English stimuli. Across all three experiments, interference effects were always evident for addition/deletion neighbors with word-outer overlap, usually present for those with word-initial overlap, but never present for those with word-final overlap. Experiment 4 replicated the addition/deletion neighbor inhibitory effects in a Spanish sentence reading task in which the participants' eye movements were monitored. These findings suggest that conventional orthographic neighborhood metrics should be redefined. In addition to its methodological implications, this conclusion has significant theoretical implications for input coding schemes and the mechanisms underlying word recognition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GXTIQGFP/Davis et al. - 2009 - Re(de)fining the orthographic neighborhood The ro.pdf}
}

@article{dayan_selective_2010,
  title = {Selective {{Bayes}}: {{Attentional}} Load and Crowding},
  shorttitle = {Selective {{Bayes}}},
  author = {Dayan, Peter and Solomon, Joshua A.},
  year = {2010},
  month = oct,
  journal = {Vision Research},
  series = {Mathematical {{Models}} of {{Visual Coding}}},
  volume = {50},
  number = {22},
  pages = {2248--2260},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2010.04.014},
  abstract = {The simple neural observation that the receptive fields of visual neurons are spatially extended lies at the heart of accounts of psychophysical phenomena to do with a sometimes unrequited need for spatial selection. In this paper, we consider its role in three anomalies associated with selective attention: the apparently undue influence of distractor stimuli when decisions in the Eriksen flanker task have to be made under time pressure; the phenomenon associated with attentional load that distractors distal to a target exert more effect when the demands on selective attention are smaller rather than larger; and the observation that crowding, a breakdown in peripheral discriminability in the presence of flankers, can under some circumstances be asymmetrical with respect to the relative proximity to the fovea of target and flanker. We show how these seeming anomalies can arise from normative Bayesian inference in the face of spatially confounded input.},
  langid = {english},
  keywords = {Attention,Attentional load,Bayesian vision,Cortical magnification,Crowding,Eriksen flanker task,Selection},
  file = {/Users/xzfang/Zotero/storage/CUX7RVJF/S0042698910001902.html}
}

@article{decheveigne_harmonic_2021,
  title = {Harmonic {{Cancellation}}\textemdash{{A Fundamental}} of {{Auditory Scene Analysis}}},
  author = {{de Cheveign{\'e}}, Alain},
  year = {2021},
  month = jan,
  journal = {Trends in Hearing},
  volume = {25},
  pages = {23312165211041422},
  publisher = {{SAGE Publications Inc}},
  issn = {2331-2165},
  doi = {10.1177/23312165211041422},
  abstract = {This paper reviews the hypothesis of harmonic cancellation according to which an interfering sound is suppressed or canceled on the basis of its harmonicity (or periodicity in the time domain) for the purpose of Auditory Scene Analysis. It defines the concept, discusses theoretical arguments in its favor, and reviews experimental results that support it, or not. If correct, the hypothesis may draw on time-domain processing of temporally accurate neural representations within the brainstem, as required also by the classic equalization-cancellation model of binaural unmasking. The hypothesis predicts that a target sound corrupted by interference will be easier to hear if the interference is harmonic than inharmonic, all else being equal. This prediction is borne out in a number of behavioral studies, but not all. The paper reviews those results, with the aim to understand the inconsistencies and come up with a reliable conclusion for, or against, the hypothesis of harmonic cancellation within the auditory system.},
  langid = {english},
  keywords = {auditory scene analysis,harmonic cancellation,harmonicity,pitch perception,segregation},
  file = {/Users/xzfang/Zotero/storage/L58HQUAC/de CheveignÃ© - 2021 - Harmonic Cancellationâ€”A Fundamental of Auditory Sc.pdf}
}

@article{dedeyne_small_2019,
  title = {The ``{{Small World}} of {{Words}}'' {{English}} Word Association Norms for over 12,000 Cue Words},
  author = {De Deyne, Simon and Navarro, Danielle J. and Perfors, Amy and Brysbaert, Marc and Storms, Gert},
  year = {2019},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {3},
  pages = {987--1006},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1115-7},
  abstract = {Word associations have been used widely in psychology, but the validity of their application strongly depends on the number of cues included in the study and the extent to which they probe all associations known by an individual. In this work, we address both issues by introducing a new English word association dataset. We describe the collection of word associations for over 12,000 cue words, currently the largest such English-language resource in the world. Our procedure allowed subjects to provide multiple responses for each cue, which permits us to measure weak associations. We evaluate the utility of the dataset in several different contexts, including lexical decision and semantic categorization. We also show that measures based on a mechanism of spreading activation derived from this new resource are highly predictive of direct judgments of similarity. Finally, a comparison with existing English word association sets further highlights systematic improvements provided through these new norms.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DCJAMA36/De Deyne et al. - 2019 - The â€œSmall World of Wordsâ€ English word associatio.pdf}
}

@article{dedeyne_structure_2016,
  title = {Structure at Every Scale: {{A}} Semantic Network Account of the Similarities between Unrelated Concepts},
  shorttitle = {Structure at Every Scale},
  author = {De Deyne, Simon and Navarro, Daniel J. and Perfors, Amy and Storms, Gert},
  year = {2016},
  journal = {Journal of Experimental Psychology: General},
  volume = {145},
  number = {9},
  pages = {1228--1254},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000192},
  abstract = {Similarity plays an important role in organizing the semantic system. However, given that similarity cannot be defined on purely logical grounds, it is important to understand how people perceive similarities between different entities. Despite this, the vast majority of studies focus on measuring similarity between very closely related items. When considering concepts that are very weakly related, little is known. In this article, we present 4 experiments showing that there are reliable and systematic patterns in how people evaluate the similarities between very dissimilar entities. We present a semantic network account of these similarities showing that a spreading activation mechanism defined over a word association network naturally makes correct predictions about weak similarities, whereas, though simpler, models based on direct neighbors between word pairs derived using the same network cannot. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Concepts,Semantics,Stimulus Similarity,Word Associations},
  file = {/Users/xzfang/Zotero/storage/AYKHLRR4/De Deyne et al. - 2016 - Structure at every scale A semantic network accou.pdf;/Users/xzfang/Zotero/storage/CNVN56R9/2016-41092-005.html}
}

@article{dedeyne_word_2008,
  title = {Word Associations: {{Network}} and Semantic Properties},
  shorttitle = {Word Associations},
  author = {De Deyne, Simon and Storms, Gert},
  year = {2008},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {40},
  number = {1},
  pages = {213--231},
  issn = {1554-3528},
  doi = {10.3758/BRM.40.1.213},
  abstract = {A number of properties of word associations, generated in a continuous task, were investigated. First, we investigated the correspondence of word class in association cues and responses. Nouns were the modal word class response, regardless of the word class of the cue, indicating a dominant paradigmatic response style. Next, the word association data were used to build an associative network to investigate the centrality of nodes. The study of node centrality showed that central nodes in the network tended to be highly frequent and acquired early. Small-world properties of the association network were investigated and compared with a large English association network (Steyvers \& Tenenbaum, 2005). Networks based on a multiple association procedure showed small-world properties despite being denser than networks based on a discrete task. Finally, a semantic taxonomy was used to investigate the composition of semantic types in association responses. The majority of responses were thematically related situation responses and entity responses referring to parts, shape, or color. Since the association task required multiple responses per cue, the interaction between generation position and semantic role could be investigated and discussed in the framework of recent theories of natural concept representations (Barsalou, Santos, Simmons, \& Wilson, in press).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZFEZBLLD/De Deyne and Storms - 2008 - Word associations Network and semantic properties.pdf}
}

@article{deen_processing_2020,
  title = {Processing Communicative Facial and Vocal Cues in the Superior Temporal Sulcus},
  author = {Deen, Ben and Saxe, Rebecca and Kanwisher, Nancy},
  year = {2020},
  month = nov,
  journal = {NeuroImage},
  volume = {221},
  pages = {117191},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.117191},
  abstract = {Facial and vocal cues provide critical social information about other humans, including their emotional and attentional states and the content of their speech. Recent work has shown that the face-responsive region of posterior superior temporal sulcus (``fSTS'') also responds strongly to vocal sounds. Here, we investigate the functional role of this region and the broader STS by measuring responses to a range of face movements, vocal sounds, and hand movements using fMRI. We find that the fSTS responds broadly to different types of audio and visual face action, including both richly social communicative actions, as well as minimally social noncommunicative actions, ruling out hypotheses of specialization for processing speech signals, or communicative signals more generally. Strikingly, however, responses to hand movements were very low, whether communicative or not, indicating a specific role in the analysis of face actions (facial and vocal), not a general role in the perception of any human action. Furthermore, spatial patterns of response in this region were able to decode communicative from noncommunicative face actions, both within and across modality (facial/vocal cues), indicating sensitivity to an abstract social dimension. These functional properties of the fSTS contrast with a region of middle STS that has a selective, largely unimodal auditory response to speech sounds over both communicative and noncommunicative vocal nonspeech sounds, and nonvocal sounds. Region of interest analyses were corroborated by a data-driven independent component analysis, identifying face-voice and auditory speech responses as dominant sources of voxelwise variance across the STS. These results suggest that the STS contains separate processing streams for the audiovisual analysis of face actions and auditory speech processing.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ILSSFUQ5/Deen et al. - 2020 - Processing communicative facial and vocal cues in .pdf;/Users/xzfang/Zotero/storage/WPNX4RUI/S1053811920306777.html}
}

@article{deheering_rapid_2015,
  title = {Rapid Categorization of Natural Face Images in the Infant Right Hemisphere},
  author = {{de Heering}, Ad{\'e}la{\"i}de and Rossion, Bruno},
  editor = {Culham, Jody C},
  year = {2015},
  month = jun,
  journal = {eLife},
  volume = {4},
  pages = {e06564},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.06564},
  abstract = {Human performance at categorizing natural visual images surpasses automatic algorithms, but how and when this function arises and develops remain unanswered. We recorded scalp electrical brain activity in 4\textendash 6 months infants viewing images of objects in their natural background at a rapid rate of 6 images/second (6 Hz). Widely variable face images appearing every 5 stimuli generate an electrophysiological response over the right hemisphere exactly at 1.2 Hz (6 Hz/5). This face-selective response is absent for phase-scrambled images and therefore not due to low-level information. These findings indicate that right lateralized face-selective processes emerge well before reading acquisition in the infant brain, which can perform figure-ground segregation and generalize face-selective responses across changes in size, viewpoint, illumination as well as expression, age and gender. These observations made with a highly sensitive and objective approach open an avenue for clarifying the developmental course of natural image categorization in the human brain.},
  keywords = {face perception,infant,natural image,right hemisphere,visual categorization},
  file = {/Users/xzfang/Zotero/storage/XNHITVXR/de Heering and Rossion - 2015 - Rapid categorization of natural face images in the.pdf}
}

@article{delaney-busch_neural_2019,
  title = {Neural {{Evidence}} for {{Bayesian Trial-by-Trial Adaptation}} on the {{N400}} during {{Semantic Priming}}},
  author = {{Delaney-Busch}, Nathaniel and Morgan, Emily and Lau, Ellen and Kuperberg, Gina R.},
  year = {2019},
  month = jun,
  journal = {Cognition},
  volume = {187},
  pages = {10--20},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.01.001},
  abstract = {When semantic information is activated by a context prior to new bottom-up input (i.e. when a word is predicted), semantic processing of that incoming word is typically facilitated, attenuating the amplitude of the N400 event related potential (ERP) \textendash{} a direct neural measure of semantic processing. N400 modulation is observed even when the context is a single semantically related ``prime'' word. This so-called ``N400 semantic priming effect'' is sensitive to the probability of encountering a related prime-target pair within an experimental block, suggesting that participants may be adapting the strength of their predictions to the predictive validity of their broader experimental environment. We formalize this adaptation using a Bayesian learning model that estimates and updates the probability of encountering a related versus an unrelated prime-target pair on each successive trial. We found that our model's trial-by-trial estimates of target word probability accounted for significant variance in trial-by-trial N400 amplitude. These findings suggest that Bayesian principles contribute to how comprehenders adapt their semantic predictions to the statistical structure of their broader environment, with implications for the functional significance of the N400 component and the predictive nature of language processing.},
  pmcid = {PMC6552672},
  pmid = {30797099},
  file = {/Users/xzfang/Zotero/storage/8LP6QRE4/Delaney-Busch et al. - 2019 - Neural Evidence for Bayesian Trial-by-Trial Adapta.pdf}
}

@article{deleeuw_jspsych_2015,
  title = {{{jsPsych}}: {{A JavaScript}} Library for Creating Behavioral Experiments in a {{Web}} Browser},
  shorttitle = {{{jsPsych}}},
  author = {{de Leeuw}, Joshua R.},
  year = {2015},
  month = mar,
  journal = {Behavior Research Methods},
  volume = {47},
  number = {1},
  pages = {1--12},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0458-y},
  abstract = {Online experiments are growing in popularity, and the increasing sophistication of Web technology has made it possible to run complex behavioral experiments online using only a Web browser. Unlike with offline laboratory experiments, however, few tools exist to aid in the development of browser-based experiments. This makes the process of creating an experiment slow and challenging, particularly for researchers who lack a Web development background. This article introduces jsPsych, a JavaScript library for the development of Web-based experiments. jsPsych formalizes a way of describing experiments that is much simpler than writing the entire experiment from scratch. jsPsych then executes these descriptions automatically, handling the flow from one task to another. The jsPsych library is open-source and designed to be expanded by the research community. The project is available online at www.jspsych.org.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3LRKB9UT/de Leeuw - 2015 - jsPsych A JavaScript library for creating behavio.pdf}
}

@article{deleeuw_similar_2019,
  title = {Similar Event-Related Potentials to Structural Violations in Music and Language: {{A}} Replication of {{Patel}}, {{Gibson}}, {{Ratner}}, {{Besson}}, \& {{Holcomb}} (1998)},
  shorttitle = {Similar Event-Related Potentials to Structural Violations in Music and Language},
  author = {{de Leeuw}, Joshua and Andrews, Jan and Altman, Zariah and Andrews, Rebecca and Appleby, Robert and Bonnano, James and DeStefano, Isabella and {Doyle-Samay}, Eileen and Faruqui, Ayela and Griesmer, Christina and Hwang, Jackie and Lee, Rena and Liang, Yunfei and Mernacaj, John and Molina, Henry and Ng, Hui Xin and Park, Steven and Possidente, Thomas and Shriver, Anne},
  year = {2019},
  month = nov,
  journal = {Meta-Psychology},
  volume = {3},
  issn = {2003-2714},
  doi = {10.15626/MP.2018.1481},
  abstract = {We report a replication of Patel, Gibson, Ratner, Besson, and Holcomb (1998). The results of our replication are largely consistent with the conclusions of the original study. We found evidence of a P600 component of the event-related potential (ERP) in response to syntactic violations in language and harmonic inconsistencies in music. There were some minor differences in the spatial distribution of the P600 on the scalp between the replication and the original. The experiment was pre-registered at https://osf.io/g3b5j/. We conducted this experiment as part of an undergraduate cognitive science research methods class at Vassar College; we discuss the practice of integrating replication work into research methods courses.},
  copyright = {Copyright (c) 2019 Joshua de Leeuw},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZZTQPL5X/Leeuw et al. - 2019 - Similar event-related potentials to structural vio.pdf}
}

@article{dell_adaptable_2021,
  title = {The Adaptable Speaker: {{A}} Theory of Implicit Learning in Language Production},
  shorttitle = {The Adaptable Speaker},
  author = {Dell, Gary S. and Kelley, Amanda C. and Hwang, Suyeon and Bian, Yuan},
  year = {2021},
  journal = {Psychological Review},
  volume = {128},
  number = {3},
  pages = {446--487},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/rev0000275},
  abstract = {The language production system continually learns. The system adapts to recent experiences while also reflecting the experience accumulated over the lifetime. This article presents a theory that explains how speakers implicitly learn novel phonotactic patterns as they produce syllables. The learning is revealed in their speech errors. For example, if speakers produce syllable strings in which the consonant /f/ is always a syllable onset, their slips will obey this rule; /f/'s will then slip mostly to onset positions. The article reviews over 30 phenomena related to this finding. To explain phonotactic learning, the article presents four linked ``mini-theories,'' each of which addresses components of the data. The first mini-theory, the production theory, provides an account of how speech errors arise during the assembly of word forms. The second, the learning theory, characterizes the implicit learning of phoneme distributions within the production system. The third mini-theory, the consolidation theory, augments the learning theory to explain instances in which this learning depends on a period of time, possibly a sleep period, before it is expressed. The final mini-theory, the developmental theory, addresses cases in which learning varies between children and adults, and depends on speakers' early linguistic experience. The resulting theory forges links between these diverse aspects of psychology. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Errors,Implicit Learning,Learning Theory,Memory Consolidation,Oral Communication,Phonemes,Phonetics,Syllables},
  file = {/Users/xzfang/Zotero/storage/AFAJ5ZUS/2021-24538-001.html}
}

@article{dell_effects_1990,
  title = {Effects of {{Frequency}} and {{Vocabulary Type}} on {{Phonological Speech Errors}}},
  author = {Dell, Gary S.},
  year = {1990},
  month = oct,
  journal = {Language and Cognitive Processes},
  volume = {5},
  number = {4},
  pages = {313--349},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690969008407066},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3KKBXFZV/Dell - 1990 - Effects of Frequency and Vocabulary Type on Phonol.pdf}
}

@article{dell_retrieval_1988,
  title = {The Retrieval of Phonological Forms in Production: {{Tests}} of Predictions from a Connectionist Model},
  shorttitle = {The Retrieval of Phonological Forms in Production},
  author = {Dell, Gary S},
  year = {1988},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {27},
  number = {2},
  pages = {124--142},
  issn = {0749-596X},
  doi = {10.1016/0749-596X(88)90070-8},
  abstract = {This article reviews a model of retrieval processes in language production that accounts for phonological speech-error data (Dell, 1986, Psychological Review, 93, 283\textendash 321). A distinction is drawn between empirical phenomena that are built into the model and phenomena that can be considered predictions from it. Data from experiments creating phonological speech errors are presented as tests of those predictions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6WH7FWBZ/Dell - 1988 - The retrieval of phonological forms in production.pdf}
}

@article{dell_thirty_2016,
  title = {Thirty Years of Structural Priming: {{An}} Introduction to the Special Issue},
  shorttitle = {Thirty Years of Structural Priming},
  author = {Dell, Gary S. and Ferreira, Victor S.},
  year = {2016},
  month = dec,
  journal = {Journal of Memory and Language},
  volume = {91},
  pages = {1--4},
  issn = {0749596X},
  doi = {10.1016/j.jml.2016.05.005},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JVAGU77A/Dell and Ferreira - 2016 - Thirty years of structural priming An introductio.pdf}
}

@article{delong_predictability_2014,
  title = {Predictability, Plausibility, and Two Late {{ERP}} Positivities during Written Sentence Comprehension},
  author = {DeLong, Katherine A. and Quante, Laura and Kutas, Marta},
  year = {2014},
  month = aug,
  journal = {Neuropsychologia},
  volume = {61},
  pages = {150--162},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2014.06.016},
  abstract = {literature survey of late positive ERP components elicited by more or less predictable words during sentence processing led them to propose two topographically and functionally distinct positivities: a parietal one associated with semantically incongruent words related to semantic reanalysis and a frontal one with unknown significance associated with congruent but lexically unpredicted words. With the goal of testing this hypothesis within a single set of experimental materials and participants, we report results from two ERP studies: Experiment 1, a post hoc analysis of a dataset that varied on dimensions of both cloze probability (predictability) and plausibility, and Experiment 2, a follow-up study in which these factors were manipulated in a controlled fashion. In both studies, we observed distinct post-N400 positivities: a more anterior one to plausible, but not anomalous, low cloze probability sentence medial words, and a more posterior one to semantically anomalous sentence continuations. Taken together with an observed canonical cloze-modulated N400, these dual positivities indicate a dissociation between brain processes relating to written words' sentential predictability versus plausibility, clearly an important distinction for any viable neural or psycholinguistic model of written sentence processing.},
  pmcid = {PMC4124880},
  pmid = {24953958},
  file = {/Users/xzfang/Zotero/storage/MTC5ALXU/DeLong et al. - 2014 - Predictability, plausibility, and two late ERP pos.pdf}
}

@article{delong_probabilistic_2005,
  title = {Probabilistic Word Pre-Activation during Language Comprehension Inferred from Electrical Brain Activity},
  author = {DeLong, Katherine A. and Urbach, Thomas P. and Kutas, Marta},
  year = {2005},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {8},
  number = {8},
  pages = {1117--1121},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1504},
  abstract = {Despite the numerous examples of anticipatory cognitive processes at micro and macro levels in many animal species, the idea that anticipation of specific words plays an integral role in real-time language processing has been contentious. Here we exploited a phonological regularity of English indefinite articles ('an' precedes nouns beginning with vowel sounds, whereas 'a' precedes nouns beginning with consonant sounds) in combination with event-related brain potential recordings from the human scalp to show that readers' brains can pre-activate individual words in a graded fashion to a degree that can be estimated from the probability that each word is given as a continuation for a sentence fragment offline. These findings are evidence that readers use the words in a sentence (as cues to their world knowledge) to estimate relative likelihoods for upcoming words.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/Users/xzfang/Zotero/storage/335U6IDG/DeLong et al. - 2005 - Probabilistic word pre-activation during language .pdf;/Users/xzfang/Zotero/storage/QI3K4SAR/nn1504.html}
}

@article{delorme_eeglab_2004,
  title = {{{EEGLAB}}: An Open Source Toolbox for Analysis of Single-Trial {{EEG}} Dynamics Including Independent Component Analysis},
  shorttitle = {{{EEGLAB}}},
  author = {Delorme, Arnaud and Makeig, Scott},
  year = {2004},
  month = mar,
  journal = {Journal of Neuroscience Methods},
  volume = {134},
  number = {1},
  pages = {9--21},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2003.10.009},
  abstract = {We have developed a toolbox and graphic user interface, EEGLAB, running under the crossplatform MATLAB environment (The Mathworks, Inc.) for processing collections of single-trial and/or averaged EEG data of any number of channels. Available functions include EEG data, channel and event information importing, data visualization (scrolling, scalp map and dipole model plotting, plus multi-trial ERP-image plots), preprocessing (including artifact rejection, filtering, epoch selection, and averaging), independent component analysis (ICA) and time/frequency decompositions including channel and component cross-coherence supported by bootstrap statistical methods based on data resampling. EEGLAB functions are organized into three layers. Top-layer functions allow users to interact with the data through the graphic interface without needing to use MATLAB syntax. Menu options allow users to tune the behavior of EEGLAB to available memory. Middle-layer functions allow users to customize data processing using command history and interactive `pop' functions. Experienced MATLAB users can use EEGLAB data structures and stand-alone signal processing functions to write custom and/or batch analysis scripts. Extensive function help and tutorial information are included. A `plug-in' facility allows easy incorporation of new EEG modules into the main menu. EEGLAB is freely available (http://www.sccn.ucsd.edu/eeglab/) under the GNU public license for noncommercial use and open source development, together with sample data, user tutorial and extensive documentation.},
  langid = {english},
  keywords = {EEG,ERP,ICA,Matlab,Single-trial,Software,Spectral decomposition},
  file = {/Users/xzfang/Zotero/storage/8WEHPT2J/Delorme and Makeig - 2004 - EEGLAB an open source toolbox for analysis of sin.pdf;/Users/xzfang/Zotero/storage/TYEFZ42E/S0165027003003479.html}
}

@inproceedings{demberg_syntactic_2012,
  title = {Syntactic {{Surprisal Affects Spoken Word Duration}} in {{Conversational Contexts}}},
  booktitle = {Proceedings of the 2012 {{Joint Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and {{Computational Natural Language Learning}}},
  author = {Demberg, Vera and Sayeed, Asad and Gorinski, Philip and Engonopoulos, Nikolaos},
  year = {2012},
  month = jul,
  pages = {356--367},
  publisher = {{Association for Computational Linguistics}},
  address = {{Jeju Island, Korea}},
  file = {/Users/xzfang/Zotero/storage/CS92J9SE/Demberg et al. - 2012 - Syntactic Surprisal Affects Spoken Word Duration i.pdf}
}

@article{denison_dynamic_2021,
  title = {A Dynamic Normalization Model of Temporal Attention},
  author = {Denison, Rachel N. and Carrasco, Marisa and Heeger, David J.},
  year = {2021},
  month = jun,
  journal = {Nature Human Behaviour},
  pages = {1--12},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01129-1},
  abstract = {Vision is dynamic, handling a continuously changing stream of input, yet most models of visual attention are static. Here, we develop a dynamic normalization model of visual temporal attention and constrain it with new psychophysical human data. We manipulated temporal attention\textemdash the prioritization of visual information at specific points in time\textemdash to a sequence of two stimuli separated by a variable time interval. Voluntary temporal attention improved perceptual sensitivity only over a specific interval range. To explain these data, we modelled voluntary and involuntary attentional gain dynamics. Voluntary gain enhancement took the form of a limited resource over short time intervals, which recovered over time. Taken together, our theoretical and experimental results formalize and generalize the idea of limited attentional resources across space at a single moment to limited resources across time at a single location.},
  copyright = {2021 The Author(s), under exclusive license to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Human behaviour Subject\_term\_id: computational-neuroscience;human-behaviour},
  file = {/Users/xzfang/Zotero/storage/8YRVMZ6D/Denison et al. - 2021 - A dynamic normalization model of temporal attentio.pdf;/Users/xzfang/Zotero/storage/U8RF96HE/s41562-021-01129-1.html}
}

@article{derakhshan_effect_2008,
  title = {Effect of Subinhibitory Concentrations of Cumin ({{Cuminum}} Cyminum {{L}}.) Seed Essential Oil and Alcoholic Extract on the Morphology, Capsule Expression and Urease Activity of {{Klebsiella}} Pneumoniae},
  author = {Derakhshan, Safoura and Sattari, Morteza and Bigdeli, Mohsen},
  year = {2008},
  month = nov,
  journal = {International Journal of Antimicrobial Agents},
  volume = {32},
  number = {5},
  pages = {432--436},
  issn = {0924-8579},
  doi = {10.1016/j.ijantimicag.2008.05.009},
  abstract = {Cuminum cyminum L., commonly known as cumin, is a plant with a considerable reputation. The aim of this work was to study the activity of cumin seed essential oil and alcoholic extract against Klebsiella pneumoniae ATCC 13883 and clinical K. pneumoniae isolates by evaluating the effect of subminimum inhibitory concentrations (sub-MICs) on cell morphology, capsule expression and urease activity. Growth of K. pneumoniae strains exposed to sub-MICs of C. cyminum extracts resulted in cell elongation and repression of capsule expression. Urease activity was decreased. The major constituent of the oil determined by gas chromatography/mass spectrometry was cumin aldehyde.},
  keywords = {Antibacterial effect,Cuminum cyminum L.,Klebsiella pneumoniae},
  file = {/Users/xzfang/Zotero/storage/GQ822DV9/Derakhshan et al. - 2008 - Effect of subinhibitory concentrations of cumin (C.html}
}

@article{desai_generalizable_2021,
  title = {Generalizable {{EEG Encoding Models}} with {{Naturalistic Audiovisual Stimuli}}},
  author = {Desai, Maansi and Holder, Jade and Villarreal, Cassandra and Clark, Nat and Hoang, Brittany and Hamilton, Liberty S.},
  year = {2021},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {41},
  number = {43},
  pages = {8946--8962},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2891-20.2021},
  abstract = {In natural conversations, listeners must attend to what others are saying while ignoring extraneous background sounds. Recent studies have used encoding models to predict electroencephalography (EEG) responses to speech in noise-free listening situations, sometimes referred to as ``speech tracking.'' Researchers have analyzed how speech tracking changes with different types of background noise. It is unclear, however, whether neural responses from acoustically rich, naturalistic environments with and without background noise can be generalized to more controlled stimuli. If encoding models for acoustically rich, naturalistic stimuli are generalizable to other tasks, this could aid in data collection from populations of individuals who may not tolerate listening to more controlled and less engaging stimuli for long periods of time. We recorded noninvasive scalp EEG while 17 human participants (8 male/9 female) listened to speech without noise and audiovisual speech stimuli containing overlapping speakers and background sounds. We fit multivariate temporal receptive field encoding models to predict EEG responses to pitch, the acoustic envelope, phonological features, and visual cues in both stimulus conditions. Our results suggested that neural responses to naturalistic stimuli were generalizable to more controlled datasets. EEG responses to speech in isolation were predicted accurately using phonological features alone, while responses to speech in a rich acoustic background were more accurate when including both phonological and acoustic features. Our findings suggest that naturalistic audiovisual stimuli can be used to measure receptive fields that are comparable and generalizable to more controlled audio-only stimuli. SIGNIFICANCE STATEMENT Understanding spoken language in natural environments requires listeners to parse acoustic and linguistic information in the presence of other distracting stimuli. However, most studies of auditory processing rely on highly controlled stimuli with no background noise, or with background noise inserted at specific times. Here, we compare models where EEG data are predicted based on a combination of acoustic, phonetic, and visual features in highly disparate stimuli\textemdash sentences from a speech corpus and speech embedded within movie trailers. We show that modeling neural responses to highly noisy, audiovisual movies can uncover tuning for acoustic and phonetic information that generalizes to simpler stimuli typically used in sensory neuroscience experiments.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2021 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {34503996},
  keywords = {electroencephalography,encoding models,natural stimuli,spectrotemporal receptive field,speech perception},
  file = {/Users/xzfang/Zotero/storage/76G3WGV6/Desai et al. - 2021 - Generalizable EEG Encoding Models with Naturalisti.pdf;/Users/xzfang/Zotero/storage/Y62WCF4S/8946.html}
}

@article{desroches_investigating_2009,
  title = {Investigating the {{Time Course}} of {{Spoken Word Recognition}}: {{Electrophysiological Evidence}} for the {{Influences}} of {{Phonological Similarity}}},
  shorttitle = {Investigating the {{Time Course}} of {{Spoken Word Recognition}}},
  author = {Desroches, Amy S. and Newman, Randy Lynn and Joanisse, Marc F.},
  year = {2009},
  month = oct,
  journal = {Journal of Cognitive Neuroscience},
  volume = {21},
  number = {10},
  pages = {1893--1906},
  issn = {0898-929X},
  doi = {10.1162/jocn.2008.21142},
  abstract = {Behavioral and modeling evidence suggests that words compete for recognition during auditory word identification, and that phonological similarity is a driving factor in this competition. The present study used event-related potentials (ERPs) to examine the temporal dynamics of different types of phonological competition (i.e., cohort and rhyme). ERPs were recorded during a novel picture\textendash word matching task, where a target picture was followed by an auditory word that either matched the target (CONE\textendash cone), or mismatched in one of three ways: rhyme (CONE\textendash bone), cohort (CONE\textendash comb), and unrelated (CONE\textendash fox). Rhymes and cohorts differentially modulated two distinct ERP components, the phonological mismatch negativity and the N400, revealing the influences of prelexical and lexical processing components in speech recognition. Cohort mismatches resulted in late increased negativity in the N400, reflecting disambiguation of the later point of miscue and the combined influences of top\textendash down expectations and misleading bottom\textendash up phonological information on processing. In contrast, we observed a reduction in the N400 for rhyme mismatches, reflecting lexical activation of rhyme competitors. Moreover, the observed rhyme effects suggest that there is an interaction between phoneme-level and lexical-level information in the recognition of spoken words. The results support the theory that both levels of information are engaged in parallel during auditory word recognition in a way that permits both bottom\textendash up and top\textendash down competition effects.},
  file = {/Users/xzfang/Zotero/storage/PK3MRRJI/Desroches et al. - 2009 - Investigating the Time Course of Spoken Word Recog.pdf;/Users/xzfang/Zotero/storage/JHB6AHQM/Investigating-the-Time-Course-of-Spoken-Word.html}
}

@article{desroches_specific_2006,
  title = {Specific Phonological Impairments in Dyslexia Revealed by Eyetracking},
  author = {Desroches, Amy S. and Joanisse, Marc F. and Robertson, Erin K.},
  year = {2006},
  month = jul,
  journal = {Cognition},
  volume = {100},
  number = {3},
  pages = {B32-B42},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2005.09.001},
  abstract = {Phonological deficits in dyslexia are typically assessed using metalinguistic tasks vulnerable to extraneous factors such as attention and memory. The present work takes the novel approach of measuring phonology using eyetracking. Eye movements of dyslexic children were monitored during an auditory word recognition task in which target items in a display (e.g., candle) were accompanied by distractors sharing a cohort (candy) or rhyme (sandal). Like controls, dyslexics showed slower recognition times when a cohort distractor was present than in a baseline condition with only phonologically unrelated distractors. However, unlike controls, dyslexic children did not show slowed recognition of targets with a rhyme distractor, suggesting they had not encoded rhyme relationships. This was further explored in an overt phonological awareness test of cohort and rhyme. Surprisingly, dyslexics showed normal rhyme performance but poorer judgment of initial sounds on these overt tests. The results implicate impaired knowledge of rhyme information in dyslexia; however they also indicate that testing methodology plays a critical role in how such problems are identified.},
  langid = {english},
  keywords = {Auditory word recognition,Dyslexia,Eyetracking,Phonological awareness,Phonological development},
  file = {/Users/xzfang/Zotero/storage/FHUKSWDU/Desroches et al. - 2006 - Specific phonological impairments in dyslexia reve.pdf;/Users/xzfang/Zotero/storage/F2T5PHBC/S001002770500154X.html}
}

@article{deutsch_absolute_2021,
  title = {Absolute Pitch Is Disrupted by a Memory Illusion},
  author = {Deutsch, Diana and Edelstein, Miren and Dooley, Kevin and Henthorn, Trevor},
  year = {2021},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {149},
  number = {4},
  pages = {2829--2835},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/10.0004776},
  abstract = {An experiment is reported, showing that short-term memory for pitch in absolute pitch (AP) possessors, while substantially more accurate than in AP nonpossessors, is also subject to illusory conjunctions of pitch and time and so can be distorted or enhanced by a single tone embedded in a sequence of six other tones. Both AP possessors and AP nonpossessors performed a short-term memory task. A test tone was presented, then a sequence of six intervening tones, and then a probe tone. The test and probe tones either were identical in pitch or differed by a semitone. The AP nonpossessors judged whether the test and probe tones were the same or different, and the AP possessors identified the test and probe tones by name. In some conditions, a tone of identical pitch to the probe tone or an octave removed from this tone was included in the intervening sequence. In both the AP possessors and AP nonpossessors, this illusion-producing tone increased judgments that the test and probe tones were identical. These results accord with a model of the system underlying short-term memory for pitch proposed earlier and show that this system is bidimensional in nature, involving both pitch height and pitch class.},
  file = {/Users/xzfang/Zotero/storage/6IJCCCUH/Deutsch et al. - 2021 - Absolute pitch is disrupted by a memory illusion.pdf;/Users/xzfang/Zotero/storage/A8PV92BB/10.html}
}

@article{devries_largescale_2020,
  title = {A Large-Scale Standardized Physiological Survey Reveals Functional Organization of the Mouse Visual Cortex},
  author = {{de Vries}, Saskia E. J. and Lecoq, Jerome A. and Buice, Michael A. and Groblewski, Peter A. and Ocker, Gabriel K. and Oliver, Michael and Feng, David and Cain, Nicholas and Ledochowitsch, Peter and Millman, Daniel and Roll, Kate and Garrett, Marina and Keenan, Tom and Kuan, Leonard and Mihalas, Stefan and Olsen, Shawn and Thompson, Carol and Wakeman, Wayne and Waters, Jack and Williams, Derric and Barber, Chris and Berbesque, Nathan and Blanchard, Brandon and Bowles, Nicholas and Caldejon, Shiella D. and Casal, Linzy and Cho, Andrew and Cross, Sissy and Dang, Chinh and Dolbeare, Tim and Edwards, Melise and Galbraith, John and Gaudreault, Nathalie and Gilbert, Terri L. and Griffin, Fiona and Hargrave, Perry and Howard, Robert and Huang, Lawrence and Jewell, Sean and Keller, Nika and Knoblich, Ulf and Larkin, Josh D. and Larsen, Rachael and Lau, Chris and Lee, Eric and Lee, Felix and Leon, Arielle and Li, Lu and Long, Fuhui and Luviano, Jennifer and Mace, Kyla and Nguyen, Thuyanh and Perkins, Jed and Robertson, Miranda and Seid, Sam and {Shea-Brown}, Eric and Shi, Jianghong and Sjoquist, Nathan and Slaughterbeck, Cliff and Sullivan, David and Valenza, Ryan and White, Casey and Williford, Ali and Witten, Daniela M. and Zhuang, Jun and Zeng, Hongkui and Farrell, Colin and Ng, Lydia and Bernard, Amy and Phillips, John W. and Reid, R. Clay and Koch, Christof},
  year = {2020},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {23},
  number = {1},
  pages = {138--151},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0550-9},
  abstract = {By comparing neural responses to diverse visual stimuli measured with a standardized two-photon imaging pipeline, the authors reveal response specializations within the mouse visual cortex.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QY3NRWUE/de Vries et al. - 2020 - A large-scale standardized physiological survey re.pdf;/Users/xzfang/Zotero/storage/4QZ5MIJV/s41593-019-0550-9.html}
}

@article{dhariwal_jukebox_2020,
  title = {Jukebox: {{A Generative Model}} for {{Music}}},
  shorttitle = {Jukebox},
  author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  year = {2020},
  month = apr,
  abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YB8PW2Y8/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf;/Users/xzfang/Zotero/storage/WBUQ4CIJ/2005.html}
}

@article{diachek_domaingeneral_2020,
  title = {The Domain-General Multiple Demand ({{MD}}) Network Does Not Support Core Aspects of Language Comprehension: A Large-Scale {{fMRI}} Investigation},
  shorttitle = {The Domain-General Multiple Demand ({{MD}}) Network Does Not Support Core Aspects of Language Comprehension},
  author = {Diachek, Evgeniia and Blank, Idan and Siegelman, Matthew and Affourtit, Josef and Fedorenko, Evelina},
  year = {2020},
  month = apr,
  journal = {Journal of Neuroscience},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2036-19.2020},
  abstract = {Aside from the language-selective left-lateralized fronto-temporal network, language comprehension sometimes recruits a domain-general bilateral fronto-parietal network implicated in executive functions: the multiple demand (MD) network. However, the nature of the MD network's contributions to language comprehension remains debated. To illuminate the role of this network in language processing in humans, we conducted a large-scale fMRI investigation using data from 30 diverse word and sentence comprehension experiments (481 unique participants (female and male), 678 scanning sessions). In line with prior findings, the MD network was active during many language tasks. Moreover, similar to the language-selective network, which is robustly lateralized to the left hemisphere, these responses were stronger in the left-hemisphere MD regions. However, in contrast with the language-selective network, the MD network responded more strongly (i) to lists of unconnected words than to sentences, and (ii) in paradigms with an explicit task compared to passive comprehension paradigms. In fact, many passive comprehension tasks failed to elicit a response above the fixation baseline in the MD network, in contrast to strong responses in the language-selective network. Together, these results argue against a role for the MD network in core aspects of sentence comprehension like inhibiting irrelevant meanings or parses, keeping intermediate representations active in working memory, or predicting upcoming words or structures. These results align with recent evidence of relatively poor tracking of the linguistic signal by the MD regions during naturalistic comprehension, and instead suggest that the MD network's engagement during language processing reflects effort associated with extraneous task demands. Significance Statement Domain-general executive processes, like working memory and cognitive control, have long been implicated in language comprehension, including in neuroimaging studies that have reported activation in domain-general multiple demand (MD) regions for linguistic manipulations. However, much prior evidence has come from paradigms where language interpretation is accompanied by extraneous tasks. Using a large fMRI dataset (30 experiments/481 participants/678 sessions), we demonstrate that MD regions are engaged during language comprehension in the presence of task demands, but not during passive reading/listening\textemdash conditions that strongly activate the fronto-temporal language network. These results present a fundamental challenge to proposals whereby linguistic computations, like inhibiting irrelevant meanings, keeping representations active in working memory, or predicting upcoming elements, draw on domain-general executive resources.},
  chapter = {Research Report: Regular Manuscript},
  copyright = {Copyright \textcopyright{} 2020 Diachek et al.},
  langid = {english},
  pmid = {32317387},
  file = {/Users/xzfang/Zotero/storage/FEZSX53C/Diachek et al. - 2020 - The domain-general multiple demand (MD) network do.pdf;/Users/xzfang/Zotero/storage/NNM7QS8Z/JNEUROSCI.2036-19.html}
}

@article{diachek_effect_2022,
  title = {The {{Effect}} of {{Disfluency}} on {{Memory}} for {{What}} Was {{Said}}},
  author = {Diachek, Evgeniia},
  year = {2022},
  pages = {46},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/D5ND6NQL/Diachek - The Effect of Disfluency on Memory for What was Sa.pdf}
}

@article{dias_dissociable_1997,
  title = {Dissociable {{Forms}} of {{Inhibitory Control}} within {{Prefrontal Cortex}} with an {{Analog}} of the {{Wisconsin Card Sort Test}}: {{Restriction}} to {{Novel Situations}} and {{Independence}} from ``{{On-Line}}'' {{Processing}}},
  shorttitle = {Dissociable {{Forms}} of {{Inhibitory Control}} within {{Prefrontal Cortex}} with an {{Analog}} of the {{Wisconsin Card Sort Test}}},
  author = {Dias, R. and Robbins, T. W. and Roberts, A. C.},
  year = {1997},
  month = dec,
  journal = {The Journal of Neuroscience},
  volume = {17},
  number = {23},
  pages = {9285},
  publisher = {{Society for Neuroscience}},
  doi = {10.1523/JNEUROSCI.17-23-09285.1997},
  abstract = {Attentional set-shifting and discrimination reversal are sensitive to prefrontal damage in the marmoset in a manner qualitatively similar to that seen in man and Old World monkeys, respectively (). Preliminary findings have demonstrated that although ...},
  langid = {english},
  pmid = {9364074},
  file = {/Users/xzfang/Zotero/storage/HV9C5PBI/Dias et al. - 1997 - Dissociable Forms of Inhibitory Control within Pre.pdf}
}

@article{dibono_deep_2013,
  title = {Deep Generative Learning of Location-Invariant Visual Word Recognition},
  author = {Di Bono, Maria Grazia and Zorzi, Marco},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00635},
  abstract = {It is widely believed that orthographic processing implies an approximate, flexible coding of letter position, as shown by relative-position and transposition priming effects in visual word recognition. These findings have inspired alternative proposals about the representation of letter position, ranging from noisy coding across the ordinal positions to relative position coding based on open bigrams. This debate can be cast within the broader problem of learning location-invariant representations of written words, that is, a coding scheme abstracting the identity and position of letters (and combinations of letters) from their eye-centred (i.e., retinal) locations. We asked whether location-invariance would emerge from deep unsupervised learning on letter strings and what type of intermediate coding would emerge in the resulting hierarchical generative model. We trained a deep network with three hidden layers on an artificial dataset of letter strings presented at five possible retinal locations. Though word-level information (i.e., word identity) was never provided to the network during training, linear decoding from the activity of the deepest hidden layer yielded near-perfect accuracy in location-invariant word recognition. Conversely, decoding from lower layers yielded a large number of transposition errors. Analyses of emergent internal representations showed that word selectivity and location invariance increased as a function of layer depth. Conversely, there was no evidence for bigram coding. Finally, the distributed internal representation of words at the deepest layer showed higher similarity to the representation elicited by the two exterior letters than by other combinations of two contiguous letters, in agreement with the hypothesis that word edges have special status. These results reveal that the efficient coding of written words \textendash{} which was the model's learning objective \textendash{} is largely based on letter-level information.},
  langid = {english},
  keywords = {connectionist modelling,deep unsupervised learning,hierarchical generative models,open-bigrams,orthographic coding},
  file = {/Users/xzfang/Zotero/storage/IAKHIP6H/Di Bono and Zorzi - 2013 - Deep generative learning of location-invariant vis.pdf}
}

@article{diependaele_semantic_2009,
  title = {Semantic Transparency and Masked Morphological Priming: {{The}} Case of Prefixed Words},
  shorttitle = {Semantic Transparency and Masked Morphological Priming},
  author = {Diependaele, Kevin and Sandra, Dominiek and Grainger, Jonathan},
  year = {2009},
  month = sep,
  journal = {Memory \& Cognition},
  volume = {37},
  number = {6},
  pages = {895--908},
  issn = {0090-502X, 1532-5946},
  doi = {10.3758/MC.37.6.895},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VPJD9XPQ/Diependaele et al. - 2009 - Semantic transparency and masked morphological pri.pdf}
}

@article{dijkstra_identifying_2015,
  title = {Identifying the {{Attended Speaker Using Electrocorticographic}} ({{ECoG}}) {{Signals}}},
  author = {Dijkstra, K. and Brunner, P. and Gunduz, A. and Coon, W. and Ritaccio, A.L. and Farquhar, J. and Schalk, G.},
  year = {2015},
  journal = {Brain computer interfaces (Abingdon, England)},
  volume = {2},
  number = {4},
  pages = {161--173},
  issn = {2326-263X},
  doi = {10.1080/2326263X.2015.1063363},
  abstract = {People affected by severe neuro-degenerative diseases (e.g., late-stage amyotrophic lateral sclerosis (ALS) or locked-in syndrome) eventually lose all muscular control. Thus, they cannot use traditional assistive communication devices that depend on muscle control, or brain-computer interfaces (BCIs) that depend on the ability to control gaze. While auditory and tactile BCIs can provide communication to such individuals, their use typically entails an artificial mapping between the stimulus and the communication intent. This makes these BCIs difficult to learn and use., In this study, we investigated the use of selective auditory attention to natural speech as an avenue for BCI communication. In this approach, the user communicates by directing his/her attention to one of two simultaneously presented speakers. We used electrocorticographic (ECoG) signals in the gamma band (70\textendash 170 Hz) to infer the identity of attended speaker, thereby removing the need to learn such an artificial mapping., Our results from twelve human subjects show that a single cortical location over superior temporal gyrus or pre-motor cortex is typically sufficient to identify the attended speaker within 10 s and with 77\% accuracy (50\% accuracy due to chance). These results lay the groundwork for future studies that may determine the real-time performance of BCIs based on selective auditory attention to speech.},
  pmcid = {PMC4776341},
  pmid = {26949710},
  file = {/Users/xzfang/Zotero/storage/NCP7AUB9/Dijkstra et al. - 2015 - Identifying the Attended Speaker Using Electrocort.pdf}
}

@article{dikker_early_2010,
  title = {Early {{Occipital Sensitivity}} to {{Syntactic Category Is Based}} on {{Form Typicality}}},
  author = {Dikker, Suzanne and Rabagliati, Hugh and Farmer, Thomas A. and Pylkk{\"a}nen, Liina},
  year = {2010},
  month = may,
  journal = {Psychological Science},
  volume = {21},
  number = {5},
  pages = {629--634},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797610367751},
  abstract = {Syntactic factors can rapidly affect behavioral and neural responses during language processing; however, the mechanisms that allow this rapid extraction of syntactically relevant information remain poorly understood. We addressed this issue using magnetoencephalography and found that an unexpected word category (e.g., ``The recently princess . . . '') elicits enhanced activity in visual cortex as early as 120 ms after exposure, and that this activity occurs as a function of the compatibility of a word's form with the form properties associated with a predicted word category. Because no sensitivity to linguistic factors has been previously reported for words in isolation at this stage of visual analysis, we propose that predictions about upcoming syntactic categories are translated into form-based estimates, which are made available to sensory cortices. This finding may be a key component to elucidating the mechanisms that allow the extreme rapidity and efficiency of language comprehension.},
  langid = {english},
  keywords = {language,magnetoencephalography,prediction,syntax,vision},
  file = {/Users/xzfang/Zotero/storage/L5PQS84V/Dikker et al. - 2010 - Early Occipital Sensitivity to Syntactic Category .pdf}
}

@article{dikker_early_2010a,
  title = {Early {{Occipital Sensitivity}} to {{Syntactic Category Is Based}} on {{Form Typicality}}},
  author = {Dikker, Suzanne and Rabagliati, Hugh and Farmer, Thomas A. and Pylkk{\"a}nen, Liina},
  year = {2010},
  month = may,
  journal = {Psychological Science},
  volume = {21},
  number = {5},
  pages = {629--634},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797610367751},
  abstract = {Syntactic factors can rapidly affect behavioral and neural responses during language processing; however, the mechanisms that allow this rapid extraction of syntactically relevant information remain poorly understood. We addressed this issue using magnetoencephalography and found that an unexpected word category (e.g., ``The recently princess . . . '') elicits enhanced activity in visual cortex as early as 120 ms after exposure, and that this activity occurs as a function of the compatibility of a word's form with the form properties associated with a predicted word category. Because no sensitivity to linguistic factors has been previously reported for words in isolation at this stage of visual analysis, we propose that predictions about upcoming syntactic categories are translated into form-based estimates, which are made available to sensory cortices. This finding may be a key component to elucidating the mechanisms that allow the extreme rapidity and efficiency of language comprehension.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MPWJE2CL/Dikker et al. - 2010 - Early Occipital Sensitivity to Syntactic Category .pdf}
}

@article{diliberto_accurate_2021,
  title = {Accurate {{Decoding}} of {{Imagined}} and {{Heard Melodies}}},
  author = {Di Liberto, Giovanni M. and Marion, Guilhem and Shamma, Shihab A.},
  year = {2021},
  journal = {Frontiers in Neuroscience},
  volume = {0},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2021.673401},
  abstract = {Music perception requires the human brain to process a variety of acoustic and music-related properties. Recent research used encoding models to tease apart and study the various cortical contributors to music perception. To do so, such approaches study temporal response functions that summarise the neural activity over several minutes of data. Here we tested the possibility of assessing the neural processing of individual musical units (bars) with electroencephalography (EEG). We devised a decoding methodology based on a maximum correlation metric across EEG segments (maxCorr) and used it to decode melodies from EEG based on an experiment where professional musicians listened and imagined four Bach melodies multiple times. We demonstrate here that accurate decoding of melodies in single-subjects and at the level of individual musical units is possible, both from EEG signals recorded during listening and imagination. Furthermore, we find that greater decoding accuracies are measured for the maxCorr method than for an envelope reconstruction approach based on backward temporal response functions (bTRFenv). These results indicate that low-frequency neural signals encode information beyond note timing, especially with respect to low-frequency cortical signals below 1 Hz, which are shown to encode pitch-related information. Along with the theoretical implications of these results, we discuss the potential applications of this decoding methodology in the context of novel brain-computer interface solutions.},
  langid = {english},
  keywords = {cortical,Decoding,EEG,imagery,melody,Music,Neural tracking,pitch,Signal processing,TRF}
}

@article{diliberto_cortical_2020,
  title = {Cortical Encoding of Melodic Expectations in Human Temporal Cortex},
  author = {Di Liberto, Giovanni M and Pelofi, Claire and Bianco, Roberta and Patel, Prachi and Mehta, Ashesh D and Herrero, Jose L and {de Cheveign{\'e}}, Alain and Shamma, Shihab and Mesgarani, Nima},
  year = {2020},
  journal = {eLife},
  volume = {9},
  pages = {e51784},
  issn = {2050-084X},
  doi = {10.7554/eLife.51784},
  abstract = {Humans engagement in music rests on underlying elements such as the listeners' cultural background and interest in music. These factors modulate how listeners anticipate musical events, a process inducing instantaneous neural responses as the music confronts these expectations. Measuring such neural correlates would represent a direct window into high-level brain processing. Here we recorded cortical signals as participants listened to Bach melodies. We assessed the relative contributions of acoustic versus melodic components of the music to the neural signal. Melodic features included information on pitch progressions and their tempo, which were extracted from a predictive model of musical structure based on Markov chains. We related the music to brain activity with temporal response functions demonstrating, for the first time, distinct cortical encoding of pitch and note-onset expectations during naturalistic music listening. This encoding was most pronounced at response latencies up to 350 ms, and in both planum temporale and Heschl's gyrus.},
  pmcid = {PMC7053998},
  pmid = {32122465},
  file = {/Users/xzfang/Zotero/storage/K5Y2LMLM/Di Liberto et al. - Cortical encoding of melodic expectations in human.pdf}
}

@article{diliberto_lowfrequency_2015,
  title = {Low-{{Frequency Cortical Entrainment}} to {{Speech Reflects Phoneme-Level Processing}}},
  author = {Di~Liberto, Giovanni~M. and O'Sullivan, James~A. and Lalor, Edmund~C.},
  year = {2015},
  month = oct,
  journal = {Current Biology},
  volume = {25},
  number = {19},
  pages = {2457--2465},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.08.030},
  abstract = {The human ability to understand speech is underpinned by a hierarchical auditory system whose successive stages process increasingly complex attributes of the acoustic input. It has been suggested that to produce categorical speech perception, this system must elicit consistent neural responses to speech tokens (e.g., phonemes) despite variations in their acoustics. Here, using electroencephalography (EEG), we provide evidence for this categorical phoneme-level speech processing by showing that the relationship between continuous speech and neural activity is best described when that speech is represented using both low-level spectrotemporal information and categorical labeling of phonetic features. Furthermore, the mapping between phonemes and EEG becomes more discriminative for phonetic features at longer latencies, in line with what one might expect from a hierarchical system. Importantly, these effects are not seen for time-reversed speech. These findings may form the basis for future research on natural language processing in specific cohorts of interest and for broader insights into how brains transform acoustic input into meaning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XB4AU9VQ/DiÂ Liberto et al. - 2015 - Low-Frequency Cortical Entrainment to Speech Refle.pdf;/Users/xzfang/Zotero/storage/6NINR2CU/S0960982215010015.html}
}

@article{diliberto_lowfrequency_2019,
  title = {Low-Frequency Cortical Responses to Natural Speech Reflect Probabilistic Phonotactics},
  author = {Di Liberto, Giovanni M. and Wong, Daniel and Melnik, Gerda Ana and {de Cheveign{\'e}}, Alain},
  year = {2019},
  month = aug,
  journal = {NeuroImage},
  volume = {196},
  pages = {237--247},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.04.037},
  abstract = {Humans comprehend speech despite the various challenges such as mispronunciation and noisy environments. Our auditory system is robust to these thanks to the integration of the sensory input with prior knowledge and expectations built on language-specific regularities. One such regularity regards the permissible phoneme sequences, which determine the likelihood that a word belongs to a given language (phonotactic probability; ``blick'' is more likely to be an English word than ``bnick''). Previous research demonstrated that violations of these rules modulate brain-evoked responses. However, several fundamental questions remain unresolved, especially regarding the neural encoding and integration strategy of phonotactics in naturalistic conditions, when there are no (or few) violations. Here, we used linear modelling to assess the influence of phonotactic probabilities on the brain responses to narrative speech measured with non-invasive EEG. We found that the relationship between continuous speech and EEG responses is best described when the stimulus descriptor includes phonotactic probabilities. This indicates that low-frequency cortical signals ({$<$}9\,Hz) reflect the integration of phonotactic information during natural speech perception, providing us with a measure of phonotactic processing at the individual subject-level. Furthermore, phonotactics-related signals showed the strongest speech-EEG interactions at latencies of 100\textendash 500\,ms, supporting a pre-lexical role of phonotactic information.},
  langid = {english},
  keywords = {cortical tracking,EEG,Language,Neighbourhood density,Phonemes},
  file = {/Users/xzfang/Zotero/storage/94ZZUUIP/Di Liberto et al. - 2019 - Low-frequency cortical responses to natural speech.pdf;/Users/xzfang/Zotero/storage/ABPR6S7X/S1053811919303234.html}
}

@article{diliberto_music_2021,
  title = {The {{Music}} of {{Silence}}: {{Part II}}: {{Music Listening Induces Imagery Responses}}},
  shorttitle = {The {{Music}} of {{Silence}}},
  author = {Di Liberto, Giovanni M and Marion, Guilhem and Shamma, Shihab A.},
  year = {2021},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {41},
  number = {35},
  pages = {7449--7460},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0184-21.2021},
  abstract = {During music listening, humans routinely acquire the regularities of the acoustic sequences and use them to anticipate and interpret the ongoing melody. Specifically, in line with this predictive framework, it is thought that brain responses during such listening reflect a comparison between the bottom-up sensory responses and top-down prediction signals generated by an internal model that embodies the music exposure and expectations of the listener. To attain a clear view of these predictive responses, previous work has eliminated the sensory inputs by inserting artificial silences (or sound omissions) that leave behind only the corresponding predictions of the thwarted expectations. Here, we demonstrate a new alternate approach in which we decode the predictive electroencephalography (EEG) responses to the silent intervals that are naturally interspersed within the music. We did this as participants (experiment 1, 20 participants, 10 female; experiment 2, 21 participants, 6 female) listened or imagined Bach piano melodies. Prediction signals were quantified and assessed via a computational model of the melodic structure of the music and were shown to exhibit the same response characteristics when measured during listening or imagining. These include an inverted polarity for both silence and imagined responses relative to listening, as well as response magnitude modulations that precisely reflect the expectations of notes and silences in both listening and imagery conditions. These findings therefore provide a unifying view that links results from many previous paradigms, including omission reactions and the expectation modulation of sensory responses, all in the context of naturalistic music listening. SIGNIFICANCE STATEMENT Music perception depends on our ability to learn and detect melodic structures. It has been suggested that our brain does so by actively predicting upcoming music notes, a process inducing instantaneous neural responses as the music confronts these expectations. Here, we studied this prediction process using EEGs recorded while participants listen to and imagine Bach melodies. Specifically, we examined neural signals during the ubiquitous musical pauses (or silent intervals) in a music stream and analyzed them in contrast to the imagery responses. We find that imagined predictive responses are routinely co-opted during ongoing music listening. These conclusions are revealed by a new paradigm using listening and imagery of naturalistic melodies.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2021 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {34341154},
  keywords = {EEG,expectation,omission,predictive processing,TRF},
  file = {/Users/xzfang/Zotero/storage/HYCNLDPX/Liberto et al. - 2021 - The Music of Silence Part II Music Listening Ind.pdf;/Users/xzfang/Zotero/storage/IPGUTEVC/7449.html}
}

@article{dilks_occipital_2013,
  title = {The {{Occipital Place Area Is Causally}} and {{Selectively Involved}} in {{Scene Perception}}},
  author = {Dilks, Daniel D. and Julian, Joshua B. and Paunov, Alexander M. and Kanwisher, Nancy},
  year = {2013},
  month = jan,
  journal = {The Journal of Neuroscience},
  volume = {33},
  number = {4},
  pages = {1331--1336},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.4081-12.2013},
  abstract = {Functional magnetic resonance imaging has revealed a set of regions selectively engaged in visual scene processing: the parahippocampal place area (PPA), the retrosplenial complex (RSC), and a region around the transverse occipital sulcus (previously known as ``TOS''), here renamed the ``occipital place area'' (OPA). Are these regions not only preferentially activated by, but also causally involved in scene perception? Although past neuropsychological data imply a causal role in scene processing for PPA and RSC, no such evidence exists for OPA. Thus, to test the causal role of OPA in human adults, we delivered transcranial magnetic stimulation (TMS) to the right OPA (rOPA) or the nearby face-selective right occipital face area (rOFA) while participants performed fine-grained perceptual discrimination tasks on scenes or faces. TMS over rOPA impaired discrimination of scenes but not faces, while TMS over rOFA impaired discrimination of faces but not scenes. In a second experiment, we delivered TMS to rOPA, or the object-selective right lateral occipital complex (rLOC), while participants performed categorization tasks involving scenes and objects. TMS over rOPA impaired categorization accuracy of scenes but not objects, while TMS over rLOC impaired categorization accuracy of objects but not scenes. These findings provide the first evidence that OPA is causally involved in scene processing, and further show that this causal role is selective for scene perception. Our findings illuminate the functional architecture of the scene perception system, and also argue against the ``distributed coding'' view in which each category-selective region participates in the representation of all objects.},
  pmcid = {PMC3711611},
  pmid = {23345209},
  file = {/Users/xzfang/Zotero/storage/7KT2DVTY/Dilks et al. - 2013 - The Occipital Place Area Is Causally and Selective.pdf}
}

@article{dimigen_coregistration_2011,
  title = {Coregistration of Eye Movements and {{EEG}} in Natural Reading: {{Analyses}} and Review},
  shorttitle = {Coregistration of Eye Movements and {{EEG}} in Natural Reading},
  author = {Dimigen, Olaf and Sommer, Werner and Hohlfeld, Annette and Jacobs, Arthur M. and Kliegl, Reinhold},
  year = {2011},
  journal = {Journal of Experimental Psychology: General},
  volume = {140},
  number = {4},
  pages = {552--572},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0023885},
  abstract = {Brain-electric correlates of reading have traditionally been studied with word-by-word presentation, a condition that eliminates important aspects of the normal reading process and precludes direct comparisons between neural activity and oculomotor behavior. In the present study, we investigated effects of word predictability on eye movements (EM) and fixation-related brain potentials (FRPs) during natural sentence reading. Electroencephalogram (EEG) and EM (via video-based eye tracking) were recorded simultaneously while subjects read heterogeneous German sentences, moving their eyes freely over the text. FRPs were time-locked to first-pass reading fixations and analyzed according to the cloze probability of the currently fixated word. We replicated robust effects of word predictability on EMs and the N400 component in FRPs. The data were then used to model the relation among fixation duration, gaze duration, and N400 amplitude, and to trace the time course of EEG effects relative to effects in EM behavior. In an extended Methodological Discussion section, we review 4 technical and data-analytical problems that need to be addressed when FRPs are recorded in free-viewing situations (such as reading, visual search, or scene perception) and propose solutions. Results suggest that EEG recordings during normal vision are feasible and useful to consolidate findings from EEG and eye-tracking studies. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Electroencephalography,Eye Fixation,Eye Movements,Prediction,Reading,Sentences,Visual Evoked Potentials,Visual Tracking,Words (Phonetic Units)},
  file = {/Users/xzfang/Zotero/storage/496UBQH5/2011-14094-001.html}
}

@article{dimigen_regressionbased_2021,
  title = {Regression-Based Analysis of Combined {{EEG}} and Eye-Tracking Data: {{Theory}} and Applications},
  shorttitle = {Regression-Based Analysis of Combined {{EEG}} and Eye-Tracking Data},
  author = {Dimigen, Olaf and Ehinger, Benedikt V.},
  year = {2021},
  month = jan,
  journal = {Journal of Vision},
  volume = {21},
  number = {1},
  pages = {3--3},
  publisher = {{The Association for Research in Vision and Ophthalmology}},
  issn = {1534-7362},
  doi = {10.1167/jov.21.1.3},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BTSY6MF4/Dimigen and Ehinger - 2021 - Regression-based analysis of combined EEG and eye-.pdf;/Users/xzfang/Zotero/storage/KTRFDF6D/article.html}
}

@article{ding_attention_2018,
  title = {Attention {{Is Required}} for {{Knowledge-Based Sequential Grouping}}: {{Insights}} from the {{Integration}} of {{Syllables}} into {{Words}}},
  shorttitle = {Attention {{Is Required}} for {{Knowledge-Based Sequential Grouping}}},
  author = {Ding, Nai and Pan, Xunyi and Luo, Cheng and Su, Naifei and Zhang, Wen and Zhang, Jianfeng},
  year = {2018},
  month = jan,
  journal = {The Journal of Neuroscience},
  volume = {38},
  number = {5},
  pages = {1178--1188},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2606-17.2017},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/P63RMMIG/Ding et al. - 2018 - Attention Is Required for Knowledge-Based Sequenti.pdf}
}

@article{ding_characterizing_2017,
  title = {Characterizing {{Neural Entrainment}} to {{Hierarchical Linguistic Units}} Using {{Electroencephalography}} ({{EEG}})},
  author = {Ding, Nai and Melloni, Lucia and Yang, Aotian and Wang, Yu and Zhang, Wen and Poeppel, David},
  year = {2017},
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  issn = {1662-5161},
  abstract = {To understand speech, listeners have to combine the words they hear into phrases and sentences. Recent magnetoencephalography (MEG) and electrocorticography (ECoG) studies show that cortical activity is concurrently entrained/synchronized to the rhythms of multiple levels of linguistic units including words, phrases, and sentences. Here we investigate whether this phenomenon can be observed using electroencephalography (EEG), a technique that is more widely available than MEG and ECoG. We show that the EEG responses concurrently track the rhythms of hierarchical linguistic units such as syllables/words, phrases, and sentences. The strength of the sentential-rate response correlates with how well each subject can detect random words embedded in a sequence of sentences. In contrast, only a syllabic-rate response is observed for an unintelligible control stimulus. In sum, EEG provides a useful tool to characterize neural encoding of hierarchical linguistic units, potentially even in individual participants.},
  file = {/Users/xzfang/Zotero/storage/S6LRSRFB/Ding et al. - 2017 - Characterizing Neural Entrainment to Hierarchical .pdf}
}

@article{ding_cortical_2016,
  title = {Cortical Tracking of Hierarchical Linguistic Structures in Connected Speech},
  author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
  year = {2016},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {19},
  number = {1},
  pages = {158--164},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4186},
  abstract = {Language consists of a hierarchy of linguistic units: words, phrases and sentences. The authors explore whether and how these abstract linguistic units are represented in the brain during speech comprehension. They find that cortical rhythms track the timescales of these linguistic units, revealing a hierarchy of neural processing timescales underlying internal construction of hierarchical linguistic structure.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Language,Psychology,Sensory processing},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Language;Psychology;Sensory processing Subject\_term\_id: language;psychology;sensory-processing},
  file = {/Users/xzfang/Zotero/storage/GG2PHGKS/Ding et al. - 2016 - Cortical tracking of hierarchical linguistic struc.pdf;/Users/xzfang/Zotero/storage/BGCM922L/nn.html}
}

@article{ding_emergence_2012,
  title = {Emergence of Neural Encoding of Auditory Objects While Listening to Competing Speakers},
  author = {Ding, N. and Simon, J. Z.},
  year = {2012},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {29},
  pages = {11854--11859},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1205381109},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IV68V3U7/Ding and Simon - 2012 - Emergence of neural encoding of auditory objects w.pdf}
}

@article{ding_temporal_2017,
  title = {Temporal Modulations in Speech and Music},
  author = {Ding, Nai and Patel, Aniruddh D. and Chen, Lin and Butler, Henry and Luo, Cheng and Poeppel, David},
  year = {2017},
  month = oct,
  journal = {Neuroscience \& Biobehavioral Reviews},
  series = {The {{Biology}} of {{Language}}},
  volume = {81},
  pages = {181--187},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2017.02.011},
  abstract = {Speech and music have structured rhythms. Here we discuss a major acoustic correlate of spoken and musical rhythms, the slow (0.25\textendash 32Hz) temporal modulations in sound intensity and compare the modulation properties of speech and music. We analyze these modulations using over 25h of speech and over 39h of recordings of Western music. We show that the speech modulation spectrum is highly consistent across 9 languages (including languages with typologically different rhythmic characteristics). A different, but similarly consistent modulation spectrum is observed for music, including classical music played by single instruments of different types, symphonic, jazz, and rock. The temporal modulations of speech and music show broad but well-separated peaks around 5 and 2Hz, respectively. These acoustically dominant time scales may be intrinsic features of speech and music, a possibility which should be investigated using more culturally diverse samples in each domain. Distinct modulation timescales for speech and music could facilitate their perceptual analysis and its neural processing.},
  langid = {english},
  keywords = {Modulation spectrum,Music,Rhythm,Speech,Temporal modulations},
  file = {/Users/xzfang/Zotero/storage/SFSLDTH7/Ding et al. - 2017 - Temporal modulations in speech and music.pdf}
}

@article{ditman_investigation_2007,
  title = {An Investigation of Concurrent {{ERP}} and Self-Paced Reading Methodologies},
  author = {Ditman, Tali and Holcomb, Phillip J. and Kuperberg, Gina R.},
  year = {2007},
  journal = {Psychophysiology},
  volume = {44},
  number = {6},
  pages = {927--935},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.2007.00593.x},
  abstract = {Traditionally, event-related brain potential (ERP) studies of language processing have presented words at a fixed rate using rapid serial visual presentation. Recent studies suggest, however, that the processes engaged during sentence comprehension are contingent on word presentation rate. These findings underscore the importance of allowing participants to read at a natural pace. The present study employed simultaneous self-paced reading and ERP methodologies to examine behavioral and neural responses while participants read sentences containing pragmatic or morphosyntactic violations or no violations. ERP and self-paced reading results replicated previous findings. This novel combination of behavioral and ERP methodologies combines the high temporal resolution and direct neural measures offered by ERPs with the more natural reading environment and information about processing load provided by self-paced reading.},
  langid = {english},
  keywords = {Event-related potentials,Language,Methodology,N400,P600,Self-paced reading,Sentence},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2007.00593.x},
  file = {/Users/xzfang/Zotero/storage/UMP84J8J/Ditman et al. - 2007 - An investigation of concurrent ERP and self-paced .pdf;/Users/xzfang/Zotero/storage/B4XFGILR/j.1469-8986.2007.00593.html}
}

@article{ditterich_microstimulation_2003,
  title = {Microstimulation of Visual Cortex Affects the Speed of Perceptual Decisions},
  author = {Ditterich, Jochen and Mazurek, Mark E. and Shadlen, Michael N.},
  year = {2003},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {6},
  number = {8},
  pages = {891--898},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1094},
  abstract = {Direction-selective neurons in the middle temporal visual area (MT) are crucially involved in motion perception, although it is not known exactly how the activity of these neurons is interpreted by the rest of the brain. Here we report that in a two-alternative task, the activity of MT neurons is interpreted as evidence for one direction and against the other. We measured the speed and accuracy of decisions as rhesus monkeys performed a direction-discrimination task. On half of the trials, we stimulated direction-selective neurons in area MT, thereby causing the monkeys to choose the neurons' preferred direction more often. Microstimulation quickened decisions in favor of the preferred direction and slowed decisions in favor of the opposite direction. Even on trials in which microstimulation did not induce a preferred direction choice, it still affected response times. Our findings suggest that during the formation of a decision, sensory evidence for competing propositions is compared and accumulates to a decision-making threshold.},
  copyright = {2003 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/xzfang/Zotero/storage/YK5EM52Y/Ditterich et al. - 2003 - Microstimulation of visual cortex affects the spee.pdf;/Users/xzfang/Zotero/storage/CDPN829U/nn1094.html}
}

@incollection{ditzinger_second_2021,
  title = {Second {{Journey}}: {{The Geometrical-Optical Illusions}}},
  shorttitle = {Second {{Journey}}},
  booktitle = {Illusions of {{Seeing}}: {{Exploring}} the {{World}} of {{Visual Perception}}},
  author = {Ditzinger, Thomas},
  editor = {Ditzinger, Thomas},
  year = {2021},
  pages = {17--40},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-63635-7_2},
  abstract = {The right perspective is crucial! Depending on the perspective from which we see things, we interpret them in this or that way.},
  isbn = {978-3-030-63635-7},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/R2S4KAGY/Ditzinger - 2021 - Second Journey The Geometrical-Optical Illusions.pdf}
}

@article{dobs_how_2019,
  title = {How Face Perception Unfolds over Time},
  author = {Dobs, Katharina and Isik, Leyla and Pantazis, Dimitrios and Kanwisher, Nancy},
  year = {2019},
  month = mar,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1--10},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09239-1},
  abstract = {We can rapidly determine the gender, age and identity of a face, but the exact steps involved are unclear. Here, the authors show using magnetoencephalography (MEG) that gender and age are encoded in the brain before identity, and reveal the role of familiarity in the earliest stages of face processing.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HMW5VCR6/Dobs et al. - 2019 - How face perception unfolds over time.pdf;/Users/xzfang/Zotero/storage/CJQTQ7ZR/s41467-019-09239-1.html}
}

@article{doelling_cortical_2015,
  title = {Cortical Entrainment to Music and Its Modulation by Expertise},
  author = {Doelling, Keith B. and Poeppel, David},
  year = {2015},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {45},
  pages = {E6233-E6242},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1508431112},
  abstract = {Recent studies establish that cortical oscillations track naturalistic speech in a remarkably faithful way. Here, we test whether such neural activity, particularly low-frequency ({$<$}8 Hz; delta\textendash theta) oscillations, similarly entrain to music and whether experience modifies such a cortical phenomenon. Music of varying tempi was used to test entrainment at different rates. In three magnetoencephalography experiments, we recorded from nonmusicians, as well as musicians with varying years of experience. Recordings from nonmusicians demonstrate cortical entrainment that tracks musical stimuli over a typical range of tempi, but not at tempi below 1 note per second. Importantly, the observed entrainment correlates with performance on a concurrent pitch-related behavioral task. In contrast, the data from musicians show that entrainment is enhanced by years of musical training, at all presented tempi. This suggests a bidirectional relationship between behavior and cortical entrainment, a phenomenon that has not previously been reported. Additional analyses focus on responses in the beta range ({$\sim$}15\textendash 30 Hz)\textemdash often linked to delta activity in the context of temporal predictions. Our findings provide evidence that the role of beta in temporal predictions scales to the complex hierarchical rhythms in natural music and enhances processing of musical content. This study builds on important findings on brainstem plasticity and represents a compelling demonstration that cortical neural entrainment is tightly coupled to both musical training and task performance, further supporting a role for cortical oscillatory activity in music perception and cognition.},
  chapter = {PNAS Plus},
  langid = {english},
  pmid = {26504238},
  keywords = {beta,entrainment,MEG,musical expertise,theta},
  file = {/Users/xzfang/Zotero/storage/WZV5T5RZ/Doelling and Poeppel - 2015 - Cortical entrainment to music and its modulation b.pdf}
}

@article{doelling_neural_2021,
  title = {Neural Oscillations Are a Start toward Understanding Brain Activity Rather than the End},
  author = {Doelling, Keith B. and Assaneo, M. Florencia},
  year = {2021},
  month = may,
  journal = {PLOS Biology},
  volume = {19},
  number = {5},
  pages = {e3001234},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3001234},
  abstract = {Does rhythmic neural activity merely echo the rhythmic features of the environment, or does it reflect a fundamental computational mechanism of the brain? This debate has generated a series of clever experimental studies attempting to find an answer. Here, we argue that the field has been obstructed by predictions of oscillators that are based more on intuition rather than biophysical models compatible with the observed phenomena. What follows is a series of cautionary examples that serve as reminders to ground our hypotheses in well-developed theories of oscillatory behavior put forth by theoretical study of dynamical systems. Ultimately, our hope is that this exercise will push the field to concern itself less with the vague question of ``oscillation or not'' and more with specific biophysical models that can be readily tested.},
  langid = {english},
  keywords = {Acoustics,Auditory cortex,Behavior,Biophysics,Circadian oscillators,Dynamical systems,Sensory perception,Sine waves},
  file = {/Users/xzfang/Zotero/storage/HS6VXV79/Doelling and Assaneo - 2021 - Neural oscillations are a start toward understandi.pdf;/Users/xzfang/Zotero/storage/FRWLDECU/article.html}
}

@article{doerig_crowding_2020,
  title = {Crowding Reveals Fundamental Differences in Local vs. Global Processing in Humans and Machines},
  author = {Doerig, A. and Bornet, A. and Choung, O. H. and Herzog, M. H.},
  year = {2020},
  month = feb,
  journal = {Vision Research},
  volume = {167},
  pages = {39--45},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2019.12.006},
  abstract = {Feedforward Convolutional Neural Networks (ffCNNs) have become state-of-the-art models both in computer vision and neuroscience. However, human-like performance of ffCNNs does not necessarily imply human-like computations. Previous studies have suggested that current ffCNNs do not make use of global shape information. However, it is currently unclear whether this reflects fundamental differences between ffCNN and human processing or is merely an artefact of how ffCNNs are trained. Here, we use visual crowding as a well-controlled, specific probe to test global shape computations. Our results provide evidence that ffCNNs cannot produce human-like global shape computations for principled architectural reasons. We lay out approaches that may address shortcomings of ffCNNs to provide better models of the human visual system.},
  langid = {english},
  keywords = {Convolutional Neural Networks,Crowding,Deep Neural Networks,Global processing,Grouping,Segmentation},
  file = {/Users/xzfang/Zotero/storage/7LUB2T7W/Doerig et al. - 2020 - Crowding reveals fundamental differences in local .pdf;/Users/xzfang/Zotero/storage/89GS9SDU/S0042698919302299.html}
}

@article{doerig_crowding_2020a,
  title = {Crowding Reveals Fundamental Differences in Local vs. Global Processing in Humans and Machines},
  author = {Doerig, A. and Bornet, A. and Choung, O. H. and Herzog, M. H.},
  year = {2020},
  month = feb,
  journal = {Vision Research},
  volume = {167},
  pages = {39--45},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2019.12.006},
  abstract = {Feedforward Convolutional Neural Networks (ffCNNs) have become state-of-the-art models both in computer vision and neuroscience. However, human-like performance of ffCNNs does not necessarily imply human-like computations. Previous studies have suggested that current ffCNNs do not make use of global shape information. However, it is currently unclear whether this reflects fundamental differences between ffCNN and human processing or is merely an artefact of how ffCNNs are trained. Here, we use visual crowding as a well-controlled, specific probe to test global shape computations. Our results provide evidence that ffCNNs cannot produce human-like global shape computations for principled architectural reasons. We lay out approaches that may address shortcomings of ffCNNs to provide better models of the human visual system.},
  langid = {english},
  keywords = {Convolutional Neural Networks,Crowding,Deep Neural Networks,Global processing,Grouping,Segmentation},
  file = {/Users/xzfang/Zotero/storage/UQPSXIDF/Doerig et al. - 2020 - Crowding reveals fundamental differences in local .pdf;/Users/xzfang/Zotero/storage/JHH3AADU/S0042698919302299.html}
}

@article{donhauser_two_2020,
  title = {Two {{Distinct Neural Timescales}} for {{Predictive Speech Processing}}},
  author = {Donhauser, Peter W. and Baillet, Sylvain},
  year = {2020},
  month = jan,
  journal = {Neuron},
  volume = {105},
  number = {2},
  pages = {385-393.e9},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.10.019},
  abstract = {During speech listening, the brain could use contextual predictions to optimize sensory sampling and processing. We asked if such predictive processing is organized dynamically into separate oscillatory timescales. We trained a neural network that uses context to predict speech at the phoneme level. Using this model, we estimated contextual uncertainty and surprise of natural speech as factors to explain neurophysiological activity in human listeners. We show, first, that speech-related activity is hierarchically organized into two timescales: fast responses (theta: 4\textendash 10~Hz), restricted to early auditory regions, and slow responses (delta: 0.5\textendash 4~Hz), dominating in downstream auditory regions. Neural activity in these bands is selectively modulated by predictions: the gain of early theta responses varies according to the contextual uncertainty of speech, while later delta responses are selective to surprising speech inputs. We conclude that theta sensory sampling is tuned to maximize expected information gain, while delta encodes only non-redundant information. Video Abstract},
  langid = {english},
  keywords = {auditory processing,delta oscillations,MEG,neural networks,predictive coding,speech processing,surprise,temporal response functions,theta oscillations,uncertainty},
  file = {/Users/xzfang/Zotero/storage/UBDQRGFG/Donhauser and Baillet - 2020 - Two Distinct Neural Timescales for Predictive Spee.pdf;/Users/xzfang/Zotero/storage/J3WTVI7E/S0896627319308931.html}
}

@article{doron_intact_2015,
  title = {Intact Crowding and Temporal Masking in Dyslexia},
  author = {Doron, Adi and Manassi, Mauro and Herzog, Michael H. and Ahissar, Merav},
  year = {2015},
  month = oct,
  journal = {Journal of Vision},
  volume = {15},
  number = {14},
  pages = {13--13},
  publisher = {{The Association for Research in Vision and Ophthalmology}},
  issn = {1534-7362},
  doi = {10.1167/15.14.13},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/V3L4KCGM/Doron et al. - 2015 - Intact crowding and temporal masking in dyslexia.pdf}
}

@article{dosenbach_distinct_2007,
  title = {Distinct Brain Networks for Adaptive and Stable Task Control in Humans},
  author = {Dosenbach, Nico U. F. and Fair, Damien A. and Miezin, Francis M. and Cohen, Alexander L. and Wenger, Kristin K. and Dosenbach, Ronny A. T. and Fox, Michael D. and Snyder, Abraham Z. and Vincent, Justin L. and Raichle, Marcus E. and Schlaggar, Bradley L. and Petersen, Steven E.},
  year = {2007},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {26},
  pages = {11073--11078},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0704320104},
  abstract = {Control regions in the brain are thought to provide signals that configure the brain's moment-to-moment information processing. Previously, we identified regions that carried signals related to task-control initiation, maintenance, and adjustment. Here we characterize the interactions of these regions by applying graph theory to resting state functional connectivity MRI data. In contrast to previous, more unitary models of control, this approach suggests the presence of two distinct task-control networks. A frontoparietal network included the dorsolateral prefrontal cortex and intraparietal sulcus. This network emphasized start-cue and error-related activity and may initiate and adapt control on a trial-by-trial basis. The second network included dorsal anterior cingulate/medial superior frontal cortex, anterior insula/frontal operculum, and anterior prefrontal cortex. Among other signals, these regions showed activity sustained across the entire task epoch, suggesting that this network may control goal-directed behavior through the stable maintenance of task sets. These two independent networks appear to operate on different time scales and affect downstream processing via dissociable mechanisms.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2007 by The National Academy of Sciences of the USA},
  langid = {english},
  pmid = {17576922},
  keywords = {attention,connectivity,executive control,functional MRI,task set},
  file = {/Users/xzfang/Zotero/storage/7BWQNLFD/Dosenbach et al. - 2007 - Distinct brain networks for adaptive and stable ta.pdf;/Users/xzfang/Zotero/storage/LVMZ6DNE/11073.html}
}

@article{dotsch_statistical_2016,
  title = {Statistical Learning Shapes Face Evaluation},
  author = {Dotsch, Ron and Hassin, Ran R. and Todorov, Alexander},
  year = {2016},
  month = nov,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {1--6},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0001},
  abstract = {The belief in physiognomy\textemdash the art of reading character from faces\textemdash has been with us for centuries1\textendash 3. People everywhere infer traits (for example, trustworthiness) from faces, and these inferences predict economic, legal and even voting decisions2,4. Research has identified many configurations of facial features that predict specific trait inferences2,5\textendash 14, and detailed computational models of such inferences have recently been developed5\textendash 7,15\textendash 17. However, these configurations do not fully account for trait inferences from faces. Here, we propose a new direction in the study of inferences from faces, inspired by a cognitive\textendash ecological18\textendash 20 and implicit-learning approach21,22. Any face can be positioned in a statistical distribution of faces extracted from the environment. We argue that understanding inferences from faces requires consideration of the statistical position of the faces in this learned distribution. Four experiments show that the mere statistical position of faces imbues them with social meaning: faces are evaluated more negatively the more they deviate from a learned central tendency. Our findings open new possibilities for the study of face evaluation, providing a potential model for explaining both individual and cross-cultural variation, as individuals are immersed in varying environments that contain different distributions of facial features.},
  copyright = {2016 Nature Publishing Group},
  langid = {english},
  keywords = {Decision making,Human behaviour,Perception,Psychology,Social neuroscience},
  file = {/Users/xzfang/Zotero/storage/4KB7EXYB/Dotsch et al. - 2016 - Statistical learning shapes face evaluation.pdf}
}

@article{drew_invisible_2013,
  title = {``{{The}} Invisible Gorilla Strikes Again: {{Sustained}} Inattentional Blindness in Expert Observers''},
  shorttitle = {``{{The}} Invisible Gorilla Strikes Again},
  author = {Drew, Trafton and Vo, Melissa L. H. and Wolfe, Jeremy M.},
  year = {2013},
  month = sep,
  journal = {Psychological science},
  volume = {24},
  number = {9},
  pages = {1848--1853},
  issn = {0956-7976},
  doi = {10.1177/0956797613479386},
  abstract = {We like to think that we would notice the occurrence of an unexpected yet salient event in our world. However, we know that people often miss such events if they are engaged in a different task, a phenomenon known as ``inattentional blindness.'' Still, these demonstrations typically involve na\"ive observers engaged in an unfamiliar task. What about expert searchers who have spent years honing their ability to detect small abnormalities in specific types of image? We asked 24 radiologists to perform a familiar lung nodule detection task. A gorilla, 48 times larger than the average nodule, was inserted in the last case. 83\% of radiologists did not see the gorilla. Eye-tracking revealed that the majority of the those who missed the gorilla looked directly at the location of the gorilla. Even expert searchers, operating in their domain of expertise, are vulnerable to inattentional blindness.},
  pmcid = {PMC3964612},
  pmid = {23863753},
  file = {/Users/xzfang/Zotero/storage/QRIFWVT9/Drew et al. - 2013 - â€œThe invisible gorilla strikes again Sustained in.pdf}
}

@misc{dubey_aha_2021,
  title = {Aha! Moments Correspond to Meta-Cognitive Prediction Errors},
  author = {Dubey, Rachit and Ho, Mark K. and Mehta, Hermish and Griffiths, Tom},
  year = {2021},
  month = jun,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/c5v42},
  abstract = {Psychologists have long been fascinated with understanding the nature of Aha! moments, moments when we transition from not knowing to suddenly realizing the solution to a problem. In this work, we present a theoretical framework that explains when and why we experience Aha! moments. Our theory posits that during problem-solving, in addition to solving the problem, people also maintain a meta-cognitive model of their ability to solve the problem as well as a prediction about the time it would take them to solve that problem. Aha! moments arise when we experience a positive error in this meta-cognitive prediction, i.e. when we solve a problem much faster than we expected to solve it. We posit that this meta-cognitive error is analogous to a positive reward prediction error thereby explaining why we feel so good after an Aha! moment. A large-scale pre-registered experiment on anagram solving supports this theory, showing that people's time prediction errors are strongly correlated with their ratings of an Aha! experience while solving anagrams. A second experiment provides further evidence to our theory by demonstrating a causal link between time prediction errors and the Aha! experience. These results highlight the importance of meta-cognitive prediction errors and deepen our understanding of human meta-reasoning.},
  keywords = {Aha! moment,Cognitive Psychology,Creativity,Engineering Psychology,Insight,Judgment and Decision Making,metacognition,monitoring and control,prediction errors,problem solving,Problem Solving,reinforcement learning,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/8EP6MQRG/Dubey et al. - 2021 - Aha! moments correspond to meta-cognitive predicti.pdf}
}

@article{dubossarsky_quantifying_2017,
  title = {Quantifying the Structure of Free Association Networks across the Life Span.},
  author = {Dubossarsky, Haim and De Deyne, Simon and Hills, Thomas T.},
  year = {2017},
  month = aug,
  journal = {Developmental Psychology},
  volume = {53},
  number = {8},
  pages = {1560--1570},
  issn = {1939-0599, 0012-1649},
  doi = {10.1037/dev0000347},
  abstract = {We investigate how the mental lexicon changes over the life span using free association data from over 8,000 individuals, ranging from 10 to 84 years of age, with more than 400 cue words per age group. Using network analysis, with words as nodes and edges defined by the strength of shared associations, we find that associative networks evolve in a nonlinear (U-shaped) fashion over the life span. During early life, the network converges and becomes increasingly structured, with reductions in average path length, entropy, clustering coefficient, and small world index. Into late life, the pattern reverses but shows clear differences from early life. The pattern is independent of the increasing number of word types produced per cue across the life span, consistent with a network encoding an increasing number of relations between words as individuals age. Lifetime variability is dominantly driven by associative change in the least well-connected words.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RPF5VP88/Dubossarsky et al. - 2017 - Quantifying the structure of free association netw.pdf}
}

@article{duff_hippocampus_2012,
  title = {The Hippocampus and the Flexible Use and Processing of Language},
  author = {Duff, Melissa C. and {Brown-Schmidt}, Sarah},
  year = {2012},
  month = apr,
  journal = {Frontiers in Human Neuroscience},
  volume = {6},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2012.00069},
  abstract = {Fundamental to all human languages is an unlimited expressive capacity and creative flexibility that allow speakers to rapidly generate novel and complex utterances. In turn, listeners interpret language ``on-line,'' incrementally integrating multiple sources of information as words unfold over time. A challenge for theories of language processing has been to understand how speakers and listeners generate, gather, integrate, and maintain representations in service of language processing. We propose that many of the processes by which we use language place high demands on and receive contributions from the hippocampal declarative memory system. The hippocampal declarative memory system is long known to support relational binding and representational flexibility. Recent findings demonstrate that these same functions are engaged during the real-time processes that support behavior in-the-moment. Such findings point to the hippocampus as a potentially key contributor to cognitive functions that require on-line integration of multiple sources of information, such as on-line language processing. Evidence supporting this view comes from findings that individuals with hippocampal amnesia show deficits in the use of language flexibly and on-line. We conclude that the relational binding and representational flexibility afforded by the hippocampal declarative memory system positions the hippocampus as a key contributor to language use and processing.},
  pmcid = {PMC3319917},
  pmid = {22493573},
  file = {/Users/xzfang/Zotero/storage/8Q5XPEYN/Duff and Brown-Schmidt - 2012 - The hippocampus and the flexible use and processin.pdf}
}

@article{dumay_sleepassociated_2007,
  title = {Sleep-{{Associated Changes}} in the {{Mental Representation}} of {{Spoken Words}}},
  author = {Dumay, Nicolas and Gaskell, M. Gareth},
  year = {2007},
  month = jan,
  journal = {Psychological Science},
  volume = {18},
  number = {1},
  pages = {35--39},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2007.01845.x},
  abstract = {The integration of a newly learned spoken word form with existing knowledge in the mental lexicon is characterized by the word form's ability to compete with similar-sounding entries during auditory word recognition. Here we show that although the mere acquisition of a spoken form is swift, its engagement in lexical competition requires an incubation-like period that is crucially associated with sleep. Words learned at 8 p.m. do not induce (inhibitory) competition effects immediately, but do so after a 12-hr interval including a night's sleep, and continue to induce such effects after 24 hr. In contrast, words learned at 8 a.m. do not show such effects immediately or after 12 hr ofwakefulness, but show the effects only after 24 hr, after sleep has occurred. This time-course dissociation is best accommodated by connectionist and neural models of learning in which sleep provides an opportunity for hippocampal information to be fed into long-term neocortical memory.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6B2D9EM3/Dumay and Gaskell - 2007 - Sleep-Associated Changes in the Mental Representat.pdf}
}

@article{duncan_common_2000,
  title = {Common Regions of the Human Frontal Lobe Recruited by Diverse Cognitive Demands},
  author = {Duncan, John and Owen, Adrian M},
  year = {2000},
  month = oct,
  journal = {Trends in Neurosciences},
  volume = {23},
  number = {10},
  pages = {475--483},
  issn = {0166-2236},
  doi = {10.1016/S0166-2236(00)01633-7},
  abstract = {Though many neuroscientific methods have been brought to bear in the search for functional specializations within prefrontal cortex, little consensus has emerged. To assess the contribution of functional neuroimaging, this article reviews patterns of frontal-lobe activation associated with a broad range of different cognitive demands, including aspects of perception, response selection, executive control, working memory, episodic memory and problem solving. The results show a striking regularity: for many demands, there is a similar recruitment of mid-dorsolateral, mid-ventrolateral and dorsal anterior cingulate cortex. Much of the remainder of frontal cortex, including most of the medial and orbital surfaces, is largely insensitive to these demands. Undoubtedly, these results provide strong evidence for regional specialization of function within prefrontal cortex. This specialization, however, takes an unexpected form: a specific frontal-lobe network that is consistently recruited for solution of diverse cognitive problems.},
  langid = {english},
  keywords = {Cognition,Frontal-lobe activation,Neuroimaging,Prefrontal cortex,Regional specialization},
  file = {/Users/xzfang/Zotero/storage/SYIFDYWI/Duncan and Owen - 2000 - Common regions of the human frontal lobe recruited.pdf;/Users/xzfang/Zotero/storage/GRZN6AIA/S0166223600016337.html}
}

@article{duncan_selective_,
  title = {Selective {{Attention}} and the {{Organization}} of {{Visual Information}}},
  author = {Duncan, John},
  pages = {17},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YGSYGV6B/Duncan - Selective Attention and the Organization of Visual.pdf}
}

@article{dux_attentional_2009,
  title = {The Attentional Blink: {{A}} Review of Data and Theory},
  shorttitle = {The Attentional Blink},
  author = {Dux, Paul E. and Marois, Ren{\'e}},
  year = {2009},
  month = nov,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {71},
  number = {8},
  pages = {1683--1700},
  issn = {1943-393X},
  doi = {10.3758/APP.71.8.1683},
  abstract = {Under conditions of rapid serial visual presentation, subjects display a reduced ability to report the second of two targets(Target2; T2) in a stream of distractors if it appearswithin200-500 msec of Target 1 (Tl). This effect. known as the attentional blink(AB),has been central in characterizing the limits of humans' ability to consciously perceive stimuli distributed across time. Here, we review theoretical accounts of the AB and examine how they explain key findings in the literature. We conclude that the AB arises from attentional demands of Tl for selection, working memory encoding, episodic registration,and response selection, which prevents this high-level central resource from being applied to T2 at shortT1-T2 lags. Tl processing also transiently impairs the redeployment of these attentional resources to subsequent targets and the inhibition of distractors that appear in close temporal proximity to T2. Although these findings are consistent with a multifactorial account of the AB,they can also be largely explained by assuming that the activation of these multiple processes depends on a common capacity-limited attentional process for selecting behaviorally relevant events presented among temporally distributed distractors. Thus, at its core, the attentional blink may ultimately reveal the temporal limits of the deployment of selective attention.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PXYFIK7U/Dux and Marois - 2009 - The attentional blink A review of data and theory.pdf}
}

@article{dye_visual_2009,
  title = {Is {{Visual Selective Attention}} in {{Deaf Individuals Enhanced}} or {{Deficient}}? {{The Case}} of the {{Useful Field}} of {{View}}},
  shorttitle = {Is {{Visual Selective Attention}} in {{Deaf Individuals Enhanced}} or {{Deficient}}?},
  author = {Dye, Matthew W. G. and Hauser, Peter C. and Bavelier, Daphne},
  year = {2009},
  month = may,
  journal = {PLOS ONE},
  volume = {4},
  number = {5},
  pages = {e5640},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005640},
  abstract = {Background Early deafness leads to enhanced attention in the visual periphery. Yet, whether this enhancement confers advantages in everyday life remains unknown, as deaf individuals have been shown to be more distracted by irrelevant information in the periphery than their hearing peers. Here, we show that, in a complex attentional task, a performance advantage results for deaf individuals. Methodology/Principal Findings We employed the Useful Field of View (UFOV) which requires central target identification concurrent with peripheral target localization in the presence of distractors \textendash{} a divided, selective attention task. First, the comparison of deaf and hearing adults with or without sign language skills establishes that deafness and not sign language use drives UFOV enhancement. Second, UFOV performance was enhanced in deaf children, but only after 11 years of age. Conclusions/Significance This work demonstrates that, following early auditory deprivation, visual attention resources toward the periphery slowly get augmented to eventually result in a clear behavioral advantage by pre-adolescence on a selective visual attention task.},
  langid = {english},
  keywords = {Age groups,Attention,Children,Deafness,Hearing disorders,Language,Schools,Sign language},
  file = {/Users/xzfang/Zotero/storage/EX6WXDFL/Dye et al. - 2009 - Is Visual Selective Attention in Deaf Individuals .pdf;/Users/xzfang/Zotero/storage/E6JU7RV8/article.html}
}

@article{e_meta_2012,
  title = {A Meta-analysis of Cerebellar Contributions to Higher Cognition from {{PET}} and {{fMRI}} Studies},
  author = {E, Keren-Happuch and Chen, Shen-Hsing Annabel and Ho, Moon-Ho Ringo and Desmond, John E.},
  year = {2012},
  month = nov,
  journal = {Human Brain Mapping},
  volume = {35},
  number = {2},
  pages = {593--615},
  issn = {1065-9471},
  doi = {10.1002/hbm.22194},
  abstract = {A growing interest in cerebellar function and its involvement in higher cognition have prompted much research in recent years. Cerebellar presence in a wide range of cognitive functions examined within an increasing body of neuroimaging literature has been observed. We applied a meta-analytic approach, which employed the activation likelihood estimate method, to consolidate results of cerebellar involvement accumulated in different cognitive tasks of interest and systematically identified similarities among the studies. The current analysis included 88 neuroimaging studies demonstrating cerebellar activations in higher cognitive domains involving emotion, executive function, language, music, timing and working memory. While largely consistent with a prior meta-analysis by Stoodley and Schmahmann (: Neuroimage 44:489-501), our results extended their findings to include music and timing domains to provide further insights into cerebellar involvement and elucidate its role in higher cognition. In addition, we conducted inter- and intradomain comparisons for the cognitive domains of emotion, language, and working memory. We also considered task differences within the domain of verbal working memory by conducting a comparison of the Sternberg with the n-back task, as well as an analysis of the differential components within the Sternberg task. Results showed a consistent cerebellar presence in the timing domain, providing evidence for a role in time keeping. Unique clusters identified within the domain further refine the topographic organization of the cerebellum. Hum Brain Mapp 35:593\textendash 615, 2014. \textcopyright{} 2012 Wiley Periodicals, Inc.},
  pmcid = {PMC3866223},
  pmid = {23125108},
  file = {/Users/xzfang/Zotero/storage/DB7DYEKB/E et al. - 2012 - A metaâ€analysis of cerebellar contributions to hig.pdf}
}

@article{eagleman_motion_2000,
  title = {Motion {{Integration}} and {{Postdiction}} in {{Visual Awareness}}},
  author = {Eagleman, David M. and Sejnowski, Terrence J.},
  year = {2000},
  month = mar,
  journal = {Science},
  volume = {287},
  number = {5460},
  pages = {2036--2038},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.287.5460.2036},
  abstract = {In the flash-lag illusion, a flash and a moving object in the same location appear to be offset. A series of psychophysical experiments yields data inconsistent with two previously proposed explanations: motion extrapolation (a predictive model) and latency difference (an online model). We propose an alternative in which visual awareness is neither predictive nor online but is postdictive, so that the percept attributed to the time of the flash is a function of events that happen in the {$\sim$}80 milliseconds after the flash. The results here show how interpolation of the past is the only framework of the three models that provides a unified explanation for the flash-lag phenomenon.},
  chapter = {Report},
  langid = {english},
  pmid = {10720334},
  file = {/Users/xzfang/Zotero/storage/4KHB4K7Y/Eagleman and Sejnowski - 2000 - Motion Integration and Postdiction in Visual Aware.pdf;/Users/xzfang/Zotero/storage/B6ZMVNBB/2036.html}
}

@article{eagleman_visual_2001,
  title = {Visual Illusions and Neurobiology},
  author = {Eagleman, David M.},
  year = {2001},
  month = dec,
  journal = {Nature Reviews Neuroscience},
  volume = {2},
  number = {12},
  pages = {920--926},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/35104092},
  abstract = {The complex structure of the visual system is sometimes exposed by its illusions. The historical study of systematic misperceptions, combined with a recent explosion of techniques to measure and stimulate neural activity, has provided a rich source for guiding neurobiological frameworks and experiments.},
  copyright = {2001 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews},
  file = {/Users/xzfang/Zotero/storage/8NPWNP83/Eagleman - 2001 - Visual illusions and neurobiology.pdf;/Users/xzfang/Zotero/storage/5BHB2YDE/35104092.html}
}

@article{eben_persisting_2020,
  title = {The Persisting Influence of Unattended Auditory Information: {{Negative}} Priming in Intentional Auditory Attention Switching},
  shorttitle = {The Persisting Influence of Unattended Auditory Information},
  author = {Eben, Charlotte and Koch, Iring and Jolicoeur, Pierre and Nolden, Sophie},
  year = {2020},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {4},
  pages = {1835--1846},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01909-y},
  abstract = {We studied negative priming (NP) in auditory attention switching. In a cued variant of dichotic listening, two spoken number words were presented, one to each ear, one spoken by a female, and one spoken by a male voice. A visual cue indicated whether the male or female voice was the target. A numerical magnitude judgement of the target number was required. The selection criterion could either switch or repeat across trials, so there were attention switch and repetition trials. Two experiments examined NP (distractor becomes target) and also included a ``competitor priming'' (CP) condition (target becomes distractor), relative to a ``no priming'' condition (target and distractor not related to previous trial). In Experiment 1, we investigated the basic priming effects. In Experiment 2, we additionally varied the response-cue interval (RCI; 100 ms vs. 1,900 ms) to examine time-related changes in priming. We found longer response times (RT) for switch trials compared with repetition trials (attention switch costs)\textemdash that is, when the internal processing context changed. In addition, we found longer RT for NP trials as well as reduced switch costs in long RCI, suggesting that previously relevant attentional settings dissipate over longer time. However, NP was not influenced by attention switches, and it was also not affected by~RCI. Hence, NP in auditory attention switching does not seem strongly context or time sensitive.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CV6W9YCM/Eben et al. - 2020 - The persisting influence of unattended auditory in.pdf}
}

@article{ebitz_population_2021,
  title = {The Population Doctrine Revolution in Cognitive Neurophysiology},
  author = {Ebitz, R. Becket and Hayden, Benjamin Y.},
  year = {2021},
  month = mar,
  journal = {arXiv:2104.00145 [q-bio]},
  eprint = {2104.00145},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {A major shift is happening within neurophysiology: a population doctrine is drawing level with the single-neuron doctrine that has long dominated the field. Population-level ideas have so far had their greatest impact in motor neurophysiology, but they hold incredible promise for resolving open questions in cognition. Here, we codify the population doctrine and survey recent work that leverages this view to probe cognition. Our discussion is organized around five core concepts that provide a foundation for population-level thinking: (1) state spaces, (2) manifolds, (3) coding dimensions, (4) subspaces, and (5) dynamics. The work we review illustrates the progress and promise that population neurophysiology holds for cognitive neuroscience\$-\$for delivering new insight into attention, working memory, decision-making, executive function, and learning.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/Users/xzfang/Zotero/storage/C2UEDPBG/Ebitz and Hayden - 2021 - The population doctrine revolution in cognitive ne.pdf;/Users/xzfang/Zotero/storage/K6Z8MH3P/2104.html}
}

@article{eckstein_humans_2017,
  title = {Humans, but {{Not Deep Neural Networks}}, {{Often Miss Giant Targets}} in {{Scenes}}},
  author = {Eckstein, Miguel P. and Koehler, Kathryn and Welbourne, Lauren E. and Akbas, Emre},
  year = {2017},
  month = sep,
  journal = {Current Biology},
  volume = {27},
  number = {18},
  pages = {2827-2832.e3},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2017.07.068},
  abstract = {Even with great advances in machine vision, animals are still unmatched in their ability to visually search complex scenes. Animals from bees [1, 2] to birds [3] to humans [4, 5, 6, 7, 8, 9, 10, 11, 12] learn about the statistical relations in visual environments to guide and aid their search for targets. Here, we investigate a novel manner in which humans utilize rapidly acquired information about scenes by guiding search toward likely target sizes. We show that humans often miss targets when their size is inconsistent with the rest of the scene, even when the targets were made larger and more salient and observers fixated the target. In contrast, we show that state-of-the-art deep neural networks do not exhibit such deficits in finding mis-scaled targets but, unlike humans, can be fooled by target-shaped distractors that are inconsistent with the expected target's size within the scene. Thus, it is not a human deficiency to miss targets when they are inconsistent in size with the scene; instead, it is a byproduct of a useful strategy that the brain has implemented to rapidly discount potential distractors.},
  langid = {english},
  keywords = {computer vision,convolutional neural networks,deep neural networks,guided search,object detection,perception,scene context,search errors,visual attention,visual search},
  file = {/Users/xzfang/Zotero/storage/V96SLWNX/Eckstein et al. - 2017 - Humans, but Not Deep Neural Networks, Often Miss G.pdf;/Users/xzfang/Zotero/storage/7ILVWQGW/S0960982217309727.html}
}

@article{eisner_specificity_2005,
  title = {The Specificity of Perceptual Learning in Speech Processing},
  author = {Eisner, Frank and Mcqueen, James M.},
  year = {2005},
  month = feb,
  journal = {Perception \& Psychophysics},
  volume = {67},
  number = {2},
  pages = {224--238},
  issn = {1532-5962},
  doi = {10.3758/BF03206487},
  abstract = {We conducted four experiments to investigate the specificity of perceptual adjustments made to unusual speech sounds. Dutch listeners heard a female talker produce an ambiguous fricative [?] (between [f] and [s]) in [f]- or [s]-biased lexical contexts. Listeners with [f]-biased exposure (e.g., [witlo?]; fromwitlof, ``chicory'';witlos is meaningless) subsequently categorized more sounds on an [ef]-[es] continuum as [f] than did listeners with [s]-biased exposure. This occurred when the continuum was based on the exposure talker's speech (Experiment 1), and when the same test fricatives appeared after vowels spoken by novel female and male talkers (Experiments 1 and 2). When the continuum was made entirely from a novel talker's speech, there was no exposure effect (Experiment 3) unless fricatives from that talker had been spliced into the exposure talker's speech during exposure (Experiment 4). We conclude that perceptual learning about idiosyncratic speech is applied at a segmental level and is, under these exposure conditions, talker specific.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TL9K5GDN/Eisner and Mcqueen - 2005 - The specificity of perceptual learning in speech p.pdf}
}

@article{ekman_timecompressed_2017,
  title = {Time-Compressed Preplay of Anticipated Events in Human Primary Visual Cortex},
  author = {Ekman, Matthias and Kok, Peter and {de Lange}, Floris P.},
  year = {2017},
  month = may,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {15276},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/ncomms15276},
  abstract = {Perception is guided by the anticipation of future events. It has been hypothesized that this process may be implemented by pattern completion in early visual cortex, in which a stimulus sequence is recreated after only a subset of the visual input is provided. Here we test this hypothesis using ultra-fast functional magnetic resonance imaging to measure BOLD activity at precisely defined receptive field locations in visual cortex (V1) of human volunteers. We find that after familiarizing subjects with a spatial sequence, flashing only the starting point of the sequence triggers an activity wave in V1 that resembles the full stimulus sequence. This preplay activity is temporally compressed compared to the actual stimulus sequence and remains present even when attention is diverted from the stimulus sequence. Preplay might therefore constitute an automatic prediction mechanism for temporal sequences in V1.},
  copyright = {2017 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YERZSZMV/Ekman et al. - 2017 - Time-compressed preplay of anticipated events in h.pdf;/Users/xzfang/Zotero/storage/W28TWDW2/ncomms15276.html}
}

@article{el-ghorab_comparative_2010,
  title = {A {{Comparative Study}} on {{Chemical Composition}} and {{Antioxidant Activity}} of {{Ginger}} ({{Zingiber}} Officinale) and {{Cumin}} ({{Cuminum}} Cyminum)},
  author = {{El-Ghorab}, Ahmed Hassan and Nauman, Muhammad and Anjum, Faqir Muhammad and Hussain, Shahzad and Nadeem, Muhammad},
  year = {2010},
  month = jul,
  journal = {Journal of Agricultural and Food Chemistry},
  volume = {58},
  number = {14},
  pages = {8231--8237},
  issn = {0021-8561},
  doi = {10.1021/jf101202x}
}

@article{el-sawi_cumin_2002,
  title = {Cumin Herb as a New Source of Essential Oils and Its Response to Foliar Spray with Some Micro-Elements},
  author = {{El-Sawi}, Salma A and Mohamed, M. A},
  year = {2002},
  month = may,
  journal = {Food Chemistry},
  volume = {77},
  number = {1},
  pages = {75--80},
  issn = {0308-8146},
  doi = {10.1016/S0308-8146(01)00326-0},
  abstract = {The effects of 50 mg/l levels of micronutrients (Zn and Mn), as single and combined treatments, on the growth, oil yield and oil constituents of cumin plants, were studied. Application of micronutrients had significant positive effects, in most cases, on growth measurements and chemical composition of cumin plants. A combined treatment of the two micronutrients gave the highest values in this respect. In the herb and seed oils, 21 constituents were identified, representing 90.2 and 95.6\% of the total amounts, respectively. Eleven components were similar in both herb and seed oils. Cumin aldehyde was found as the main component at concentrations of 53.6\% for seed oil and 40.5\% for herb oil. Among the new identified components in the seed oil were perilla aldehyde, {$\alpha$}-cis bergamotene, acoradiene and benzoic acid 4-(1-methylethyl). These components were found in the herb oil, as well. The oils of herb and seeds of cumin contained considerable amounts of oxygenated monoterpenes. Both oils were characterized by small amounts of monoterpenoid and sesquiterpene hydrocarbons. Qualitative and quantitative data indicated that oil production from cumin herb is a possibility.},
  keywords = {Cumin (Cuminum cyminum),Essential oil,GCâ€“MS,Growth,Herb,Micronutrients,Mn,Seeds,Zn},
  file = {/Users/xzfang/Zotero/storage/M92IQNCF/El-Sawi and Mohamed - 2002 - Cumin herb as a new source of essential oils and i.html}
}

@article{eldridge_remembering_2000,
  title = {Remembering Episodes: A Selective Role for the Hippocampus during Retrieval},
  shorttitle = {Remembering Episodes},
  author = {Eldridge, Laura L. and Knowlton, Barbara J. and Furmanski, Christopher S. and Bookheimer, Susan Y. and Engel, Stephen A.},
  year = {2000},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {3},
  number = {11},
  pages = {1149--1152},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/80671},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CVXCI7XJ/Eldridge et al. - 2000 - Remembering episodes a selective role for the hip.pdf}
}

@article{elliott_acoustic_2013,
  title = {Acoustic Structure of the Five Perceptual Dimensions of Timbre in Orchestral Instrument Tones},
  author = {Elliott, Taffeta M. and Hamilton, Liberty S. and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {2013},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {133},
  number = {1},
  pages = {389--404},
  issn = {0001-4966},
  doi = {10.1121/1.4770244},
  abstract = {Attempts to relate the perceptual dimensions of timbre to quantitative acoustical dimensions have been tenuous, leading to claims that timbre is an emergent property, if measurable at all. Here, a three-pronged analysis shows that the timbre space of sustained instrument tones occupies 5 dimensions and that a specific combination of acoustic properties uniquely determines gestalt perception of timbre. Firstly, multidimensional scaling (MDS) of dissimilarity judgments generated a perceptual timbre space in which 5 dimensions were cross-validated and selected by traditional model comparisons. Secondly, subjects rated tones on semantic scales. A discriminant function analysis (DFA) accounting for variance of these semantic ratings across instruments and between subjects also yielded 5 significant dimensions with similar stimulus ordination. The dimensions of timbre space were then interpreted semantically by rotational and reflectional projection of the MDS solution into two DFA dimensions. Thirdly, to relate this final space to acoustical structure, the perceptual MDS coordinates of each sound were regressed with its joint spectrotemporal modulation power spectrum. Sound structures correlated significantly with distances in perceptual timbre space. Contrary to previous studies, most perceptual timbre dimensions are not the result of purely temporal or spectral features but instead depend on signature spectrotemporal patterns.},
  pmcid = {PMC3548835},
  pmid = {23297911},
  file = {/Users/xzfang/Zotero/storage/QSBTZXBD/Elliott et al. - 2013 - Acoustic structure of the five perceptual dimensio.pdf}
}

@article{elliott_modulation_2009,
  title = {The {{Modulation Transfer Function}} for {{Speech Intelligibility}}},
  author = {Elliott, Taffeta M. and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {2009},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {5},
  number = {3},
  pages = {e1000302},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000302},
  abstract = {We systematically determined which spectrotemporal modulations in speech are necessary for comprehension by human listeners. Speech comprehension has been shown to be robust to spectral and temporal degradations, but the specific relevance of particular degradations is arguable due to the complexity of the joint spectral and temporal information in the speech signal. We applied a novel modulation filtering technique to recorded sentences to restrict acoustic information quantitatively and to obtain a joint spectrotemporal modulation transfer function for speech comprehension, the speech MTF. For American English, the speech MTF showed the criticality of low modulation frequencies in both time and frequency. Comprehension was significantly impaired when temporal modulations {$<$}12 Hz or spectral modulations {$<$}4 cycles/kHz were removed. More specifically, the MTF was bandpass in temporal modulations and low-pass in spectral modulations: temporal modulations from 1 to 7 Hz and spectral modulations {$<$}1 cycles/kHz were the most important. We evaluated the importance of spectrotemporal modulations for vocal gender identification and found a different region of interest: removing spectral modulations between 3 and 7 cycles/kHz significantly increases gender misidentifications of female speakers. The determination of the speech MTF furnishes an additional method for producing speech signals with reduced bandwidth but high intelligibility. Such compression could be used for audio applications such as file compression or noise removal and for clinical applications such as signal processing for cochlear implants.},
  langid = {english},
  keywords = {Acoustic signals,Audio signal processing,Bioacoustics,Modulation,Signal bandwidth,Signal filtering,Speech,Speech signal processing},
  file = {/Users/xzfang/Zotero/storage/2DW2SDZB/Elliott and Theunissen - 2009 - The Modulation Transfer Function for Speech Intell.pdf;/Users/xzfang/Zotero/storage/U4A7DD49/article.html}
}

@article{elman_meaning_2009,
  title = {On the {{Meaning}} of {{Words}} and {{Dinosaur Bones}}: {{Lexical Knowledge Without}} a {{Lexicon}}},
  shorttitle = {On the {{Meaning}} of {{Words}} and {{Dinosaur Bones}}},
  author = {Elman, Jeffrey L.},
  year = {2009},
  journal = {Cognitive Science},
  volume = {33},
  number = {4},
  pages = {547--582},
  issn = {1551-6709},
  doi = {10.1111/j.1551-6709.2009.01023.x},
  abstract = {Although for many years a sharp distinction has been made in language research between rules and words\textemdash with primary interest on rules\textemdash this distinction is now blurred in many theories. If anything, the focus of attention has shifted in recent years in favor of words. Results from many different areas of language research suggest that the lexicon is representationally rich, that it is the source of much productive behavior, and that lexically specific information plays a critical and early role in the interpretation of grammatical structure. But how much information can or should be placed in the lexicon? This is the question I address here. I review a set of studies whose results indicate that event knowledge plays a significant role in early stages of sentence processing and structural analysis. This poses a conundrum for traditional views of the lexicon. Either the lexicon must be expanded to include factors that do not plausibly seem to belong there; or else virtually all information about word meaning is removed, leaving the lexicon impoverished. I suggest a third alternative, which provides a way to account for lexical knowledge without a mental lexicon.},
  langid = {english},
  keywords = {Ambiguity resolution,Dynamical systems,Lexical representation,Sentence processing,Simple recurrent network},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6709.2009.01023.x},
  file = {/Users/xzfang/Zotero/storage/GDFLFZGA/Elman - 2009 - On the Meaning of Words and Dinosaur Bones Lexica.pdf;/Users/xzfang/Zotero/storage/AQDG9XH3/j.1551-6709.2009.01023.html}
}

@article{elmer_increased_2013,
  title = {Increased Cortical Surface Area of the Left Planum Temporale in Musicians Facilitates the Categorization of Phonetic and Temporal Speech Sounds},
  author = {Elmer, Stefan and H{\"a}nggi, J{\"u}rgen and Meyer, Martin and J{\"a}ncke, Lutz},
  year = {2013},
  month = nov,
  journal = {Cortex},
  volume = {49},
  number = {10},
  pages = {2812--2821},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2013.03.007},
  abstract = {We measured musicians and non-musicians by using structural magnetic resonance imaging to investigate relationships between cortical features of the left planum temporale (PT) and the categorization of consonant-vowel (CV) syllables and their reduced-spectrum analogues. The present work is based on previous functional studies consistently showing that the left PT is particularly responsive to transient acoustic features in CV syllables and their reduced-spectrum analogues, and on striking evidence pointing to structural alterations of the left PT as a function of musicianship. By combining these two observations, we hypothesized to find that differences in cortical surface area (SA) and cortical thickness (CT) of the left PT in musicians may facilitate the categorization of fast-changing phonetic cues. Behavioural results indicated that musicians and non-musicians achieved a comparable performance in the categorization of CV syllables, whereas the musicians performed significantly better than the controls in the more demanding reduced-spectrum condition. This better behavioural performance corresponds to an increased cortical SA of the left PT in musicians compared to non-musicians. No differences in CT of the left PT were found between groups. In line with our predictions, we revealed a positive correlation between cortical SA of the left PT in musicians and the behavioural performance during the acoustically more demanding reduced-spectrum condition. Hence, we provide first evidence for a relationship between musical expertise, cortical SA of the left PT, and the processing of fast-changing phonetic cues.},
  langid = {english},
  keywords = {Cortical surface area,Musicianship,Phonetic and temporal processing,Planum temporale,Structural MRI,Transfer effects,Voice-onset time},
  file = {/Users/xzfang/Zotero/storage/MJ8NQ2KY/S0010945213000907.html}
}

@article{epstein_cognitive_2017,
  title = {The Cognitive Map in Humans: {{Spatial}} Navigation and Beyond},
  shorttitle = {The Cognitive Map in Humans},
  author = {Epstein, Russell A. and Patai, Eva Zita and Julian, Joshua B. and Spiers, Hugo J.},
  year = {2017},
  month = oct,
  journal = {Nature neuroscience},
  volume = {20},
  number = {11},
  pages = {1504--1513},
  issn = {1097-6256},
  doi = {10.1038/nn.4656},
  abstract = {The `cognitive map' hypothesis proposes that brain builds a unified representation of the spatial environment to support memory and guide future action. Forty years of electrophysiological research in rodents suggests that cognitive maps are neurally instantiated by place, grid, border, and head direction cells in the hippocampal formation and related structures. Here we review recent work that suggests a similar functional organization in the human brain and reveals novel insights into how cognitive maps are used during spatial navigation. Specifically, these studies indicate that: (i) the human hippocampus and entorhinal cortex support map-like spatial codes; (ii) posterior brain regions such as parahippocampal and retrosplenial cortices provide critical inputs that allow cognitive maps to be anchored to fixed environmental landmarks; (iii) hippocampal and entorhinal spatial codes are used in conjunction with frontal lobe mechanisms to plan routes during navigation. We also discuss how these three basic elements of cognitive map based navigation spatial coding, landmark anchoring, and route planning might be applied to non-spatial domains to provide the building blocks for many core elements of human thought.},
  pmcid = {PMC6028313},
  pmid = {29073650},
  file = {/Users/xzfang/Zotero/storage/CZCDIUIM/Epstein et al. - 2017 - The cognitive map in humans Spatial navigation an.pdf}
}

@article{erhan_visualizing_,
  title = {Visualizing {{Higher-Layer Features}} of a {{Deep Network}}},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal and Box, P O},
  pages = {14},
  abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IF3WWJHL/Erhan et al. - Visualizing Higher-Layer Features of a Deep Networ.pdf}
}

@article{espana-boquera_improving_2011,
  title = {Improving {{Offline Handwritten Text Recognition}} with {{Hybrid HMM}}/{{ANN Models}}},
  author = {{Espa{\~n}a-Boquera}, S. and {Castro-Bleda}, M.J. and {Gorbe-Moya}, J. and {Zamora-Martinez}, F.},
  year = {2011},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {4},
  pages = {767--779},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2010.141},
  abstract = {This paper proposes the use of hybrid Hidden Markov Model (HMM)/Artificial Neural Network (ANN) models for recognizing unconstrained offline handwritten texts. The structural part of the optical models has been modeled with Markov chains, and a Multilayer Perceptron is used to estimate the emission probabilities. This paper also presents new techniques to remove slope and slant from handwritten text and to normalize the size of text images with supervised learning methods. Slope correction and size normalization are achieved by classifying local extrema of text contours with Multilayer Perceptrons. Slant is also removed in a nonuniform way by using Artificial Neural Networks. Experiments have been conducted on offline handwritten text lines from the IAM database, and the recognition rates achieved, in comparison to the ones reported in the literature, are among the best for the same task.},
  keywords = {Artificial neural networks,Handwriting recognition,Hidden Markov models,HMM,hybrid HMM/ANN,image normalization.,Image segmentation,Markov processes,multilayer perceptron,neural networks,offline handwriting,Pixel,Text recognition},
  file = {/Users/xzfang/Zotero/storage/MFFS52XH/EspaÃ±a-Boquera et al. - 2011 - Improving Offline Handwritten Text Recognition wit.pdf;/Users/xzfang/Zotero/storage/46JIU39K/5551147.html}
}

@article{etard_neural_2019,
  title = {Neural {{Speech Tracking}} in the {{Theta}} and in the {{Delta Frequency Band Differentially Encode Clarity}} and {{Comprehension}} of {{Speech}} in {{Noise}}},
  author = {Etard, Octave and Reichenbach, Tobias},
  year = {2019},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {29},
  pages = {5750--5759},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1828-18.2019},
  abstract = {Humans excel at understanding speech even in adverse conditions such as background noise. Speech processing may be aided by cortical activity in the delta and theta frequency bands, which have been found to track the speech envelope. However, the rhythm of non-speech sounds is tracked by cortical activity as well. It therefore remains unclear which aspects of neural speech tracking represent the processing of acoustic features, related to the clarity of speech, and which aspects reflect higher-level linguistic processing related to speech comprehension. Here we disambiguate the roles of cortical tracking for speech clarity and comprehension through recording EEG responses to native and foreign language in different levels of background noise, for which clarity and comprehension vary independently. We then use a both a decoding and an encoding approach to relate clarity and comprehension to the neural responses. We find that cortical tracking in the theta frequency band is mainly correlated to clarity, whereas the delta band contributes most to speech comprehension. Moreover, we uncover an early neural component in the delta band that informs on comprehension and that may reflect a predictive mechanism for language processing. Our results disentangle the functional contributions of cortical speech tracking in the delta and theta bands to speech processing. They also show that both speech clarity and comprehension can be accurately decoded from relatively short segments of EEG recordings, which may have applications in future mind-controlled auditory prosthesis. SIGNIFICANCE STATEMENT Speech is a highly complex signal whose processing requires analysis from lower-level acoustic features to higher-level linguistic information. Recent work has shown that neural activity in the delta and theta frequency bands track the rhythm of speech, but the role of this tracking for speech processing remains unclear. Here we disentangle the roles of cortical entrainment in different frequency bands and at different temporal lags for speech clarity, reflecting the acoustics of the signal, and speech comprehension, related to linguistic processing. We show that cortical speech tracking in the theta frequency band encodes mostly speech clarity, and thus acoustic aspects of the signal, whereas speech tracking in the delta band encodes the higher-level speech comprehension.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2019 Etard and Reichenbach. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {31109963},
  keywords = {envelope tracking,neural oscillations,speech comprehension,speech processing},
  file = {/Users/xzfang/Zotero/storage/P4HSJAT9/Etard and Reichenbach - 2019 - Neural Speech Tracking in the Theta and in the Del.pdf;/Users/xzfang/Zotero/storage/VQGDKKPK/5750.html}
}

@article{ettinger_what_2020,
  title = {What {{BERT Is Not}}: {{Lessons}} from a {{New Suite}} of {{Psycholinguistic Diagnostics}} for {{Language Models}}},
  shorttitle = {What {{BERT Is Not}}},
  author = {Ettinger, Allyson},
  year = {2020},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {34--48},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00298},
  abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\textemdash{} and, in particular, it shows clear insensitivity to the contextual impacts of negation.},
  file = {/Users/xzfang/Zotero/storage/GMPS8E5L/Ettinger - 2020 - What BERT Is Not Lessons from a New Suite of Psyc.pdf;/Users/xzfang/Zotero/storage/PC2F87HF/What-BERT-Is-Not-Lessons-from-a-New-Suite-of.html}
}

@article{etzel_searchlight_2013,
  title = {Searchlight Analysis: Promise, Pitfalls, and Potential},
  shorttitle = {Searchlight Analysis},
  author = {Etzel, Joset A. and Zacks, Jeffrey M. and Braver, Todd S.},
  year = {2013},
  month = sep,
  journal = {NeuroImage},
  volume = {78},
  pages = {261--269},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2013.03.041},
  abstract = {Multivariate pattern analysis (MVPA) is an increasingly popular approach for characterizing the information present in neural activity as measured by fMRI. For neuroimaging researchers, the searchlight technique serves as the most intuitively appealing means of implementing MVPA with fMRI data. However, searchlight approaches carry with them a number of special concerns and limitations that can lead to serious interpretation errors in practice, such as misidentifying a cluster as informative, or failing to detect truly informative voxels. Here we describe how such distorted results can occur, using both schematic illustrations and examples from actual fMRI datasets. We recommend that confirmatory and sensitivity tests, such as the ones prescribed here, should be considered a necessary stage of searchlight analysis interpretation, and that their adoption will allow the full potential of searchlight analysis to be realized.},
  pmcid = {PMC3988828},
  pmid = {23558106},
  file = {/Users/xzfang/Zotero/storage/R6D2NCAP/Etzel et al. - 2013 - Searchlight analysis promise, pitfalls, and poten.pdf}
}

@article{evans_getting_2015,
  title = {Getting the {{Cocktail Party Started}}: {{Masking Effects}} in {{Speech Perception}}},
  shorttitle = {Getting the {{Cocktail Party Started}}},
  author = {Evans, Samuel and McGettigan, Carolyn and Agnew, Zarinah K. and Rosen, Stuart and Scott, Sophie K.},
  year = {2015},
  month = dec,
  journal = {Journal of Cognitive Neuroscience},
  volume = {28},
  number = {3},
  pages = {483--500},
  publisher = {{MIT Press}},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_00913},
  abstract = {Spoken conversations typically take place in noisy environments, and different kinds of masking sounds place differing demands on cognitive resources. Previous studies, examining the modulation of neural activity associated with the properties of competing sounds, have shown that additional speech streams engage the superior temporal gyrus. However, the absence of a condition in which target speech was heard without additional masking made it difficult to identify brain networks specific to masking and to ascertain the extent to which competing speech was processed equivalently to target speech. In this study, we scanned young healthy adults with continuous fMRI, while they listened to stories masked by sounds that differed in their similarity to speech. We show that auditory attention and control networks are activated during attentive listening to masked speech in the absence of an overt behavioral task. We demonstrate that competing speech is processed predominantly in the left hemisphere within the same pathway as target speech but is not treated equivalently within that stream and that individuals who perform better in speech in noise tasks activate the left mid-posterior superior temporal gyrus more. Finally, we identify neural responses associated with the onset of sounds in the auditory environment; activity was found within right lateralized frontal regions consistent with a phasic alerting response. Taken together, these results provide a comprehensive account of the neural processes involved in listening in noise.},
  file = {/Users/xzfang/Zotero/storage/FIW9LL8M/Evans et al. - 2015 - Getting the Cocktail Party Started Masking Effect.pdf;/Users/xzfang/Zotero/storage/4QVLG2NJ/jocn_a_00913.html}
}

@article{eymond_featurebased_2019,
  title = {Feature-Based Attention across Saccades: {{Pop-out}} in Color Search Is Spatiotopic},
  shorttitle = {Feature-Based Attention across Saccades},
  author = {Eymond, C{\'e}cile and Cavanagh, Patrick and Collins, Th{\'e}r{\`e}se},
  year = {2019},
  month = jan,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {1},
  pages = {85--97},
  issn = {1943-393X},
  doi = {10.3758/s13414-018-1597-5},
  abstract = {Our perception of the world remains stable despite the retinal shifts that occur with each saccade. The role of spatial attention in matching pre- to postsaccadic visual information has been well established, but the role of feature-based attention remains unclear. In this study, we examined the transsaccadic processing of a color pop-out target. Participants made a saccade towards a neutral target and performed a search task on a peripheral array presented once the saccade landed. A similar array was presented just before the saccade and we analyzed what aspect of this preview benefitted the postsaccadic search task. We assessed the preview effect in the spatiotopic and retinotopic reference frames, and the potential transfer of feature selectivity across the saccade. In the first experiment, the target and distractor colors remained identical for the preview and the postsaccadic array and performance improved. The largest benefit was observed at the spatiotopic location. In the second experiment, the target and distractor colors were swapped across the saccade. All responses were slowed but the cost was least at the spatiotopic location. Our results show that the preview attracted spatial attention to the target location, which was then remapped, and suggest that previewed features, specifically colors, were transferred across the saccade. Furthermore, the preview induced a spatiotopic advantage regardless of whether the target switched color or not, suggesting that spatiotopy was established independently of feature processing. Our results support independent priming effects of features versus location and underline the role of feature-based selection in visual stability.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LVULHPG7/Eymond et al. - 2019 - Feature-based attention across saccades Pop-out i.pdf}
}

@article{failor_learning_2021,
  title = {Learning Orthogonalizes Visual Cortical Population Codes},
  author = {Failor, Samuel W. and Carandini, Matteo and Harris, Kenneth D.},
  year = {2021},
  month = may,
  journal = {bioRxiv},
  pages = {2021.05.23.445338},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.05.23.445338},
  abstract = {{$<$}p{$>$}The response of a neuronal population to a stimulus can be summarized by a vector in a high-dimensional space. Learning theory suggests that the brain should be most able to produce distinct behavioral responses to two stimuli when the rate vectors they evoke are close to orthogonal. To investigate how learning modifies population codes, we measured the orientation tuning of 4,000-neuron populations in visual cortex before and after training on a visual discrimination task. Learning suppressed responses to the task-informative stimuli, most strongly amongst weakly-tuned neurons. This suppression reflected a simple change at the population level: sparsening of population responses to relevant stimuli, resulting in orthogonalization of their rate vectors. A model of F-I curve modulation, requiring no synaptic plasticity, quantitatively predicted the learning effect.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/838Y3FIJ/Failor et al. - 2021 - Learning orthogonalizes visual cortical population.pdf;/Users/xzfang/Zotero/storage/3YZ3W6P8/2021.05.23.html}
}

@misc{falandays_emergence_2021,
  title = {The {{Emergence}} of {{Cultural Attractors}}: {{How Dynamic Populations}} of {{Learners Achieve Collective Cognitive Alignment}}},
  shorttitle = {The {{Emergence}} of {{Cultural Attractors}}},
  author = {Falandays, J. Benjamin and Smaldino, Paul E.},
  year = {2021},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/v5fsr},
  abstract = {Cultural attractor landscapes describe the time-evolution of cultural variants over successive transmission events. Because these landscapes are statistical patterns that emerge from the interactions of dynamic learners in dynamic populations/environments, stable landscapes cannot be taken for granted. However, they are often modeled as such, and little is known about how attractors form, change, and/or stabilize. We present a model of cultural attractor dynamics, which adapts a model of unsupervised category learning in individuals to a multi-agent setting, wherein learners provide the training input to each other. Agents in our populations spontaneously align their cognitive category structures, producing emergent cultural attractor points. We highlight three interesting behaviors exhibited by our model: (1) noise enhances stability of cultural category structures; (2) short ``critical'' periods of learning early in life enhance stability; and (3) larger populations produce more stable but less complex attractor landscapes, and cliquish network structure can mitigate the latter.},
  keywords = {agent-based modeling,Animal Learning and Behavior,categorization,cultural attraction,cultural evolution,Perception,Perceptual Organization,Social and Behavioral Sciences,symbolic cognition},
  file = {/Users/xzfang/Zotero/storage/E34DHNQM/Falandays and Smaldino - 2021 - The Emergence of Cultural Attractors How Dynamic .pdf}
}

@article{farbood_neural_2015,
  title = {The Neural Processing of Hierarchical Structure in Music and Speech at Different Timescales},
  author = {Farbood, Morwaread M. and Heeger, David J. and Marcus, Gary and Hasson, Uri and Lerner, Yulia},
  year = {2015},
  journal = {Frontiers in Neuroscience},
  volume = {9},
  issn = {1662-453X},
  doi = {10.3389/fnins.2015.00157},
  abstract = {Music, like speech, is a complex auditory signal that contains structures at multiple timescales, and as such a potentially powerful entry point into the question of how the brain integrates complex streams of information. Using an experimental design modeled after previous studies that used scrambled versions of a spoken story (Lerner, Honey, Silbert, \& Hasson, 2011) and a silent movie (Hasson, Yang, Vallines, Heeger, \& Rubin, 2008), we investigate whether listeners perceive hierarchical structure in music beyond short (\textasciitilde 6 sec) time windows and whether there is cortical overlap between music and language processing at multiple timescales. Experienced pianists were presented with an extended musical excerpt scrambled at multiple timescales\textendash\textendash by measure, phrase, and section\textendash\textendash while measuring brain activity with functional magnetic resonance imaging (fMRI). The reliability of evoked activity, as quantified by inter-subject correlation of the fMRI responses was measured. We found that response reliability depended systematically on musical structural coherence, revealing a topographically organized hierarchy of processing timescales. Early auditory areas (at the bottom of the hierarchy) responded reliably in all conditions. For brain areas at the top of the hierarchy, the original (unscrambled) excerpt evoked more reliable responses than any of the scrambled excerpts, indicating that these brain areas process long-timescale musical structures, on the order of minutes. The topography of processing timescales was analogous with that reported previously for speech, but the timescale gradients for music and speech overlapped with one another only partially, suggesting that temporally analogous structures\textendash\textendash words/measures, sentences/musical phrases, paragraph/sections\textendash\textendash are processed separately.},
  langid = {english},
  keywords = {fMRI,hierarchical structure,Music,processing timescales,Speech},
  file = {/Users/xzfang/Zotero/storage/N7G4V58L/Farbood et al. - 2015 - The neural processing of hierarchical structure in.pdf}
}

@article{farmer_phonological_2006,
  title = {Phonological Typicality Influences On-Line Sentence Comprehension},
  author = {Farmer, Thomas A. and Christiansen, Morten H. and Monaghan, Padraic},
  year = {2006},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {103},
  number = {32},
  pages = {12203--12208},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0602173103},
  abstract = {Since Saussure, the relationship between the sound and the meaning of words has been regarded as largely arbitrary. Here, however, we show that a probabilistic relationship exists between the sound of a word and its lexical category. Corpus analyses of nouns and verbs indicate that the phonological properties of the individual words in these two lexical categories form relatively separate and coherent clusters, with some nouns sounding more typical of the noun category than others and likewise for verbs. Additional analyses reveal that the phonological properties of nouns and verbs affect lexical access, and we also demonstrate the influence of such properties during the on-line processing of both simple unambiguous and syntactically ambiguous sentences. Thus, although the sound of a word may not provide cues to its specific meaning, phonological typicality, the degree to which the sound properties of an individual word are typical of other words in its lexical category, affects both word- and sentence-level language processing. The findings are consistent with a perspective on language comprehension in which sensitivity to multiple syntactic constraints in adulthood emerges as a product of language-development processes that are driven by the integration of multiple cues to linguistic structure, including phonological typicality.},
  chapter = {Social Sciences},
  copyright = {\textcopyright{} 2006 by The National Academy of Sciences of the USA},
  langid = {english},
  pmid = {16882728},
  file = {/Users/xzfang/Zotero/storage/R7NWT4VZ/Farmer et al. - 2006 - Phonological typicality influences on-line sentenc.pdf;/Users/xzfang/Zotero/storage/IANFALYL/12203.html}
}

@article{farris-trimble_testretest_2013,
  title = {Test-Retest Reliability of Eye Tracking in the Visual World Paradigm for the Study of Real-Time Spoken Word Recognition},
  author = {{Farris-Trimble}, Ashley and McMurray, Bob},
  year = {2013},
  month = aug,
  journal = {Journal of speech, language, and hearing research : JSLHR},
  volume = {56},
  number = {4},
  pages = {10.1044/1092-4388(2012/12-0145)},
  issn = {1092-4388},
  doi = {10.1044/1092-4388(2012/12-0145)},
  abstract = {Purpose Researchers have begun to use eye tracking in the visual world paradigm (VWP) to study clinical differences in language processing, but the reliability of such laboratory tests has rarely been assessed. This paper assesses test-retest reliability of the VWP for spoken word recognition. Methods Participants performed an auditory VWP task in repeated sessions and a visual-only VWP task in a third session. We performed correlation and regression analyses on several parameters to determine which reflect reliable behavior and which are predictive of behavior in later sessions. Results Results showed that the fixation parameters most closely related to timing and degree of fixations were moderately-to-strongly correlated across days, while the parameters related to rate of increase or decrease of fixations to particular items were less strongly correlated. Moreover, when including factors derived from the visual-only task, the performance of the regression model was at least moderately correlated with Day 2 performance on all parameters (R {$>$} .30). Conclusions The VWP is stable enough (with some caveats) to serve as an individual measure. These findings suggest guidelines for future use of the paradigm and for areas of improvement in both methodology and analysis.},
  pmcid = {PMC3875834},
  pmid = {23926331},
  file = {/Users/xzfang/Zotero/storage/J6QWPGVU/Farris-Trimble and McMurray - 2013 - Test-retest reliability of eye tracking in the vis.pdf}
}

@article{feather_metamers_2019,
  title = {Metamers of Neural Networks Reveal Divergence from Human Perceptual Systems},
  author = {Feather, Jenelle and Durango, Alex and Gonzalez, Ray and McDermott, Josh},
  year = {2019},
  pages = {25},
  abstract = {Deep neural networks have been embraced as models of sensory systems, instantiating representational transformations that appear to resemble those in the visual and auditory systems. To more thoroughly investigate their similarity to biological systems, we synthesized model metamers \textendash{} stimuli that produce the same responses at some stage of a network's representation. We generated model metamers for natural stimuli by performing gradient descent on a noise signal, matching the responses of individual layers of image and audio networks to a natural image or speech signal. The resulting signals reflect the invariances instantiated in the network up to the matched layer. We then measured whether model metamers were recognizable to human observers \textendash a necessary condition for the model representations to replicate those of humans. Although model metamers from early network layers were recognizable to humans, those from deeper layers were not. Auditory model metamers became more human-recognizable with architectural modifications that reduced aliasing from pooling operations, but those from the deepest layers remained unrecognizable. We also used the metamer test to compare model representations. Cross-model metamer recognition dropped off for deeper layers, roughly at the same point that human recognition deteriorated, indicating divergence across model representations. The results reveal discrepancies between model and human representations, but also show how metamers can help guide model refinement and elucidate model representations.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VU3JME4G/Feather et al. - Metamers of neural networks reveal divergence from.pdf}
}

@article{federmeier_multiple_2007,
  title = {Multiple Effects of Sentential Constraint on Word Processing},
  author = {Federmeier, Kara D. and Wlotko, Edward W. and {De Ochoa-Dewald}, Esmeralda and Kutas, Marta},
  year = {2007},
  month = may,
  journal = {Brain research},
  volume = {1146},
  pages = {75--84},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2006.06.101},
  abstract = {Behavioral and electrophysiological studies have uncovered different patterns of constraint effects on the processing of words in sentences. Whereas response time measures have indicated a reduced scope of facilitation from strongly constraining contexts, event-related brain potential (ERP) measures have instead revealed enhanced facilitation for semantically related endings in such sentences. Given this disparity, and the concomitant possibility of functionally separable stages of context effects, the current study jointly examined expectancy (cloze probability) and constraint effects on the ERP response to words. Expected and unexpected (but plausible) words completed strongly and weakly constraining sentences; unexpected items were matched for contextual fit across the two levels of constraint and were semantically unrelated to the most expected endings. N400 amplitudes were graded by expectancy but unaffected by constraint and seemed to index the benefit of contextual information. However, a later effect, in the form of increased frontal positivity from 500 to 900 ms post-stimulus-onset, indicated a possible cost associated with the processing of unexpected words in strongly constraining contexts.},
  pmcid = {PMC2704150},
  pmid = {16901469},
  file = {/Users/xzfang/Zotero/storage/GE25BB3S/Federmeier et al. - 2007 - Multiple effects of sentential constraint on word .pdf}
}

@article{fedorenko_broad_2013,
  title = {Broad Domain Generality in Focal Regions of Frontal and Parietal Cortex},
  author = {Fedorenko, Evelina and Duncan, John and Kanwisher, Nancy},
  year = {2013},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {41},
  pages = {16616--16621},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1315235110},
  abstract = {Unlike brain regions that respond selectively to specific kinds of information content, a number of frontal and parietal regions are thought to be domain- and process-general: that is, active during a wide variety of demanding cognitive tasks. However, most previous evidence for this functional generality in humans comes from methods that overestimate activation overlap across tasks. Here we present functional MRI evidence from single-subject analyses for broad functional generality of a specific set of brain regions: the same sets of voxels are engaged across tasks ranging from arithmetic to storing information in working memory, to inhibiting irrelevant information. These regions have a specific topography, often lying directly adjacent to domain-specific regions. Thus, in addition to domain-specific brain regions tailored to solve particular problems of longstanding importance to our species, the human brain also contains a set of functionally general regions that plausibly endow us with the cognitive flexibility necessary to solve novel problems.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  isbn = {9781315235110},
  langid = {english},
  pmid = {24062451},
  keywords = {cognitive control,Multiple-demand system},
  file = {/Users/xzfang/Zotero/storage/78TWHC88/Fedorenko et al. - 2013 - Broad domain generality in focal regions of fronta.pdf;/Users/xzfang/Zotero/storage/UTFLEADF/16616.html}
}

@article{fedorenko_broca_2020,
  title = {Broca's {{Area Is Not}} a {{Natural Kind}}},
  author = {Fedorenko, Evelina and Blank, Idan A.},
  year = {2020},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {4},
  pages = {270--284},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2020.01.001},
  langid = {english},
  pmid = {32160565},
  keywords = {articulation,Brocaâ€™s area,domain-specificity,executive functions,language,LIFG},
  file = {/Users/xzfang/Zotero/storage/F2RM6Z9T/S1364-6613(20)30003-6.html}
}

@article{fedorenko_functional_2011,
  title = {Functional Specificity for High-Level Linguistic Processing in the Human Brain},
  author = {Fedorenko, E. and Behr, M. K. and Kanwisher, N.},
  year = {2011},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {39},
  pages = {16428--16433},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1112937108},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7IZFXDSG/Fedorenko et al. - 2011 - Functional specificity for high-level linguistic p.pdf}
}

@article{fedorenko_lack_2020,
  title = {Lack of Selectivity for Syntax Relative to Word Meanings throughout the Language Network},
  author = {Fedorenko, Evelina and Blank, Idan Asher and Siegelman, Matthew and Mineroff, Zachary},
  year = {2020},
  month = oct,
  journal = {Cognition},
  volume = {203},
  pages = {104348},
  issn = {00100277},
  doi = {10.1016/j.cognition.2020.104348},
  abstract = {To understand what you are reading now, your mind retrieves the meanings of words and constructions from a linguistic knowledge store (lexico-semantic processing) and identifies the relationships among them to construct a complex meaning (syntactic or combinatorial processing). Do these two sets of processes rely on distinct, specialized mechanisms or, rather, share a common pool of resources? Linguistic theorizing, empirical evidence from language acquisition and processing, and computational modeling have jointly painted a picture whereby lexico-semantic and syntactic processing are deeply inter-connected and perhaps not separable. In contrast, many current proposals of the neural architecture of language continue to endorse a view whereby certain brain regions selectively support syntactic/combinatorial processing, although the locus of such ``syntactic hub'', and its nature, vary across proposals. Here, we searched for selectivity for syntactic over lexico-semantic processing using a powerful individual-subjects fMRI approach across three sentence comprehension paradigms that have been used in prior work to argue for such selectivity: responses to lexico-semantic vs. morpho-syntactic violations (Experiment 1); recovery from neural suppression across pairs of sentences differing in only lexical items vs. only syntactic structure (Experiment 2); and same/different meaning judgments on such sentence pairs (Experiment 3). Across experiments, both lexico-semantic and syntactic conditions elicited robust responses throughout the left fronto-temporal language network. Critically, however, no regions were more strongly engaged by syntactic than lexico-semantic processing, although some regions showed the opposite pattern. Thus, contra many current proposals of the neural architecture of language, syntactic/combinatorial processing is not separable from lexico-semantic processing at the level of brain regions\textemdash or even voxel subsets\textemdash within the language network, in line with strong integration between these two processes that has been consistently observed in behavioral and computational language research. The results further suggest that the language network may be generally more strongly concerned with meaning than syntactic form, in line with the primary function of language\textemdash to share meanings across minds.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/B9UDFXGY/Fedorenko et al. - 2020 - Lack of selectivity for syntax relative to word me.pdf}
}

@article{fedorenko_language_2016,
  title = {Language and Thought Are Not the Same Thing: Evidence from Neuroimaging and Neurological Patients: {{Language}} versus Thought},
  shorttitle = {Language and Thought Are Not the Same Thing},
  author = {Fedorenko, Evelina and Varley, Rosemary},
  year = {2016},
  month = apr,
  journal = {Annals of the New York Academy of Sciences},
  volume = {1369},
  number = {1},
  pages = {132--153},
  issn = {00778923},
  doi = {10.1111/nyas.13046},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EP9FR53K/Fedorenko and Varley - 2016 - Language and thought are not the same thing evide.pdf}
}

@article{fedorenko_nature_2006,
  title = {The Nature of Working Memory Capacity in Sentence Comprehension: {{Evidence}} against Domain-Specific Working Memory Resources\ding{73}},
  shorttitle = {The Nature of Working Memory Capacity in Sentence Comprehension},
  author = {Fedorenko, E and Gibson, E and Rohde, D},
  year = {2006},
  month = may,
  journal = {Journal of Memory and Language},
  volume = {54},
  number = {4},
  pages = {541--553},
  issn = {0749596X},
  doi = {10.1016/j.jml.2005.12.006},
  abstract = {This paper reports the results of a dual-task experiment which investigates the nature of working memory resources used in sentence comprehension. Participants read sentences of varying syntactic complexity (containing subject- and object-extracted relative clauses) while remembering one or three nouns (similar to or dissimilar from the sentence-nouns). A significant on-line interaction was found between syntactic complexity and similarity between the memory-nouns and the sentence-nouns in the three memory-nouns conditions, such that the similarity between the memory-nouns and the sentence-nouns affected the more complex object-extracted relative clauses to a greater extent than the less complex subject-extracted relative clauses. These results extend Gordon, Hendrick, and Levine's (2002) report of a trend of such an interaction. The results argue against the domain-specific view of working memory resources in sentence comprehension (Caplan \& Waters, 1999).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GGDSMXHI/Fedorenko et al. - 2006 - The nature of working memory capacity in sentence .pdf}
}

@article{fedorenko_neural_2016,
  title = {Neural Correlate of the Construction of Sentence Meaning},
  author = {Fedorenko, Evelina and Scott, Terri L. and Brunner, Peter and Coon, William G. and Pritchett, Brianna and Schalk, Gerwin and Kanwisher, Nancy},
  year = {2016},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {41},
  pages = {E6256-E6262},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1612132113},
  abstract = {The neural processes that underlie your ability to read and understand this sentence are unknown. Sentence comprehension occurs very rapidly, and can only be understood at a mechanistic level by discovering the precise sequence of underlying computational and neural events. However, we have no continuous and online neural measure of sentence processing with high spatial and temporal resolution. Here we report just such a measure: intracranial recordings from the surface of the human brain show that neural activity, indexed by {$\gamma$}-power, increases monotonically over the course of a sentence as people read it. This steady increase in activity is absent when people read and remember nonword-lists, despite the higher cognitive demand entailed, ruling out accounts in terms of generic attention, working memory, and cognitive load. Response increases are lower for sentence structure without meaning (``Jabberwocky'' sentences) and word meaning without sentence structure (word-lists), showing that this effect is not explained by responses to syntax or word meaning alone. Instead, the full effect is found only for sentences, implicating compositional processes of sentence understanding, a striking and unique feature of human language not shared with animal communication systems. This work opens up new avenues for investigating the sequence of neural events that underlie the construction of linguistic meaning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6AXLV2IW/Fedorenko et al. - 2016 - Neural correlate of the construction of sentence m.pdf}
}

@article{fedorenko_new_2010,
  title = {New {{Method}} for {{fMRI Investigations}} of {{Language}}: {{Defining ROIs Functionally}} in {{Individual Subjects}}},
  shorttitle = {New {{Method}} for {{fMRI Investigations}} of {{Language}}},
  author = {Fedorenko, Evelina and Hsieh, Po-Jang and {Nieto-Casta{\~n}{\'o}n}, Alfonso and {Whitfield-Gabrieli}, Susan and Kanwisher, Nancy},
  year = {2010},
  month = aug,
  journal = {Journal of Neurophysiology},
  volume = {104},
  number = {2},
  pages = {1177--1194},
  issn = {0022-3077},
  doi = {10.1152/jn.00032.2010},
  abstract = {Previous neuroimaging research has identified a number of brain regions sensitive to different aspects of linguistic processing, but precise functional characterization of these regions has proven challenging. We hypothesize that clearer functional specificity may emerge if candidate language-sensitive regions are identified functionally within each subject individually, a method that has revealed striking functional specificity in visual cortex but that has rarely been applied to neuroimaging studies of language. This method enables pooling of data from corresponding functional regions across subjects rather than from corresponding locations in stereotaxic space (which may differ functionally because of the anatomical variability across subjects). However, it is far from obvious a priori that this method will work as it requires that multiple stringent conditions be met. Specifically, candidate language-sensitive brain regions must be identifiable functionally within individual subjects in a short scan, must be replicable within subjects and have clear correspondence across subjects, and must manifest key signatures of language processing (e.g., a higher response to sentences than nonword strings, whether visual or auditory). We show here that this method does indeed work: we identify 13 candidate language-sensitive regions that meet these criteria, each present in {$\geq$}80\% of subjects individually. The selectivity of these regions is stronger using our method than when standard group analyses are conducted on the same data, suggesting that the future application of this method may reveal clearer functional specificity than has been evident in prior neuroimaging research on language.},
  pmcid = {PMC2934923},
  pmid = {20410363},
  file = {/Users/xzfang/Zotero/storage/TR5PJ4B5/Fedorenko et al. - 2010 - New Method for fMRI Investigations of Language De.pdf}
}

@article{feigenson_core_2004,
  title = {Core Systems of Number},
  author = {Feigenson, Lisa and Dehaene, Stanislas and Spelke, Elizabeth},
  year = {2004},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  volume = {8},
  number = {7},
  pages = {307--314},
  issn = {13646613},
  doi = {10.1016/j.tics.2004.05.002},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VPA4DNMM/Feigenson et al. - 2004 - Core systems of number.pdf}
}

@article{feigenson_limits_2005,
  title = {On the Limits of Infants' Quantification of Small Object Arrays},
  author = {Feigenson, Lisa and Carey, Susan},
  year = {2005},
  month = oct,
  journal = {Cognition},
  volume = {97},
  number = {3},
  pages = {295--313},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2004.09.010},
  abstract = {Recent work suggests that infants rely on mechanisms of object-based attention and short-term memory to represent small numbers of objects. Such work shows that infants discriminate arrays containing 1, 2, or 3 objects, but fail with arrays greater than 3 [Feigenson, L., \& Carey, S. (2003). Tracking individuals via object-files: Evidence from infants' manual search. Developmental Science, 6, 568\textendash 584; Feigenson, L., Carey, S., \& Hauser, M. (2002). The representations underlying infants' choice of more: Object files versus analog magnitudes. Psychological Science, 13(2), 150\textendash 156]. However, little is known about how infants represent arrays exceeding the 3-item limit of parallel representation. We explored possible formats by which infants might represent a 4-object array. Experiment 1 used a manual search paradigm to show that infants successfully discriminated between arrays of 1 vs. 2, 2 vs. 3, and 1 vs. 3 objects. However, infants failed to discriminate 1 vs. 4 despite the highly discriminable ratio, providing the strongest evidence to date for object-file representations underlying performance in this task. Experiment 2 replicated this dramatic failure to discriminate 1 from 4 in a second paradigm, a cracker choice task. We then showed that infants in the choice task succeeded at choosing the larger quantity with 0 vs. 4 crackers and with 1 small vs. 4 large crackers. These results suggest that while infants failed to represent 4 as ``exactly 4'', ``approximately 4'', ``3'', or as even as ``a plurality'', they did represent information about the array, including the existence of a cracker or cracker-material and the size of the individual objects in the array.},
  langid = {english},
  keywords = {Number,Numerosity,Object-files,Plural,Short-term memory limits,Singular},
  file = {/Users/xzfang/Zotero/storage/T9TKL4Q7/Feigenson and Carey - 2005 - On the limits of infants' quantification of small .pdf;/Users/xzfang/Zotero/storage/RNKRH584/S0010027704002082.html}
}

@article{feigenson_tracking_2003,
  title = {Tracking Individuals via Object-Files: Evidence from Infants' Manual Search},
  shorttitle = {Tracking Individuals via Object-Files},
  author = {Feigenson, Lisa and Carey, Susan},
  year = {2003},
  journal = {Developmental Science},
  volume = {6},
  number = {5},
  pages = {568--584},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00313},
  abstract = {In two experiments, a manual search task explored 12- to 14-month-old infants' representations of small sets of objects. In this paradigm, patterns of searching revealed the number of objects infants represented as hidden in an opaque box. In Experiment 1, we obtained the set-size signature of object-file representations: infants succeeded at representing precisely 1, precisely 2, and precisely 3 objects in the box, but failed at representing 4 (or even that 4 is greater than 2). In Experiment 2, we showed that infants' expectations about the contents of the box were based on number of individual objects, and not on a continuous property such as total object volume. These findings support the hypothesis that infants maintained representations of individuals, that object-files were the underlying means of representing these individuals, and that object-file models can be compared via one-to-one correspondence to establish numerical equivalence.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-7687.00313},
  file = {/Users/xzfang/Zotero/storage/TWL2TZK7/Feigenson and Carey - 2003 - Tracking individuals via object-files evidence fr.pdf;/Users/xzfang/Zotero/storage/624FGYD6/1467-7687.html}
}

@article{feinman_learning_2021,
  title = {Learning {{Task-General Representations}} with {{Generative Neuro-Symbolic Modeling}}},
  author = {Feinman, Reuben and Lake, Brenden M.},
  year = {2021},
  month = jan,
  journal = {arXiv:2006.14448 [cs, stat]},
  eprint = {2006.14448},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {People can learn rich, general-purpose conceptual representations from only raw perceptual inputs. Current machine learning approaches fall well short of these human standards, although different modeling traditions often have complementary strengths. Symbolic models can capture the compositional and causal knowledge that enables flexible generalization, but they struggle to learn from raw inputs, relying on strong abstractions and simplifying assumptions. Neural network models can learn directly from raw data, but they struggle to capture compositional and causal structure and typically must retrain to tackle new tasks. We bring together these two traditions to learn generative models of concepts that capture rich compositional and causal structure, while learning from raw data. We develop a generative neuro-symbolic (GNS) model of handwritten character concepts that uses the control flow of a probabilistic program, coupled with symbolic stroke primitives and a symbolic image renderer, to represent the causal and compositional processes by which characters are formed. The distributions of parts (strokes), and correlations between parts, are modeled with neural network subroutines, allowing the model to learn directly from raw data and express nonparametric statistical relationships. We apply our model to the Omniglot challenge of human-level concept learning, using a background set of alphabets to learn an expressive prior distribution over character drawings. In a subsequent evaluation, our GNS model uses probabilistic inference to learn rich conceptual representations from a single training image that generalize to 4 unique tasks, succeeding where previous work has fallen short.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/KK3K9EP7/Feinman and Lake - 2021 - Learning Task-General Representations with Generat.pdf}
}

@article{feldman_bayes_2009,
  title = {Bayes and the Simplicity Principle in Perception.},
  author = {Feldman, Jacob},
  year = {2009},
  month = oct,
  journal = {Psychological Review},
  volume = {116},
  number = {4},
  pages = {875--887},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0017144},
  abstract = {Discussions of the foundations of perceptual inference have often centered on 2 governing principles, the likelihood principle and the simplicity principle. Historically, these principles have usually been seen as opposed, but contemporary statistical (e.g., Bayesian) theory tends to see them as consistent, because for a variety of reasons simpler models (i.e., those with fewer dimensions or free parameters) make better predictors than more complex ones. In perception, many interpretation spaces are naturally hierarchical, meaning that they consist of a set of mutually embedded model classes of various levels of complexity, including simpler (lower dimensional) classes that are special cases of more complex ones. This article shows how such spaces can be regarded as algebraic structures, for example, as partial orders or lattices, with interpretations ordered in terms of dimensionality. The natural inference rule in such a space is a kind of simplicity rule: Among all interpretations qualitatively consistent with the image, draw the one that is lowest in the partial order, called the maximum-depth interpretation. This interpretation also maximizes the Bayesian posterior under certain simplifying assumptions, consistent with a unification of simplicity and likelihood principles. Moreover, the algebraic approach brings out the compositional structure inherent in such spaces, showing how perceptual interpretations are composed from a lexicon of primitive perceptual descriptors.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/G3TFUV8W/Feldman - 2009 - Bayes and the simplicity principle in perception..pdf}
}

@article{feldman_infants_2021,
  title = {Do {{Infants Really Learn Phonetic Categories}}?},
  author = {Feldman, Naomi H. and Goldwater, Sharon and Dupoux, Emmanuel and Schatz, Thomas},
  year = {2021},
  month = nov,
  journal = {Open Mind},
  volume = {5},
  pages = {113--131},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00046},
  abstract = {Early changes in infants' ability to perceive native and nonnative speech sound contrasts are typically attributed to their developing knowledge of phonetic categories. We critically examine this hypothesis and argue that there is little direct evidence of category knowledge in infancy. We then propose an alternative account in which infants' perception changes because they are learning a perceptual space that is appropriate to represent speech, without yet carving up that space into phonetic categories. If correct, this new account has substantial implications for understanding early language development.},
  file = {/Users/xzfang/Zotero/storage/FTID3NX7/Feldman et al. - 2021 - Do Infants Really Learn Phonetic Categories.pdf;/Users/xzfang/Zotero/storage/98BDYZ2E/Do-Infants-Really-Learn-Phonetic-Categories.html}
}

@article{feldman_influence_2009,
  title = {The Influence of Categories on Perception: {{Explaining}} the Perceptual Magnet Effect as Optimal Statistical Inference},
  shorttitle = {The Influence of Categories on Perception},
  author = {Feldman, Naomi H. and Griffiths, Thomas L. and Morgan, James L.},
  year = {2009},
  month = oct,
  journal = {Psychological review},
  volume = {116},
  number = {4},
  pages = {752--782},
  issn = {0033-295X},
  doi = {10.1037/a0017196},
  abstract = {A variety of studies have demonstrated that organizing stimuli into categories can affect the way the stimuli are perceived. We explore the influence of categories on perception through one such phenomenon, the perceptual magnet effect, in which discriminability between vowels is reduced near prototypical vowel sounds. We present a Bayesian model to explain why this reduced discriminability might occur: it arises as a consequence of optimally solving the statistical problem of perception in noise. In the optimal solution to this problem, listeners' perception is biased toward phonetic category means because they use knowledge of these categories to guide their inferences about speakers' target productions. Simulations show that model predictions closely correspond to previously published human data, and novel experimental results provide evidence for the predicted link between perceptual warping and noise. The model unifies several previous accounts of the perceptual magnet effect and provides a framework for exploring categorical effects in other domains.},
  pmcid = {PMC2785510},
  pmid = {19839683},
  file = {/Users/xzfang/Zotero/storage/7UTVGP76/Feldman et al. - 2009 - The influence of categories on perception Explain.pdf}
}

@article{feldman_mutual_2021,
  title = {Mutual {{Information}} and {{Categorical Perception}}},
  author = {Feldman, Jacob},
  year = {2021},
  month = aug,
  journal = {Psychological Science},
  volume = {32},
  number = {8},
  pages = {1298--1310},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797621996663},
  abstract = {Categorical perception refers to the enhancement of perceptual sensitivity near category boundaries, generally along dimensions that are informative about category membership. However, it remains unclear exactly which dimensions are treated as informative and why. This article reports a series of experiments in which subjects were asked to learn statistically defined categories in a novel, unfamiliar 2D perceptual space of shapes. Perceptual discrimination was tested before and after category learning of various features in the space, each defined by its position and orientation relative to the maximally informative dimension. The results support a remarkably simple generalization: The magnitude of improvement in perceptual discrimination of each feature is proportional to the mutual information between the feature and the category variable. This finding suggests a rational basis for categorical perception in which the precision of perceptual discrimination is tuned to the statistical structure of the environment.},
  langid = {english},
  keywords = {categorical perception,categorization,mutual information},
  file = {/Users/xzfang/Zotero/storage/VD3B8A9C/Feldman - 2021 - Mutual Information and Categorical Perception.pdf}
}

@article{fernandino_decoding_2021,
  title = {Decoding the {{Information Structure Underlying}} the {{Neural Representation}} of {{Concepts}}},
  author = {Fernandino, Leonardo and Conant, Lisa L. and Humphries, Colin J. and Binder, Jeffrey R.},
  year = {2021},
  month = mar,
  journal = {bioRxiv},
  pages = {2021.03.16.435524},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.03.16.435524},
  abstract = {{$<$}p{$>$}The nature of the neural code underlying conceptual knowledge remains a major unsolved problem in cognitive neuroscience. Three main types of information have been proposed as candidates for the neural representations of lexical concepts: taxonomic (i.e., information about category membership and inter-category relations), distributional (i.e., information about patterns of word co-occurrence in natural language use), and experiential (i.e., information about sensory-motor, affective, and other features of phenomenal experience engaged during concept acquisition). In two experiments, we investigated the extent to which these three types of information are encoded in the neural activation patterns associated with hundreds of English nouns from a wide variety of conceptual categories. Participants made familiarity judgments on the meaning of written nouns while undergoing functional MRI. A high-resolution, whole-brain activation map was generated for each noun in each participant{${'}$}s native space. These word-specific activation maps were used to evaluate different representational spaces corresponding to the three types of information described above. In both studies, we found a striking advantage for experience-based models in most brain areas previously associated with concept representation. Partial correlation analyses revealed that only experiential information successfully predicted concept similarity structure when inter-model correlations were taken into account. This pattern of results was found independently for object concepts and event concepts. Our findings indicate that the neural representation of conceptual knowledge primarily encodes information about features of experience, and that - to the extent that it is represented in the brain - taxonomic and distributional information may rely on such an experience-based code.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PB8AVHYU/Fernandino et al. - 2021 - Decoding the Information Structure Underlying the .pdf;/Users/xzfang/Zotero/storage/FP6764B6/2021.03.16.435524v1.html}
}

@article{fernandino_decoding_2022,
  title = {Decoding the Information Structure Underlying the Neural Representation of Concepts},
  author = {Fernandino, Leonardo and Tong, Jia-Qing and Conant, Lisa L. and Humphries, Colin J. and Binder, Jeffrey R.},
  year = {2022},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {6},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2108091119},
  abstract = {The nature of the representational code underlying conceptual knowledge remains a major unsolved problem in cognitive neuroscience. We assessed the extent to which different representational systems contribute to the instantiation of lexical concepts in high-level, heteromodal cortical areas previously associated with semantic cognition. We found that lexical semantic information can be reliably decoded from a wide range of heteromodal cortical areas in the frontal, parietal, and temporal cortex. In most of these areas, we found a striking advantage for experience-based representational structures (i.e., encoding information about sensory-motor, affective, and other features of phenomenal experience), with little evidence for independent taxonomic or distributional organization. These results were found independently for object and event concepts. Our findings indicate that concept representations in the heteromodal cortex are based, at least in part, on experiential information. They also reveal that, in most heteromodal areas, event concepts have more heterogeneous representations (i.e., they are more easily decodable) than object concepts and that other areas beyond the traditional ``semantic hubs'' contribute to semantic cognition, particularly the posterior cingulate gyrus and the precuneus.},
  chapter = {Social Sciences},
  copyright = {Copyright \textcopyright{} 2022 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {35115397},
  keywords = {concept representation,embodied semantics,lexical semantics,representational similarity analysis,semantic memory},
  file = {/Users/xzfang/Zotero/storage/54MWZHIP/e2108091119.html}
}

@article{ferreira_good_2007,
  title = {The `{{Good Enough}}' {{Approach}} to {{Language Comprehension}}: {{The}} `{{Good Enough}}' {{Approach}}},
  shorttitle = {The `{{Good Enough}}' {{Approach}} to {{Language Comprehension}}},
  author = {Ferreira, Fernanda and Patson, Nikole D.},
  year = {2007},
  month = mar,
  journal = {Language and Linguistics Compass},
  volume = {1},
  number = {1-2},
  pages = {71--83},
  issn = {1749818X},
  doi = {10.1111/j.1749-818X.2007.00007.x},
  abstract = {Ferreira and colleagues argued that the language comprehension system creates syntactic and semantic representations that are merely `good enough' (GE) given the task that the comprehender needs to perform. GE representations contrast with ones that are detailed, complete, and accurate with respect to the input. In this article, we review the original argument for GE processing, and we present new evidence that supports the concept: first, local interpretations are computed, which can interfere with global ones; second, new findings based on the recording of event-related potentials show the use of simple heuristics rather than compositional algorithms for constructing sentence meaning; and recent studies show that the comprehension system has mechanisms for handling disfluencies, but they work imperfectly. We argue that the GE approach to language comprehension is similar to the use of fast and frugal heuristics for decision-making, and that future research should explore this connection more thoroughly.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HT7NXHDX/Ferreira and Patson - 2007 - The â€˜Good Enoughâ€™ Approach to Language Comprehensi.pdf}
}

@article{ferreira_goodenough_2002,
  title = {Good-{{Enough Representations}} in {{Language Comprehension}}},
  author = {Ferreira, Fernanda and Bailey, Karl G.D. and Ferraro, Vittoria},
  year = {2002},
  month = feb,
  journal = {Current Directions in Psychological Science},
  volume = {11},
  number = {1},
  pages = {11--15},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1111/1467-8721.00158},
  abstract = {People comprehend utterances rapidly and without conscious effort. Traditional theories assume that sentence processing is algorithmic and that meaning is derived compositionally. The language processor is believed to generate representations of the linguistic input that are complete, detailed, and accurate. However, recent findings challenge these assumptions. Investigations of the misinterpretation of both garden-path and passive sentences have yielded support for the idea that the meaning people obtain for a sentence is often not a reflection of its true content. Moreover, incorrect interpretations may persist even after syntactic reanalysis has taken place. Our good-enough approach to language comprehension holds that language processing is sometimes only partial and that semantic representations are often incomplete. Future work will elucidate the conditions under which sentence processing is simply good enough.},
  langid = {english},
  keywords = {language comprehension,linguistic ambiguity,satisficing,syntax},
  file = {/Users/xzfang/Zotero/storage/9E497GJF/Ferreira et al. - 2002 - Good-Enough Representations in Language Comprehens.pdf}
}

@article{fiedler_late_2019,
  title = {Late Cortical Tracking of Ignored Speech Facilitates Neural Selectivity in Acoustically Challenging Conditions},
  author = {Fiedler, Lorenz and W{\"o}stmann, Malte and Herbst, Sophie K. and Obleser, Jonas},
  year = {2019},
  month = feb,
  journal = {NeuroImage},
  volume = {186},
  pages = {33--42},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.10.057},
  abstract = {Listening requires selective neural processing of the incoming sound mixture, which in humans is borne out by a surprisingly clean representation of attended-only speech in auditory cortex. How this neural selectivity is achieved even at negative signal-to-noise ratios (SNR) remains unclear. We show that, under such conditions, a late cortical representation (i.e., neural tracking) of the ignored acoustic signal is key to successful separation of attended and distracting talkers (i.e., neural selectivity). We recorded and modeled the electroencephalographic response of 18 participants who attended to one of two simultaneously presented stories, while the SNR between the two talkers varied dynamically between +6 and -6~dB. The neural tracking showed an increasing early-to-late attention-biased selectivity. Importantly, acoustically dominant (i.e., louder) ignored talkers were tracked neurally by late involvement of fronto-parietal regions, which contributed to enhanced neural selectivity. This neural selectivity, by way of representing the ignored talker, poses a mechanistic neural account of attention under real-life acoustic conditions.},
  langid = {english},
  keywords = {Attention,Auditory cortex,EEG,Fronto-parietal attention network,SNR,Speech encoding},
  file = {/Users/xzfang/Zotero/storage/2E4NLSPK/Fiedler et al. - 2019 - Late cortical tracking of ignored speech facilitat.pdf;/Users/xzfang/Zotero/storage/EWZU5334/S1053811918320299.html}
}

@article{filik_processing_2008,
  title = {Processing Local Pragmatic Anomalies in Fictional Contexts: {{Evidence}} from the {{N400}}},
  shorttitle = {Processing Local Pragmatic Anomalies in Fictional Contexts},
  author = {Filik, Ruth and Leuthold, Hartmut},
  year = {2008},
  journal = {Psychophysiology},
  volume = {45},
  number = {4},
  pages = {554--558},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.2008.00656.x},
  abstract = {Readers typically experience processing difficulty when they encounter a word that is anomalous within the local context, such as ``The cat picked up the chainsaw.'' In an ERP study, we demonstrate that by placing such a sentence in a fictional scenario that is well known to the reader (e.g., a Tom and Jerry cartoon), the N400 effect usually associated with these pragmatic anomalies can be eliminated. This finding suggests that readers can rapidly integrate information from their common ground while interpreting incoming text and provides further evidence that incoming words are immediately evaluated within the global discourse.},
  langid = {english},
  keywords = {Common ground,Discourse context,ERPs,Language,N400,Pragmatic anomalies},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2008.00656.x},
  file = {/Users/xzfang/Zotero/storage/MSAVLYDT/Filik and Leuthold - 2008 - Processing local pragmatic anomalies in fictional .pdf;/Users/xzfang/Zotero/storage/CU6Q9545/j.1469-8986.2008.00656.html}
}

@article{fine_rapid_2013,
  title = {Rapid {{Expectation Adaptation}} during {{Syntactic Comprehension}}},
  author = {Fine, Alex B. and Jaeger, T. Florian and Farmer, Thomas A. and Qian, Ting},
  year = {2013},
  month = oct,
  journal = {PLOS ONE},
  volume = {8},
  number = {10},
  pages = {e77661},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0077661},
  abstract = {When we read or listen to language, we are faced with the challenge of inferring intended messages from noisy input. This challenge is exacerbated by considerable variability between and within speakers. Focusing on syntactic processing (parsing), we test the hypothesis that language comprehenders rapidly adapt to the syntactic statistics of novel linguistic environments (e.g., speakers or genres). Two self-paced reading experiments investigate changes in readers' syntactic expectations based on repeated exposure to sentences with temporary syntactic ambiguities (so-called ``garden path sentences''). These sentences typically lead to a clear expectation violation signature when the temporary ambiguity is resolved to an a priori less expected structure (e.g., based on the statistics of the lexical context). We find that comprehenders rapidly adapt their syntactic expectations to converge towards the local statistics of novel environments. Specifically, repeated exposure to a priori unexpected structures can reduce, and even completely undo, their processing disadvantage (Experiment 1). The opposite is also observed: a priori expected structures become less expected (even eliciting garden paths) in environments where they are hardly ever observed (Experiment 2). Our findings suggest that, when changes in syntactic statistics are to be expected (e.g., when entering a novel environment), comprehenders can rapidly adapt their expectations, thereby overcoming the processing disadvantage that mistaken expectations would otherwise cause. Our findings take a step towards unifying insights from research in expectation-based models of language processing, syntactic priming, and statistical learning.},
  langid = {english},
  keywords = {Forecasting,Grammar,Language,Language acquisition,Linguistic morphology,Phonology,Psycholinguistics,Syntax},
  file = {/Users/xzfang/Zotero/storage/EUUNUSIP/Fine et al. - 2013 - Rapid Expectation Adaptation during Syntactic Comp.pdf;/Users/xzfang/Zotero/storage/G6AKSKZH/article.html}
}

@article{firestone_cognition_2016,
  title = {Cognition Does Not Affect Perception: {{Evaluating}} the Evidence for ``Top-down'' Effects},
  shorttitle = {Cognition Does Not Affect Perception},
  author = {Firestone, Chaz and Scholl, Brian J.},
  year = {2016/ed},
  journal = {Behavioral and Brain Sciences},
  volume = {39},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X15000965},
  abstract = {What determines what we see? In contrast to the traditional ``modular'' understanding of perception, according to which visual processing is encapsulated from higher-level cognition, a tidal wave of recent research alleges that states such as beliefs, desires, emotions, motivations, intentions, and linguistic representations exert direct, top-down influences on what we see. There is a growing consensus that such effects are ubiquitous, and that the distinction between perception and cognition may itself be unsustainable. We argue otherwise: None of these hundreds of studies~\textendash ~either individually or collectively~\textendash ~provides compelling evidence for true top-down effects on perception, or ``cognitive penetrability.'' In particular, and despite their variety, we suggest that these studies all fall prey to only a handful of pitfalls. And whereas abstract theoretical challenges have failed to resolve this debate in the past, our presentation of these pitfalls is empirically anchored: In each case, we show not only how certain studies could be susceptible to the pitfall (in principle), but also how several alleged top-down effects actually are explained by the pitfall (in practice). Moreover, these pitfalls are perfectly general, with each applying to dozens of other top-down effects. We conclude by extracting the lessons provided by these pitfalls into a checklist that future work could use to convincingly demonstrate top-down effects on visual perception. The discovery of substantive top-down effects of cognition on perception would revolutionize our understanding of how the mind is organized; but without addressing these pitfalls, no such empirical report will license such exciting conclusions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4EA3N2BP/Firestone and Scholl - 2016 - Cognition does not affect perception Evaluating t.pdf;/Users/xzfang/Zotero/storage/2KYZKLZQ/920E2AE74C642DD3CB3FA8160EA1D84A.html}
}

@article{firestone_performance_2020,
  title = {Performance vs. Competence in Human\textendash Machine Comparisons},
  author = {Firestone, Chaz},
  year = {2020},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {43},
  pages = {26562--26571},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1905334117},
  abstract = {Does the human mind resemble the machines that can behave like it? Biologically inspired machine-learning systems approach ``human-level'' accuracy in an astounding variety of domains, and even predict human brain activity\textemdash raising the exciting possibility that such systems represent the world like we do. However, even seemingly intelligent machines fail in strange and ``unhumanlike'' ways, threatening their status as models of our minds. How can we know when human\textendash machine behavioral differences reflect deep disparities in their underlying capacities, vs. when such failures are only superficial or peripheral? This article draws on a foundational insight from cognitive science\textemdash the distinction between performance and competence\textemdash to encourage ``species-fair'' comparisons between humans and machines. The performance/competence distinction urges us to consider whether the failure of a system to behave as ideally hypothesized, or the failure of one creature to behave like another, arises not because the system lacks the relevant knowledge or internal capacities (``competence''), but instead because of superficial constraints on demonstrating that knowledge (``performance''). I argue that this distinction has been neglected by research comparing human and machine behavior, and that it should be essential to any such comparison. Focusing on the domain of image classification, I identify three factors contributing to the species-fairness of human\textendash machine comparisons, extracted from recent work that equates such constraints. Species-fair comparisons level the playing field between natural and artificial intelligence, so that we can separate more superficial differences from those that may be deep and enduring.},
  chapter = {Perspective},
  copyright = {\textcopyright{} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  isbn = {9781905334117},
  langid = {english},
  pmid = {33051296},
  keywords = {artificial intelligence,cognition,deep learning,development,perception},
  file = {/Users/xzfang/Zotero/storage/LCSJGDDE/Firestone - 2020 - Performance vs. competence in humanâ€“machine compar.pdf;/Users/xzfang/Zotero/storage/S7ZBN7EI/26562.html}
}

@article{fischer_objectlevel_2011,
  title = {Object-Level Visual Information Gets through the Bottleneck of Crowding},
  author = {Fischer, Jason and Whitney, David},
  year = {2011},
  month = sep,
  journal = {Journal of Neurophysiology},
  volume = {106},
  number = {3},
  pages = {1389--1398},
  issn = {0022-3077},
  doi = {10.1152/jn.00904.2010},
  abstract = {Natural visual scenes are cluttered. In such scenes, many objects in the periphery can be crowded, blocked from identification, simply because of the dense array of clutter. Outside of the fovea, crowding constitutes the fundamental limitation on object recognition and is thought to arise from the limited resolution of the neural mechanisms that select and bind visual features into coherent objects. Thus it is widely believed that in the visual processing stream, a crowded object is reduced to a collection of dismantled features with no surviving holistic properties. Here, we show that this is not so: an entire face can survive crowding and contribute its holistic attributes to the perceived average of the set, despite being blocked from recognition. Our results show that crowding does not dismantle high-level object representations to their component features.},
  pmcid = {PMC3174808},
  pmid = {21676930}
}

@article{fischer_serial_2014,
  title = {Serial Dependence in Visual Perception},
  author = {Fischer, Jason and Whitney, David},
  year = {2014},
  month = may,
  journal = {Nature Neuroscience},
  volume = {17},
  number = {5},
  pages = {738--743},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3689},
  abstract = {Visual input is often noisy and discontinuous, even though the physical environment is generally stable. The authors show that the visual system trades off change sensitivity to capitalize on physical continuity via serial dependence: present perception is biased toward past visual input. This bias is modulated by attention and governed by a spatiotemporally-tuned operator, a continuity field.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8JAQXHGC/Fischer and Whitney - 2014 - Serial dependence in visual perception.pdf;/Users/xzfang/Zotero/storage/D8TAGGYT/nn.html}
}

@article{fischler_semantic_1977,
  title = {Semantic Facilitation without Association in a Lexical Decision Task},
  author = {Fischler, Ira},
  year = {1977},
  month = may,
  journal = {Memory \& Cognition},
  volume = {5},
  number = {3},
  pages = {335--339},
  issn = {0090-502X, 1532-5946},
  doi = {10.3758/BF03197580},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XL5GRJWE/Fischler - 1977 - Semantic facilitation without association in a lex.pdf}
}

@article{fitz_language_2019,
  title = {Language {{ERPs}} Reflect Learning through Prediction Error Propagation},
  author = {Fitz, Hartmut and Chang, Franklin},
  year = {2019},
  month = jun,
  journal = {Cognitive Psychology},
  volume = {111},
  pages = {15--52},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2019.03.002},
  abstract = {Event-related potentials (ERPs) provide a window into how the brain is processing language. Here, we propose a theory that argues that ERPs such as the N400 and P600 arise as side effects of an error-based learning mechanism that explains linguistic adaptation and language learning. We instantiated this theory in a connectionist model that can simulate data from three studies on the N400 (amplitude modulation by expectancy, contextual constraint, and sentence position), five studies on the P600 (agreement, tense, word category, subcategorization and garden-path sentences), and a study on the semantic P600 in role reversal anomalies. Since ERPs are learning signals, this account explains adaptation of ERP amplitude to within-experiment frequency manipulations and the way ERP effects are shaped by word predictability in earlier sentences. Moreover, it predicts that ERPs can change over language development. The model provides an account of the sensitivity of ERPs to expectation mismatch, the relative timing of the N400 and P600, the semantic nature of the N400, the syntactic nature of the P600, and the fact that ERPs can change with experience. This approach suggests that comprehension ERPs are related to sentence production and language acquisition mechanisms.},
  langid = {english},
  keywords = {Comprehension,Connectionist model,Development,Error back-propagation,Event-related potentials,Learning,Linguistic adaptation,N400,P600,Semantic P600},
  file = {/Users/xzfang/Zotero/storage/BRHH5XQH/Fitz and Chang - 2019 - Language ERPs reflect learning through prediction .pdf;/Users/xzfang/Zotero/storage/HWRSNAE4/S0010028518300124.html}
}

@article{flick_building_2018,
  title = {Building Words and Phrases in the Left Temporal Lobe},
  author = {Flick, Graham and Oseki, Yohei and Kaczmarek, Amanda R. and Al Kaabi, Meera and Marantz, Alec and Pylkk{\"a}nen, Liina},
  year = {2018},
  month = sep,
  journal = {Cortex},
  volume = {106},
  pages = {213--236},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.06.004},
  abstract = {A central part of knowing a language is the ability to combine basic linguistic units to form complex representations. While our neurobiological understanding of how words combine into larger structures has significantly advanced in recent years, the combinatory operations that build words themselves remain unknown. Are complex words such as tombstone and starlet built with the same mechanisms that construct phrases from words, such as grey stone or bright star? Here we addressed this with two magnetoencephalography (MEG) experiments, which simultaneously varied demands associated with phrasal composition, and the processing of morphological complexity in compound and suffixed nouns. Replicating previous findings, we show that portions of the left anterior temporal lobe (LATL) are engaged in the combination of modifiers and monomorphemic nouns in phrases (e.g., brown rabbit). As regards compounding, we show that semantically transparent compounds (e.g., tombstone) also engage left anterior temporal cortex, though the spatiotemporal details of this effect differed from phrasal composition. Further, when a phrase was constructed from a modifier and a transparent compound (e.g., granite tombstone), the typical LATL phrasal composition response appeared at a delayed latency, which follows if an initial within-word operation (tomb~+~stone) must take place before the combination of the compound with the preceding modifier (granite~+~tombstone). In contrast to compounding, suffixation (i.e., star~+~let) did not engage the LATL in any consistent way, suggesting a distinct processing route. Finally, our results suggest an intriguing generalization that morpho-orthographic complexity that does not recruit the LATL may block the engagement of the LATL in subsequent phrase building. In sum, our findings offer a detailed spatiotemporal characterization of the lowest level combinatory operations that ultimately feed the composition of full sentences.},
  langid = {english},
  keywords = {Anterior temporal cortex,Complex word recognition,Magnetoencephalography,Semantic composition},
  file = {/Users/xzfang/Zotero/storage/MB6CSR94/Flick et al. - 2018 - Building words and phrases in the left temporal lo.pdf;/Users/xzfang/Zotero/storage/IHY522TK/S0010945218301904.html}
}

@article{flick_letters_2021,
  title = {From Letters to Composed Concepts: {{A}} Magnetoencephalography Study of Reading},
  shorttitle = {From Letters to Composed Concepts},
  author = {Flick, Graham and Abdullah, Osama and Pylkk{\"a}nen, Liina},
  year = {2021},
  journal = {Human Brain Mapping},
  volume = {n/a},
  number = {n/a},
  issn = {1097-0193},
  doi = {10.1002/hbm.25608},
  abstract = {Language comprehension requires the recognition of individual words and the combination of their meanings to yield complex concepts or interpretations. This combinatory process often requires the insertion of unstated semantic material between words, based on thematic or feature knowledge. For example, the phrase horse barn is not interpreted as a blend of a horse and a barn, but specifically a barn where horses are kept. Previous neuroscientific evidence suggests that left posterior and anterior temporal cortex underpin thematic and feature-based concept knowledge, respectively, but much remains unclear about how these areas contribute to combinatory language processing. Using magnetoencephalography, we contrasted source-localized responses to modifier-noun phrases involving thematic relations versus feature modifications, while also examining how lower-level orthographic processing fed composition. Participants completed three procedures examining responses to letter-strings, adjective-noun phrases, and noun\textendash noun combinations that varied the semantic relations between words. We found that sections of the left anterior temporal lobe, posterior temporal lobe, and cortex surrounding the angular gyrus were all engaged in the minimal composition of adjective-noun phrases, a more distributed network than in most prior studies of minimal composition. Of these regions, only the left posterior temporal lobe was additionally sensitive to implicit thematic relations between composing words, suggesting that it houses a specialized relational processing component in a wider composition network. We additionally identified a left occipitotemporal progression from orthographic to lexical processing, feeding ventral anterior areas engaged in the combination of word meanings. Finally, by examining source signal leakage, we characterized the degree to which these responses could be distinguished from one another using source estimation.},
  langid = {english},
  keywords = {angular gyrus,anterior temporal lobe,composition,magnetoencephalography,posterior temporal lobe,semantics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.25608},
  file = {/Users/xzfang/Zotero/storage/ZVD2494D/Flick et al. - From letters to composed concepts A magnetoenceph.pdf;/Users/xzfang/Zotero/storage/EQMSTXWV/hbm.html}
}

@article{formisano_who_2008,
  title = {"{{Who}}" {{Is Saying}} "{{What}}"? {{Brain-Based Decoding}} of {{Human Voice}} and {{Speech}}},
  shorttitle = {"{{Who}}" {{Is Saying}} "{{What}}"?},
  author = {Formisano, E. and De Martino, F. and Bonte, M. and Goebel, R.},
  year = {2008},
  month = nov,
  journal = {Science},
  volume = {322},
  number = {5903},
  pages = {970--973},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1164318},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZJWEWINQ/Formisano et al. - 2008 - Who Is Saying What Brain-Based Decoding of Hu.pdf}
}

@article{fornaciai_attractive_2018,
  title = {Attractive {{Serial Dependence}} in the {{Absence}} of an {{Explicit Task}}},
  author = {Fornaciai, Michele and Park, Joonkoo},
  year = {2018},
  month = mar,
  journal = {Psychological Science},
  volume = {29},
  number = {3},
  pages = {437--446},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797617737385},
  abstract = {Attractive serial dependence refers to an adaptive change in the representation of sensory information, whereby a current stimulus appears to be similar to a previous one. The nature of this phenomenon is controversial, however, as serial dependence could arise from biased perceptual representations or from biased traces of working memory representation at a decisional stage. Here, we demonstrated a neural signature of serial dependence in numerosity perception emerging early in the visual processing stream even in the absence of an explicit task. Furthermore, a psychophysical experiment revealed that numerosity perception is biased by a previously presented stimulus in an attractive way, not by repulsive adaptation. These results suggest that serial dependence is a perceptual phenomenon starting from early levels of visual processing and occurring independently from a decision process, which is consistent with the view that these biases smooth out noise from neural signals to establish perceptual continuity.},
  langid = {english},
  keywords = {numerosity perception,serial dependence,visual perception},
  file = {/Users/xzfang/Zotero/storage/YDEU5CSY/Fornaciai and Park - 2018 - Attractive Serial Dependence in the Absence of an .pdf}
}

@article{forster_repetition_1984,
  title = {Repetition Priming and Frequency Attenuation in Lexical Access.},
  author = {Forster, Kenneth I. and Davis, Chris},
  year = {1984},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {10},
  number = {4},
  pages = {680--698},
  issn = {0278-7393},
  doi = {10.1037/0278-7393.10.4.680},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/C48AAF52/Forster and Davis - 1984 - Repetition priming and frequency attenuation in le.pdf}
}

@article{foster_common_2013,
  title = {Common Parietal Activation in Musical Mental Transformations across Pitch and Time},
  author = {Foster, Nicholas E. V. and Halpern, Andrea R. and Zatorre, Robert J.},
  year = {2013},
  month = jul,
  journal = {NeuroImage},
  volume = {75},
  pages = {27--35},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2013.02.044},
  abstract = {We previously observed that mental manipulation of the pitch level or temporal organization of melodies results in functional activation in the human intraparietal sulcus (IPS), a region also associated with visuospatial transformation and numerical calculation. Two outstanding questions about these musical transformations are whether pitch and time depend on separate or common processing in IPS, and whether IPS recruitment in melodic tasks varies depending upon the degree of transformation required (as it does in mental rotation). In the present study we sought to answer these questions by applying functional magnetic resonance imaging while musicians performed closely matched mental transposition (pitch transformation) and melody reversal (temporal transformation) tasks. A voxel-wise conjunction analysis showed that in individual subjects, both tasks activated overlapping regions in bilateral IPS, suggesting that a common neural substrate subserves both types of mental transformation. Varying the magnitude of mental pitch transposition resulted in variation of IPS BOLD signal in correlation with the musical key-distance of the transposition, but not with the pitch distance, indicating that the cognitive metric relevant for this type of operation is an abstract one, well described by music-theoretic concepts. These findings support a general role for the IPS in systematically transforming auditory stimulus representations in a nonspatial context.},
  langid = {english},
  keywords = {Auditory,fMRI,Music,Parietal lobe,Transformation},
  file = {/Users/xzfang/Zotero/storage/JR9IDGGJ/Foster et al. - 2013 - Common parietal activation in musical mental trans.pdf;/Users/xzfang/Zotero/storage/F63CF22T/S105381191300178X.html}
}

@article{foster_common_2013a,
  title = {Common Parietal Activation in Musical Mental Transformations across Pitch and Time},
  author = {Foster, Nicholas E. V. and Halpern, Andrea R. and Zatorre, Robert J.},
  year = {2013},
  month = jul,
  journal = {NeuroImage},
  volume = {75},
  pages = {27--35},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2013.02.044},
  abstract = {We previously observed that mental manipulation of the pitch level or temporal organization of melodies results in functional activation in the human intraparietal sulcus (IPS), a region also associated with visuospatial transformation and numerical calculation. Two outstanding questions about these musical transformations are whether pitch and time depend on separate or common processing in IPS, and whether IPS recruitment in melodic tasks varies depending upon the degree of transformation required (as it does in mental rotation). In the present study we sought to answer these questions by applying functional magnetic resonance imaging while musicians performed closely matched mental transposition (pitch transformation) and melody reversal (temporal transformation) tasks. A voxel-wise conjunction analysis showed that in individual subjects, both tasks activated overlapping regions in bilateral IPS, suggesting that a common neural substrate subserves both types of mental transformation. Varying the magnitude of mental pitch transposition resulted in variation of IPS BOLD signal in correlation with the musical key-distance of the transposition, but not with the pitch distance, indicating that the cognitive metric relevant for this type of operation is an abstract one, well described by music-theoretic concepts. These findings support a general role for the IPS in systematically transforming auditory stimulus representations in a nonspatial context.},
  langid = {english},
  keywords = {Auditory,fMRI,Music,Parietal lobe,Transformation},
  file = {/Users/xzfang/Zotero/storage/IF22V4B4/Foster et al. - 2013 - Common parietal activation in musical mental trans.pdf;/Users/xzfang/Zotero/storage/6RN39U56/S105381191300178X.html}
}

@article{foster_reverse_2006,
  title = {Reverse Replay of Behavioural Sequences in Hippocampal Place Cells during the Awake State},
  author = {Foster, David J. and Wilson, Matthew A.},
  year = {2006},
  month = mar,
  journal = {Nature},
  volume = {440},
  number = {7084},
  pages = {680--683},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature04587},
  abstract = {During sleep, neurons in the rat hippocampus are known to replay sequences of activity that took place when the rat was awake. A new study, in rats running around a track, eating and grooming, shows that replay also occurs repeatedly during the awake state, and that behavioural sequences are replayed in reverse order. Theories of spatial learning have previously suggested that reverse replay might be useful. Replay during the awake state might also explain in part why learning can be more effective if learning sessions are spaced out in time rather than clustered together, why hyperactivity causes learning problems, and why simply being awake and resting can help learning.},
  copyright = {2006 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/Users/xzfang/Zotero/storage/XAJHCNFG/Foster and Wilson - 2006 - Reverse replay of behavioural sequences in hippoca.pdf;/Users/xzfang/Zotero/storage/GWTV2WSS/nature04587.html}
}

@article{foster_using_2021,
  title = {Using {{EEG}} to Decode Semantics during an Artificial Language Learning Task},
  author = {Foster, Chris and Williams, Chad C. and Krigolson, Olave E. and Fyshe, Alona},
  year = {2021},
  journal = {Brain and Behavior},
  volume = {n/a},
  number = {n/a},
  issn = {2162-3279},
  doi = {10.1002/brb3.2234},
  abstract = {Background As we learn a new nonnative language (L2), we begin to build a new map of concepts onto orthographic representations. Eventually, L2 can conjure as rich a semantic representation as our native language (L1). However, the neural processes for mapping a new orthographic representation to a familiar meaning are not well understood or characterized. Methods Using electroencephalography and an artificial language that maps symbols to English words, we show that it is possible to use machine learning models to detect a newly formed semantic mapping as it is acquired. Results Through a trial-by-trial analysis, we show that we can detect when a new semantic mapping is formed. Our results show that, like word meaning representations evoked by a L1, the localization of the newly formed neural representations is highly distributed, but the representation may emerge more slowly after the onset of the symbol. Furthermore, our mapping of word meanings to symbols removes the confound of the semantics to the visual characteristics of the stimulus, a confound that has been difficult to disentangle previously. Conclusion We have shown that the L1 semantic representation conjured by a newly acquired L2 word can be detected using decoding techniques, and we give the first characterization of the emergence of that mapping. Our work opens up new possibilities for the study of semantic representations during L2 learning.},
  langid = {english},
  keywords = {electroencephalography,language,language learning,machine learning,semantics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/brb3.2234},
  file = {/Users/xzfang/Zotero/storage/X6ZQP2Q4/Foster et al. - Using EEG to decode semantics during an artificial.pdf;/Users/xzfang/Zotero/storage/Z6MHPX3X/brb3.html}
}

@article{frank_erp_2015,
  title = {The {{ERP}} Response to the Amount of Information Conveyed by Words in Sentences},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  year = {2015},
  month = jan,
  journal = {Brain and Language},
  volume = {140},
  pages = {1--11},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2014.10.006},
  abstract = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence's hierarchical structure for generating expectations about the upcoming word.},
  langid = {english},
  keywords = {Entropy,Event-related potentials,Information theory,Reading,Sentence comprehension,Surprisal},
  file = {/Users/xzfang/Zotero/storage/9ETNIJA5/Frank et al. - 2015 - The ERP response to the amount of information conv.pdf;/Users/xzfang/Zotero/storage/IFCZGU7W/S0093934X14001515.html}
}

@article{frank_hierarchical_2018,
  title = {Hierarchical and Sequential Processing of Language},
  author = {Frank, Stefan L. and Christiansen, Morten H.},
  year = {2018},
  month = oct,
  journal = {Language, Cognition and Neuroscience},
  volume = {33},
  number = {9},
  pages = {1213--1218},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1424347},
  abstract = {Ding et al. (2017) contrast their view that language processing is based on hierarchical syntactic structures, to a view that relies on word-level input statistics. In this response to their paper, we clarify how, exactly, the two views differ (and how they do not), and make a case for the importance of sequential, as opposed to hierarchical, structure for language processing.},
  keywords = {abstraction and generalisation,Hierarchical structure,language statistics,sequential structure},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2018.1424347},
  file = {/Users/xzfang/Zotero/storage/HS99KK75/Frank and Christiansen - 2018 - Hierarchical and sequential processing of language.pdf;/Users/xzfang/Zotero/storage/BJTQFMI3/23273798.2018.html}
}

@article{frank_how_2012,
  title = {How Hierarchical Is Language Use?},
  author = {Frank, Stefan L. and Bod, Rens and Christiansen, Morten H.},
  year = {2012},
  month = nov,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {279},
  number = {1747},
  pages = {4522--4531},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2012.1741},
  abstract = {It is generally assumed that hierarchical phrase structure plays a central role in human language. However, considerations of simplicity and evolutionary continuity suggest that hierarchical structure should not be invoked too hastily. Indeed, recent neurophysiological, behavioural and computational studies show that sequential sentence structure has considerable explanatory power and that hierarchical processing is often not involved. In this paper, we review evidence from the recent literature supporting the hypothesis that sequential structure may be fundamental to the comprehension, production and acquisition of human language. Moreover, we provide a preliminary sketch outlining a non-hierarchical model of language use and discuss its implications and testable predictions. If linguistic phenomena can be explained by sequential rather than hierarchical structure, this will have considerable impact in a wide range of fields, such as linguistics, ethology, cognitive neuroscience, psychology and computer science.},
  keywords = {cognitive neuroscience,computational linguistics,language evolution,language structure,psycholinguistics},
  file = {/Users/xzfang/Zotero/storage/V5E2G5PW/Frank et al. - 2012 - How hierarchical is language use.pdf}
}

@article{frank_lexical_2018,
  title = {Lexical Representation Explains Cortical Entrainment during Speech Comprehension},
  author = {Frank, Stefan L. and Yang, Jinbiao},
  year = {2018},
  month = may,
  journal = {PLOS ONE},
  volume = {13},
  number = {5},
  pages = {e0197304},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0197304},
  abstract = {Results from a recent neuroimaging study on spoken sentence comprehension have been interpreted as evidence for cortical entrainment to hierarchical syntactic structure. We present a simple computational model that predicts the power spectra from this study, even though the model's linguistic knowledge is restricted to the lexical level, and word-level representations are not combined into higher-level units (phrases or sentences). Hence, the cortical entrainment results can also be explained from the lexical properties of the stimuli, without recourse to hierarchical syntax.},
  langid = {english},
  keywords = {Fats,Grammar,Linguistic morphology,Psycholinguistics,Semantics,Speech signal processing,Syllables,Syntax},
  file = {/Users/xzfang/Zotero/storage/9NG2LKM2/Frank and Yang - 2018 - Lexical representation explains cortical entrainme.pdf;/Users/xzfang/Zotero/storage/QESE2NTL/article.html}
}

@article{frank_predicting_2012,
  title = {Predicting {{Pragmatic Reasoning}} in {{Language Games}}},
  author = {Frank, Michael C. and Goodman, Noah D.},
  year = {2012},
  month = may,
  journal = {Science},
  volume = {336},
  number = {6084},
  pages = {998--998},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1218633},
  abstract = {One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. We examined judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative and that listeners use Bayesian inference to recover speakers' intended referents. Our model provides a close, parameter-free fit to human judgments, suggesting that the use of information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication. A Bayesian inference model predicts how listeners decode communications. A Bayesian inference model predicts how listeners decode communications.},
  chapter = {Brevia},
  copyright = {Copyright \textcopyright{} 2012, American Association for the Advancement of Science},
  langid = {english},
  pmid = {22628647},
  file = {/Users/xzfang/Zotero/storage/9IGY6JUJ/Frank and Goodman - 2012 - Predicting Pragmatic Reasoning in Language Games.pdf;/Users/xzfang/Zotero/storage/WEGXR6EY/998.html}
}

@inproceedings{frank_word_2013,
  title = {Word Surprisal Predicts {{N400}} Amplitude during Reading},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  year = {2013},
  month = aug,
  pages = {878--883},
  publisher = {{Association for Computational Linguistics}},
  address = {{Sofia, Bulgaria}},
  file = {/Users/xzfang/Zotero/storage/PDVVS87H/Frank et al. - 2013 - Word surprisal predicts N400 amplitude during read.pdf}
}

@article{frankland_architecture_2015,
  title = {An Architecture for Encoding Sentence Meaning in Left Mid-Superior Temporal Cortex},
  author = {Frankland, Steven M. and Greene, Joshua D.},
  year = {2015},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {37},
  pages = {11732--11737},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1421236112},
  abstract = {Human brains flexibly combine the meanings of words to compose structured thoughts. For example, by combining the meanings of ``bite,'' ``dog,'' and ``man,'' we can think about a dog biting a man, or a man biting a dog. Here, in two functional magnetic resonance imaging (fMRI) experiments using multivoxel pattern analysis (MVPA), we identify a region of left mid-superior temporal cortex (lmSTC) that flexibly encodes ``who did what to whom'' in visually presented sentences. We find that lmSTC represents the current values of abstract semantic variables (``Who did it?'' and ``To whom was it done?'') in distinct subregions. Experiment 1 first identifies a broad region of lmSTC whose activity patterns (               i               ) facilitate decoding of structure-dependent sentence meaning (``Who did what to whom?'') and (               ii               ) predict affect-related amygdala responses that depend on this information (e.g., ``the baby kicked the grandfather'' vs. ``the grandfather kicked the baby''). Experiment 2 then identifies distinct, but neighboring, subregions of lmSTC whose activity patterns carry information about the identity of the current ``agent'' (``Who did it?'') and the current ``patient'' (               ``               To whom was it done?''). These neighboring subregions lie along the upper bank of the superior temporal sulcus and the lateral bank of the superior temporal gyrus, respectively. At a high level, these regions may function like topographically defined data registers, encoding the fluctuating values of abstract semantic variables. This functional architecture, which in key respects resembles that of a classical computer, may play a critical role in enabling humans to flexibly generate complex thoughts.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QEACQ6MH/Frankland and Greene - 2015 - An architecture for encoding sentence meaning in l.pdf}
}

@article{frankland_concepts_2020,
  title = {Concepts and {{Compositionality}}: {{In Search}} of the {{Brain}}'s {{Language}} of {{Thought}}},
  shorttitle = {Concepts and {{Compositionality}}},
  author = {Frankland, Steven M. and Greene, Joshua D.},
  year = {2020},
  journal = {Annual Review of Psychology},
  volume = {71},
  number = {1},
  pages = {273--303},
  doi = {10.1146/annurev-psych-122216-011829},
  abstract = {Imagine Genghis Khan, Aretha Franklin, and the Cleveland Cavaliers performing an opera on Maui. This silly sentence makes a serious point: As humans, we can flexibly generate and comprehend an unbounded number of complex ideas. Little is known, however, about how our brains accomplish this. Here we assemble clues from disparate areas of cognitive neuroscience, integrating recent research on language, memory, episodic simulation, and computational models of high-level cognition. Our review is framed by Fodor's classic language of thought hypothesis, according to which our minds employ an amodal, language-like system for combining and recombining simple concepts to form more complex thoughts. Here, we highlight emerging work on combinatorial processes in the brain and consider this work's relation to the language of thought. We review evidence for distinct, but complementary, contributions of map-like representations in subregions of the default mode network and sentence-like representations of conceptual relations in regions of the temporal and prefrontal cortex.},
  pmid = {31550985},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-psych-122216-011829},
  file = {/Users/xzfang/Zotero/storage/WAYJG23Z/Frankland and Greene - 2020 - Concepts and Compositionality In Search of the Br.pdf}
}

@article{frankland_two_2020,
  title = {Two {{Ways}} to {{Build}} a {{Thought}}: {{Distinct Forms}} of {{Compositional Semantic Representation}} across {{Brain Regions}}},
  shorttitle = {Two {{Ways}} to {{Build}} a {{Thought}}},
  author = {Frankland, Steven M and Greene, Joshua D},
  year = {2020},
  month = may,
  journal = {Cerebral Cortex},
  volume = {30},
  number = {6},
  pages = {3838--3855},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhaa001},
  abstract = {To understand a simple sentence such as ``the woman chased the dog'', the human mind must dynamically organize the relevant concepts to represent who did what to whom. This structured recombination of concepts (woman, dog, chased) enables the representation of novel events, and is thus a central feature of intelligence. Here, we use functional magnetic resonance (fMRI) and encoding models to delineate the contributions of three brain regions to the representation of relational combinations. We identify a region of anterior-medial prefrontal cortex (amPFC) that shares representations of noun-verb conjunctions across sentences: for example, a combination of ``woman'' and ``chased'' to encode woman-as-chaser, distinct from woman-as-chasee. This PFC region differs from the left-mid superior temporal cortex (lmSTC) and hippocampus, two regions previously implicated in representing relations. lmSTC represents broad role combinations that are shared across verbs (e.g., woman-as-agent), rather than narrow roles, limited to specific actions (woman-as-chaser). By contrast, a hippocampal sub-region represents events sharing narrow conjunctions as dissimilar. The success of the hippocampal conjunctive encoding model is anti-correlated with generalization performance in amPFC on a trial-by-trial basis, consistent with a pattern separation mechanism. Thus, these three regions appear to play distinct, but complementary, roles in encoding compositional event structure.},
  file = {/Users/xzfang/Zotero/storage/VA38X6Q5/Frankland and Greene - 2020 - Two Ways to Build a Thought Distinct Forms of Com.pdf;/Users/xzfang/Zotero/storage/JBPA6KAE/5818044.html}
}

@article{fraundorf_readers_2016,
  title = {Readers Generalize Adaptation to Newly-Encountered Dialectal Structures to Other Unfamiliar Structures},
  author = {Fraundorf, Scott H. and Jaeger, T. Florian},
  year = {2016},
  month = dec,
  journal = {Journal of memory and language},
  volume = {91},
  pages = {28--58},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2016.05.006},
  abstract = {Growing evidence suggests that syntactic processing may be guided in part by expectations about the statistics of the input that comprehenders have encountered; however, these statistics and even the syntactic structures themselves vary from situation to situation. Some recent work suggests that readers can adapt to variability in the frequencies of known, but infrequent syntactic structures. But, the relation between adaptation to altered frequencies of familiar structures and learning to process unfamiliar, never-before-seen structures is under-explored. In two self-paced reading experiments, we investigated readers' adaptation to an unfamiliar structure used in some regional dialects of American English: the needs+past participle structure, such as using The car needs washed to mean The car needs to be washed. Study 1 used a novel Web-based recruitment method to target regions where participants were likely to be familiar (Ohio, western Pennsylvania) or unfamiliar (Colorado) with the needs+past participle structure. Participants unfamiliar with the structure initially read the structure more slowly, but over the course of the experiment came to read it more like the familiar participants. Study 2 further demonstrated that participants who have adapted to needs+past participle generalize this adaptation to a different, but related structure. These results suggest (a) that readers adapt to unfamiliar syntactic structures, (b) that, in doing so, they become more like existing users of those structures, and (c) that they can generalize this other structures that they may also be more likely to encounter. We discuss these results in the context of implicit learning accounts of exposure effects on syntactic processing.},
  pmcid = {PMC5376074},
  pmid = {28377640},
  file = {/Users/xzfang/Zotero/storage/PE8J2KJK/Fraundorf and Jaeger - 2016 - Readers generalize adaptation to newly-encountered.pdf}
}

@article{freedman_categorical_2001,
  title = {Categorical {{Representation}} of {{Visual Stimuli}} in the {{Primate Prefrontal Cortex}}},
  author = {Freedman, David J. and Riesenhuber, Maximilian and Poggio, Tomaso and Miller, Earl K.},
  year = {2001},
  month = jan,
  journal = {Science},
  volume = {291},
  number = {5502},
  pages = {312--316},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.291.5502.312},
  abstract = {The ability to group stimuli into meaningful categories is a fundamental cognitive process. To explore its neural basis, we trained monkeys to categorize computer-generated stimuli as ``cats'' and ``dogs.'' A morphing system was used to systematically vary stimulus shape and precisely define the category boundary. Neural activity in the lateral prefrontal cortex reflected the category of visual stimuli, even when a monkey was retrained with the stimuli assigned to new categories.},
  chapter = {Report},
  langid = {english},
  pmid = {11209083},
  file = {/Users/xzfang/Zotero/storage/4J6UGV4J/Freedman et al. - 2001 - Categorical Representation of Visual Stimuli in th.pdf;/Users/xzfang/Zotero/storage/HVCMS48A/312.html}
}

@article{fridriksson_revealing_2016,
  title = {Revealing the Dual Streams of Speech Processing},
  author = {Fridriksson, Julius and Yourganov, Grigori and Bonilha, Leonardo and Basilakos, Alexandra and Den Ouden, Dirk-Bart and Rorden, Christopher},
  year = {2016},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {113},
  number = {52},
  pages = {15108--15113},
  issn = {0027-8424},
  doi = {10.1073/pnas.1614038114},
  abstract = {Relatively recently, the concept of dual route neural architecture, where dorsal and ventral brain regions process information synergistically, has been applied to study of speech processing. Although a large body of work has investigated these streams in relation to human speech processing, there is little consensus regarding specific cortical regions implicated. Relying on extensive behavioral and neuroimaging data from a large sample of stroke survivors, we used a data-driven approach to localize regions crucial for motor\textendash phonological and lexical\textendash semantic aspects of speech processing. Results revealed distinct anatomical boundaries between a dorsal frontoparietal stream supporting a form-to-articulation pathway and a ventral temporal\textendash frontal stream supporting a form-to-meaning pathway. This study shows clear division between two processing routes underlying human speech., Several dual route models of human speech processing have been proposed suggesting a large-scale anatomical division between cortical regions that support motor\textendash phonological aspects vs. lexical\textendash semantic aspects of speech processing. However, to date, there is no complete agreement on what areas subserve each route or the nature of interactions across these routes that enables human speech processing. Relying on an extensive behavioral and neuroimaging assessment of a large sample of stroke survivors, we used a data-driven approach using principal components analysis of lesion-symptom mapping to identify brain regions crucial for performance on clusters of behavioral tasks without a priori separation into task types. Distinct anatomical boundaries were revealed between a dorsal frontoparietal stream and a ventral temporal\textendash frontal stream associated with separate components. Collapsing over the tasks primarily supported by these streams, we characterize the dorsal stream as a form-to-articulation pathway and the ventral stream as a form-to-meaning pathway. This characterization of the division in the data reflects both the overlap between tasks supported by the two streams as well as the observation that there is a bias for phonological production tasks supported by the dorsal stream and lexical\textendash semantic comprehension tasks supported by the ventral stream. As such, our findings show a division between two processing routes that underlie human speech processing and provide an empirical foundation for studying potential computational differences that distinguish between the two routes.},
  pmcid = {PMC5206517},
  pmid = {27956600},
  file = {/Users/xzfang/Zotero/storage/KGFIFJTK/Fridriksson et al. - 2016 - Revealing the dual streams of speech processing.pdf}
}

@article{friederici_brain_2002,
  title = {Brain Signatures of Artificial Language Processing: {{Evidence}} Challenging the Critical Period Hypothesis},
  shorttitle = {Brain Signatures of Artificial Language Processing},
  author = {Friederici, Angela D. and Steinhauer, Karsten and Pfeifer, Erdmut},
  year = {2002},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {99},
  number = {1},
  pages = {529--534},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.012611199},
  abstract = {Adult second language learning seems to be more difficult and less efficient than first language acquisition during childhood. By using event-related brain potentials, we show that adults who learned a miniature artificial language display a similar real-time pattern of brain activation when processing this language as native speakers do when processing natural languages. Participants trained in the artificial language showed two event-related brain potential components taken to reflect early automatic and late controlled syntactic processes, whereas untrained participants did not. This result challenges the common view that late second language learners process language in a principally different way from native speakers. Our findings demonstrate that a small system of grammatical rules can be syntactically instantiated by the adult speaker in a way that strongly resembles native-speaker sentence processing.},
  chapter = {Biological Sciences},
  copyright = {Copyright \textcopyright{} 2002, The National Academy of Sciences},
  langid = {english},
  pmid = {11773629},
  file = {/Users/xzfang/Zotero/storage/DE6JWE2W/Friederici et al. - 2002 - Brain signatures of artificial language processing.pdf;/Users/xzfang/Zotero/storage/53K3C66Q/529.html}
}

@article{fries_mechanism_2005,
  title = {A Mechanism for Cognitive Dynamics: Neuronal Communication through Neuronal Coherence},
  shorttitle = {A Mechanism for Cognitive Dynamics},
  author = {Fries, Pascal},
  year = {2005},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {9},
  number = {10},
  pages = {474--480},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2005.08.011},
  abstract = {At any one moment, many neuronal groups in our brain are active. Microelectrode recordings have characterized the activation of single neurons and fMRI has unveiled brain-wide activation patterns. Now it is time to understand how the many active neuronal groups interact with each other and how their communication is flexibly modulated to bring about our cognitive dynamics. I hypothesize that neuronal communication is mechanistically subserved by neuronal coherence. Activated neuronal groups oscillate and thereby undergo rhythmic excitability fluctuations that produce temporal windows for communication. Only coherently oscillating neuronal groups can interact effectively, because their communication windows for input and for output are open at the same times. Thus, a flexible pattern of coherence defines a flexible communication structure, which subserves our cognitive flexibility.},
  langid = {english}
}

@article{friston_theory_2005,
  title = {A Theory of Cortical Responses},
  author = {Friston, Karl},
  year = {2005},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {360},
  number = {1456},
  pages = {815--836},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2005.1622},
  abstract = {This article concerns the nature of evoked brain responses and the principles underlying their generation. We start with the premise that the sensory brain has evolved to represent or infer the causes of changes in its sensory inputs. The problem of inference is well formulated in statistical terms. The statistical fundaments of inference may therefore afford important constraints on neuronal implementation. By formulating the original ideas of Helmholtz on perception, in terms of modern-day statistical theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts. It turns out that the problems of inferring the causes of sensory input (perceptual inference) and learning the relationship between input and cause (perceptual learning) can be resolved using exactly the same principle. Specifically, both inference and learning rest on minimizing the brain's free energy, as defined in statistical physics. Furthermore, inference and learning can proceed in a biologically plausible fashion. Cortical responses can be seen as the brain's attempt to minimize the free energy induced by a stimulus and thereby encode the most likely cause of that stimulus. Similarly, learning emerges from changes in synaptic efficacy that minimize the free energy, averaged over all stimuli encountered. The underlying scheme rests on empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organization and responses. The aim of this article is to encompass many apparently unrelated anatomical, physiological and psychophysical attributes of the brain within a single theoretical perspective. In terms of cortical architectures, the theoretical treatment predicts that sensory cortex should be arranged hierarchically, that connections should be reciprocal and that forward and backward connections should show a functional asymmetry (forward connections are driving, whereas backward connections are both driving and modulatory). In terms of synaptic physiology, it predicts associative plasticity and, for dynamic models, spike-timing-dependent plasticity. In terms of electrophysiology, it accounts for classical and extra classical receptive field effects and long-latency or endogenous components of evoked cortical responses. It predicts the attenuation of responses encoding prediction error with perceptual learning and explains many phenomena such as repetition suppression, mismatch negativity (MMN) and the P300 in electroencephalography. In psychophysical terms, it accounts for the behavioural correlates of these physiological phenomena, for example, priming and global precedence. The final focus of this article is on perceptual learning as measured with the MMN and the implications for empirical studies of coupling among cortical areas using evoked sensory responses.},
  keywords = {Bayesian,cortical,generative models,hierarchical,inference,predictive coding},
  file = {/Users/xzfang/Zotero/storage/STG9NIN9/Friston - 2005 - A theory of cortical responses.pdf}
}

@article{fritsche_bayesian_2020,
  title = {A {{Bayesian}} and Efficient Observer Model Explains Concurrent Attractive and Repulsive History Biases in Visual Perception},
  author = {Fritsche, Matthias and Spaak, Eelke and {de Lange}, Floris P},
  editor = {Serences, John T and Gold, Joshua I and Sheehan, Timothy},
  year = {2020},
  month = jun,
  journal = {eLife},
  volume = {9},
  pages = {e55389},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.55389},
  abstract = {Human perceptual decisions can be repelled away from (repulsive adaptation) or attracted towards recent visual experience (attractive serial dependence). It is currently unclear whether and how these repulsive and attractive biases interact during visual processing and what computational principles underlie these history dependencies. Here we disentangle repulsive and attractive biases by exploring their respective timescales. We find that perceptual decisions are concurrently attracted towards the short-term perceptual history and repelled from stimuli experienced up to minutes into the past. The temporal pattern of short-term attraction and long-term repulsion cannot be captured by an ideal Bayesian observer model alone. Instead, it is well captured by an ideal observer model with efficient encoding and Bayesian decoding of visual information in a slowly changing environment. Concurrent attractive and repulsive history biases in perceptual decisions may thus be the consequence of the need for visual processing to simultaneously satisfy constraints of efficiency and stability.},
  keywords = {Bayesian inference,efficient coding,history biases,sensory adaptation,serial dependence,visual perception},
  file = {/Users/xzfang/Zotero/storage/PB6DKXZE/Fritsche et al. - 2020 - A Bayesian and efficient observer model explains c.pdf}
}

@article{fritz_auditory_2007,
  title = {Auditory Attention\textemdash Focusing the Searchlight on Sound},
  author = {Fritz, Jonathan B and Elhilali, Mounya and David, Stephen V and Shamma, Shihab A},
  year = {2007},
  month = aug,
  journal = {Current Opinion in Neurobiology},
  series = {Sensory Systems},
  volume = {17},
  number = {4},
  pages = {437--455},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2007.07.011},
  abstract = {Some fifty years after the first physiological studies of auditory attention, the field is now ripening, with exciting recent insights into the psychophysics, psychology, and neural basis of auditory attention. Current research seeks to unravel the complex interactions of pre-attentive and attentive processing of the acoustic scene, the role of auditory attention in mediating receptive-field plasticity in both auditory spatial and auditory feature processing, the contrasts and parallels between auditory and visual attention pathways and mechanisms, the interplay of bottom-up and top-down attentional mechanisms, the influential role of attention, goals, and expectations in shaping auditory processing, and the orchestration of diverse attentional effects at multiple levels from the cochlea to the cortex.},
  langid = {english}
}

@article{frost_domain_2015,
  title = {Domain Generality versus Modality Specificity: The Paradox of Statistical Learning},
  shorttitle = {Domain Generality versus Modality Specificity},
  author = {Frost, Ram and Armstrong, Blair C. and Siegelman, Noam and Christiansen, Morten H.},
  year = {2015},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {19},
  number = {3},
  pages = {117--125},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2014.12.010},
  abstract = {Statistical learning (SL) is typically considered to be a domain-general mechanism by which cognitive systems discover the underlying distributional properties of the input. However, recent studies examining whether there are commonalities in the learning of distributional information across different domains or modalities consistently reveal modality and stimulus specificity. Therefore, important questions are how and why a hypothesized domain-general learning mechanism systematically produces such effects. Here, we offer a theoretical framework according to which SL is not a unitary mechanism, but a set of domain-general computational principles that operate in different modalities and, therefore, are subject to the specific constraints characteristic of their respective brain regions. This framework offers testable predictions and we discuss its computational and neurobiological plausibility.},
  langid = {english},
  keywords = {domain-general mechanisms,modality specificity,neurobiologically plausible models,statistical learning,stimulus specificity},
  file = {/Users/xzfang/Zotero/storage/89S6SFDG/Frost et al. - 2015 - Domain generality versus modality specificity the.pdf;/Users/xzfang/Zotero/storage/KSWHYLWT/S1364661314002770.html}
}

@article{fruchter_lexical_2015,
  title = {Lexical {{Preactivation}} in {{Basic Linguistic Phrases}}},
  author = {Fruchter, Joseph and Linzen, Tal and Westerlund, Masha and Marantz, Alec},
  year = {2015},
  month = oct,
  journal = {Journal of Cognitive Neuroscience},
  volume = {27},
  number = {10},
  pages = {1912--1935},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_00822},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WKS84EM5/Fruchter et al. - 2015 - Lexical Preactivation in Basic Linguistic Phrases.pdf}
}

@article{fuglsang_noiserobust_2017,
  title = {Noise-Robust Cortical Tracking of Attended Speech in Real-World Acoustic Scenes},
  author = {Fuglsang, S{\o}ren Asp and Dau, Torsten and Hjortkj{\ae}r, Jens},
  year = {2017},
  month = aug,
  journal = {NeuroImage},
  volume = {156},
  pages = {435--444},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2017.04.026},
  abstract = {Selectively attending to one speaker in a multi-speaker scenario is thought to synchronize low-frequency cortical activity to the attended speech signal. In recent studies, reconstruction of speech from single-trial electroencephalogram (EEG) data has been used to decode which talker a listener is attending to in a two-talker situation. It is currently unclear how this generalizes to more complex sound environments. Behaviorally, speech perception is robust to the acoustic distortions that listeners typically encounter in everyday life, but it is unknown whether this is mirrored by a noise-robust neural tracking of attended speech. Here we used advanced acoustic simulations to recreate real-world acoustic scenes in the laboratory. In virtual acoustic realities with varying amounts of reverberation and number of interfering talkers, listeners selectively attended to the speech stream of a particular talker. Across the different listening environments, we found that the attended talker could be accurately decoded from single-trial EEG data irrespective of the different distortions in the acoustic input. For highly reverberant environments, speech envelopes reconstructed from neural responses to the distorted stimuli resembled the original clean signal more than the distorted input. With reverberant speech, we observed a late cortical response to the attended speech stream that encoded temporal modulations in the speech signal without its reverberant distortion. Single-trial attention decoding accuracies based on 40\textendash 50s long blocks of data from 64 scalp electrodes were equally high (80\textendash 90\% correct) in all considered listening environments and remained statistically significant using down to 10 scalp electrodes and short ({$<$}30-s) unaveraged EEG segments. In contrast to the robust decoding of the attended talker we found that decoding of the unattended talker deteriorated with the acoustic distortions. These results suggest that cortical activity tracks an attended speech signal in a way that is invariant to acoustic distortions encountered in real-life sound environments. Noise-robust attention decoding additionally suggests a potential utility of stimulus reconstruction techniques in attention-controlled brain-computer interfaces.},
  langid = {english},
  keywords = {Acoustic simulations,Auditory attention,Cortical entrainment,Decoding,Delta rhythms,EEG,Speech,Theta rhythms}
}

@article{fukushima_neocognitron_1988,
  title = {Neocognitron: {{A}} Hierarchical Neural Network Capable of Visual Pattern Recognition},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  year = {1988},
  month = jan,
  journal = {Neural Networks},
  volume = {1},
  number = {2},
  pages = {119--130},
  issn = {08936080},
  doi = {10.1016/0893-6080(88)90014-7},
  abstract = {A neural network modelfor visual pattern recognition, called the "neocognitron, "' was previously proposed by the author In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism o f the neocognitron.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZG4LJMQP/Fukushima - 1988 - Neocognitron A hierarchical neural network capabl.pdf}
}

@article{futrell_lossy_2020,
  title = {Lossy-{{Context Surprisal}}: {{An Information}}-{{Theoretic Model}} of {{Memory Effects}} in {{Sentence Processing}}},
  shorttitle = {Lossy-{{Context Surprisal}}},
  author = {Futrell, Richard and Gibson, Edward and Levy, Roger P.},
  year = {2020},
  month = mar,
  journal = {Cognitive Science},
  volume = {44},
  number = {3},
  issn = {0364-0213, 1551-6709},
  doi = {10.1111/cogs.12814},
  abstract = {A key component of research on human sentence processing is to characterize the processing difficulty associated with the comprehension of words in context. Models that explain and predict this difficulty can be broadly divided into two kinds, expectation-based and memory-based. In this work, we present a new model of incremental sentence processing difficulty that unifies and extends key features of both kinds of models. Our model, lossy-context surprisal, holds that the processing difficulty at a word in context is proportional to the surprisal of the word given a lossy memory representation of the context\textemdash that is, a memory representation that does not contain complete information about previous words. We show that this model provides an intuitive explanation for an outstanding puzzle involving interactions of memory and expectations: language-dependent structural forgetting, where the effects of memory on sentence processing appear to be moderated by language statistics. Furthermore, we demonstrate that dependency locality effects, a signature prediction of memory-based theories, can be derived from lossy-context surprisal as a special case of a novel, more general principle called information locality.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DBANMN9L/Futrell et al. - 2020 - Lossyâ€Context Surprisal An Informationâ€Theoretic .pdf}
}

@article{futrell_natural_2021,
  title = {The {{Natural Stories}} Corpus: A Reading-Time Corpus of {{English}} Texts Containing Rare Syntactic Constructions},
  shorttitle = {The {{Natural Stories}} Corpus},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T. and Fedorenko, Evelina},
  year = {2021},
  month = mar,
  journal = {Language Resources and Evaluation},
  volume = {55},
  number = {1},
  pages = {63--77},
  issn = {1574-0218},
  doi = {10.1007/s10579-020-09503-7},
  abstract = {It is now a common practice to compare models of human language processing by comparing how well they predict behavioral and neural measures of processing difficulty, such as reading times, on corpora of rich naturalistic linguistic materials. However, many of these corpora, which are based on naturally-occurring text, do not contain many of the low-frequency syntactic constructions that are often required to distinguish between processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected Penn Treebank-style parse trees and includes self-paced reading time data and aligned audio recordings. We give an overview of the content of the corpus, review recent work using the corpus, and release the data.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3SAUBY5S/Futrell et al. - 2021 - The Natural Stories corpus a reading-time corpus .pdf}
}

@inproceedings{futrell_noisycontext_2017,
  title = {Noisy-Context Surprisal as a Human Sentence Processing Cost Model},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  author = {Futrell, Richard and Levy, Roger},
  year = {2017},
  month = apr,
  pages = {688--698},
  publisher = {{Association for Computational Linguistics}},
  address = {{Valencia, Spain}},
  abstract = {We use the noisy-channel theory of human sentence comprehension to develop an incremental processing cost model that unifies and extends key features of expectation-based and memory-based models. In this model, which we call noisy-context surprisal, the processing cost of a word is the surprisal of the word given a noisy representation of the preceding context. We show that this model accounts for an outstanding puzzle in sentence comprehension, language-dependent structural forgetting effects (Gibson and Thomas, 1999; Vasishth et al., 2010; Frank et al., 2016), which are previously not well modeled by either expectation-based or memory-based approaches. Additionally, we show that this model derives and generalizes locality effects (Gibson, 1998; Demberg and Keller, 2008), a signature prediction of memory-based models. We give corpus-based evidence for a key assumption in this derivation.},
  file = {/Users/xzfang/Zotero/storage/N2MKC32C/Futrell and Levy - 2017 - Noisy-context surprisal as a human sentence proces.pdf}
}

@misc{fyall_dynamic_2017,
  title = {Dynamic Representation of Partially Occluded Objects in Primate Prefrontal and Visual Cortex},
  author = {Fyall, Amber M. and {El-Shamayleh}, Yasmine and Choi, Hannah and {Shea-Brown}, Eric and Pasupathy, Anitha},
  year = {2017},
  month = sep,
  journal = {eLife},
  publisher = {{eLife Sciences Publications Limited}},
  doi = {10.7554/eLife.25784},
  abstract = {Complementary neural codes in frontal and visual cortex support a role for feedback signals in the representation and recognition of partially occluded objects.},
  copyright = {\textcopyright{} 2017 Fyall et al.. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
  howpublished = {https://elifesciences.org/articles/25784/figures},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XFCXND9H/Fyall et al. - 2017 - Dynamic representation of partially occluded objec.pdf;/Users/xzfang/Zotero/storage/86SBYV8R/figures.html}
}

@article{fyshe_lexical_2019,
  title = {The Lexical Semantics of Adjective\textendash Noun Phrases in the Human Brain},
  author = {Fyshe, Alona and Sudre, Gustavo and Wehbe, Leila and Rafidi, Nicole and Mitchell, Tom M.},
  year = {2019},
  journal = {Human Brain Mapping},
  volume = {40},
  number = {15},
  pages = {4457--4469},
  issn = {1097-0193},
  doi = {10.1002/hbm.24714},
  abstract = {As a person reads, the brain performs complex operations to create higher order semantic representations from individual words. While these steps are effortless for competent readers, we are only beginning to understand how the brain performs these actions. Here, we explore lexical semantics using magnetoencephalography (MEG) recordings of people reading adjective\textendash noun phrases presented one word at a time. We track the neural representation of single word representations over time, through different brain regions. Our results reveal two novel findings: (a) a neural representation of the adjective is present during noun presentation, but this representation is different from that observed during adjective presentation and (b) the neural representation of adjective semantics observed during adjective reading is reactivated after phrase reading, with remarkable consistency. We also note that while the semantic representation of the adjective during the reading of the adjective is very distributed, the later representations are concentrated largely to temporal and frontal areas previously associated with composition. Taken together, these results paint a picture of information flow in the brain as phrases are read and understood.},
  copyright = {\textcopyright{} 2019 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {language comprehension,magnetoencephalography,semantic composition,semantic representations},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.24714},
  file = {/Users/xzfang/Zotero/storage/AP74U3SR/Fyshe et al. - 2019 - The lexical semantics of adjectiveâ€“noun phrases in.pdf;/Users/xzfang/Zotero/storage/DXIVM6FL/hbm.html}
}

@article{gabry_visualization_2017,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2017},
  month = sep,
  doi = {10.1111/rssa.12378},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MGYEY6Y3/Gabry et al. - 2017 - Visualization in Bayesian workflow.pdf}
}

@article{gabry_visualization_2019,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  number = {2},
  pages = {389--402},
  issn = {1467-985X},
  doi = {10.1111/rssa.12378},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high dimensional models that are used by applied researchers.},
  langid = {english},
  keywords = {Bayesian data analysis,Statistical graphics,Statistical workflow},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12378},
  file = {/Users/xzfang/Zotero/storage/UW8Y72QA/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf;/Users/xzfang/Zotero/storage/MQDZBBXA/rssa.html}
}

@article{gachkar_chemical_2007,
  title = {Chemical and Biological Characteristics of {{Cuminum}} Cyminum and {{Rosmarinus}} Officinalis Essential Oils},
  author = {Gachkar, Latif and Yadegari, Davood and Rezaei, Mohammad Bagher and Taghizadeh, Masood and Astaneh, Shakiba Alipoor and Rasooli, Iraj},
  year = {2007},
  journal = {Food Chemistry},
  volume = {102},
  number = {3},
  pages = {898--904},
  issn = {0308-8146},
  doi = {10.1016/j.foodchem.2006.06.035},
  abstract = {Essential oils extracted by hydrodistillation from Cuminum cyminum and Rosmarinus officinalis were characterized by means of GC and GC\textendash MS. C. cyminum and R. officinalis contained {$\alpha$}-pinene (29.1\%, 14.9\%), 1,8-cineole (17.9\%, 7.43\%) and linalool (10.4\%, 14.9\%), respectively, as the major compounds. C. cyminum oil exhibited stronger antimicrobial activity than did R. officinalis oil against E. coli, S. aureus and L. monocytogenes. Complete death time on exposure to Cuminum cyminum L. and Rosmarinus officinalis L. oils were 20 and 25 min 180 and 240 min and 90 and 120 min for E. coli, S. aureus and L. monocytogenes, respectively. Radical-scavenging and antioxidant properties were tested by means of 1,1-diphenyl-2-picrylhydrazyl (DPPH) assay and the {$\beta$}-carotene bleaching test. These properties were compared to those of Thymus x-porlock essential oil, used as a reference ingredient. The radical scavenging performance of the rosemary oil was better than that of C. cyminum. Results from the antioxidant test were better than those provided by the radical-scavenging activity. C. cyminum and R. officinalis essential oils may be considered as potent agents in food preservation.},
  keywords = {Antimicrobial,Antioxidant,Cuminum cyminum,E. coli,Essential oil,L. monocytogenes,Radical scavenging,Rosmarinus officinalis,S. aureus},
  file = {/Users/xzfang/Zotero/storage/RBWP2SCA/Gachkar et al. - 2007 - Chemical and biological characteristics of Cuminum.html}
}

@article{gagl_eye_2021,
  title = {Eye Movements during Text Reading Align with the Rate of Speech Production},
  author = {Gagl, Benjamin and Gregorova, Klara and Golch, Julius and Hawelka, Stefan and Sassenhagen, Jona and Tavano, Alessandro and Poeppel, David and Fiebach, Christian J.},
  year = {2021},
  month = dec,
  journal = {Nature Human Behaviour},
  pages = {1--14},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01215-4},
  abstract = {Across languages, the speech signal is characterized by a predominant modulation of the amplitude spectrum between about 4.3 and 5.5\,Hz, reflecting the production and processing of linguistic information chunks (syllables and words) every \textasciitilde 200\,ms. Interestingly, \textasciitilde 200\,ms is also the typical duration of eye fixations during reading. Prompted by this observation, we demonstrate that German readers sample written text at \textasciitilde 5\,Hz. A subsequent meta-analysis of 142 studies from 14 languages replicates this result and shows that sampling frequencies vary across languages between 3.9\,Hz and 5.2\,Hz. This variation systematically depends on the complexity of the writing systems (character-based versus alphabetic systems and orthographic transparency). Finally, we empirically demonstrate a positive correlation between speech spectrum and eye movement sampling in low-skilled non-native readers, with tentative evidence from post hoc analysis suggesting the same relationship in low-skilled native readers. On the basis of this convergent evidence, we propose that during reading, our brain's linguistic processing systems imprint a preferred processing rate\textemdash that is, the rate of spoken language production and perception\textemdash onto the oculomotor system.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Language,Reading,Saccades},
  file = {/Users/xzfang/Zotero/storage/UAYC4IJX/Gagl et al. - 2021 - Eye movements during text reading align with the r.pdf;/Users/xzfang/Zotero/storage/YAJAPG3T/s41562-021-01215-4.html}
}

@article{galantucci_we_2014,
  title = {Do {{We Notice}} When {{Communication Goes Awry}}? {{An Investigation}} of {{People}}'s {{Sensitivity}} to {{Coherence}} in {{Spontaneous Conversation}}},
  shorttitle = {Do {{We Notice}} When {{Communication Goes Awry}}?},
  author = {Galantucci, Bruno and Roberts, Gareth},
  year = {2014},
  month = jul,
  journal = {PLOS ONE},
  volume = {9},
  number = {7},
  pages = {e103182},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0103182},
  abstract = {In the dominant theoretical framework, human communication is modeled as the faithful transmission of information. This implies that when people are involved in communicational exchanges, they should be sensitive to the success with which information is transmitted, easily detecting when conversations lack coherence. The expectation that humans are good at detecting conversational incoherence is in line with common intuition, but there are several reasons to suspect that it might be unrealistic. First, similar intuitions have been shown to be unrealistic for a number of psychological processes. Second, faithful information transmission may conflict with other conversational goals. Third, mechanisms supporting information transmission may themselves lead to cases of incoherence being missed. To ascertain the extent to which people are insensitive to patches of serious conversational incoherence, we generated such patches in the laboratory by repeatedly crossing two unrelated conversations. Across two studies, involving both narrowly and broadly focused conversations, between 27\% and 42\% of the conversants did not notice that their conversations had been crossed. The results of these studies suggest that it may indeed be unrealistic to model spontaneous conversation as faithful information transmission. Rather, our results are more consistent with models of communication that view it as involving noisy and error-prone inferential processes, serving multiple independent goals.},
  langid = {english},
  keywords = {Colleges,Color vision,Communications,Computer software,Experimental psychology,Social communication,Verbal communication},
  file = {/Users/xzfang/Zotero/storage/PNZ8AJ7F/Galantucci and Roberts - 2014 - Do We Notice when Communication Goes Awry An Inve.pdf;/Users/xzfang/Zotero/storage/2YUY935G/article.html}
}

@article{gallant_human_2000,
  title = {A {{Human Extrastriate Area Functionally Homologous}} to {{Macaque V4}}},
  author = {Gallant, Jack L and Shoup, Rachel E and Mazer, James A},
  year = {2000},
  month = aug,
  journal = {Neuron},
  volume = {27},
  number = {2},
  pages = {227--235},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(00)00032-5},
  abstract = {Extrastriate area V4 is crucial for intermediate form vision and visual attention in nonhuman primates. Human neuroimaging suggests that an area in the lingual sulcus/fusiform gyrus may correspond to ventral V4 (V4v). We studied a human neurological patient, AR, with a putative V4v lesion. The lesion does not affect early visual processing (luminance, orientation, and motion perception). However, it does impair hue perception, intermediate form vision, and visual attention in the upper contralateral visual field. Form deficits occur during discrimination of illusory borders, Glass patterns, curvature, and non-Cartesian patterns. Attention deficits occur during discrimination of the relative positions of object parts, detection of low-salience targets, and orientation discrimination in the presence of distractors. This pattern of deficits is consistent with the known properties of area V4 in nonhuman primates, indicating that AR's lesion affects a cortical region functionally homologous to macaque V4.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DML35GFE/Gallant et al. - 2000 - A Human Extrastriate Area Functionally Homologous .pdf;/Users/xzfang/Zotero/storage/7T2AZ238/S0896627300000325.html}
}

@article{galle_development_2014,
  title = {The Development of Voicing Categories: {{A}} Quantitative Review of over 40~Years of Infant Speech Perception Research},
  shorttitle = {The Development of Voicing Categories},
  author = {Galle, Marcus E. and McMurray, Bob},
  year = {2014},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {4},
  pages = {884--906},
  issn = {1531-5320},
  doi = {10.3758/s13423-013-0569-y},
  abstract = {Most research on infant speech categories has relied on measures of discrimination. Such work often employs categorical perception as a linking hypothesis to enable inferences about categorization on the basis of discrimination measures. However, a large number of studies with adults challenge the utility of categorical perception in describing adult speech perception, and this in turn calls into question how to interpret measures of infant speech discrimination. We propose here a parallel channels model of discrimination (built on Pisoni and Tash Perception \& Psychophysics, 15(2), 285\textendash 290, 1974), which posits that both a noncategorical or veridical encoding of speech cues and category representations can simultaneously contribute to discrimination. This can thus produce categorical perception effects without positing any warping of the acoustic signal, but it also reframes how we think about infant discrimination and development. We test this model by conducting a quantitative review of 20 studies examining infants' discrimination of voice onset time contrasts. This review suggests that within-category discrimination is surprisingly prevalent even in classic studies and that, averaging across studies, discrimination is related to continuous acoustic distance. It also identifies several methodological factors that may mask our ability to see this. Finally, it suggests that infant discrimination may improve over development, contrary to commonly held notion of perceptual narrowing. These results are discussed in terms of theories of speech development that may require such continuous sensitivity.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WGJMEFDQ/Galle and McMurray - 2014 - The development of voicing categories A quantitat.pdf}
}

@article{galle_what_2019,
  title = {What Are You Waiting for? {{Real-time}} Integration of Cues for Fricatives Suggests Encapsulated Auditory Memory.},
  shorttitle = {What Are You Waiting For?},
  author = {Galle, Marcus E. and {Klein-Packard}, Jamie and Schreiber, Kayleen and McMurray, Bob},
  year = {2019},
  month = jan,
  journal = {Cognitive science},
  volume = {43},
  number = {1},
  issn = {0364-0213},
  doi = {10.1111/cogs.12700},
  abstract = {Speech unfolds over time and the cues for even a single phoneme are rarely available simultaneously. Consequently, to recognize a single phoneme listeners must integrate material over several hundred milliseconds. Prior work contrasts two accounts: 1) a memory buffer account in which listeners accumulate auditory information in memory and only access higher level representations (i.e., lexical representations) when sufficient information has arrived; and 2) an immediate integration scheme in which lexical representations can be partially activated on the basis of early cues and then updated when more information arises. These studies have uniformly shown evidence for immediate integration for a variety of phonetic distinctions. We attempted to extend this to fricatives, a class of speech sounds which requires not only temporal integration of asynchronous cues (the frication, followed by the formant transitions 150\textendash 350 msec later), but also integration across different frequency bands, and compensation for contextual factors like coarticulation. Eye-movements in the visual world paradigm and showed clear evidence for a memory buffer. Results were replicated in five experiments, ruling out methodological factors and tying the release of the buffer to the onset of the vowel. These findings support a general auditory account for speech by suggesting that the acoustic nature of particular speech sounds may have large effects on how they are processed. It also has major implications for theories of auditory and speech perception by raising the possibility of a encapsulated memory buffers in early auditory processing.},
  pmcid = {PMC6338078},
  pmid = {30648798},
  file = {/Users/xzfang/Zotero/storage/MYGQFMQ4/Galle et al. - 2019 - What are you waiting for Real-time integration of.pdf}
}

@article{gandhi_emergence_2017,
  title = {Emergence of Categorical Face Perception after Extended Early-Onset Blindness},
  author = {Gandhi, Tapan K. and Singh, Amy Kalia and Swami, Piyush and Ganesh, Suma and Sinha, Pawan},
  year = {2017},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {23},
  pages = {6139--6143},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1616050114},
  abstract = {It is unknown whether the ability to visually distinguish between faces and nonfaces is subject to a critical period during development. Would a congenitally blind child who gains sight several years after birth be able to acquire this skill? This question has remained unanswered because of the rarity of cases of late sight onset. We had the opportunity to work with five early-blind individuals who gained sight late in childhood after treatment for dense bilateral cataracts. We tested their ability to categorize patterns as faces, using natural images that spanned a spectrum of face semblance. The results show that newly sighted individuals are unable to distinguish between faces and nonfaces immediately after sight onset, but improve markedly in the following months. These results demonstrate preserved plasticity for acquiring face/nonface categorization ability even late in life, and set the stage for investigating the informational and neural basis of this skill acquisition.},
  copyright = {\textcopyright{}  . http://www.pnas.org/site/misc/userlicense.xhtml},
  langid = {english},
  pmid = {28533387},
  keywords = {blindness,face classification,plasticity,sight restoration,visual learning},
  file = {/Users/xzfang/Zotero/storage/5XLB7XZD/Gandhi et al. - 2017 - Emergence of categorical face perception after ext.pdf;/Users/xzfang/Zotero/storage/6A2MHKXV/6139.html}
}

@article{ganong_phonetic_1980,
  title = {Phonetic Categorization in Auditory Word Perception},
  author = {Ganong, William F.},
  year = {1980},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {6},
  number = {1},
  pages = {110--125},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.6.1.110},
  abstract = {To investigate the interaction in speech perception of auditory information and lexical knowledge (in particular, knowledge of which phonetic sequences are words), acoustic continua varying in voice onset time were constructed so that for each acoustic continuum, 1 of the 2 possible phonetic categorizations made a word and the other did not. For example, one continuum ranged between the word dash and the nonword tash; another used the nonword dask and the word task. In 2 experiments with 35 Ss from an S pool, a significant lexical effect\textemdash a tendency to make phonetic categorizations that make words\textemdash was found. This effect was greater at the phoneme boundary (where auditory information is ambiguous) than at the ends of the continua. Hence the lexical effect must arise at a stage of processing sensitive to both lexical knowledge and auditory information. (30 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Auditory Discrimination,Phonetics,Speech Perception,Words (Phonetic Units)},
  file = {/Users/xzfang/Zotero/storage/U53TQZG5/Ganong - 1980 - Phonetic categorization in auditory word perceptio.pdf;/Users/xzfang/Zotero/storage/RSTD5S7J/1981-07020-001.html}
}

@article{gardner_online_2021,
  title = {Online Pragmatic Interpretations of Scalar Adjectives Are Affected by Perceived Speaker Reliability},
  author = {Gardner, Bethany and Dix, Sadie and Lawrence, Rebecca and Morgan, Cameron and Sullivan, Anaclare and Kurumada, Chigusa},
  year = {2021},
  month = feb,
  journal = {PLOS ONE},
  volume = {16},
  number = {2},
  pages = {e0245130},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0245130},
  abstract = {Linguistic communication requires understanding of words in relation to their context. Among various aspects of context, one that has received relatively little attention until recently is the speakers themselves. We asked whether comprehenders' online language comprehension is affected by the perceived reliability with which a speaker formulates pragmatically well-formed utterances. In two eye-tracking experiments, we conceptually replicated and extended a seminal work by Grodner and Sedivy (2011). A between-participant manipulation was used to control reliability with which a speaker follows implicit pragmatic conventions (e.g., using a scalar adjective in accordance with contextual contrast). Experiment 1 replicated Grodner and Sedivy's finding that contrastive inference in response to scalar adjectives was suspended when both the spoken input and the instructions provided evidence of the speaker's (un)reliability: For speech from the reliable speaker, comprehenders exhibited the early fixations attributable to a contextually-situated, contrastive interpretation of a scalar adjective. In contrast, for speech from the unreliable speaker, comprehenders did not exhibit such early fixations. Experiment 2 provided novel evidence of the reliability effect in the absence of explicit instructions. In both experiments, the effects emerged in the earliest expected time window given the stimuli sentence structure. The results suggest that real-time interpretations of spoken language are optimized in the context of a speaker identity, characteristics of which are extrapolated across utterances.},
  langid = {english},
  keywords = {Behavior,Communications,Eye movements,Language,Learning,Sociolinguistics,Speech,Vision},
  file = {/Users/xzfang/Zotero/storage/566LEYME/Gardner et al. - 2021 - Online pragmatic interpretations of scalar adjecti.pdf;/Users/xzfang/Zotero/storage/QH9W73YX/article.html}
}

@article{garrido_mismatch_2009,
  title = {The Mismatch Negativity: {{A}} Review of Underlying Mechanisms},
  shorttitle = {The Mismatch Negativity},
  author = {Garrido, Marta I. and Kilner, James M. and Stephan, Klaas E. and Friston, Karl J.},
  year = {2009},
  month = mar,
  journal = {Clinical Neurophysiology},
  volume = {120},
  number = {3},
  pages = {453--463},
  issn = {1388-2457},
  doi = {10.1016/j.clinph.2008.11.029},
  abstract = {The mismatch negativity (MMN) is a brain response to violations of a rule, established by a sequence of sensory stimuli (typically in the auditory domain) [N\"a\"at\"anen R. Attention and brain function. Hillsdale, NJ: Lawrence Erlbaum; 1992]. The MMN reflects the brain's ability to perform automatic comparisons between consecutive stimuli and provides an electrophysiological index of sensory learning and perceptual accuracy. Although the MMN has been studied extensively, the neurophysiological mechanisms underlying the MMN are not well understood. Several hypotheses have been put forward to explain the generation of the MMN; amongst these accounts, the ``adaptation hypothesis'' and the ``model adjustment hypothesis'' have received the most attention. This paper presents a review of studies that focus on neuronal mechanisms underlying the MMN generation, discusses the two major explanatory hypotheses, and proposes predictive coding as a general framework that attempts to unify both.},
  pmcid = {PMC2671031},
  pmid = {19181570},
  file = {/Users/xzfang/Zotero/storage/7VCP29UZ/Garrido et al. - 2009 - The mismatch negativity A review of underlying me.pdf}
}

@misc{garvert_hippocampal_2021,
  title = {Hippocampal Spatio-Temporal Cognitive Maps Adaptively Guide Reward Generalization},
  author = {Garvert, Mona M. and Saanum, Tankred and Schulz, Eric and Schuck, Nicolas W. and Doeller, Christian F.},
  year = {2021},
  month = oct,
  pages = {2021.10.22.465012},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.10.22.465012},
  abstract = {The brain forms cognitive maps of relational knowledge, an organizing principle thought to underlie our ability to generalize and make inferences. However, how can a relevant map be selected in situations where a stimulus is embedded in multiple relational structures? Here, we find that both spatial and temporal cognitive maps influence generalization in a choice task, where spatial location determines reward magnitude. Mirroring behavior, the hippocampus not only builds a map of spatial relationships but also encodes temporal distances. As the task progresses, participants' choices become more influenced by spatial relationships, reflected in a strengthening of the spatial and a weakening of the temporal map. This change is driven by orbitofrontal cortex, which represents the evidence that an observed outcome is generated from the spatial rather than the temporal map and updates hippocampal representations accordingly. Taken together, this demonstrates how hippocampal cognitive maps are used and updated flexibly for inference.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MXVIS9T9/Garvert et al. - 2021 - Hippocampal spatio-temporal cognitive maps adaptiv.pdf;/Users/xzfang/Zotero/storage/XX6QF2U4/2021.10.22.465012v1.html}
}

@article{gaser_brain_2003,
  title = {Brain {{Structures Differ}} between {{Musicians}} and {{Non-Musicians}}},
  author = {Gaser, Christian and Schlaug, Gottfried},
  year = {2003},
  month = oct,
  journal = {The Journal of Neuroscience},
  volume = {23},
  number = {27},
  pages = {9240--9245},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-27-09240.2003},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/F4NY3S35/Gaser and Schlaug - 2003 - Brain Structures Differ between Musicians and Non-.pdf}
}

@article{gaspelin_problem_2016,
  title = {The {{Problem}} of {{Latent Attentional Capture}}: {{Easy Visual Search Conceals Capture}} by {{Task-Irrelevant Abrupt Onsets}}},
  shorttitle = {The {{Problem}} of {{Latent Attentional Capture}}},
  author = {Gaspelin, Nicholas and Ruthruff, Eric and Lien, Mei-Ching},
  year = {2016},
  month = aug,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {42},
  number = {8},
  pages = {1104--1120},
  issn = {0096-1523},
  doi = {10.1037/xhp0000214},
  abstract = {Researchers are sharply divided regarding whether irrelevant abrupt onsets capture spatial attention. Numerous studies report that they do and a roughly equal number report that they do not. This puzzle has inspired numerous attempts at reconciliation, none gaining general acceptance. We propose that abrupt onsets routinely capture attention, but the size of observed capture effects depends critically on how long attention dwells on distractor items which, in turn, depends critically on search difficulty. In a series of spatial cuing experiments, we show that irrelevant abrupt onsets produce robust capture effects when visual search is difficult, but not when search is easy. Critically, this effect occurs even when search difficulty varies randomly across trials, preventing any strategic adjustments of the attentional set that could modulate probability of capture by the onset cue. We argue that easy visual search provides an insensitive test for stimulus-driven capture by abrupt onsets: even though onsets truly capture attention, the effects of capture can be latent. This observation helps to explain previous failures to find capture by onsets, nearly all of which employed an easy visual search.},
  pmcid = {PMC4977216},
  pmid = {26854530},
  file = {/Users/xzfang/Zotero/storage/5VSZZJNA/Gaspelin et al. - 2016 - The Problem of Latent Attentional Capture Easy Vi.pdf}
}

@article{gaston_time_2018,
  title = {The Time Course of Contextual Cohort Effects in Auditory Processing of Category-Ambiguous Words: {{MEG}} Evidence for a Single ``Clash'' as Noun or Verb},
  shorttitle = {The Time Course of Contextual Cohort Effects in Auditory Processing of Category-Ambiguous Words},
  author = {Gaston, Phoebe and Marantz, Alec},
  year = {2018},
  month = apr,
  journal = {Language, Cognition and Neuroscience},
  volume = {33},
  number = {4},
  pages = {402--423},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2017.1395466},
  abstract = {The size and probability distribution of a word-form's cohort of lexical competitors influence auditory processing and can be constrained by syntactic category information. This experiment employs noun/verb homonyms (e.g. ``ache'') presented in syntactic context to clarify the mechanisms and representations involved in context-based cohort restriction. Implications for theories positing single versus multiple word-forms in cases of category ambiguity also arise. Using correlations between neural activity in auditory cortex, measured by magnetoencephalography (MEG), and standard and context-dependent cohort entropy and phoneme surprisal variables, we consider the possibility of cohort restriction on the basis of form or on the basis of category usage. Crucially, the form-conditional measure is consistent only with a single word-form view of category ambiguity. Our results show that noun/verb homonyms are derived from single category-neutral word-forms and that the cohort is restricted incrementally in context, by form and then by usage.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZAZ9H59R/Gaston and Marantz - 2018 - The time course of contextual cohort effects in au.pdf}
}

@article{gauthier_does_2018,
  title = {Does the Brain Represent Words? {{An}} Evaluation of Brain Decoding Studies of Language Understanding},
  shorttitle = {Does the Brain Represent Words?},
  author = {Gauthier, Jon and Ivanova, Anna},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.00591 [cs]},
  eprint = {1806.00591},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language decoding studies have identified word representations which can be used to predict brain activity in response to novel words and sentences (Anderson et al., 2016; Pereira et al., 2018). The unspoken assumption of these studies is that, during processing, linguistic information is transformed into some shared semantic space, and those semantic representations are then used for a variety of linguistic and non-linguistic tasks. We claim that current studies vastly underdetermine the content of these representations, the algorithms which the brain deploys to produce and consume them, and the computational tasks which they are designed to solve. We illustrate this indeterminacy with an extension of the sentence-decoding experiment of Pereira et al. (2018), showing how standard evaluations fail to distinguish between language processing models which deploy different mechanisms and which are optimized to solve very different tasks. We conclude by suggesting changes to the brain decoding paradigm which can support stronger claims of neural representation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/V2BBXQSQ/Gauthier and Ivanova - 2018 - Does the brain represent words An evaluation of b.pdf}
}

@article{gauthier_expertise_2000,
  title = {Expertise for Cars and Birds Recruits Brain Areas Involved in Face Recognition},
  author = {Gauthier, Isabel and Skudlarski, Pawel and Gore, John C. and Anderson, Adam W.},
  year = {2000},
  month = feb,
  journal = {Nature Neuroscience},
  volume = {3},
  number = {2},
  pages = {191--197},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/72140},
  abstract = {Expertise with unfamiliar objects (`greebles') recruits face-selective areas in the fusiform gyrus (FFA) and occipital lobe (OFA). Here we extend this finding to other homogeneous categories. Bird and car experts were tested with functional magnetic resonance imaging during tasks with faces, familiar objects, cars and birds. Homogeneous categories activated the FFA more than familiar objects. Moreover, the right FFA and OFA showed significant expertise effects. An independent behavioral test of expertise predicted relative activation in the right FFA for birds versus cars within each group. The results suggest that level of categorization and expertise, rather than superficial properties of objects, determine the specialization of the FFA.},
  copyright = {2000 Nature America Inc.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/Users/xzfang/Zotero/storage/L3GAKA39/Gauthier et al. - 2000 - Expertise for cars and birds recruits brain areas .pdf;/Users/xzfang/Zotero/storage/9EMGVVH4/nn0200_191.html}
}

@article{gauthier_linking_2019,
  title = {Linking Artificial and Human Neural Representations of Language},
  author = {Gauthier, Jon and Levy, Roger},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.01244 [cs, q-bio]},
  eprint = {1910.01244},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Quantitative Biology - Neurons and Cognition},
  file = {/Users/xzfang/Zotero/storage/2HSVV23G/Gauthier and Levy - 2019 - Linking artificial and human neural representation.pdf}
}

@article{gelman_bayesian_2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JWTVK843/Gelman et al. - 2020 - Bayesian Workflow.pdf}
}

@article{gelman_power_2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691614551642},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  keywords = {design calculation,exaggeration ratio,power analysis,replication crisis,statistical significance,Type M error,Type S error},
  file = {/Users/xzfang/Zotero/storage/F6BYN2WK/Gelman and Carlin - 2014 - Beyond Power Calculations Assessing Type S (Sign).pdf}
}

@article{gershman_gradual_2013,
  title = {Gradual Extinction Prevents the Return of Fear: Implications for the Discovery of State},
  shorttitle = {Gradual Extinction Prevents the Return of Fear},
  author = {Gershman, Samuel J. and Jones, Carolyn E. and Norman, Kenneth A. and Monfils, Marie-H. and Niv, Yael},
  year = {2013},
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {7},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2013.00164},
  abstract = {Fear memories are notoriously difficult to erase, often recovering over time. The longstanding explanation for this finding is that, in extinction training, a new memory is formed that competes with the old one for expression but does not otherwise modify it. This explanation is at odds with traditional models of learning such as Rescorla-Wagner and reinforcement learning. A possible reconciliation that was recently suggested is that extinction training leads to the inference of a new state that is different from the state that was in effect in the original training. This solution, however, raises a new question: under what conditions are new states, or new memories formed? Theoretical accounts implicate persistent large prediction errors in this process. As a test of this idea, we reasoned that careful design of the reinforcement schedule during extinction training could reduce these prediction errors enough to prevent the formation of a new memory, while still decreasing reinforcement sufficiently to drive modification of the old fear memory. In two Pavlovian fear-conditioning experiments, we show that gradually reducing the frequency of aversive stimuli, rather than eliminating them abruptly, prevents the recovery of fear. This finding has important implications for theories of state discovery in reinforcement learning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6RKG6VAH/Gershman et al. - 2013 - Gradual extinction prevents the return of fear im.pdf}
}

@book{gerstner_spiking_2002,
  title = {Spiking {{Neuron Models}}: {{Single Neurons}}, {{Populations}}, {{Plasticity}}},
  shorttitle = {Spiking {{Neuron Models}}},
  author = {Gerstner, Wulfram and Kistler, Werner M.},
  year = {2002},
  month = aug,
  publisher = {{Cambridge University Press}},
  abstract = {"Neurons in the brain communicate by short electrical pulses, the so-called action potentials or spikes. How can we understand the process of spike generation? How can we understand information transmission by neurons? What happens if thousands of neurons are coupled together in a seemingly random network? How does the network connectivity determine the activity patterns? And, vice versa, how does the spike activity influence the connectivity pattern? These questions are addressed in this introduction to spiking neurons aimed at those taking courses in computational neuroscience, theoretical biology, biophysics, or neural networks. The approach will suit students of physics, mathematics, or computer science; it will also be useful for biologists who are interested in mathematical modelling. The text is enhanced by many worked examples and illustrations. There are no mathematical prerequisites beyond what the audience would meet as undergraduates: more advanced techniques are introduced in an elementary, concrete fashion when needed."--Publisher's description},
  googlebooks = {Rs4oc7HfxIUC},
  isbn = {978-0-521-89079-3},
  langid = {english},
  keywords = {Computers / Bioinformatics,Computers / Data Science / Neural Networks,Mathematics / Applied,Science / Life Sciences / Biophysics,Science / Life Sciences / Neuroscience}
}

@article{gert_wild_,
  title = {1 {{Wild}} Lab: {{A}} Naturalistic Free Viewing Experiment Reveals 2 Previously Unknown {{EEG}} Signatures of Face Processing.},
  author = {Gert, Anna L and Ehinger, Benedikt V and Timm, Silja and Kietzmann, Tim C},
  pages = {34},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MT58FS4D/Gert et al. - 1 Wild lab A naturalistic free viewing experiment.pdf}
}

@article{gert_wild_2021,
  title = {Wild Lab: {{A}} Naturalistic Free Viewing Experiment Reveals Previously Unknown {{EEG}} Signatures of Face Processing.},
  shorttitle = {Wild Lab},
  author = {Gert, Anna L. and Ehinger, Benedikt V. and Timm, Silja and Kietzmann, Tim C. and Koenig, Peter},
  year = {2021},
  month = jul,
  journal = {bioRxiv},
  pages = {2021.07.02.450779},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.07.02.450779},
  abstract = {{$<$}p{$>$}Neural mechanisms of face perception are predominantly studied in well-controlled experimental settings that involve random stimulus sequences and fixed eye positions. While powerful, the employed paradigms are far from what constitutes natural vision. Here, we demonstrate the feasibility of ecologically more valid experimental paradigms using natural viewing behavior, by combining a free viewing paradigm on natural scenes, free of photographer bias, with advanced data processing techniques that correct for overlap effects and co-varying nonlinear dependencies of multiple eye movement parameters. We validate this approach by replicating classic N170 effects in neural responses, triggered by fixation onsets (fERPs). Importantly, our more natural stimulus paradigm yielded smaller variability between subjects than the classic setup. Moving beyond classic temporal and spatial effect locations, our experiment furthermore revealed previously unknown signatures of face processing. This includes modulation of early fERP components, as well as category-specific adaptation effects across subsequent fixations that emerge even before fixation onset.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8NTKUXUW/Gert et al. - 2021 - Wild lab A naturalistic free viewing experiment r.pdf;/Users/xzfang/Zotero/storage/2DVEHKRA/2021.07.02.html}
}

@article{getz_cortical_2018,
  title = {Cortical Tracking of Constituent Structure in Language Acquisition},
  author = {Getz, Heidi and Ding, Nai and Newport, Elissa L. and Poeppel, David},
  year = {2018},
  month = dec,
  journal = {Cognition},
  volume = {181},
  pages = {135--140},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.08.019},
  abstract = {Linguistic units are organized at multiple levels: words combine to form phrases, which combine to form sentences. Ding, Melloni, Zhang, Tian, and Poeppel (2016) discovered that the brain tracks units at each level of hierarchical structure simultaneously. Such tracking requires knowledge of how words and phrases are structurally related. Here we asked how neural tracking emerges as knowledge of phrase structure is acquired. We recorded electrophysiological (MEG) data while adults listened to a miniature language with distributional cues to phrase structure or to a control language which lacked the crucial distributional cues. Neural tracking of phrases developed rapidly, only in the condition in which participants formed mental representations of phrase structure as measured behaviorally. These results illuminate the mechanisms through which abstract mental representations are acquired and processed by the brain.},
  langid = {english},
  keywords = {Entrainment,Hierarchical structure,MEG,Statistical learning,Syntax},
  file = {/Users/xzfang/Zotero/storage/I4RIDQ2I/Getz et al. - 2018 - Cortical tracking of constituent structure in lang.pdf;/Users/xzfang/Zotero/storage/LVRBHZ7F/S0010027718302312.html}
}

@article{getzmann_switching_2017,
  title = {Switching of Auditory Attention in ``Cocktail-Party'' Listening: {{ERP}} Evidence of Cueing Effects in Younger and Older Adults},
  shorttitle = {Switching of Auditory Attention in ``Cocktail-Party'' Listening},
  author = {Getzmann, Stephan and Jasny, Julian and Falkenstein, Michael},
  year = {2017},
  month = feb,
  journal = {Brain and Cognition},
  volume = {111},
  pages = {1--12},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2016.09.006},
  abstract = {Verbal communication in a ``cocktail-party situation'' is a major challenge for the auditory system. In particular, changes in target speaker usually result in declined speech perception. Here, we investigated whether speech cues indicating a subsequent change in target speaker reduce the costs of switching in younger and older adults. We employed event-related potential (ERP) measures and a speech perception task, in which sequences of short words were simultaneously presented by four speakers. Changes in target speaker were either unpredictable or semantically cued by a word within the target stream. Cued changes resulted in a less decreased performance than uncued changes in both age groups. The ERP analysis revealed shorter latencies in the change-related N400 and late positive complex (LPC) after cued changes, suggesting an acceleration in context updating and attention switching. Thus, both younger and older listeners used semantic cues to prepare changes in speaker setting.},
  langid = {english},
  keywords = {Aging,Attention,Cueing,Event-related potentials,Speech perception},
  file = {/Users/xzfang/Zotero/storage/9IMPBX3B/Getzmann et al. - 2017 - Switching of auditory attention in â€œcocktail-party.pdf;/Users/xzfang/Zotero/storage/N69XLI8H/S0278262616302408.html}
}

@inproceedings{ghorshi_comparative_2006,
  title = {Comparative Analysis of Formants of {{British}}, American and Australian Accents},
  author = {Ghorshi, Seyed and Vaseghi, Saeed and Yan, Qin},
  year = {2006},
  month = sep,
  doi = {10.21437/Interspeech.2006-35},
  abstract = {This paper compares and quantifies the differences between formants of speech across accents. The cross entropy information measure is used to compare the differences between the formants of the vowels of three major English accents namely British, American and Australian. An improved formant estimation method, based on a linear prediction (LP) model feature analysis and a hidden Markov model (HMM) of formants, is employed for estimation of formant trajectories of vowels and diphthongs. Comparative analysis of the formant space of the three accents indicates that these accents are mostly conveyed by the first two formants. The third and fourth formants exhibit some significant differences across accents for only a few phonemes most notably the variants of vowel 'r' in the American (rhotic) accent compared to British (non-rhotic accent). The issue of speaker variability versus accent variability is examined by comparing the cross-entropies of speech models trained on different groups of speakers within and across the accents. Index Terms: accent, formants, cross entropy, speech recognition.},
  file = {/Users/xzfang/Zotero/storage/UI65QZUN/Ghorshi et al. - 2006 - Comparative analysis of formants of British, ameri.pdf}
}

@article{gibson_color_2017,
  title = {Color Naming across Languages Reflects Color Use},
  author = {Gibson, Edward and Futrell, Richard and {Jara-Ettinger}, Julian and Mahowald, Kyle and Bergen, Leon and Ratnasingam, Sivalogeswaran and Gibson, Mitchell and Piantadosi, Steven T. and Conway, Bevil R.},
  year = {2017},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {40},
  pages = {10785--10790},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1619666114},
  abstract = {What determines how languages categorize colors? We analyzed results of the World Color Survey (WCS) of 110 languages to show that despite gross differences across languages, communication of chromatic chips is always better for warm colors (yellows/reds) than cool colors (blues/greens). We present an analysis of color statistics in a large databank of natural images curated by human observers for salient objects and show that objects tend to have warm rather than cool colors. These results suggest that the cross-linguistic similarity in color-naming efficiency reflects colors of universal usefulness and provide an account of a principle (color use) that governs how color categories come about. We show that potential methodological issues with the WCS do not corrupt information-theoretic analyses, by collecting original data using two extreme versions of the color-naming task, in three groups: the Tsimane', a remote Amazonian hunter-gatherer isolate; Bolivian-Spanish speakers; and English speakers. These data also enabled us to test another prediction of the color-usefulness hypothesis: that differences in color categorization between languages are caused by differences in overall usefulness of color to a culture. In support, we found that color naming among Tsimane' had relatively low communicative efficiency, and the Tsimane' were less likely to use color terms when describing familiar objects. Color-naming among Tsimane' was boosted when naming artificially colored objects compared with natural objects, suggesting that industrialization promotes color usefulness.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {28923921},
  keywords = {basic color terms,color categorization,color cognition,information theory,Whorfian hypothesis},
  file = {/Users/xzfang/Zotero/storage/ZXFIB27F/Gibson et al. - 2017 - Color naming across languages reflects color use.pdf;/Users/xzfang/Zotero/storage/INY8PJP4/10785.html}
}

@article{gibson_don_2017,
  title = {Don't {{Underestimate}} the {{Benefits}} of {{Being Misunderstood}}},
  author = {Gibson, Edward and Tan, Caitlin and Futrell, Richard and Mahowald, Kyle and Konieczny, Lars and Hemforth, Barbara and Fedorenko, Evelina},
  year = {2017},
  month = jun,
  journal = {Psychological Science},
  volume = {28},
  number = {6},
  pages = {703--712},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797617690277},
  abstract = {Being a nonnative speaker of a language poses challenges. Individuals often feel embarrassed by the errors they make when talking in their second language. However, here we report an advantage of being a nonnative speaker: Native speakers give foreign-accented speakers the benefit of the doubt when interpreting their utterances; as a result, apparently implausible utterances are more likely to be interpreted in a plausible way when delivered in a foreign than in a native accent. Across three replicated experiments, we demonstrated that native English speakers are more likely to interpret implausible utterances, such as ``the mother gave the candle the daughter,'' as similar plausible utterances (``the mother gave the candle to the daughter'') when the speaker has a foreign accent. This result follows from the general model of language interpretation in a noisy channel, under the hypothesis that listeners assume a higher error rate in foreign-accented than in nonaccented speech.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4KZFT3MG/Gibson et al. - 2017 - Donâ€™t Underestimate the Benefits of Being Misunder.pdf}
}

@article{gibson_how_2019,
  title = {How {{Efficiency Shapes Human Language}}},
  author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  year = {2019},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {5},
  pages = {389--407},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.02.003},
  abstract = {Cognitive science applies diverse tools and perspectives to study human language. Recently, an exciting body of work has examined linguistic phenomena through the lens of efficiency in usage: what otherwise puzzling features of language find explanation in formal accounts of how language might be optimized for communication and learning? Here, we review studies that deploy formal tools from probability and information theory to understand how and why language works the way that it does, focusing on phenomena ranging from the lexicon through syntax. These studies show how a pervasive pressure for efficiency guides the forms of natural language and indicate that a rich future for language research lies in connecting linguistics to cognitive psychology and mathematical theories of communication and inference.},
  langid = {english},
  keywords = {communication,cross-linguistic universals,language complexity,language efficiency,language evolution,language learnability},
  file = {/Users/xzfang/Zotero/storage/MV4UINNF/Gibson et al. - 2019 - How Efficiency Shapes Human Language.pdf;/Users/xzfang/Zotero/storage/37A85Y7M/S1364661319300580.html}
}

@article{gibson_linguistic_1998,
  title = {Linguistic Complexity: Locality of Syntactic Dependencies},
  shorttitle = {Linguistic Complexity},
  author = {Gibson, Edward},
  year = {1998},
  month = aug,
  journal = {Cognition},
  volume = {68},
  number = {1},
  pages = {1--76},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(98)00034-1},
  abstract = {This paper proposes a new theory of the relationship between the sentence processing mechanism and the available computational resources. This theory \textendash{} the Syntactic Prediction Locality Theory (SPLT) \textendash{} has two components: an integration cost component and a component for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is hypothesized to be quantified in terms of the number of syntactic categories that are necessary to complete the current input string as a grammatical sentence. Furthermore, in accordance with results from the working memory literature both memory cost and integration cost are hypothesized to be heavily influenced by locality (1) the longer a predicted category must be kept in memory before the prediction is satisfied, the greater is the cost for maintaining that prediction; and (2) the greater the distance between an incoming word and the most local head or dependent to which it attaches, the greater the integration cost. The SPLT is shown to explain a wide range of processing complexity phenomena not previously accounted for under a single theory, including (1) the lower complexity of subject-extracted relative clauses compared to object-extracted relative clauses, (2) numerous processing overload effects across languages, including the unacceptability of multiply center-embedded structures, (3) the lower complexity of cross-serial dependencies relative to center-embedded dependencies, (4) heaviness effects, such that sentences are easier to understand when larger phrases are placed later and (5) numerous ambiguity effects, such as those which have been argued to be evidence for the Active Filler Hypothesis.},
  langid = {english},
  keywords = {Computational resources,Linguistic complexity,Sentence processing,Syntactic dependency},
  file = {/Users/xzfang/Zotero/storage/R8MYRTPU/Gibson - 1998 - Linguistic complexity locality of syntactic depen.pdf}
}

@book{gibson_processing_2011,
  title = {The {{Processing}} and {{Acquisition}} of {{Reference}}},
  author = {Gibson, Edward A. and Pearlmutter, Neal J.},
  year = {2011},
  month = mar,
  publisher = {{MIT Press}},
  abstract = {How people refer to objects in the world, how people comprehend reference, and how children acquire an understanding of and an ability to use reference.This volume brings together contributions by prominent researchers in the fields of language processing and language acquisition on topics of common interest: how people refer to objects in the world, how people comprehend such referential expressions, and how children acquire the ability to refer and to understand reference. The contributors first discuss issues related to children's acquisition and processing of reference, then consider evidence of adults' processing of reference from eye-tracking methods (the visual-world paradigm) and from corpora and reading experiments. They go on to discuss such topics as how children resolve ambiguity, children's difficulty in understanding coreference, the use of eye movements to physical objects to measure the accessibility of different referents, the uses of probabilistic and pragmatic information in language comprehension, antecedent accessibility and salience in reference, and neuropsychological data from the event-related potential (ERP) recording literature.},
  googlebooks = {RsvxCwAAQBAJ},
  isbn = {978-0-262-29472-0},
  langid = {english},
  keywords = {Language Arts \& Disciplines / Linguistics / Psycholinguistics}
}

@article{gibson_rational_2013,
  title = {Rational Integration of Noisy Evidence and Prior Semantic Expectations in Sentence Interpretation},
  author = {Gibson, Edward and Bergen, Leon and Piantadosi, Steven T.},
  year = {2013},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {20},
  pages = {8051--8056},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1216438110},
  abstract = {Sentence processing theories typically assume that the input to our language processing mechanisms is an error-free sequence of words. However, this assumption is an oversimplification because noise is present in typical language use (for instance, due to a noisy environment, producer errors, or perceiver errors). A complete theory of human sentence comprehension therefore needs to explain how humans understand language given imperfect input. Indeed, like many cognitive systems, language processing mechanisms may even be ``well designed''\textendash in this case for the task of recovering intended meaning from noisy utterances. In particular, comprehension mechanisms may be sensitive to the types of information that an idealized statistical comprehender would be sensitive to. Here, we evaluate four predictions about such a rational (Bayesian) noisy-channel language comprehender in a sentence comprehension task: (i) semantic cues should pull sentence interpretation towards plausible meanings, especially if the wording of the more plausible meaning is close to the observed utterance in terms of the number of edits; (ii) this process should asymmetrically treat insertions and deletions due to the Bayesian ``size principle''; such nonliteral interpretation of sentences should (iii) increase with the perceived noise rate of the communicative situation and (iv) decrease if semantically anomalous meanings are more likely to be communicated. These predictions are borne out, strongly suggesting that human language relies on rational statistical inference over a noisy channel.},
  langid = {english},
  pmid = {23637344},
  keywords = {communication,psycholinguistics,rational inference},
  file = {/Users/xzfang/Zotero/storage/BGM2YCIH/Gibson et al. - 2013 - Rational integration of noisy evidence and prior s.pdf;/Users/xzfang/Zotero/storage/N5IT8WTZ/8051.html}
}

@article{gibson_rational_2016,
  title = {A Rational Inference Approach to Aphasic Language Comprehension},
  author = {Gibson, Edward and Sandberg, Chaleece and Fedorenko, Evelina and Bergen, Leon and Kiran, Swathi},
  year = {2016},
  month = nov,
  journal = {Aphasiology},
  volume = {30},
  number = {11},
  pages = {1341--1360},
  issn = {0268-7038, 1464-5041},
  doi = {10.1080/02687038.2015.1111994},
  abstract = {Background: It has long been observed that, when confronted with an implausible sentence like The ball kicked the girl, individuals with aphasia rely more on plausibility information from world knowledge (such that a girl is likely to kick a ball, but not vice versa) than control non-impaired populations do. We here offer a novel hypothesis to explain this greater reliance on plausibility information for individuals with aphasia. The hypothesis is couched with the rational inference approach to language processing. A key idea in this approach is that to derive an interpretation for an input string, individuals combine their priors (about messages that are likely to be communicated) with their knowledge about how messages can get corrupted by noise (due to production or perception errors). Aims: We hypothesise that language comprehension in aphasia works in the same way, except with a greater amount of noise, which leads to stronger reliance on syntactic and semantic priors. Methods \& Procedures: We evaluated this hypothesis in an act-out task in three groups of participants (8 individuals with aphasia, 7 older controls, 11 younger controls) on two sets of materials: (a) implausible double-object (DO)/prepositional-phrase object (PO) materials, where a single added or deleted word could lead to a plausible meaning; and (b) implausible active-passive materials, where at least two added or deleted words are needed to arrive at a plausible meaning. Outcomes \& Results: We observed that, similar to controls, individuals with aphasia rely on plausibility to a greater extent in the DO/PO than in the active/passive alternation. Critically, however, as predicted, individuals with aphasia rely less on the literal syntax overall than either of the control groups, and use their world knowledge prior (plausibility) in both the active/passive and DO/ PO alternations, whereas controls rely on plausibility only in the DO/PO alternation. In addition, older persons and persons with aphasia made more errors on the DO structures (which are less frequent than PO structures) independent of plausibility, thus providing evidence for reliance on a syntactic prior, the more frequent structure. Conclusions: These results are as predicted by the rational inference approach to language processing in individuals with aphasia.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XLADW852/Gibson et al. - 2016 - A rational inference approach to aphasic language .pdf}
}

@article{gidaris_unsupervised_2018,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.07728 [cs]},
  eprint = {1803.07728},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/55WG56GL/Gidaris et al. - 2018 - Unsupervised Representation Learning by Predicting.pdf;/Users/xzfang/Zotero/storage/G2LC2FJW/1803.html}
}

@article{gidon_dendritic_2020,
  title = {Dendritic Action Potentials and Computation in Human Layer 2/3 Cortical Neurons},
  author = {Gidon, Albert and Zolnik, Timothy Adam and Fidzinski, Pawel and Bolduan, Felix and Papoutsi, Athanasia and Poirazi, Panayiota and Holtkamp, Martin and Vida, Imre and Larkum, Matthew Evan},
  year = {2020},
  month = jan,
  journal = {Science},
  volume = {367},
  number = {6473},
  pages = {83--87},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax6239},
  abstract = {Human dendrites are special A special developmental program in the human brain drives the disproportionate thickening of cortical layer 2/3. This suggests that the expansion of layer 2/3, along with its numerous neurons and their large dendrites, may contribute to what makes us human. Gidon et al. thus investigated the dendritic physiology of layer 2/3 pyramidal neurons in slices taken from surgically resected brain tissue in epilepsy patients. Dual somatodendritic recordings revealed previously unknown classes of action potentials in the dendrites of these neurons, which make their activity far more complex than has been previously thought. These action potentials allow single neurons to solve two long-standing computational problems in neuroscience that were considered to require multilayer neural networks. Science, this issue p. 83 The active electrical properties of dendrites shape neuronal input and output and are fundamental to brain function. However, our knowledge of active dendrites has been almost entirely acquired from studies of rodents. In this work, we investigated the dendrites of layer 2 and 3 (L2/3) pyramidal neurons of the human cerebral cortex ex vivo. In these neurons, we discovered a class of calcium-mediated dendritic action potentials (dCaAPs) whose waveform and effects on neuronal output have not been previously described. In contrast to typical all-or-none action potentials, dCaAPs were graded; their amplitudes were maximal for threshold-level stimuli but dampened for stronger stimuli. These dCaAPs enabled the dendrites of individual human neocortical pyramidal neurons to classify linearly nonseparable inputs\textemdash a computation conventionally thought to require multilayered networks. Dendritic action potentials extend the repertoire of computations available to human neurons. Dendritic action potentials extend the repertoire of computations available to human neurons.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {31896716},
  file = {/Users/xzfang/Zotero/storage/MW4EFERJ/Gidon et al. - 2020 - Dendritic action potentials and computation in hum.pdf;/Users/xzfang/Zotero/storage/U4E8SMMX/83.html}
}

@article{gillis_hearing_2022,
  title = {Hearing Loss Is Associated with Delayed Neural Responses to Continuous Speech},
  author = {Gillis, Marlies and Decruy, Lien and Vanthornhout, Jonas and Francart, Tom},
  year = {2022},
  journal = {European Journal of Neuroscience},
  volume = {55},
  number = {6},
  pages = {1671--1690},
  issn = {1460-9568},
  doi = {10.1111/ejn.15644},
  abstract = {We investigated the impact of hearing loss on the neural processing of speech. Using a forward modelling approach, we compared the neural responses to continuous speech of 14 adults with sensorineural hearing loss with those of age-matched normal-hearing peers. Compared with their normal-hearing peers, hearing-impaired listeners had increased neural tracking and delayed neural responses to continuous speech in quiet. The latency also increased with the degree of hearing loss. As speech understanding decreased, neural tracking decreased in both populations; however, a significantly different trend was observed for the latency of the neural responses. For normal-hearing listeners, the latency increased with increasing background noise level. However, for hearing-impaired listeners, this increase was not observed. Our results support the idea that the neural response latency indicates the efficiency of neural speech processing: More or different brain regions are involved in processing speech, which causes longer communication pathways in the brain. These longer communication pathways hamper the information integration among these brain regions, reflected in longer processing times. Altogether, this suggests decreased neural speech processing efficiency in HI listeners as more time and more or different brain regions are required to process speech. Our results suggest that this reduction in neural speech processing efficiency occurs gradually as hearing deteriorates. From our results, it is apparent that sound amplification does not solve hearing loss. Even when listening to speech in silence at a comfortable loudness, hearing-impaired listeners process speech less efficiently.},
  langid = {english},
  keywords = {EEG,hearing loss,neural tracking,speech},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.15644},
  file = {/Users/xzfang/Zotero/storage/S7XDJWZG/Gillis et al. - 2022 - Hearing loss is associated with delayed neural res.pdf;/Users/xzfang/Zotero/storage/89HCEP49/ejn.html}
}

@article{gillis_neural_2021,
  title = {Neural {{Markers}} of {{Speech Comprehension}}: {{Measuring EEG Tracking}} of {{Linguistic Speech Representations}}, {{Controlling}} the {{Speech Acoustics}}},
  shorttitle = {Neural {{Markers}} of {{Speech Comprehension}}},
  author = {Gillis, Marlies and Vanthornhout, Jonas and Simon, Jonathan Z. and Francart, Tom and Brodbeck, Christian},
  year = {2021},
  month = dec,
  journal = {Journal of Neuroscience},
  volume = {41},
  number = {50},
  pages = {10316--10329},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0812-21.2021},
  abstract = {When listening to speech, our brain responses time lock to acoustic events in the stimulus. Recent studies have also reported that cortical responses track linguistic representations of speech. However, tracking of these representations is often described without controlling for acoustic properties. Therefore, the response to these linguistic representations might reflect unaccounted acoustic processing rather than language processing. Here, we evaluated the potential of several recently proposed linguistic representations as neural markers of speech comprehension. To do so, we investigated EEG responses to audiobook speech of 29 participants (22 females). We examined whether these representations contribute unique information over and beyond acoustic neural tracking and each other. Indeed, not all of these linguistic representations were significantly tracked after controlling for acoustic properties. However, phoneme surprisal, cohort entropy, word surprisal, and word frequency were all significantly tracked over and beyond acoustic properties. We also tested the generality of the associated responses by training on one story and testing on another. In general, the linguistic representations are tracked similarly across different stories spoken by different readers. These results suggests that these representations characterize the processing of the linguistic content of speech. SIGNIFICANCE STATEMENT For clinical applications, it would be desirable to develop a neural marker of speech comprehension derived from neural responses to continuous speech. Such a measure would allow for behavior-free evaluation of speech understanding; this would open doors toward better quantification of speech understanding in populations from whom obtaining behavioral measures may be difficult, such as young children or people with cognitive impairments, to allow better targeted interventions and better fitting of hearing devices.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2021 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {34732519},
  file = {/Users/xzfang/Zotero/storage/SJSWKFLI/Gillis et al. - 2021 - Neural Markers of Speech Comprehension Measuring .pdf;/Users/xzfang/Zotero/storage/5856IFP6/10316.html}
}

@article{giraud_cortical_2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  shorttitle = {Cortical Oscillations and Speech Processing},
  author = {Giraud, Anne-Lise and Poeppel, David},
  year = {2012},
  month = mar,
  journal = {Nature neuroscience},
  volume = {15},
  number = {4},
  pages = {511--517},
  issn = {1097-6256},
  doi = {10.1038/nn.3063},
  abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, `packaging' incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
  pmcid = {PMC4461038},
  pmid = {22426255},
  file = {/Users/xzfang/Zotero/storage/ZU3WJ772/Giraud and Poeppel - 2012 - Cortical oscillations and speech processing emerg.pdf}
}

@misc{giroud_channel_2021,
  title = {The Channel Capacity of Multilevel Linguistic Features Constrains Speech Comprehension},
  author = {Giroud, Jeremy and Lerousseau, Jacques Pesnot and Pellegrino, Francois and Morillon, Benjamin},
  year = {2021},
  month = dec,
  pages = {2021.12.08.471750},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.12.08.471750},
  abstract = {Humans are expert at processing speech but how this feat is accomplished remains a major question in cognitive neuroscience. Capitalizing on the concept of channel capacity, we developed a unified measurement framework to investigate the respective influence of seven acoustic and linguistic features on speech comprehension, encompassing acoustic, sub-lexical, lexical and supra-lexical levels of description. We show that comprehension is independently impacted by all these features, but at varying degrees and with a clear dominance of the syllabic rate. Comparing comprehension of French words and sentences further reveals that when supra-lexical contextual information is present, the impact of all other features is dramatically reduced. Finally, we estimated the channel capacity associated with each linguistic feature and compared them with their generic distribution in natural speech. Our data point towards supra-lexical contextual information as the feature limiting the flow of natural speech. Overall, this study reveals how multilevel linguistic features constrain speech comprehension.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/B7JMCDIN/Giroud et al. - 2021 - The channel capacity of multilevel linguistic feat.pdf;/Users/xzfang/Zotero/storage/AXQ96LJW/2021.12.08.html}
}

@article{glennerster_marr_2007,
  title = {Marr's Vision: {{Twenty-five}} Years On},
  shorttitle = {Marr's Vision},
  author = {Glennerster, Andrew},
  year = {2007},
  month = jun,
  journal = {Current Biology},
  volume = {17},
  number = {11},
  pages = {R397-R399},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2007.03.035},
  abstract = {It is twenty-five years since the posthumous publication of David Marr's book Vision[1]. Only 35 years old when he died, Marr had already dramatically influenced vision research. His book, and the series of papers that preceded it, have had a lasting impact on the way that researchers approach human and computer vision.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9L9MKXYE/Glennerster - 2007 - Marr's vision Twenty-five years on.pdf}
}

@article{goddard_role_,
  ids = {staub_role_2019},
  title = {The Role of Preview Validity in Predictability and Frequency Effects on Eye Movements in Reading},
  author = {Goddard, K.},
  doi = {10/31305},
  abstract = {Epub 2018 Apr 12A word's predictability, as measured by its cloze probability, has a robust influence on the time a reader's eyes spend on the word, with more predictable words receiving shorter fixations. However, several previous studies using the boundary paradigm have found no apparent effect of predictability on early reading time measures when the reader does not have valid parafoveal preview of the target word. The present study directly assesses this pattern in two experiments, demonstrating evidence for a null effect of predictability on first fixation and gaze duration with invalid preview, supported by Bayes factor analyses. While the effect of context independent word frequency is shown to survive with invalid preview, consistent with previous studies, the effect of predictability is eliminated with both unrelated word previews and random letter string previews. These results suggest that a word's predictability influences early stages of orthographic processing, and does so only when perceptual evidence is equivocal, as is the case when the word is initially viewed in parafoveal vision. Word frequency may influence not only early orthographic processing, but also later processing stages. (PsycINFO Database Record (c) 2018 APA, all rights reserved)This work was supported by a grant from the National Science Foundation (BCS 1732008) to the Adrian Stau},
  pmid = {29648870},
  keywords = {Adult,Eye Movements,Fovea Centralis,Humans,Pattern Recognition; Visual,Psycholinguistics,Reading,Young Adult},
  file = {/Users/xzfang/Zotero/storage/CD5TBIN9/Goddard - The role of preview validity in predictability and.pdf;/Users/xzfang/Zotero/storage/ABVANW9R/185553867.html}
}

@article{goetschalckx_image_2018,
  title = {Image Memorability across Longer Time Intervals},
  author = {Goetschalckx, Lore and Moors, Pieter and Wagemans, Johan},
  year = {2018},
  month = may,
  journal = {Memory (Hove, England)},
  volume = {26},
  number = {5},
  pages = {581--588},
  issn = {1464-0686},
  doi = {10.1080/09658211.2017.1383435},
  abstract = {You may find some images easier to remember than others. Recent studies of visual memory have found remarkable levels of consistency for this inter-item variability across observers, suggesting that memorability can be considered an intrinsic image property. The current study replicated and extended previous results, while adopting a more traditional visual long-term memory task with retention intervals of 20 min, one day, and one week, as opposed to the previously used repeat-detection task, which typically relied on short retention intervals (5 min). Our memorability rank scores show levels of consistency across observers in line with those reported in previous research. They correlate strongly with previous quantifications and appear stable over time. Furthermore, we show that the way consistency of memorability scores increases with the number of responses per image follows the Spearman-Brown formula. Interestingly, our results also seem to show an increase in consistency with an increase in retention interval. Supported by simulated data, this effect is attributed to a decrease of extraneous influences on recognition over time. Finally, we also provide evidence for a log-linear, rather than linear, decline of the raw memorability scores over time, with more memorable images declining less strongly.},
  langid = {english},
  pmid = {28974153},
  keywords = {Adolescent,Adult,Female,Humans,Image memorability,long-term memory,Male,Memory,Memory; Long-Term,Middle Aged,Neuropsychological Tests,Photic Stimulation,scenes,time,Time Factors,visual memory,Visual Perception,Young Adult},
  file = {/Users/xzfang/Zotero/storage/KPQ6H7JK/Goetschalckx et al. - 2018 - Image memorability across longer time intervals.pdf}
}

@article{goldinger_echoes_1998,
  title = {Echoes of Echoes? {{An}} Episodic Theory of Lexical Access},
  shorttitle = {Echoes of Echoes?},
  author = {Goldinger, S. D.},
  year = {1998},
  month = apr,
  journal = {Psychological Review},
  volume = {105},
  number = {2},
  pages = {251--279},
  issn = {0033-295X},
  doi = {10.1037/0033-295x.105.2.251},
  abstract = {In this article the author proposes an episodic theory of spoken word representation, perception, and production. By most theories, idiosyncratic aspects of speech (voice details, ambient noise, etc.) are considered noise and are filtered in perception. However, episodic theories suggest that perceptual details are stored in memory and are integral to later perception. In this research the author tested an episodic model (MINERVA 2; D. L. Hintzman, 1986) against speech production data from a word-shadowing task. The model predicted the shadowing-response-time patterns, and it correctly predicted a tendency for shadowers to spontaneously imitate the acoustic patterns of words and nonwords. It also correctly predicted imitation strength as a function of "abstract" stimulus properties, such as word frequency. Taken together, the data and theory suggest that detailed episodes constitute the basic substrate of the mental lexicon.},
  langid = {english},
  pmid = {9577239},
  keywords = {Computer Simulation,Humans,Memory,Speech,Speech Perception,Verbal Learning,Vocabulary},
  file = {/Users/xzfang/Zotero/storage/IQDVIXTQ/Goldinger - 1998 - Echoes of echoes An episodic theory of lexical ac.pdf}
}

@article{goldinger_words_1996,
  title = {Words and Voices: {{Episodic}} Traces in Spoken Word Identification and Recognition Memory.},
  shorttitle = {Words and Voices},
  author = {Goldinger, Stephen D.},
  year = {1996},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {22},
  number = {5},
  pages = {1166},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1285},
  doi = {10.1037/0278-7393.22.5.1166},
  file = {/Users/xzfang/Zotero/storage/QH95M93U/Goldinger - Words and voices Episodic traces in spoken word i.pdf;/Users/xzfang/Zotero/storage/VPSPZWF8/1996-05780-008.html}
}

@misc{goldstein_thinking_2021,
  title = {Thinking Ahead: Spontaneous Prediction in Context as a Keystone of Language in Humans and Machines},
  shorttitle = {Thinking Ahead},
  author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
  year = {2021},
  month = mar,
  pages = {2020.12.02.403477},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.02.403477},
  abstract = {Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). These models are trained to generate appropriate linguistic responses in a given context using a self-supervised prediction task. We provide empirical evidence that the human brain and autoregressive DLMs share two computational principles: 1) both are engaged in continuous prediction; 2) both represent words as a function of the previous context. Behaviorally, we demonstrate a match between humans and DLM's next-word predictions given sufficient contextual windows during the processing of a real-life narrative. Neurally, we demonstrate that the brain, like autoregressive DLMs, constantly predicts upcoming words in natural speech, hundreds of milliseconds before they are perceived. Finally, we show that DLM's contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embeddings. Our findings suggest that autoregressive DLMs provide a novel and biologically feasible computational framework for studying the neural basis of language.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Y4QG2D3Q/Goldstein et al. - 2021 - Thinking ahead spontaneous prediction in context .pdf;/Users/xzfang/Zotero/storage/7KA6RBK9/2020.12.02.403477v3.html}
}

@article{gomez_developmental_2005,
  title = {The {{Developmental Trajectory}} of {{Nonadjacent Dependency Learning}}},
  author = {G{\'o}mez, Rebecca and Maye, Jessica},
  year = {2005},
  journal = {Infancy},
  volume = {7},
  number = {2},
  pages = {183--206},
  issn = {1532-7078},
  doi = {10.1207/s15327078in0702_4},
  abstract = {We investigated the developmental trajectory of nonadjacent dependency learning in an artificial language. Infants were exposed to 1 of 2 artificial languages with utterances of the form [aXc or bXd] (Grammar 1) or [aXd or bXc] (Grammar 2). In both languages, the grammaticality of an utterance depended on the relation between the 1 st and 3rd elements, whereas the intervening element varied freely. High variability of the middle element is known to contribute to perception of nonadjacent dependencies (G\'oomez, 2002), but the developmental trajectory of such learning is unknown. Experiment 1 replicated the study of G\'omez with a younger age group and a more subtle variability manipulation. Twelve-month-olds failed to track nonadjacent dependencies under conditions tested here (Experiments 2a and 2b), but by 15 months, infants are beginning to track this structure (Experiment 3). Such learning has implications for understanding how infants might begin to acquire similar structure in natural language.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15327078in0702\_4},
  file = {/Users/xzfang/Zotero/storage/IQY37TW5/GÃ³mez and Maye - 2005 - The Developmental Trajectory of Nonadjacent Depend.pdf;/Users/xzfang/Zotero/storage/C2WRHSUC/s15327078in0702_4.html}
}

@article{gomez_extensive_2019,
  title = {Extensive Childhood Experience with {{Pok\'emon}} Suggests Eccentricity Drives Organization of Visual Cortex},
  author = {Gomez, Jesse and Barnett, Michael and {Grill-Spector}, Kalanit},
  year = {2019},
  month = jun,
  journal = {Nature Human Behaviour},
  volume = {3},
  number = {6},
  pages = {611--624},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0592-8},
  abstract = {The functional organization of human high-level visual cortex, such as the face- and place-selective regions, is strikingly consistent across individuals. An unanswered question in neuroscience concerns which dimensions of visual information constrain the development and topography of this shared brain organization. To answer this question, we used functional magnetic resonance imaging to scan a unique group of adults who, as children, had extensive visual experience with Pok\'emon. These animal-like, pixelated characters are dissimilar from other ecological categories, such as faces and places, along critical dimensions (foveal bias, rectilinearity, size, animacy). We show not only that adults who have Pok\'emon experience demonstrate distinct distributed cortical responses to Pok\'emon, but also that the experienced retinal eccentricity during childhood can predict the locus of Pok\'emon responses in adulthood. These data demonstrate that inherent functional representations in the visual cortex\textemdash retinal eccentricity\textemdash combined with consistent viewing behaviour of particular stimuli during childhood result in a shared functional topography in adulthood.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Extrastriate cortex;Object vision;Perception Subject\_term\_id: extrastriate-cortex;object-vision;perception},
  file = {/Users/xzfang/Zotero/storage/9QRSCJF5/Gomez et al. - 2019 - Extensive childhood experience with PokÃ©mon sugges.pdf;/Users/xzfang/Zotero/storage/FJAK4BJE/s41562-019-0592-8.html}
}

@article{gomez_mathematical_2015,
  title = {Mathematical Difficulties in Developmental Coordination Disorder: {{Symbolic}} and Nonsymbolic Number Processing},
  shorttitle = {Mathematical Difficulties in Developmental Coordination Disorder},
  author = {Gomez, Alice and Piazza, Manuela and Jobert, Antoinette and {Dehaene-Lambertz}, Ghislaine and Dehaene, Stanislas and Huron, Caroline},
  year = {2015},
  month = aug,
  journal = {Research in Developmental Disabilities},
  volume = {43--44},
  pages = {167--178},
  issn = {08914222},
  doi = {10.1016/j.ridd.2015.06.011},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8QGVXH7R/Gomez et al. - 2015 - Mathematical difficulties in developmental coordin.pdf}
}

@article{gomez_overlap_2008,
  title = {The {{Overlap Model}}: {{A Model}} of {{Letter Position Coding}}},
  shorttitle = {The {{Overlap Model}}},
  author = {Gomez, Pablo and Ratcliff, Roger and Perea, Manuel},
  year = {2008},
  month = jul,
  journal = {Psychological review},
  volume = {115},
  number = {3},
  pages = {577--600},
  issn = {0033-295X},
  doi = {10.1037/a0012667},
  abstract = {Recent research has shown that letter identity and letter position are not integral perceptual dimensions (e.g., jugde primes judge in word-recognition experiments). Most comprehensive computational models of visual word recognition (e.g., the interactive activation model, , and its successors) assume that the position of each letter within a word is perfectly encoded. Thus, these models are unable to explain the presence of effects of letter transposition (trial-trail), letter migration (beard-bread), repeated letters (moose-mouse), or subset/superset effects (faulty-faculty). The authors extend  theory of order relations for encoding of letter positions and show that the model can successfully deal with these effects. The basic assumption is that letters in the visual stimulus have distributions over positions so that the representation of one letter will extend into adjacent letter positions. To test the model, the authors conducted a series of forced-choice perceptual identification experiments. The overlap model produced very good fits to the empirical data, and even a simplified 2-parameter model was capable of producing fits for 104 observed data points with a correlation coefficient of .91.},
  pmcid = {PMC2597794},
  pmid = {18729592},
  file = {/Users/xzfang/Zotero/storage/7WX6RMS7/Gomez et al. - 2008 - The Overlap Model A Model of Letter Position Codi.pdf}
}

@inproceedings{goodkind_predictive_2018,
  title = {Predictive Power of Word Surprisal for Reading Times Is a Linear Function of Language Model Quality},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Cognitive Modeling}} and           {{Computational Linguistics}} ({{CMCL}} 2018)},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2018},
  pages = {10--18},
  publisher = {{Association for Computational Linguistics}},
  address = {{Salt Lake City, Utah}},
  doi = {10.18653/v1/W18-0102},
  abstract = {Within human sentence processing, it is known that there are large effects of a word's probability in context on how long it takes to read it. This relationship has been quantified using informationtheoretic surprisal, or the amount of new information conveyed by a word. Here, we compare surprisals derived from a collection of language models derived from n-grams, neural networks, and a combination of both. We show that the models' psychological predictive power improves as a tight linear function of language model linguistic quality. We also show that the size of the effect of surprisal is estimated consistently across all types of language models. These findings point toward surprising robustness of surprisal estimates and suggest that surprisal estimated by low-quality language models are not biased.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9HBCL9IT/Goodkind and Bicknell - 2018 - Predictive power of word surprisal for reading tim.pdf}
}

@article{goodman_knowledge_2013,
  title = {Knowledge and {{Implicature}}: {{Modeling Language Understanding}} as {{Social Cognition}}},
  shorttitle = {Knowledge and {{Implicature}}},
  author = {Goodman, Noah D. and Stuhlm{\"u}ller, Andreas},
  year = {2013},
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {1},
  pages = {173--184},
  issn = {1756-8765},
  doi = {10.1111/tops.12007},
  abstract = {Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to ``invert'' this model of the speaker. We apply this framework to model scalar implicature (``some'' implies ``not all,'' and ``N'' implies ``not more than N''). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.},
  langid = {english},
  keywords = {Bayesian model,Language,Scalar implicature},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12007},
  file = {/Users/xzfang/Zotero/storage/NB5W4XBL/Goodman and StuhlmÃ¼ller - 2013 - Knowledge and Implicature Modeling Language Under.pdf}
}

@article{gordon_corticospinal_2017,
  title = {Corticospinal Excitability during the Processing of Handwritten and Typed Words and Non-Words},
  author = {Gordon, Chelsea L. and Spivey, Michael J. and Balasubramaniam, Ramesh},
  year = {2017},
  month = jun,
  journal = {Neuroscience Letters},
  volume = {651},
  pages = {232--236},
  issn = {0304-3940},
  doi = {10.1016/j.neulet.2017.05.021},
  abstract = {A number of studies have suggested that perception of actions is accompanied by motor simulation of those actions. To further explore this proposal, we applied Transcranial magnetic stimulation (TMS) to the left primary motor cortex during the observation of handwritten and typed language stimuli, including words and non-word consonant clusters. We recorded motor-evoked potentials (MEPs) from the right first dorsal interosseous (FDI) muscle to measure cortico-spinal excitability during written text perception. We observed a facilitation in MEPs for handwritten stimuli, regardless of whether the stimuli were words or non-words, suggesting potential motor simulation during observation. We did not observe a similar facilitation for the typed stimuli, suggesting that motor simulation was not occurring during observation of typed text. By demonstrating potential simulation of written language text during observation, these findings add to a growing literature suggesting that the motor system plays a strong role in the perception of written language.},
  langid = {english},
  keywords = {Embodied cognition,Language and action,Motor simulation,TMS,Word perception},
  file = {/Users/xzfang/Zotero/storage/UEQMW7UV/Gordon et al. - 2017 - Corticospinal excitability during the processing o.pdf;/Users/xzfang/Zotero/storage/SPBM47S7/S0304394017304093.html}
}

@article{gosselin_superstitious_2003,
  title = {Superstitious {{Perceptions Reveal Properties}} of {{Internal Representations}}},
  author = {Gosselin, Fr{\'e}d{\'e}ric and Schyns, Philippe G.},
  year = {2003},
  month = sep,
  journal = {Psychological Science},
  volume = {14},
  number = {5},
  pages = {505--509},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.03452},
  abstract = {Everyone has seen a human face in a cloud, a pebble, or blots on a wall. Evidence of superstitious perceptions has been documented since classical antiquity, but has received little scientific attention. In the study reported here, we used superstitious perceptions in a new principled method to reveal the properties of unobservable object representations in memory. We stimulated the visual system with unstructured white noise. Observers firmly believed that they perceived the letter S in Experiment 1 and a smile on a face in Experiment 2. Using reverse correlation and computational analyses, we rendered the memory representations underlying these superstitious perceptions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JFB3XZQ3/Gosselin and Schyns - 2003 - Superstitious Perceptions Reveal Properties of Int.pdf}
}

@article{grady_early_2014,
  title = {Early Visual Deprivation from Congenital Cataracts Disrupts Activity and Functional Connectivity in the Face Network},
  author = {Grady, Cheryl L. and Mondloch, Catherine J. and Lewis, Terri L. and Maurer, Daphne},
  year = {2014},
  month = may,
  journal = {Neuropsychologia},
  volume = {57},
  pages = {122--139},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2014.03.005},
  abstract = {The development of the face-processing network has been examined with functional neuroimaging, but the effect of visual deprivation early in life on this network is not known. We examined this question in a group of young adults who had been born with dense, central cataracts in both eyes that blocked all visual input to the retina until the cataracts were removed during infancy. We used functional magnetic resonance imaging to examine regions in the ``core'' and ``extended'' face networks as participants viewed faces and other objects, and performed a face discrimination task. This task required matching faces on the basis of facial features or on the spacing between the facial features. The Cataract group (a) had reduced discrimination performance on the Spacing task relative to Controls; (b) used the same brain regions as Controls when passively viewing faces or making judgments about faces, but showed reduced activation during passive viewing of faces, especially in extended face-network regions; and (c) unlike Controls, showed activation in face-network regions for objects. In addition, the functional connections of the fusiform gyri with the rest of the face network were altered, and these brain changes were related to Cataract participants' performance on the face discrimination task. These results provide evidence that early visual input is necessary to set up or preserve activity and functional connectivity in the face-processing network that will later mediate expert face processing.},
  langid = {english},
  keywords = {Development,Face processing,Functional magnetic resonance imaging,Fusiform gyrus,Visual deprivation},
  file = {/Users/xzfang/Zotero/storage/VJY6NBVK/Grady et al. - 2014 - Early visual deprivation from congenital cataracts.pdf;/Users/xzfang/Zotero/storage/N8Y4SENR/S002839321400089X.html}
}

@article{graf_selective_,
  title = {Selective {{Effects}} of {{Interference}} on {{Implicit}} and {{Explicit Memory}} for {{New Associations}}},
  author = {Graf, Peter and Schacter, Daniel L},
  pages = {9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CNAEAPHJ/Graf and Schacter - Selective Effects of Interference on Implicit and .pdf}
}

@article{grainger_orthographic_2012,
  title = {Orthographic {{Processing}} in {{Baboons}} ({{Papio}} Papio)},
  author = {Grainger, Jonathan and Dufau, St{\'e}phane and Montant, Marie and Ziegler, Johannes C. and Fagot, Jo{\"e}l},
  year = {2012},
  month = apr,
  journal = {Science},
  volume = {336},
  number = {6078},
  pages = {245--248},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1218152},
  abstract = {{$<$}p{$>$}Skilled readers use information about which letters are where in a word (orthographic information) in order to access the sounds and meanings of printed words. We asked whether efficient processing of orthographic information could be achieved in the absence of prior language knowledge. To do so, we trained baboons to discriminate English words from nonsense combinations of letters that resembled real words. The results revealed that the baboons were using orthographic information in order to efficiently discriminate words from letter strings that were not words. Our results demonstrate that basic orthographic processing skills can be acquired in the absence of preexisting linguistic representations.{$<$}/p{$>$}},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2012, American Association for the Advancement of Science},
  langid = {english},
  pmid = {22499949},
  file = {/Users/xzfang/Zotero/storage/33M986EW/Grainger et al. - 2012 - Orthographic Processing in Baboons (Papio papio).pdf;/Users/xzfang/Zotero/storage/TKRDASFI/245.html}
}

@article{grainger_orthographic_2018,
  title = {Orthographic Processing: {{A}} `Mid-Level' Vision of Reading: {{The}} 44th {{Sir Frederic Bartlett Lecture}}},
  shorttitle = {Orthographic Processing},
  author = {Grainger, Jonathan},
  year = {2018},
  month = feb,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {71},
  number = {2},
  pages = {335--359},
  issn = {1747-0218, 1747-0226},
  doi = {10.1080/17470218.2017.1314515},
  abstract = {I will describe how orthographic processing acts as a central interface between visual and linguistic processing during reading, and as such can be considered to be the `mid-level vision' of reading research. In order to make this case, I first summarize the evidence in favour of letter-based word recognition before examining work investigating how orthographic similarities among words influence single word reading. I describe how evidence gradually accumulated against traditional measures of orthographic similarity and the associated theories of orthographic processing, forcing a reconsideration of how letter-position information is represented by skilled readers. Then, I present the theoretical framework that was developed to explain these findings, with a focus on the distinction between location-specific and location-invariant orthographic representations. Finally, I describe work extending this theoretical framework in two main directions: first, to the realm of reading development, with the aim to specify the key changes in the processing of letters and letter strings that accompany successful learning to read, and second, to the realm of sentence reading, in order to specify how orthographic information can be processed across several words in parallel, and how skilled readers keep track of which letters belong to which words.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3JPWJAEI/Grainger - 2018 - Orthographic processing A â€˜mid-levelâ€™ vision of r.pdf}
}

@article{grainger_vision_2016,
  title = {A {{Vision}} of {{Reading}}},
  author = {Grainger, Jonathan and Dufau, St{\'e}phane and Ziegler, Johannes C.},
  year = {2016},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {20},
  number = {3},
  pages = {171--179},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2015.12.008},
  abstract = {Different fields of research within the cognitive sciences have investigated basic processes in reading, but progress has been hampered by limited cross-fertilization. We propose a theoretical framework aimed at facilitating integration of findings obtained via these different approaches with respect to the impact of visual factors on reading. We describe a specialized system for parallel letter processing that assigns letter identities to different locations along the horizontal meridian within the limits imposed by visual acuity and crowding. Spatial attention is used to set up this system during reading development, and difficulty in doing so has repercussions in terms of efficient translation of the orthographic code into its phonological counterpart, and fast access to semantics from print.},
  langid = {english},
  keywords = {dyslexia,eye movements,letter visibility,orthographic processing,reading}
}

@article{grand_semantic_2022,
  title = {Semantic Projection Recovers Rich Human Knowledge of Multiple Object Features from Word Embeddings},
  author = {Grand, Gabriel and Blank, Idan Asher and Pereira, Francisco and Fedorenko, Evelina},
  year = {2022},
  month = apr,
  journal = {Nature Human Behaviour},
  pages = {1--13},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01316-8},
  abstract = {How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts\textemdash that is, are more semantically related\textemdash are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. Here, we use a domain-general method to extract context-dependent relationships from word embeddings: `semantic projection' of word-vectors onto lines that represent features such as size (the line connecting the words `small' and `big') or danger (`safe' to `dangerous'), analogous to `mental scales'. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Language and linguistics,Psychology},
  file = {/Users/xzfang/Zotero/storage/MSPRJDHX/Grand et al. - 2022 - Semantic projection recovers rich human knowledge .pdf}
}

@article{grant_electrophysiology_2021,
  title = {Electrophysiology Reflects the Influence of Discourse Context on Auditory Semantic Processing in Bilinguals},
  author = {Grant, Angela M. and {Brisson-McKenna}, Maude and Phillips, Natalie A.},
  year = {2021},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {47},
  number = {2},
  pages = {343--386},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1285},
  doi = {10.1037/xlm0000813},
  abstract = {Most language experiences take place at the level of multiple sentences. However, previous studies of second language (L2) comprehension have typically focused on lexical- and sentence-level processing. Our study addresses this gap by examining auditory discourse comprehension in 32 English/French bilinguals. We tested the prediction of the noisy channel model (Futrell \& Gibson, 2017) that bilinguals will rely more on top-down, discourse-level cues in L2 because these are common across languages, as opposed to the language-specific associations of an often weaker L2. We further hypothesized that these effects could be influenced by individual differences, such that participants with lower L2 proficiency or working memory would have more difficulty building and maintaining discourse context. Specifically, we measured the N400 response, an index of automatic semantic processing. Participants heard three-sentence stories with prime and target words in the final sentence whose lexical association was manipulated, as was the congruence of the target with the preceding discourse. Overall, our results support the noisy channel model of language comprehension in a sample of highly proficient bilinguals. We observed larger N400 effects of discourse congruence than lexical association, and the difference between these 2 conditions was greater in the L2 than in the L1. Additionally, the effects of lexical association were limited to the L1 and predicted by individual differences in language dominance but not working memory. These findings suggest that bilinguals do indeed make greater use of top-down, supralinguistic information in their L2 compared with their L1. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Bilingualism,Comprehension,Discourse Analysis,Electrophysiology,Individual Differences,Linguistics,Semantics,Sentences,Short Term Memory,Test Construction},
  file = {/Users/xzfang/Desktop/Archive/2020-12729-001.pdf;/Users/xzfang/Zotero/storage/D2BCR6TN/2020-12729-001.html}
}

@article{graumann_spatiotemporal_2022,
  title = {The Spatiotemporal Neural Dynamics of Object Location Representations in the Human Brain},
  author = {Graumann, Monika and Ciuffi, Caterina and Dwivedi, Kshitij and Roig, Gemma and Cichy, Radoslaw M.},
  year = {2022},
  month = feb,
  journal = {Nature Human Behaviour},
  pages = {1--16},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01302-0},
  abstract = {To interact with objects in complex environments, we must know what they are and where they are in spite of challenging viewing conditions. Here, we investigated where, how and when representations of object location and category emerge in the human brain when objects appear on cluttered natural scene images using a combination of functional magnetic resonance imaging, electroencephalography and computational models. We found location representations to emerge along the ventral visual stream towards lateral occipital complex, mirrored by gradual emergence in deep neural networks. Time-resolved analysis suggested that computing object location representations involves recurrent processing in high-level visual cortex. Object category representations also emerged gradually along the ventral visual stream, with evidence for recurrent computations. These results resolve the spatiotemporal dynamics of the ventral visual stream that give rise to representations of where and what objects are present in a scene under challenging viewing conditions. Using electroencephalography and functional magnetic resonance imaging, Graumann et al. examine where, how and when representations of object location and category emerge in the human brain.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Extrastriate cortex,Learning algorithms,Neural decoding,Object vision,Perception},
  file = {/Users/xzfang/Zotero/storage/74NDKTXS/Graumann et al. - 2022 - The spatiotemporal neural dynamics of object locat.pdf;/Users/xzfang/Zotero/storage/LCXZL9X6/s41562-022-01302-0.html}
}

@article{greenberg_anticipation_2015,
  title = {Anticipation of High Arousal Aversive and Positive Movie Clips Engages Common and Distinct Neural Substrates},
  author = {Greenberg, Tsafrir and Carlson, Joshua M. and Rubin, Denis and Cha, Jiook and {Mujica-Parodi}, Lilianne},
  year = {2015},
  month = apr,
  journal = {Social Cognitive and Affective Neuroscience},
  volume = {10},
  number = {4},
  pages = {605--611},
  issn = {1749-5016},
  doi = {10.1093/scan/nsu091},
  abstract = {The neural correlates of anxious anticipation have been primarily studied with aversive and neutral stimuli. In this study, we examined the effect of valence on anticipation by using high arousal aversive and positive stimuli and a condition of uncertainty (i.e. either positive or aversive). The task consisted of predetermined cues warning participants of upcoming aversive, positive, `uncertain' (either aversive or positive) and neutral movie clips. Anticipation of all affective clips engaged common regions including the anterior insula, dorsal anterior cingulate cortex, thalamus, caudate, inferior parietal and prefrontal cortex that are associated with emotional experience, sustained attention and appraisal. In contrast, the nucleus accumbens and medial prefrontal cortex, regions implicated in reward processing, were selectively engaged during anticipation of positive clips (depicting sexually explicit content) and the mid-insula, which has been linked to processing aversive stimuli, was selectively engaged during anticipation of aversive clips (depicting graphic medical procedures); these three areas were also activated during anticipation of `uncertain' clips reflecting a broad preparatory response for both aversive and positive stimuli. These results suggest that a common circuitry is recruited in anticipation of affective clips regardless of valence, with additional areas preferentially engaged depending on whether expected stimuli are negative or positive.},
  pmcid = {PMC4381244},
  pmid = {24984958},
  file = {/Users/xzfang/Zotero/storage/LQNU59MP/Greenberg et al. - 2015 - Anticipation of high arousal aversive and positive.pdf}
}

@article{grefkes_functional_2005,
  title = {The Functional Organization of the Intraparietal Sulcus in Humans and Monkeys},
  author = {Grefkes, Christian and Fink, Gereon R},
  year = {2005},
  month = jul,
  journal = {Journal of Anatomy},
  volume = {207},
  number = {1},
  pages = {3--17},
  issn = {0021-8782},
  doi = {10.1111/j.1469-7580.2005.00426.x},
  abstract = {In macaque monkeys, the posterior parietal cortex (PPC) is concerned with the integration of multimodal information for constructing a spatial representation of the external world (in relation to the macaque's body or parts thereof), and planning and executing object-centred movements. The areas within the intraparietal sulcus (IPS), in particular, serve as interfaces between the perceptive and motor systems for controlling arm and eye movements in space. We review here the latest evidence for the existence of the IPS areas AIP (anterior intraparietal area), VIP (ventral intraparietal area), MIP (medial intraparietal area), LIP (lateral intraparietal area) and CIP (caudal intraparietal area) in macaques, and discuss putative human equivalents as assessed with functional magnetic resonance imaging. The data suggest that anterior parts of the IPS comprising areas AIP and VIP are relatively well preserved across species. By contrast, posterior areas such as area LIP and CIP have been found more medially in humans, possibly reflecting differences in the evolution of the dorsal visual stream and the inferior parietal lobule. Despite interspecies differences in the precise functional anatomy of the IPS areas, the functional relevance of this sulcus for visuomotor tasks comprising target selections for arm and eye movements, object manipulation and visuospatial attention is similar in humans and macaques, as is also suggested by studies of neurological deficits (apraxia, neglect, B\'alint's syndrome) resulting from lesions to this region.},
  pmcid = {PMC1571496},
  pmid = {16011542},
  file = {/Users/xzfang/Zotero/storage/R7UKWNAM/Grefkes and Fink - 2005 - The functional organization of the intraparietal s.pdf}
}

@article{griffiths_understanding_2020,
  title = {Understanding {{Human Intelligence}} through {{Human Limitations}}},
  author = {Griffiths, Thomas L.},
  year = {2020},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {11},
  pages = {873--883},
  issn = {13646613},
  doi = {10.1016/j.tics.2020.09.001},
  abstract = {Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may di er from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/48M7VAL9/Griffiths - 2020 - Understanding Human Intelligence through Human Lim.pdf}
}

@misc{grootswagers_thingseeg_2021,
  title = {{{THINGS-EEG}}: {{Human}} Electroencephalography Recordings for 1,854 Concepts Presented in Rapid Serial Visual Presentation Streams},
  shorttitle = {{{THINGS-EEG}}},
  author = {Grootswagers, Tijl and Zhou, Ivy and Robinson, Amanda K. and Hebart, Martin N. and Carlson, Thomas A.},
  year = {2021},
  month = jun,
  pages = {2021.06.03.447008},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.03.447008},
  abstract = {The neural basis of object recognition and semantic knowledge have been the focus of a large body of research but given the high dimensionality of object space, it is challenging to develop an overarching theory on how brain organises object knowledge. To help understand how the brain allows us to recognise, categorise, and represent objects and object categories, there is a growing interest in using large-scale image databases for neuroimaging experiments. Traditional image databases are based on manually selected object concepts and often single images per concept. In contrast, `big data' stimulus sets typically consist of images that can vary significantly in quality and may be biased in content. To address this issue, recent work developed THINGS: a large stimulus set of 1,854 object concepts and 26,107 associated images. In the current paper, we present THINGS-EEG, a dataset containing human electroencephalography responses from 50 subjects to all concepts and 22,248 images in the THINGS stimulus set. The THINGS-EEG dataset provides neuroimaging recordings to a systematic collection of objects and concepts and can therefore support a wide array of research to understand visual object processing in the human brain.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/X57T937H/Grootswagers et al. - 2021 - THINGS-EEG Human electroencephalography recording.pdf;/Users/xzfang/Zotero/storage/CNX636QS/2021.06.03.html}
}

@article{guenther_effects_1999,
  title = {Effects of Categorization and Discrimination Training  on Auditory Perceptual Space},
  author = {Guenther, Frank H. and Husain, Fatima T. and Cohen, Michael A. and {Shinn-Cunningham}, Barbara G.},
  year = {1999},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {106},
  number = {5},
  pages = {2900--2912},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.428112}
}

@article{guest_simulating_2020,
  title = {On {{Simulating Neural Damage}} in {{Connectionist Networks}}},
  author = {Guest, Olivia and Caso, Andrea and Cooper, Richard P.},
  year = {2020},
  month = sep,
  journal = {Computational Brain \& Behavior},
  volume = {3},
  number = {3},
  pages = {289--321},
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-020-00081-z},
  abstract = {A key strength of connectionist modelling is its ability to simulate both intact cognition and the behavioural effects of neural damage. We survey the literature, showing that models have been damaged in a variety of ways, e.g. by removing connections, by adding noise to connection weights, by scaling weights, by removing units and by adding noise to unit activations. While these different implementations of damage have often been assumed to be behaviourally equivalent, some theorists have made aetiological claims that rest on nonequivalence. They suggest that related deficits with different aetiologies might be accounted for by different forms of damage within a single model. We present two case studies that explore the effects of different forms of damage in two influential connectionist models, each of which has been applied to explain neuropsychological deficits. Our results indicate that the effect of simulated damage can indeed be sensitive to the way in which damage is implemented, particularly when the environment comprises subsets of items that differ in their statistical properties, but such effects are sensitive to relatively subtle aspects of the model's training environment. We argue that, as a consequence, substantial methodological care is required if aetiological claims about simulated neural damage are to be justified, and conclude more generally that implementation assumptions, including those concerning simulated damage, must be fully explored when evaluating models of neurological deficits, both to avoid over-extending the explanatory power of specific implementations and to ensure that reported results are replicable.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4RIZD8E2/Guest et al. - 2020 - On Simulating Neural Damage in Connectionist Netwo.pdf}
}

@article{guggenmos_multivariate_2018,
  title = {Multivariate Pattern Analysis for {{MEG}}: {{A}} Comparison of Dissimilarity Measures},
  shorttitle = {Multivariate Pattern Analysis for {{MEG}}},
  author = {Guggenmos, Matthias and Sterzer, Philipp and Cichy, Radoslaw Martin},
  year = {2018},
  month = jun,
  journal = {NeuroImage},
  volume = {173},
  pages = {434--447},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.02.044},
  abstract = {Multivariate pattern analysis (MVPA) methods such as decoding and representational similarity analysis (RSA) are growing rapidly in popularity for the analysis of magnetoencephalography (MEG) data. However, little is known about the relative performance and characteristics of the specific dissimilarity measures used to describe differences between evoked activation patterns. Here we used a multisession MEG data set to qualitatively characterize a range of dissimilarity measures and to quantitatively compare them with respect to decoding accuracy (for decoding) and between-session reliability of representational dissimilarity matrices (for RSA). We tested dissimilarity measures from a range of classifiers (Linear Discriminant Analysis \textendash{} LDA, Support Vector Machine \textendash{} SVM, Weighted Robust Distance \textendash{} WeiRD, Gaussian Na\"ive Bayes \textendash{} GNB) and distances (Euclidean distance, Pearson correlation). In addition, we evaluated three key processing choices: 1) preprocessing (noise normalisation, removal of the pattern mean), 2) weighting decoding accuracies by decision values, and 3) computing distances in three different partitioning schemes (non-cross-validated, cross-validated, within-class-corrected). Four main conclusions emerged from our results. First, appropriate multivariate noise normalization substantially improved decoding accuracies and the reliability of dissimilarity measures. Second, LDA, SVM and WeiRD yielded high peak decoding accuracies and nearly identical time courses. Third, while using decoding accuracies for RSA was markedly less reliable than continuous distances, this disadvantage was ameliorated by decision-value-weighting of decoding accuracies. Fourth, the cross-validated Euclidean distance provided unbiased distance estimates and highly replicable representational dissimilarity matrices. Overall, we strongly advise the use of multivariate noise normalisation as a general preprocessing step, recommend LDA, SVM and WeiRD as classifiers for decoding and highlight the cross-validated Euclidean distance as a reliable and unbiased default choice for RSA.},
  langid = {english},
  keywords = {Cross-validation,Decoding,EEG,Machine learning,MEG,Multi-voxel pattern analysis,Noise normalisation,Representational similarity analysis},
  file = {/Users/xzfang/Zotero/storage/2J89DPCA/Guggenmos et al. - 2018 - Multivariate pattern analysis for MEG A compariso.pdf;/Users/xzfang/Zotero/storage/H5Q5U7WR/S1053811918301411.html}
}

@article{guggenmos_multivariate_2018a,
  title = {Multivariate Pattern Analysis for {{MEG}}: {{A}} Comparison of Dissimilarity Measures},
  shorttitle = {Multivariate Pattern Analysis for {{MEG}}},
  author = {Guggenmos, Matthias and Sterzer, Philipp and Cichy, Radoslaw Martin},
  year = {2018},
  month = jun,
  journal = {NeuroImage},
  volume = {173},
  pages = {434--447},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.02.044},
  abstract = {Multivariate pattern analysis (MVPA) methods such as decoding and representational similarity analysis (RSA) are growing rapidly in popularity for the analysis of magnetoencephalography (MEG) data. However, little is known about the relative performance and characteristics of the specific dissimilarity measures used to describe differences between evoked activation patterns. Here we used a multisession MEG data set to qualitatively characterize a range of dissimilarity measures and to quantitatively compare them with respect to decoding accuracy (for decoding) and between-session reliability of representational dissimilarity matrices (for RSA). We tested dissimilarity measures from a range of classifiers (Linear Discriminant Analysis \textendash{} LDA, Support Vector Machine \textendash{} SVM, Weighted Robust Distance \textendash{} WeiRD, Gaussian Na\"ive Bayes \textendash{} GNB) and distances (Euclidean distance, Pearson correlation). In addition, we evaluated three key processing choices: 1) preprocessing (noise normalisation, removal of the pattern mean), 2) weighting decoding accuracies by decision values, and 3) computing distances in three different partitioning schemes (non-cross-validated, cross-validated, within-class-corrected). Four main conclusions emerged from our results. First, appropriate multivariate noise normalization substantially improved decoding accuracies and the reliability of dissimilarity measures. Second, LDA, SVM and WeiRD yielded high peak decoding accuracies and nearly identical time courses. Third, while using decoding accuracies for RSA was markedly less reliable than continuous distances, this disadvantage was ameliorated by decision-value-weighting of decoding accuracies. Fourth, the cross-validated Euclidean distance provided unbiased distance estimates and highly replicable representational dissimilarity matrices. Overall, we strongly advise the use of multivariate noise normalisation as a general preprocessing step, recommend LDA, SVM and WeiRD as classifiers for decoding and highlight the cross-validated Euclidean distance as a reliable and unbiased default choice for RSA.},
  langid = {english},
  keywords = {Cross-validation,Decoding,EEG,Machine learning,MEG,Multi-voxel pattern analysis,Noise normalisation,Representational similarity analysis},
  file = {/Users/xzfang/Zotero/storage/CU748BPI/Guggenmos et al. - 2018 - Multivariate pattern analysis for MEG A compariso.pdf;/Users/xzfang/Zotero/storage/M6WCMNIF/S1053811918301411.html}
}

@article{gunter_concerning_1999,
  title = {Concerning the Automaticity of Syntactic Processing},
  author = {Gunter, Thomas C. and Friederici, Angela D.},
  year = {1999},
  journal = {Psychophysiology},
  volume = {36},
  number = {1},
  pages = {126--137},
  issn = {1469-8986},
  doi = {10.1017/S004857729997155X},
  abstract = {In a within-subjects design, event-related potentials were compared for two types of sentence-final syntactic errors: Incorrect verb inflection and incorrect word category (phrase structure). In a grammatical judgment task, these errors triggered robust N400 and P600 components. To assess the degree of automaticity of the underlying linguistic processes, the N400 and P600 effects were measured in a task for which the participants judged whether a word in a sentence was printed in upper case. In this physical judgment task, the N400 and P600 following verb inflection errors were greatly attenuated or absent, whereas those elicited by word category violation were only slightly diminished in amplitude. The data suggest that word category information is processed more automatically than inflectional information. The P600 appears to reflect a relatively controlled language-related process.},
  langid = {english},
  keywords = {Automaticity,ERPs,Language,N400,P3b,P600,Syntax},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1017/S004857729997155X},
  file = {/Users/xzfang/Zotero/storage/NVTITJ9N/Gunter and Friederici - 1999 - Concerning the automaticity of syntactic processin.pdf;/Users/xzfang/Zotero/storage/465XERCK/S004857729997155X.html}
}

@article{guo_corticothalamic_2017,
  title = {A {{Corticothalamic Circuit}} for {{Dynamic Switching}} between {{Feature Detection}} and {{Discrimination}}},
  author = {Guo, Wei and Clause, Amanda R. and {Barth-Maron}, Asa and Polley, Daniel B.},
  year = {2017},
  month = jul,
  journal = {Neuron},
  volume = {95},
  number = {1},
  pages = {180-194.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.05.019},
  abstract = {Sensory processing must be sensitive enough to encode faint signals near the noise floor but selective enough to differentiate between similar stimuli. Here we describe a layer 6 corticothalamic (L6 CT) circuit in the mouse auditory forebrain that alternately biases sound processing toward hypersensitivity and improved behavioral sound detection or dampened excitability and enhanced sound discrimination. Optogenetic activation of L6 CT neurons could increase or decrease the gain and tuning precision in the thalamus and all layers of the cortical column, depending on the timing between L6 CT activation and sensory stimulation. The direction of neural and~perceptual modulation \textendash{} enhanced detection at the~expense of discrimination or vice versa \textendash{} arose from the interaction of L6 CT neurons and subnetworks of fast-spiking inhibitory neurons that reset the phase of low-frequency cortical rhythms. These findings suggest that L6 CT neurons contribute to the resolution of the competing demands of detection and discrimination.},
  langid = {english},
  keywords = {auditory cortex,auditory thalamus,delta rhythm,layer 6,medial geniculate body,modulation,oscillation,phase reset,plasticity,theta rhythm},
  file = {/Users/xzfang/Zotero/storage/WR6I4C9H/Guo et al. - 2017 - A Corticothalamic Circuit for Dynamic Switching be.pdf}
}

@article{gupta_visual_2021,
  title = {Visual {{Search Asymmetry}}: {{Deep Nets}} and {{Humans Share Similar Inherent Biases}}},
  author = {Gupta, Shashi Kant and Zhang, Mengmi and Wu, Chia-Chien and Wolfe, Jeremy M and Kreiman, Gabriel},
  year = {2021},
  pages = {14},
  abstract = {Visual search is a ubiquitous and often challenging daily task, exemplified by looking for the car keys at home or a friend in a crowd. An intriguing property of some classical search tasks is an asymmetry such that finding a target A among distractors B can be easier than finding B among A. To elucidate the mechanisms responsible for asymmetry in visual search, we propose a computational model that takes a target and a search image as inputs and produces a sequence of eye movements until the target is found. The model integrates eccentricity-dependent visual recognition with target-dependent top-down cues. We compared the model against human behavior in six paradigmatic search tasks that show asymmetry in humans. Without prior exposure to the stimuli or task-specific training, the model provides a plausible mechanism for search asymmetry. We hypothesized that the polarity of search asymmetry arises from experience with the natural environment. We tested this hypothesis by training the model on augmented versions of ImageNet where the biases of natural images were either removed or reversed. The polarity of search asymmetry disappeared or was altered depending on the training protocol. This study highlights how classical perceptual properties can emerge in neural network models, without the need for task-specific training, but rather as a consequence of the statistical properties of the developmental diet fed to the model. All source code and data are publicly available at https: //github.com/kreimanlab/VisualSearchAsymmetry.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Y6NVVYAD/Gupta et al. - Visual Search Asymmetry Deep Nets and Humans Shar.pdf}
}

@article{gutierrez-sigut_impact_2022,
  title = {The Impact of Visual Cues during Visual Word Recognition in Deaf Readers: {{An ERP}} Study},
  shorttitle = {The Impact of Visual Cues during Visual Word Recognition in Deaf Readers},
  author = {{Gutierrez-Sigut}, Eva and {Vergara-Mart{\'i}nez}, Marta and Perea, Manuel},
  year = {2022},
  month = jan,
  journal = {Cognition},
  volume = {218},
  pages = {104938},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104938},
  abstract = {Although evidence is still scarce, recent research suggests key differences in how deaf and hearing readers use visual information during visual word recognition. Here we compared the time course of lexical access in deaf and hearing readers of similar reading ability. We also investigated whether one visual property of words, the outline-shape, modulates visual word recognition differently in both groups. We recorded the EEG signal of twenty deaf and twenty hearing readers while they performed a lexical decision task. In addition to the effect of lexicality, we assessed the impact of outline-shape by contrasting responses to pseudowords with an outline-shape that was consistent (e.g., mofor) or inconsistent (e.g., mosor) with their baseword (motor). Despite hearing readers having higher phonological abilities, results showed a remarkably similar time course of the lexicality effect in deaf and hearing readers. We also found that only for deaf readers, inconsistent-shape pseudowords (e.g., mosor) elicited larger amplitude ERPs than consistent-shape pseudowords (e.g., mofor) from 150~ms after stimulus onset and extending into the N400 time window. This latter finding supports the view that deaf readers rely more on visual characteristics than typical hearing readers during visual word recognition. Altogether, our results suggest different mechanisms underlying effective word recognition in deaf and hearing readers.},
  langid = {english},
  keywords = {Deafness,Orthographic processing,Reading,Visual word recognition},
  file = {/Users/xzfang/Zotero/storage/5BX6VSH3/S0010027721003619.html}
}

@article{gutierrez-sigut_tracking_2019,
  title = {Tracking the Time Course of Letter Visual-Similarity Effects during Word Recognition: {{A}} Masked Priming {{ERP}} Investigation},
  shorttitle = {Tracking the Time Course of Letter Visual-Similarity Effects during Word Recognition},
  author = {{Guti{\'e}rrez-Sigut}, Eva and Marcet, Ana and Perea, Manuel},
  year = {2019},
  month = aug,
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  volume = {19},
  number = {4},
  pages = {966--984},
  issn = {1531-135X},
  doi = {10.3758/s13415-019-00696-1},
  abstract = {Visual similarity effects during the early stages of word processing have been consistently found for letter-like digits and symbols. However, despite its relevance for models of word recognition, evidence for letter visual-similarity effects is scarce and restricted to behavioral experiments. In two masked priming experiments, we measured event-related potential (ERP) responses to words preceded by an identical (dentist-DENTIST), a~visually similar (dentjst-DENTIST), or a~visually dissimilar prime (dentgst-DENTIST) to track the time course of the effects of letter visual-similarity during word processing. In the 230- to 350-ms time window, the ERPs in the visual dissimilar condition showed larger negative-going amplitudes than in the visual similar condition, which in turn behaved like the identity condition. In a later time window (400-500 ms), the visually similar condition elicited larger negative-going amplitudes than the identity condition. This pattern of findings can be accommodated within those models of word recognition that assume uncertainty concerning letter identities early in word processing that is resolved over time.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IMS2QFJE/GutiÃ©rrez-Sigut et al. - 2019 - Tracking the time course of letter visual-similari.pdf}
}

@inproceedings{gutwin_peripheral_2017,
  title = {Peripheral {{Popout}}: {{The Influence}} of {{Visual Angle}} and {{Stimulus Intensity}} on {{Popout Effects}}},
  shorttitle = {Peripheral {{Popout}}},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gutwin, Carl and Cockburn, Andy and Coveney, Ashley},
  year = {2017},
  month = may,
  pages = {208--219},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/3025453.3025984},
  abstract = {By exploiting visual popout effects, interface designers can rapidly draw a user's attention to salient information objects in a display. A variety of different visual stimuli can be used to achieve popout effects, including color, shape, size, motion, luminance, and flashing. However, there is a lack of understanding about how accurately different intensities of these effects support popout, particularly as targets move further from the center of the visual field. We therefore conducted a study to examine the accuracy of popout target identification using different visual variables, each at five different levels of intensity, and at a wide range of angles from the display center. Results show that motion is a strong popout stimulus, even at low intensities and wide angles. Identification accuracy decreases rapidly across visual angle with other popout stimuli, particularly with shape and color. The findings have relevance to a wide variety of applications, particularly as multi-display desktop environments increase in size and visual extent.},
  isbn = {978-1-4503-4655-9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XQLGP3JU/Gutwin et al. - 2017 - Peripheral Popout The Influence of Visual Angle a.pdf}
}

@article{gwilliams_extracting_2020,
  title = {Extracting Language Content from Speech Sounds: {{An}} Information Theoretic Approach},
  author = {Gwilliams, Laura and Davis, Matthew},
  year = {2020},
  pages = {17},
  abstract = {Speech comprehension involves recovering the speaker's intended meaning from the speech sounds that they produce. While the sensory-driven components of this process have been widely investigated, the impact of speech content (i.e. linguistic information) on sensory processing is much less understood. Here we summarise the growing body of research demonstrating that neural processing of speech sounds is influenced by morpheme- and word-level statistical properties of the information conveyed. We introduce and review evidence that information theoretic measures such as entropy and surprisal are apparent in neural responses. These findings help uncover fundamental organisational principles of the language system: what units are stored and how they are accessed. Modelling sensitivity to the information content of the speech signal helps explain the interface between (i) auditory processes operating on speech sounds and (ii) the words and meanings that those sounds convey.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QESCHPDB/Gwilliams and Davis - 2020 - Extracting language content from speech sounds An.pdf}
}

@techreport{gwilliams_immediate_2020,
  type = {Preprint},
  title = {Immediate Ambiguity Resolution in Speech Perception Based on Prior Acoustic Experience},
  author = {Gwilliams, Laura and Wallisch, Pascal},
  year = {2020},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/yptfr},
  abstract = {Speech perception relies on the rapid resolution of uncertainty. Here we explore whether auditory experiences contribute to this process of ambiguity resolution. \textasciitilde 8000 participants were surveyed online for their (i) subjective percept of a speech stimulus with ambiguous formant allocation; (ii) demographic profile and auditory experiences. Both linguistic and non-linguistic auditory experiences significantly predict speech perception. Listeners were more likely to perceive the ambiguous stimulus in accordance with their own name, and were biased towards lower formant allocation as a function of being exposed to lower auditory frequencies in their environment. Overall, our results show that the subjective interpretation of an ambiguous stimulus in the auditory domain is determined by prior acoustic exposure, suggesting the operation of an exposure-dependent mechanism impacting sensitivity that resolves ambiguity in speech perception.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JU2SZQQL/Gwilliams and Wallisch - 2020 - Immediate ambiguity resolution in speech perceptio.pdf}
}

@article{gwilliams_morphological_2018,
  title = {Morphological Representations Are Extrapolated from Morpho-Syntactic Rules},
  author = {Gwilliams, L. and Marantz, A.},
  year = {2018},
  month = jun,
  journal = {Neuropsychologia},
  volume = {114},
  pages = {77--87},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2018.04.015},
  abstract = {The field of psycho- and neuro-linguistics has long-debated the decompositional model of visual word processing: Are written words processed via the visual forms of stem and affix morphemes, or as complex wholes? Although many have now settled upon a decompositional view, it is unclear what heuristic the brain uses to generate these visual morpheme-forms in the first place. Here we conduct a magneto-encephalography study to test two hypotheses for how this may be done: i) the brain encodes representations of the morphemes that follow the morpho-syntactic rules governing constituents: A stem morpheme will be represented if the word obeys the grammatical behaviour associated with its suffix; ii) the brain only encodes stem morphemes that occur with multiple suffixes or as words in isolation. Our results indicate that words with morpho-syntactic wellformedness as stem-suffix combinations are decomposed by the system, thus supporting the former hypothesis. This suggests that knowledge of morpho-syntactic rules can be used to form morphological representations of written words, in absence of independent experience with all of their constituent morphemes. Possible mechanisms supporting this computation are discussed.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2WMTNPNT/Gwilliams and Marantz - 2018 - Morphological representations are extrapolated fro.pdf}
}

@misc{gwilliams_neural_2020,
  title = {Neural Dynamics of Phoneme Sequencing in Real Speech Jointly Encode Order and Invariant Content},
  author = {Gwilliams, Laura and King, Jean-Remi and Marantz, Alec and Poeppel, David},
  year = {2020},
  month = apr,
  pages = {2020.04.04.025684},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.04.04.025684},
  abstract = {Listeners experience speech as a sequence of discrete words. However, the real input is a continuously varying acoustic signal that blends words and phonemes into one another. Here we recorded two-hour magnetoencephalograms from 21 subjects listening to stories, in order to investigate how the brain concurrently solves three competing demands: 1) processing overlapping acoustic-phonetic information while 2) keeping track of the relative order of phonemic units and 3) maintaining individuated phonetic information until successful word recognition. We show that the human brain transforms speech input, roughly at the rate of phoneme duration, along a temporally-defined representational trajectory. These representations, absent from the acoustic signal, are active earlier when phonemes are predictable than when they are surprising, and are sustained until lexical ambiguity is resolved. The results reveal how phoneme sequences in natural speech are represented and how they interface with stored lexical items. One sentence summary The human brain keeps track of the relative order of speech sound sequences by jointly encoding content and elapsed processing time},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LNS4FXIV/Gwilliams et al. - 2020 - Neural dynamics of phoneme sequencing in real spee.pdf;/Users/xzfang/Zotero/storage/L9THG35R/2020.04.04.025684v1.html}
}

@article{gwilliams_recurrent_2020,
  title = {Recurrent Processes Support a Cascade of Hierarchical Decisions},
  author = {Gwilliams, Laura and King, Jean-Remi},
  editor = {Serre, Thomas and Frank, Michael J},
  year = {2020},
  month = sep,
  journal = {eLife},
  volume = {9},
  pages = {e56603},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.56603},
  abstract = {Perception depends on a complex interplay between feedforward and recurrent processing. Yet, while the former has been extensively characterized, the computational organization of the latter remains largely unknown. Here, we use magneto-encephalography to localize, track and decode the feedforward and recurrent processes of reading, as elicited by letters and digits whose level of ambiguity was parametrically manipulated. We first confirm that a feedforward response propagates through the ventral and dorsal pathways within the first 200 ms. The subsequent activity is distributed across temporal, parietal and prefrontal cortices, which sequentially generate five levels of representations culminating in action-specific motor signals. Our decoding analyses reveal that both the content and the timing of these brain responses are best explained by a hierarchy of recurrent neural assemblies, which both maintain and broadcast increasingly rich representations. Together, these results show how recurrent processes generate, over extended time periods, a cascade of decisions that ultimately accounts for subjects' perceptual reports and reaction times.},
  keywords = {decoding,human brain,magnetoencephalography,perceptual decision making,reading,recurrence},
  file = {/Users/xzfang/Zotero/storage/RI3DWKZL/Gwilliams and King - 2020 - Recurrent processes support a cascade of hierarchi.pdf}
}

@article{gwilliams_spoken_2018,
  title = {In {{Spoken Word Recognition}}, the {{Future Predicts}} the {{Past}}},
  author = {Gwilliams, Laura and Linzen, Tal and Poeppel, David and Marantz, Alec},
  year = {2018},
  month = aug,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {35},
  pages = {7585--7599},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0065-18.2018},
  abstract = {Speech is an inherently noisy and ambiguous signal. To fluently derive meaning, a listener must integrate contextual information to guide interpretations of the sensory input. Although many studies have demonstrated the influence of prior context on speech perception, the neural mechanisms supporting the integration of subsequent context remain unknown. Using MEG to record from human auditory cortex, we analyzed responses to spoken words with a varyingly ambiguous onset phoneme, the identity of which is later disambiguated at the lexical uniqueness point. Fifty participants (both male and female) were recruited across two MEG experiments. Our findings suggest that primary auditory cortex is sensitive to phonological ambiguity very early during processing at just 50 ms after onset. Subphonemic detail is preserved in auditory cortex over long timescales and re-evoked at subsequent phoneme positions. Commitments to phonological categories occur in parallel, resolving on the shorter timescale of {$\sim$}450 ms. These findings provide evidence that future input determines the perception of earlier speech sounds by maintaining sensory features until they can be integrated with top-down lexical information. SIGNIFICANCE STATEMENT The perception of a speech sound is determined by its surrounding context in the form of words, sentences, and other speech sounds. Often, such contextual information becomes available later than the sensory input. The present study is the first to unveil how the brain uses this subsequent information to aid speech comprehension. Concretely, we found that the auditory system actively maintains the acoustic signal in auditory cortex while concurrently making guesses about the identity of the words being said. Such a processing strategy allows the content of the message to be accessed quickly while also permitting reanalysis of the acoustic signal to minimize parsing mistakes.},
  copyright = {Copyright \textcopyright{} 2018 the authors 0270-6474/18/387585-15\$15.00/0},
  langid = {english},
  pmid = {30012695},
  keywords = {auditory processing,lexical access,MEG,speech},
  file = {/Users/xzfang/Zotero/storage/ZF85SCQL/Gwilliams et al. - 2018 - In Spoken Word Recognition, the Future Predicts th.pdf;/Users/xzfang/Zotero/storage/MRUUJ2GW/7585.html}
}

@article{hafri_perception_2021,
  title = {The {{Perception}} of {{Relations}}},
  author = {Hafri, Alon and Firestone, Chaz},
  year = {2021},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2021.01.006},
  abstract = {The world contains not only objects and features (red apples, glass bowls, wooden tables), but also relations holding between them (apples contained in bowls, bowls supported by tables). Representations of these relations are often developmentally precocious and linguistically privileged; but how does the mind extract them in the first place? Although relations themselves cast no light onto our eyes, a growing body of work suggests that even very sophisticated relations display key signatures of automatic visual processing. Across physical, eventive, and social domains, relations such as support, fit, cause, chase, and even socially interact are extracted rapidly, are impossible to ignore, and influence other perceptual processes. Sophisticated and structured relations are not only judged and understood, but also seen \textemdash{} revealing surprisingly rich content in visual perception itself.},
  langid = {english},
  keywords = {compositionality,core cognition,intuitive physics,role-binding,structured representations,visual psychophysics}
}

@misc{hafri_where_2022,
  title = {Where Word and World Meet: {{Language}} and Vision Share an Abstract Representation of Symmetry},
  shorttitle = {Where Word and World Meet},
  author = {Hafri, Alon and Gleitman, Lila R. and Landau, Barbara and Trueswell, John},
  year = {2022},
  month = jan,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/ahf32},
  abstract = {Symmetry is ubiquitous in nature, in logic and mathematics, and in perception, language, and thought. Although humans are exquisitely sensitive to visual symmetry (e.g., of a butterfly), symmetry in natural language goes beyond visuospatial properties: many words point to abstract concepts with symmetrical content (e.g., equal, marry). For example, if Mark marries Bill, then Bill marries Mark. In both cases (vision and language), symmetry may be formally characterized as invariance under transformation. Is this a coincidence, or is there some deeper psychological resemblance? Here we asked whether representations of symmetry correspond across language and vision. To do so, we developed a novel cross-modal matching paradigm. On each trial, participants observed a visual stimulus (either symmetrical or non-symmetrical) and had to choose between a symmetrical and non-symmetrical English predicate unrelated to the stimulus (e.g., "negotiate" vs. "propose"). In a first study with visual events (symmetrical collision or asymmetrical launch), participants reliably chose the predicate matching the event's symmetry. A second study showed that this "language-vision correspondence" generalized to objects and was weakened when the stimuli's binary nature was made less apparent (i.e., of one object, rather than two inward-facing objects). A final study showed the same effect when nonsigners guessed English translations of signs from American Sign Language, which expresses many symmetrical concepts spatially. Taken together, our findings support the existence of an abstract representation of symmetry which humans access via both perceptual and linguistic means. More broadly, this work sheds light on the rich, structured nature of the language-cognition interface.},
  langid = {american},
  keywords = {abstract language,Cognitive Psychology,Concepts and Categories,conceptual structure,cross-modal,Language,language-cognition interface,Linguistics,Psycholinguistics and Neurolinguistics,Semantics and Pragmatics,Social and Behavioral Sciences,visual relations},
  file = {/Users/xzfang/Zotero/storage/W9AYMLTZ/Hafri et al. - 2022 - Where word and world meet Language and vision sha.pdf}
}

@article{hafting_microstructure_2005,
  title = {Microstructure of a Spatial Map in the Entorhinal Cortex},
  author = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I.},
  year = {2005},
  month = aug,
  journal = {Nature},
  volume = {436},
  number = {7052},
  pages = {801--806},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature03721},
  abstract = {The ability to find one's way depends on neural algorithms that integrate information about place, distance and direction, but the implementation of these operations in cortical microcircuits is poorly understood. Here we show that the dorsocaudal medial entorhinal cortex (dMEC) contains a directionally oriented, topographically organized neural map of the spatial environment. Its key unit is the `grid cell', which is activated whenever the animal's position coincides with any vertex of a regular grid of equilateral triangles spanning the surface of the environment. Grids of neighbouring cells share a common orientation and spacing, but their vertex locations (their phases) differ. The spacing and size of individual fields increase from dorsal to ventral dMEC. The map is anchored to external landmarks, but persists in their absence, suggesting that grid cells may be part of a generalized, path-integration-based map of the spatial environment.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/59KQ3LYT/Hafting et al. - 2005 - Microstructure of a spatial map in the entorhinal .pdf;/Users/xzfang/Zotero/storage/W66YU3BI/nature03721.html}
}

@article{hagiwara_github_2019,
  title = {{{GitHub Typo Corpus}}: {{A Large-Scale Multilingual Dataset}} of {{Misspellings}} and {{Grammatical Errors}}},
  shorttitle = {{{GitHub Typo Corpus}}},
  author = {Hagiwara, Masato and Mita, Masato},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.12893 [cs]},
  eprint = {1911.12893},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The lack of large-scale datasets has been a major hindrance to the development of NLP tasks such as spelling correction and grammatical error correction (GEC). As a complementary new resource for these tasks, we present the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, a large and popular platform for hosting and sharing git repositories. The dataset, which we have made publicly available, contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date. We also describe our process for filtering true typo edits based on learned classifiers on a small annotated subset, and demonstrate that typo edits can be identified with F 1 {$\sim$} 0.9 using a very simple classifier with only three features. The detailed analyses of the dataset show that existing spelling correctors merely achieve an F-measure of approx. 0.5, suggesting that the dataset serves as a new, rich source of spelling errors that complement existing datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/CNASKU3A/Hagiwara and Mita - 2019 - GitHub Typo Corpus A Large-Scale Multilingual Dat.pdf}
}

@article{hagoort_erp_2000,
  title = {{{ERP}} Effects of Listening to Speech Compared to Reading: The {{P600}}/{{SPS}} to Syntactic Violations in Spoken Sentences and Rapid Serial Visual Presentation},
  author = {Hagoort, Peter and Brown, Colin M},
  year = {2000},
  pages = {19},
  abstract = {In this study, event-related brain potential eÂ€ects of speech processing are obtained and compared to similar eÂ€ects in sentence reading. In two experiments sentences were presented that contained three diÂ€erent types of grammatical violations. In one experiment sentences were presented word by word at a rate of four words per second. The grammatical violations elicited a Syntactic Positive Shift (P600/SPS), 500 ms after the onset of the word that rendered the sentence ungrammatical. The P600/SPS consisted of two phases, an early phase with a relatively equal anterior{$\pm$}posterior distribution and a later phase with a strong posterior distribution. We interpret the \textregistered rst phase as an indication of structural integration complexity, and the second phase as an indication of failing parsing operations and/or an attempt at reanalysis. In the second experiment the same syntactic violations were presented in sentences spoken at a normal rate and with normal intonation. These violations elicited a P600/SPS with the same onset as was observed for the reading of these sentences. In addition two of the three violations showed a preceding frontal negativity, most clearly over the left hemisphere. 7 2000 Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/U3Z4C3PC/Hagoort and Brown - 2000 - ERP eÂ€ects of listening to speech compared to read.pdf}
}

@inproceedings{hale_probabilistic_2001,
  title = {A Probabilistic Earley Parser as a Psycholinguistic Model},
  booktitle = {Second Meeting of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}} on {{Language}} Technologies 2001  - {{NAACL}} '01},
  author = {Hale, John},
  year = {2001},
  pages = {1--8},
  publisher = {{Association for Computational Linguistics}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.3115/1073336.1073357},
  abstract = {In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word wi given its prefix w0...i-1 on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke's probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2E3BPN23/Hale - 2001 - A probabilistic earley parser as a psycholinguisti.pdf}
}

@misc{halvagal_combination_2022,
  title = {The Combination of {{Hebbian}} and Predictive Plasticity Learns Invariant Object Representations in Deep Sensory Networks},
  author = {Halvagal, Manu Srinath and Zenke, Friedemann},
  year = {2022},
  month = mar,
  pages = {2022.03.17.484712},
  institution = {{bioRxiv}},
  doi = {10.1101/2022.03.17.484712},
  abstract = {Discriminating distinct objects and concepts from sensory stimuli is essential for survival. Our brains accomplish this feat by forming meaningful internal representations in deep sensory networks with plastic synaptic connections. Experience-dependent plasticity presumably exploits temporal contingencies between sensory inputs to build these internal representations. However, the precise mechanisms underlying plasticity remain elusive. We derive a local synaptic plasticity model inspired by self-supervised machine learning techniques that shares a deep conceptual connection to Bienenstock-Cooper-Munro (BCM) theory and is consistent with experimentally observed plasticity rules. We show that our plasticity model yields disentangled object representations in deep neural networks without the need for supervision and implausible negative examples. In response to altered visual experience, our model qualitatively captures neuronal selectivity changes observed in the monkey inferotemporal cortex in-vivo. Our work suggests a plausible learning rule to drive learning in sensory networks while making concrete testable predictions.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IUYAT5J9/Halvagal and Zenke - 2022 - The combination of Hebbian and predictive plastici.pdf;/Users/xzfang/Zotero/storage/Y9M7UPNQ/2022.03.17.484712v1.html}
}

@article{hamann_intact_,
  title = {Intact {{Perceptual Memory}} in the {{Absence}} of {{Conscious Memory}}},
  author = {Hamann, Stephan B and Squire, Larry R},
  pages = {5},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8IP8PH2G/Hamann and Squire - Intact Perceptual Memory in the Absence of Conscio.pdf}
}

@article{hamilton_parallel_2021,
  title = {Parallel and Distributed Encoding of Speech across Human Auditory Cortex},
  author = {Hamilton, Liberty S. and Oganian, Yulia and Hall, Jeffery and Chang, Edward F.},
  year = {2021},
  month = aug,
  journal = {Cell},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2021.07.019},
  abstract = {Speech perception is thought to rely on a cortical feedforward serial transformation of acoustic into linguistic representations. Using intracranial recordings across the entire human auditory cortex, electrocortical stimulation, and surgical ablation, we show that cortical processing across areas is not consistent with a serial hierarchical organization. Instead, response latency and receptive field analyses demonstrate parallel and distinct information processing in the primary and nonprimary auditory cortices. This functional dissociation was also observed where stimulation of the primary auditory cortex evokes auditory hallucination but does not distort or interfere with speech perception. Opposite effects were observed during stimulation of nonprimary cortex in superior temporal gyrus. Ablation of the primary auditory cortex does not affect speech perception. These results establish a distributed functional organization of parallel information processing throughout the human auditory cortex and demonstrate an essential independent role for nonprimary auditory cortex in speech processing.},
  langid = {english},
  keywords = {auditory cortex,cortical stimulation,electrocorticography,Heschl's gyrus,intracranial recordings,speech,superior temporal gyrus},
  file = {/Users/xzfang/Zotero/storage/5PDMBDVB/Hamilton et al. - 2021 - Parallel and distributed encoding of speech across.pdf;/Users/xzfang/Zotero/storage/4YEYVRX4/S0092867421008783.html}
}

@misc{hamilton_topography_2020,
  title = {Topography of Speech-Related Acoustic and Phonological Feature Encoding throughout the Human Core and Parabelt Auditory Cortex},
  author = {Hamilton, Liberty S. and Oganian, Yulia and Chang, Edward F.},
  year = {2020},
  month = jun,
  pages = {2020.06.08.121624},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.06.08.121624},
  abstract = {Speech perception involves the extraction of acoustic and phonological features from the speech signal. How those features map out across the human auditory cortex is unknown. Complementary to noninvasive imaging, the high spatial and temporal resolution of intracranial recordings has greatly contributed to recent advances in our understanding. However, these approaches are typically limited by piecemeal sampling of the expansive human temporal lobe auditory cortex. Here, we present a functional characterization of local cortical encoding throughout all major regions of the primary and non-primary human auditory cortex. We overcame previous limitations by using rare direct recordings from the surface of the temporal plane after surgical microdissection of the deep Sylvian fissure between the frontal and temporal lobes. We recorded neural responses using simultaneous high-density direct recordings over the left temporal plane and the lateral superior temporal gyrus, while participants listened to natural speech sentences and pure tone stimuli. We found an anatomical separation of simple spectral feature tuning, including tuning for pure tones and absolute pitch, on the superior surface of the temporal plane, and complex tuning for phonological features, relative pitch and speech amplitude modulations on lateral STG. Broadband onset responses are unique to posterior STG and not found elsewhere in auditory cortices. This onset region is functionally distinct from the rest of STG, with latencies similar to primary auditory areas. These findings reveal a new, detailed functional organization of response selectivity to acoustic and phonological features in speech throughout the human auditory cortex.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/W6R3HYUZ/Hamilton et al. - 2020 - Topography of speech-related acoustic and phonolog.pdf;/Users/xzfang/Zotero/storage/ETY5238N/2020.06.08.121624v1.html}
}

@article{han_scale_2020,
  title = {Scale and Translation-Invariance for Novel Objects in Human Vision},
  author = {Han, Yena and Roig, Gemma and Geiger, Gad and Poggio, Tomaso},
  year = {2020},
  month = jan,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {1411},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-57261-6},
  abstract = {Though the range of invariance in recognition of novel objects is a basic aspect of human vision, its characterization has remained surprisingly elusive. Here we report tolerance to scale and position changes in one-shot learning by measuring recognition accuracy of Korean letters presented in a flash to non-Korean subjects who had no previous experience with Korean letters. We found that humans have significant scale-invariance after only a single exposure to a novel object. The range of translation-invariance is limited, depending on the size and position of presented objects. To understand the underlying brain computation associated with the invariance properties, we compared experimental data with computational modeling results. Our results suggest that to explain invariant recognition of objects by humans, neural network models should explicitly incorporate built-in scale-invariance, by encoding different scale channels as well as eccentricity-dependent representations captured by neurons' receptive field sizes and sampling density that change with eccentricity. Our psychophysical experiments and related simulations strongly suggest that the human visual system uses a computational strategy that differs in some key aspects from current deep learning architectures, being more data efficient and relying more critically on eye-movements.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EUZGP5S2/Han et al. - 2020 - Scale and translation-invariance for novel objects.pdf;/Users/xzfang/Zotero/storage/4XE65U5W/s41598-019-57261-6.html}
}

@article{hanna_effects_2003,
  title = {The Effects of Common Ground and Perspective on Domains of Referential Interpretation},
  author = {Hanna, Joy E and Tanenhaus, Michael K and Trueswell, John C},
  year = {2003},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {49},
  number = {1},
  pages = {43--61},
  issn = {0749-596X},
  doi = {10.1016/S0749-596X(03)00022-6},
  abstract = {Addressees' eye movements were tracked as they followed instructions given by a confederate speaker hidden from view. Experiment 1 used objects in common ground (known to both participants) or privileged ground (known to the addressee). Although privileged objects interfered with reference to an identical object in common ground, addressees were always more likely to look at an object in common ground than privileged ground. Experiment 2 used definite and indefinite referring expressions with early or late points of disambiguation, depending on the uniqueness of the display objects. The speaker's and addressee's perspectives matched when the speaker was accurately informed about the display, and mismatched when the speaker was misinformed. When perspectives matched, addressees identified the target faster with early than with late disambiguation displays. When perspectives mismatched, addressees still identified the target quickly, showing an ability to use the speaker's perspective. These experiments demonstrate that although addressees cannot completely ignore information in privileged ground, common ground and perspective each have immediate effects on reference resolution.},
  langid = {english},
  keywords = {Common ground,Communication,Conversation,Language,Perspective,Reference resolution}
}

@article{hannagan_emergence_2021,
  title = {Emergence of a Compositional Neural Code for Written Words: {{Recycling}} of a Convolutional Neural Network for Reading},
  shorttitle = {Emergence of a Compositional Neural Code for Written Words},
  author = {Hannagan, T. and Agrawal, A. and Cohen, L. and Dehaene, S.},
  year = {2021},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {46},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2104779118},
  abstract = {The visual word form area (VWFA) is a region of human inferotemporal cortex that emerges at a fixed location in the occipitotemporal cortex during reading acquisition and systematically responds to written words in literate individuals. According to the neuronal recycling hypothesis, this region arises through the repurposing, for letter recognition, of a subpart of the ventral visual pathway initially involved in face and object recognition. Furthermore, according to the biased connectivity hypothesis, its reproducible localization is due to preexisting connections from this subregion to areas involved in spoken-language processing. Here, we evaluate those hypotheses in an explicit computational model. We trained a deep convolutional neural network of the ventral visual pathway, first to categorize pictures and then to recognize written words invariantly for case, font, and size. We show that the model can account for many properties of the VWFA, particularly when a subset of units possesses a biased connectivity to word output units. The network develops a sparse, invariant representation of written words, based on a restricted set of reading-selective units. Their activation mimics several properties of the VWFA, and their lesioning causes a reading-specific deficit. The model predicts that, in literate brains, written words are encoded by a compositional neural code with neurons tuned either to individual letters and their ordinal position relative to word start or word ending or to pairs of letters (bigrams).},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2021 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {34750255},
  keywords = {compositionality,literacy,neural network,reading,VWFA},
  file = {/Users/xzfang/Zotero/storage/X47YJGMX/e2104779118.html}
}

@misc{hansen_mapping_2021,
  title = {Mapping Neurotransmitter Systems to the Structural and Functional Organization of the Human Neocortex},
  author = {Hansen, Justine Y. and Shafiei, Golia and Markello, Ross D. and Smart, Kelly and Cox, Sylvia ML and Wu, Yanjun and Gallezot, Jean-Dominique and Aumont, Etienne and Servaes, Stijn and Scala, Stephanie G. and Dubois, Jonathan M. and Wainstein, Gabriel and Bezgin, Gleb and Funck, Thomas and Schmitz, Taylor W. and Spreng, R. Nathan and Soucy, Jean-Paul and Baillet, Sylvain and Guimond, Synthia and Hietala, Jarmo and Bedard, Marc-Andre and Leyton, Marco and Kobayashi, Eliane and {Rosa-Neto}, Pedro and {Palomero-Gallagher}, Nicola and Shine, James and Carson, Richard E. and Tuominen, Lauri and Dagher, Alain and Misic, Bratislav},
  year = {2021},
  month = oct,
  pages = {2021.10.28.466336},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.10.28.466336},
  abstract = {Neurotransmitter receptors support the propagation of signals in the human brain. How receptor systems are situated within macroscale neuroanatomy and how they shape emergent function remains poorly understood, and there exists no comprehensive atlas of receptors. Here we collate positron emission tomography scans in {$>$}1,200 healthy individuals to construct a whole-brain 3-D normative atlas of 18 receptors and transporters across 9 different neurotransmitter systems. We find that receptor profiles align with structural connectivity and mediate function, including neurophysiological oscillatory dynamics and resting state hemodynamic functional connectivity. Using the Neurosynth cognitive atlas, we uncover a topographic gradient of overlapping receptor distributions that separates extrinsic and intrinsic psychological processes. Finally, we find both expected and novel associations between receptor distributions and cortical thinning patterns across 13 disorders. We replicate all findings in an independently collected autoradiography dataset. This work demonstrates how chemoarchitecture shapes brain structure and function, providing a new direction for studying multi-scale brain organization.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Z2L8BBAX/Hansen et al. - 2021 - Mapping neurotransmitter systems to the structural.pdf;/Users/xzfang/Zotero/storage/Y7954AYB/2021.10.28.html}
}

@article{hanulikova_when_2012,
  title = {When One Person's Mistake Is Another's Standard Usage: The Effect of Foreign Accent on Syntactic Processing},
  shorttitle = {When One Person's Mistake Is Another's Standard Usage},
  author = {Hanul{\'i}kov{\'a}, Adriana and {van Alphen}, Petra M. and {van Goch}, Merel M. and Weber, Andrea},
  year = {2012},
  month = apr,
  journal = {Journal of Cognitive Neuroscience},
  volume = {24},
  number = {4},
  pages = {878--887},
  issn = {1530-8898},
  doi = {10.1162/jocn_a_00103},
  abstract = {How do native listeners process grammatical errors that are frequent in non-native speech? We investigated whether the neural correlates of syntactic processing are modulated by speaker identity. ERPs to gender agreement errors in sentences spoken by a native speaker were compared with the same errors spoken by a non-native speaker. In line with previous research, gender violations in native speech resulted in a P600 effect (larger P600 for violations in comparison with correct sentences), but when the same violations were produced by the non-native speaker with a foreign accent, no P600 effect was observed. Control sentences with semantic violations elicited comparable N400 effects for both the native and the non-native speaker, confirming no general integration problem in foreign-accented speech. The results demonstrate that the P600 is modulated by speaker identity, extending our knowledge about the role of speaker's characteristics on neural correlates of speech processing.},
  langid = {english},
  pmid = {21812565},
  keywords = {Acoustic Stimulation,Adolescent,Adult,Comprehension,Electroencephalography,Evoked Potentials; Auditory,Female,Humans,Language,Male,Speech Perception,Translating,Young Adult},
  file = {/Users/xzfang/Zotero/storage/YFPLTIZL/HanulÃ­kovÃ¡ et al. - 2012 - When one person's mistake is another's standard us.pdf}
}

@article{hardstone_longterm_2021,
  title = {Long-Term Priors Influence Visual Perception through Recruitment of Long-Range Feedback},
  author = {Hardstone, Richard and Zhu, Michael and Flinker, Adeen and Melloni, Lucia and Devore, Sasha and Friedman, Daniel and Dugan, Patricia and Doyle, Werner K. and Devinsky, Orrin and He, Biyu J.},
  year = {2021},
  month = nov,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {6288},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-26544-w},
  abstract = {Perception results from the interplay of sensory input and prior knowledge. Despite behavioral evidence that long-term priors powerfully shape perception, the neural mechanisms underlying these interactions remain poorly understood. We obtained direct cortical recordings in neurosurgical patients as they viewed ambiguous images that elicit constant perceptual switching. We observe top-down influences from the temporal to occipital cortex, during the preferred percept that is congruent with the long-term prior. By contrast, stronger feedforward drive is observed during the non-preferred percept, consistent with a prediction error signal. A computational model based on hierarchical predictive coding and attractor networks reproduces all key experimental findings. These results suggest a pattern of large-scale information flow change underlying long-term priors' influence on perception and provide constraints on theories about long-term priors' influence on perception.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Dynamical systems,Object vision,Perception},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Dynamical systems;Object vision;Perception Subject\_term\_id: dynamical-systems;object-vision;perception},
  file = {/Users/xzfang/Zotero/storage/S66VMWE2/Hardstone et al. - 2021 - Long-term priors influence visual perception throu.pdf;/Users/xzfang/Zotero/storage/66KZ66SB/s41467-021-26544-w.html}
}

@inproceedings{harrington_finding_2021,
  title = {Finding {{Biological Plausibility}} for {{Adversarially Robust Features}} via {{Metameric Tasks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Harrington, Anne and Deza, Arturo},
  year = {2021},
  month = sep,
  abstract = {Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such...},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/34MQZEM4/Harrington and Deza - 2021 - Finding Biological Plausibility for Adversarially .pdf;/Users/xzfang/Zotero/storage/WGXVBGXK/forum.html}
}

@article{harrison_gibbs_2020,
  title = {Gibbs {{Sampling}} with {{People}}},
  author = {Harrison, Peter M. C. and Marjieh, Raja and Adolfi, Federico and {van Rijn}, Pol and {Anglada-Tort}, Manuel and Tchernichovski, Ofer and {Larrouy-Maestri}, Pauline and Jacoby, Nori},
  year = {2020},
  month = nov,
  journal = {arXiv:2008.02595 [cs, q-bio, stat]},
  eprint = {2008.02595},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {A core problem in cognitive science and machine learning is to understand how humans derive semantic representations from perceptual objects, such as color from an apple, pleasantness from a musical chord, or seriousness from a face. Markov Chain Monte Carlo with People (MCMCP) is a prominent method for studying such representations, in which participants are presented with binary choice trials constructed such that the decisions follow a Markov Chain Monte Carlo acceptance rule. However, while MCMCP has strong asymptotic properties, its binary choice paradigm generates relatively little information per trial, and its local proposal function makes it slow to explore the parameter space and find the modes of the distribution. Here we therefore generalize MCMCP to a continuous-sampling paradigm, where in each iteration the participant uses a slider to continuously manipulate a single stimulus dimension to optimize a given criterion such as 'pleasantness'. We formulate both methods from a utility-theory perspective, and show that the new method can be interpreted as 'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation parameter to the transition step, and show that this parameter can be manipulated to flexibly shift between Gibbs sampling and deterministic optimization. In an initial study, we show GSP clearly outperforming MCMCP; we then show that GSP provides novel and interpretable results in three other domains, namely musical chords, vocal emotions, and faces. We validate these results through large-scale perceptual rating experiments. The final experiments use GSP to navigate the latent space of a state-of-the-art image synthesis network (StyleGAN), a promising approach for applying GSP to high-dimensional perceptual spaces. We conclude by discussing future cognitive applications and ethical implications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition,Statistics - Applications},
  file = {/Users/xzfang/Zotero/storage/EKTATI28/Harrison et al. - 2020 - Gibbs Sampling with People.pdf;/Users/xzfang/Zotero/storage/43ZLQILL/2008.html}
}

@article{harrison_limited_2021,
  title = {Limited Memory for Ensemble Statistics in Visual Change Detection},
  author = {Harrison, William J. and McMaster, Jessica M. V. and Bays, Paul M.},
  year = {2021},
  month = sep,
  journal = {Cognition},
  volume = {214},
  pages = {104763},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104763},
  abstract = {Accounts of working memory based on independent item representations may overlook a possible contribution of ensemble statistics, higher-order regularities of a scene such as the mean or variance of a visual attribute. Here we used change detection tasks to investigate the hypothesis that observers store ensemble statistics in working memory and use them to detect changes in the visual environment. We controlled changes to the ensemble mean or variance between memory and test displays across six experiments. We made specific predictions of observers' sensitivity using an optimal summation model that integrates evidence across separate items but does not detect changes in ensemble statistics. We found strong evidence that observers outperformed this model, but only when task difficulty was high, and only for changes in stimulus variance. Under these conditions, we estimated that the variance of items contributed to change detection sensitivity more strongly than any individual item in this case. In contrast, however, we found strong evidence against the hypothesis that the average feature value is stored in working memory: when the mean of memoranda changed, sensitivity did not differ from the optimal summation model, which was blind to the ensemble mean, in five out of six experiments. Our results reveal that change detection is primarily limited by uncertainty in the memory of individual features, but that memory for the variance of items can facilitate detection under a limited set of conditions that involve relatively high working memory demands.},
  langid = {english},
  keywords = {Change detection,Ensemble statistics,Optimal observer model,Short term memory,Signal detection theory,Visual working memory},
  file = {/Users/xzfang/Zotero/storage/N6C5W9HR/Harrison et al. - 2021 - Limited memory for ensemble statistics in visual c.pdf;/Users/xzfang/Zotero/storage/VLIFGDNW/S0010027721001827.html}
}

@article{harrison_segmental_,
  title = {Is {{Segmental Interference Position-dependent}}?},
  author = {Harrison, William and Hepner, Christopher R and Nozari, Nazbanou},
  pages = {7},
  abstract = {The paper investigates the existence of position-independent segments in written and typed word production. In two experiments, we employed the segmental interference effect to first replicate past findings that naming a picture is more difficult in the context of another picture with which it shares segments in the same position (e.g., glow-flow) compared to an unrelated word (e.g., glow-cave). We then tested a new condition, in which the same target word is paired with an anagram of the original competitor (glow-wolf). Critically, the anagram shared the same number of segments with the target word, but never in the same position. Both experiments found robust interference for targets produced in the context of anagrams, with a magnitude comparable to the interference induced by the position-overlapping word. The results suggest that not only are position-independent segments represented in the production system, but they also play a critical role in activating segmentally related words and creating competition during word production.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4QUUHF47/Harrison et al. - Is Segmental Interference Position-dependent.pdf}
}

@article{harrison_unifying_2015,
  title = {A {{Unifying Model}} of {{Orientation Crowding}} in {{Peripheral Vision}}},
  author = {Harrison, William J. and Bex, Peter J.},
  year = {2015},
  month = dec,
  journal = {Current Biology},
  volume = {25},
  number = {24},
  pages = {3213--3219},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.10.052},
  abstract = {Peripheral vision is fundamentally limited not by the visibility of features, but by the spacing between them [1]. When too close together, visual features can become ``crowded'' and perceptually indistinguishable. Crowding interferes with basic tasks such as letter and face identification and thus informs our understanding of object recognition breakdown in peripheral vision [2]. Multiple proposals have attempted to explain crowding [3], and each is supported by compelling psychophysical and neuroimaging data [4, 5, 6] that are incompatible with competing proposals. In general, perceptual failures have variously been attributed to the averaging of nearby visual signals [7, 8, 9, 10], confusion between target and distractor elements [11, 12], and a limited resolution of visual spatial attention [13]. Here we introduce a psychophysical paradigm that allows systematic study of crowded perception within the orientation domain, and we present a unifying computational model of crowding phenomena that reconciles conflicting explanations. Our results show that our single measure produces a variety of perceptual errors that are reported across the crowding literature. Critically, a simple model of the responses of populations of orientation-selective visual neurons accurately predicts all perceptual errors. We thus provide a unifying mechanistic explanation for orientation crowding in peripheral vision. Our simple model accounts for several perceptual phenomena produced by crowding of orientation and raises the possibility that multiple classes of object recognition failures in peripheral vision can be accounted for by a single mechanism.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/L54EAXCP/Harrison and Bex - 2015 - A Unifying Model of Orientation Crowding in Periph.pdf;/Users/xzfang/Zotero/storage/QMPLWNDC/S0960982215013469.html}
}

@misc{hartsuiker_research_,
  title = {Research {{Article Is Syntax Separate}} or {{Shared Between Languages}}? {{Cross-Linguistic Syntactic Priming}} in {{Spanish-English}}},
  shorttitle = {Research {{Article Is Syntax Separate}} or {{Shared Between Languages}}?},
  author = {Hartsuiker, Robert J. and Pickering, Martin J. and Veltkamp, Eline},
  abstract = {ABSTRACT\textemdash Much research in bilingualism has addressed the question of the extent to which lexical information is shared between languages. The present study investigated whether syntactic information is shared by testing if syntactic priming occurs between languages. Spanish-English bilingual partici-pants described cards to each other in a dialogue game. We found that a participant who had just heard a sentence in Spanish tended to use the same type of sentence when describing the next card in English. In particular, English passives were considerably more common following a Spanish passive than otherwise. We use the results to extend current models of the representation of grammatical information to bilinguals. Although a very high proportion of the world's population speaks two (or more) languages, the psychological study of language has con-centrated largely on monolingualism. A fundamental psycholinguistic},
  file = {/Users/xzfang/Zotero/storage/QQ78T3UF/Hartsuiker et al. - Research Article Is Syntax Separate or Shared Betw.pdf;/Users/xzfang/Zotero/storage/Q3FVWW4Q/download.html}
}

@article{hartsuiker_syntax_2004,
  title = {Is {{Syntax Separate}} or {{Shared Between Languages}}?: {{Cross-Linguistic Syntactic Priming}} in {{Spanish-English Bilinguals}}},
  shorttitle = {Is {{Syntax Separate}} or {{Shared Between Languages}}?},
  author = {Hartsuiker, Robert J. and Pickering, Martin J. and Veltkamp, Eline},
  year = {2004},
  month = jun,
  journal = {Psychological Science},
  volume = {15},
  number = {6},
  pages = {409--414},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.0956-7976.2004.00693.x},
  abstract = {Much research in bilingualism has addressed the question of the extent to which lexical information is shared between languages. The present study investigated whether syntactic information is shared by testing if syntactic priming occurs between languages. Spanish-English bilingual participants described cards to each other in a dialogue game. We found that a participant who had just heard a sentence in Spanish tended to use the same type of sentence when describing the next card in English. In particular, English passives were considerably more common following a Spanish passive than otherwise. We use the results to extend current models of the representation of grammatical information to bilinguals.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Z5XYKAP9/Hartsuiker et al. - 2004 - Is Syntax Separate or Shared Between Languages C.pdf}
}

@article{hassabis_patients_2007,
  title = {Patients with Hippocampal Amnesia Cannot Imagine New Experiences},
  author = {Hassabis, Demis and Kumaran, Dharshan and Vann, Seralynne D. and Maguire, Eleanor A.},
  year = {2007},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {5},
  pages = {1726--1731},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0610561104},
  abstract = {Amnesic patients have a well established deficit in remembering their past experiences. Surprisingly, however, the question as to whether such patients can imagine new experiences has not been formally addressed to our knowledge. We tested whether a group of amnesic patients with primary damage to the hippocampus bilaterally could construct new imagined experiences in response to short verbal cues that outlined a range of simple commonplace scenarios. Our results revealed that patients were markedly impaired relative to matched control subjects at imagining new experiences. Moreover, we identified a possible source for this deficit. The patients' imagined experiences lacked spatial coherence, consisting instead of fragmented images in the absence of a holistic representation of the environmental setting. The hippocampus, therefore, may make a critical contribution to the creation of new experiences by providing the spatial context into which the disparate elements of an experience can be bound. Given how closely imagined experiences match episodic memories, the absence of this function mediated by the hippocampus, may also fundamentally affect the ability to vividly re-experience the past.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2007 by The National Academy of Sciences of the USA.                    Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {17229836},
  keywords = {construction,episodic,hippocampus,imagination,memory},
  file = {/Users/xzfang/Zotero/storage/R93ACJCW/Hassabis et al. - 2007 - Patients with hippocampal amnesia cannot imagine n.pdf;/Users/xzfang/Zotero/storage/Y89SIVJW/1726.html}
}

@article{hassabis_using_2007,
  title = {Using {{Imagination}} to {{Understand}} the {{Neural Basis}} of {{Episodic Memory}}},
  author = {Hassabis, Demis and Kumaran, Dharshan and Maguire, Eleanor A.},
  year = {2007},
  month = dec,
  journal = {Journal of Neuroscience},
  volume = {27},
  number = {52},
  pages = {14365--14374},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4549-07.2007},
  abstract = {Functional MRI (fMRI) studies investigating the neural basis of episodic memory recall, and the related task of thinking about plausible personal future events, have revealed a consistent network of associated brain regions. Surprisingly little, however, is understood about the contributions individual brain areas make to the overall recollective experience. To examine this, we used a novel fMRI paradigm in which subjects had to imagine fictitious experiences. In contrast to future thinking, this results in experiences that are not explicitly temporal in nature or as reliant on self-processing. By using previously imagined fictitious experiences as a comparison for episodic memories, we identified the neural basis of a key process engaged in common, namely scene construction, involving the generation, maintenance and visualization of complex spatial contexts. This was associated with activations in a distributed network, including hippocampus, parahippocampal gyrus, and retrosplenial cortex. Importantly, we disambiguated these common effects from episodic memory-specific responses in anterior medial prefrontal cortex, posterior cingulate cortex and precuneus. These latter regions may support self-schema and familiarity processes, and contribute to the brain's ability to distinguish real from imaginary memories. We conclude that scene construction constitutes a common process underlying episodic memory and imagination of fictitious experiences, and suggest it may partially account for the similar brain networks implicated in navigation, episodic future thinking, and the default mode. We suggest that additional brain regions are co-opted into this core network in a task-specific manner to support functions such as episodic memory that may have additional requirements.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2007 Society for Neuroscience 0270-6474/07/2714365-10\$15.00/0},
  langid = {english},
  pmid = {18160644},
  keywords = {construction,episodic memory,fMRI,imagination,recollection,scene},
  file = {/Users/xzfang/Zotero/storage/CKX27UEA/Hassabis et al. - 2007 - Using Imagination to Understand the Neural Basis o.pdf;/Users/xzfang/Zotero/storage/SRVXZ26L/14365.html}
}

@article{hasson_hierarchy_2008,
  title = {A {{Hierarchy}} of {{Temporal Receptive Windows}} in {{Human Cortex}}},
  author = {Hasson, Uri and Yang, Eunice and Vallines, Ignacio and Heeger, David J. and Rubin, Nava},
  year = {2008},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {28},
  number = {10},
  pages = {2539--2550},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5487-07.2008},
  abstract = {Real-world events unfold at different time scales and, therefore, cognitive and neuronal processes must likewise occur at different time scales. We present a novel procedure that identifies brain regions responsive to sensory information accumulated over different time scales. We measured functional magnetic resonance imaging activity while observers viewed silent films presented forward, backward, or piecewise-scrambled in time. Early visual areas (e.g., primary visual cortex and the motion-sensitive area MT+) exhibited high response reliability regardless of disruptions in temporal structure. In contrast, the reliability of responses in several higher brain areas, including the superior temporal sulcus (STS), precuneus, posterior lateral sulcus (LS), temporal parietal junction (TPJ), and frontal eye field (FEF), was affected by information accumulated over longer time scales. These regions showed highly reproducible responses for repeated forward, but not for backward or piecewise-scrambled presentations. Moreover, these regions exhibited marked differences in temporal characteristics, with LS, TPJ, and FEF responses depending on information accumulated over longer durations ({$\sim$}36 s) than STS and precuneus ({$\sim$}12 s). We conclude that, similar to the known cortical hierarchy of spatial receptive fields, there is a hierarchy of progressively longer temporal receptive windows in the human brain.},
  copyright = {Copyright \textcopyright{} 2008 Society for Neuroscience 0270-6474/08/282539-12\$15.00/0},
  langid = {english},
  pmid = {18322098},
  keywords = {cortex,fMRI,functional organization,receptive fields,temporal coding,time},
  file = {/Users/xzfang/Zotero/storage/XNIXPZ8Y/Hasson et al. - 2008 - A Hierarchy of Temporal Receptive Windows in Human.pdf;/Users/xzfang/Zotero/storage/CWL9PVDQ/2539.html}
}

@article{hasson_hierarchy_2008a,
  title = {A {{Hierarchy}} of {{Temporal Receptive Windows}} in {{Human Cortex}}},
  author = {Hasson, Uri and Yang, Eunice and Vallines, Ignacio and Heeger, David J. and Rubin, Nava},
  year = {2008},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {28},
  number = {10},
  pages = {2539--2550},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5487-07.2008},
  abstract = {Real-world events unfold at different time scales and, therefore, cognitive and neuronal processes must likewise occur at different time scales. We present a novel procedure that identifies brain regions responsive to sensory information accumulated over different time scales. We measured functional magnetic resonance imaging activity while observers viewed silent films presented forward, backward, or piecewise-scrambled in time. Early visual areas (e.g., primary visual cortex and the motion-sensitive area MT+) exhibited high response reliability regardless of disruptions in temporal structure. In contrast, the reliability of responses in several higher brain areas, including the superior temporal sulcus (STS), precuneus, posterior lateral sulcus (LS), temporal parietal junction (TPJ), and frontal eye field (FEF), was affected by information accumulated over longer time scales. These regions showed highly reproducible responses for repeated forward, but not for backward or piecewise-scrambled presentations. Moreover, these regions exhibited marked differences in temporal characteristics, with LS, TPJ, and FEF responses depending on information accumulated over longer durations ({$\sim$}36 s) than STS and precuneus ({$\sim$}12 s). We conclude that, similar to the known cortical hierarchy of spatial receptive fields, there is a hierarchy of progressively longer temporal receptive windows in the human brain.},
  copyright = {Copyright \textcopyright{} 2008 Society for Neuroscience 0270-6474/08/282539-12\$15.00/0},
  langid = {english},
  pmid = {18322098},
  keywords = {cortex,fMRI,functional organization,receptive fields,temporal coding,time},
  file = {/Users/xzfang/Zotero/storage/BJM2IG6A/Hasson et al. - 2008 - A Hierarchy of Temporal Receptive Windows in Human.pdf;/Users/xzfang/Zotero/storage/6CILJP5S/2539.html}
}

@article{hauser_evolution_2003,
  title = {The Evolution of the Music Faculty: A Comparative Perspective},
  shorttitle = {The Evolution of the Music Faculty},
  author = {Hauser, Marc D. and McDermott, Josh},
  year = {2003},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {6},
  number = {7},
  pages = {663--668},
  issn = {1546-1726},
  doi = {10.1038/nn1080},
  abstract = {We propose a theoretical framework for exploring the evolution of the music faculty from a comparative perspective. This framework addresses questions of phylogeny, adaptive function, innate biases and perceptual mechanisms. We argue that comparative studies can make two unique contributions to investigations of the origins of music. First, musical exposure can be controlled and manipulated to an extent not possible in humans. Second, any features of music perception found in nonhuman animals must not be part of an adaptation for music, and must rather be side effects of more general features of perception or cognition. We review studies that use animal research to target specific aspects of music perception (such as octave generalization), as well as studies that investigate more general and shared systems of the mind/brain that may be relevant to music (such as rhythm perception and emotional encoding). Finally, we suggest several directions for future work, following the lead of comparative studies on the language faculty.},
  copyright = {2003 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MSGXKUNJ/Hauser and McDermott - 2003 - The evolution of the music faculty a comparative .pdf;/Users/xzfang/Zotero/storage/52JERRG6/nn1080.html}
}

@article{hausfeld_pattern_2012,
  title = {Pattern Analysis of {{EEG}} Responses to Speech and Voice: {{Influence}} of Feature Grouping},
  shorttitle = {Pattern Analysis of {{EEG}} Responses to Speech and Voice},
  author = {Hausfeld, Lars and De Martino, Federico and Bonte, Milene and Formisano, Elia},
  year = {2012},
  month = feb,
  journal = {NeuroImage},
  volume = {59},
  number = {4},
  pages = {3641--3651},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2011.11.056},
  abstract = {Pattern recognition algorithms are becoming increasingly used in functional neuroimaging. These algorithms exploit information contained in temporal, spatial, or spatio-temporal patterns of independent variables (features) to detect subtle but reliable differences between brain responses to external stimuli or internal brain states. When applied to the analysis of electroencephalography (EEG) or magnetoencephalography (MEG) data, a choice needs to be made on how the input features to the algorithm are obtained from the signal amplitudes measured at the various channels. In this article, we consider six types of pattern analyses deriving from the combination of three types of feature selection in the temporal domain (predefined windows, shifting window, whole trial) with two approaches to handle the channel dimension (channel wise, multi-channel). We combined these different types of analyses with a Gaussian Na\"ive Bayes classifier and analyzed a multi-subject EEG data set from a study aimed at understanding the task dependence of the cortical mechanisms for encoding speaker's identity and speech content (vowels) from short speech utterances (Bonte, Valente, \& Formisano, 2009). Outcomes of the analyses showed that different grouping of available features helps highlighting complementary (i.e. temporal, topographic) aspects of information content in the data. A shifting window/multi-channel approach proved especially valuable in tracing both the early build up of neural information reflecting speaker or vowel identity and the late and task-dependent maintenance of relevant information reflecting the performance of a working memory task. Because it exploits the high temporal resolution of EEG (and MEG), such a shifting window approach with sequential multi-channel classifications seems the most appropriate choice for tracing the temporal profile of neural information processing.},
  langid = {english},
  keywords = {Audition,Classification,EEG,ERP,Machine learning,Speech},
  file = {/Users/xzfang/Zotero/storage/DVY4UDVA/Hausfeld et al. - 2012 - Pattern analysis of EEG responses to speech and vo.pdf}
}

@article{hawkins_division_2020,
  title = {The Division of Labor in Communication: {{Speakers}} Help Listeners Account for Asymmetries in Visual Perspective},
  shorttitle = {The Division of Labor in Communication},
  author = {Hawkins, Robert D. and Gweon, Hyowon and Goodman, Noah D.},
  year = {2020},
  month = may,
  journal = {arXiv:1807.09000 [cs]},
  eprint = {1807.09000},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent debates over adults' theory of mind use have been fueled by surprising failures of perspective-taking in communication, suggesting that perspective-taking can be relatively effortful. How, then, should speakers and listeners allocate their resources to achieve successful communication? We begin with the observation that this shared goal induces a natural division of labor: the resources one agent chooses to allocate toward perspective-taking should depend on their expectations about the other's allocation. We formalize this idea in a resource-rational model augmenting recent probabilistic weighting accounts with a mechanism for (costly) control over the degree of perspective-taking. In a series of simulations, we first derive an intermediate degree of perspective weighting as an optimal tradeoff between expected costs and benefits of perspective-taking. We then present two behavioral experiments testing novel predictions of our model. In Experiment 1, we manipulated the presence or absence of occlusions in a director-matcher task and found that speakers spontaneously produced more informative descriptions to account for "known unknowns" in their partner's private view. In Experiment 2, we compared the scripted utterances used by confederates in prior work with those produced in interactions with unscripted directors. We found that confederates were systematically less informative than listeners would initially expect given the presence of occlusions, but listeners used violations to adaptively make fewer errors over time. Taken together, our work suggests that people are not simply "mindblind"; they use contextually appropriate expectations to navigate the division of labor with their partner. We discuss how a resource rational framework may provide a more deeply explanatory foundation for understanding flexible perspective-taking under processing constraints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/I3W7IMQT/Hawkins et al. - 2020 - The division of labor in communication Speakers h.pdf;/Users/xzfang/Zotero/storage/NRSQYAX9/1807.html}
}

@article{hawkins_division_2021,
  title = {The {{Division}} of {{Labor}} in {{Communication}}: {{Speakers Help Listeners Account}} for {{Asymmetries}} in {{Visual Perspective}}},
  shorttitle = {The {{Division}} of {{Labor}} in {{Communication}}},
  author = {Hawkins, Robert D. and Gweon, Hyowon and Goodman, Noah D.},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {3},
  pages = {e12926},
  issn = {1551-6709},
  doi = {10.1111/cogs.12926},
  abstract = {Recent debates over adults' theory of mind use have been fueled by surprising failures of perspective-taking in communication, suggesting that perspective-taking may be relatively effortful. Yet adults routinely engage in effortful processes when needed. How, then, should speakers and listeners allocate their resources to achieve successful communication? We begin with the observation that the shared goal of communication induces a natural division of labor: The resources one agent chooses to allocate toward perspective-taking should depend on their expectations about the other's allocation. We formalize this idea in a resource-rational model augmenting recent probabilistic weighting accounts with a mechanism for (costly) control over the degree of perspective-taking. In a series of simulations, we first derive an intermediate degree of perspective weighting as an optimal trade-off between expected costs and benefits of perspective-taking. We then present two behavioral experiments testing novel predictions of our model. In Experiment 1, we manipulated the presence or absence of occlusions in a director\textendash matcher task. We found that speakers spontaneously modulated the informativeness of their descriptions to account for ``known unknowns'' in their partner's private view, reflecting a higher degree of speaker perspective-taking than previously acknowledged. In Experiment 2, we then compared the scripted utterances used by confederates in prior work with those produced in interactions with unscripted directors. We found that confederates were systematically less informative than listeners would initially expect given the presence of occlusions, but listeners used violations to adaptively make fewer errors over time. Taken together, our work suggests that people are not simply ``mindblind''; they use contextually appropriate expectations to navigate the division of labor with their partner. We discuss how a resource-rational framework may provide a more deeply explanatory foundation for understanding flexible perspective-taking under processing constraints.},
  langid = {english},
  keywords = {Communication,Pragmatics,Resource rationality,Theory of mind},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12926},
  file = {/Users/xzfang/Zotero/storage/HXL6SKK7/Hawkins et al. - 2021 - The Division of Labor in Communication Speakers H.pdf;/Users/xzfang/Zotero/storage/H8U29V32/cogs.html}
}

@article{hawkins_generalizing_2020,
  title = {Generalizing Meanings from Partners to Populations: {{Hierarchical}} Inference Supports Convention Formation on Networks},
  shorttitle = {Generalizing Meanings from Partners to Populations},
  author = {Hawkins, Robert D. and Goodman, Noah D. and Goldberg, Adele E. and Griffiths, Thomas L.},
  year = {2020},
  month = may,
  journal = {arXiv:2002.01510 [cs]},
  eprint = {2002.01510},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A key property of linguistic conventions is that they hold over an entire community of speakers, allowing us to communicate efficiently even with people we have never met before. At the same time, much of our language use is partner-specific: we know that words may be understood differently by different people based on our shared history. This poses a challenge for accounts of convention formation. Exactly how do agents make the inferential leap to community-wide expectations while maintaining partner-specific knowledge? We propose a hierarchical Bayesian model to explain how speakers and listeners solve this inductive problem. To evaluate our model's predictions, we conducted an experiment where participants played an extended natural-language communication game with different partners in a small community. We examine several measures of generalization and find key signatures of both partner-specificity and community convergence that distinguish our model from alternatives. These results suggest that partner-specificity is not only compatible with the formation of community-wide conventions, but may facilitate it when coupled with a powerful inductive mechanism.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {/Users/xzfang/Zotero/storage/D2WVS74M/Hawkins et al. - 2020 - Generalizing meanings from partners to populations.pdf}
}

@article{hawkins_partners_2021,
  title = {From Partners to Populations: {{A}} Hierarchical {{Bayesian}} Account of Coordination and Convention},
  shorttitle = {From Partners to Populations},
  author = {Hawkins, Robert D. and Franke, Michael and Frank, Michael C. and Smith, Kenny and Griffiths, Thomas L. and Goodman, Noah D.},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.05857 [cs]},
  eprint = {2104.05857},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Languages are powerful solutions to coordination problems: they provide stable, shared expectations about how the words we say correspond to the beliefs and intentions in our heads. Yet language use in a variable and non-stationary social environment requires linguistic representations to be flexible: old words acquire new ad hoc or partner-specific meanings on the fly. In this paper, we introduce a hierarchical Bayesian theory of convention formation that aims to reconcile the long-standing tension between these two basic observations. More specifically, we argue that the central computational problem of communication is not simply transmission, as in classical formulations, but learning and adaptation over multiple timescales. Under our account, rapid learning within dyadic interactions allows for coordination on partner-specific common ground, while social conventions are stable priors that have been abstracted away from interactions with multiple partners. We present new empirical data alongside simulations showing how our model provides a cognitive foundation for explaining several phenomena that have posed a challenge for previous accounts: (1) the convergence to more efficient referring expressions across repeated interaction with the same partner, (2) the gradual transfer of partner-specific common ground to novel partners, and (3) the influence of communicative context on which conventions eventually form.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/L5TBGIE8/Hawkins et al. - 2021 - From partners to populations A hierarchical Bayes.pdf;/Users/xzfang/Zotero/storage/8BEFCV3Z/2104.html}
}

@article{hay_stuffed_2010,
  title = {Stuffed Toys and Speech Perception},
  author = {Hay, Jennifer and Drager, Katie},
  year = {2010},
  month = jul,
  volume = {48},
  number = {4},
  pages = {865--892},
  publisher = {{Walter de Gruyter GmbH \& Co. KG}},
  issn = {1613-396X},
  doi = {10.1515/ling.2010.027},
  abstract = {Previous research has shown that speech perception can be influenced by a speaker's social characteristics, including the expected dialect area of the speaker (Niedzielski, Journal of Language and Social Psychology 18: 62\textendash 85, 1999; Hay et al. The Linguistic Review 23: 351\textendash 379, 2006a). This article reports on an experiment designed to test to degree to which exposure to the concept of a region can also influence perception. In order to invoke the concept, we exposed participants, who were all speakers of New Zealand English, to either stuffed toy kangaroos and koalas (associated with Australia) or stuffed toy kiwis (associated with New Zealand). Participants then completed a perception task in which they matched natural vowels produced by a male New Zealander to vowels from a synthesized continuum which ranged from raised and fronted Australian-like tokens to lowered and centralized New Zealand-like tokens. Our results indicate that perception of the vowels shifted depending on which set of toys the participants had seen. This supports models of speech perception in which linguistic and nonlinguistic information are intricately entwined.},
  chapter = {Linguistics},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IXHAJL9W/html.html}
}

@article{haynes_predicting_2005,
  title = {Predicting the Orientation of Invisible Stimuli from Activity in Human Primary Visual Cortex},
  author = {Haynes, John-Dylan and Rees, Geraint},
  year = {2005},
  month = may,
  journal = {Nature Neuroscience},
  volume = {8},
  number = {5},
  pages = {686--691},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1445},
  abstract = {Humans can experience aftereffects from oriented stimuli that are not consciously perceived, suggesting that such stimuli receive cortical processing. Determining the physiological substrate of such effects has proven elusive owing to the low spatial resolution of conventional human neuroimaging techniques compared to the size of orientation columns in visual cortex. Here we show that even at conventional resolutions it is possible to use fMRI to obtain a direct measure of orientation-selective processing in V1. We found that many parts of V1 show subtle but reproducible biases to oriented stimuli, and that we could accumulate this information across the whole of V1 using multivariate pattern recognition. Using this information, we could then successfully predict which one of two oriented stimuli a participant was viewing, even when masking rendered that stimulus invisible. Our findings show that conventional fMRI can be used to reveal feature-selective processing in human cortex, even for invisible stimuli.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/xzfang/Zotero/storage/NJSN2AYQ/Haynes and Rees - 2005 - Predicting the orientation of invisible stimuli fr.pdf;/Users/xzfang/Zotero/storage/S7LJPT22/nn1445.html}
}

@article{he_breakdown_2007,
  title = {Breakdown of {{Functional Connectivity}} in {{Frontoparietal Networks Underlies Behavioral Deficits}} in {{Spatial Neglect}}},
  author = {He, Biyu J. and Snyder, Abraham Z. and Vincent, Justin L. and Epstein, Adrian and Shulman, Gordon L. and Corbetta, Maurizio},
  year = {2007},
  month = mar,
  journal = {Neuron},
  volume = {53},
  number = {6},
  pages = {905--918},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2007.02.013},
  abstract = {Spatial neglect is a syndrome following stroke manifesting attentional deficits in perceiving and responding to stimuli in the contralesional field. We examined brain network integrity in patients with neglect by measuring coherent fluctuations of fMRI signals (functional connectivity). Connectivity in two largely separate attention networks located in dorsal and ventral frontoparietal areas was assessed at both acute and chronic stages of recovery. Connectivity in the ventral network, part of which directly lesioned, was diffusely disrupted and showed no recovery. In the structurally intact dorsal network, interhemispheric connectivity in posterior parietal cortex was acutely disrupted but fully recovered. This acute disruption, and disrupted connectivity in specific pathways in the ventral network, strongly correlated with impaired attentional processing across subjects. Lastly, disconnection of the white matter tracts connecting frontal and parietal cortices was associated with more severe neglect and more disrupted functional connectivity. These findings support a network view in understanding neglect.},
  langid = {english},
  keywords = {HUMDISEASE,SYSNEURO},
  file = {/Users/xzfang/Zotero/storage/QWAMIXPK/He et al. - 2007 - Breakdown of Functional Connectivity in Frontopari.pdf;/Users/xzfang/Zotero/storage/PMKVFM7B/S0896627307001122.html}
}

@article{heald_contextual_2021,
  title = {Contextual Inference Underlies the Learning of Sensorimotor Repertoires},
  author = {Heald, James B. and Lengyel, M{\'a}t{\'e} and Wolpert, Daniel M.},
  year = {2021},
  month = nov,
  journal = {Nature},
  pages = {1--5},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04129-3},
  abstract = {Humans spend a lifetime learning, storing and refining a repertoire of motor memories. For example, through experience, we become proficient at manipulating a large range of objects with distinct dynamical properties. However, it is unknown what principle underlies how our continuous stream of sensorimotor experience is segmented into separate memories and how we adapt and use this growing repertoire. Here we develop a theory of motor learning based on the key principle that memory creation, updating and expression are all controlled by a single computation\textemdash contextual inference. Our theory reveals that adaptation can arise both by creating and updating memories (proper learning) and by changing how existing memories are differentially expressed (apparent learning). This insight enables us to account for key features of motor learning that had no unified explanation: spontaneous recovery1, savings2, anterograde interference3, how environmental consistency affects learning rate4,5 and the distinction between explicit and implicit learning6. Critically, our theory also predicts new phenomena\textemdash evoked recovery and context-dependent single-trial learning\textemdash which we confirm experimentally. These results suggest that contextual inference, rather than classical single-context mechanisms1,4,7\textendash 9, is the key principle underlying how a diverse set of experiences is reflected in our motor behaviour.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Motor control},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Human behaviour;Motor control Subject\_term\_id: human-behaviour;motor-control},
  file = {/Users/xzfang/Zotero/storage/UTW7IXSX/Heald et al. - 2021 - Contextual inference underlies the learning of sen.pdf;/Users/xzfang/Zotero/storage/4K2FPK8Q/s41586-021-04129-3.html}
}

@article{hebart_things_2019,
  title = {{{THINGS}}: {{A}} Database of 1,854 Object Concepts and More than 26,000 Naturalistic Object Images},
  shorttitle = {{{THINGS}}},
  author = {Hebart, Martin N. and Dickter, Adam H. and Kidder, Alexis and Kwok, Wan Y. and Corriveau, Anna and Wicklin, Caitlin Van and Baker, Chris I.},
  year = {2019},
  month = oct,
  journal = {PLOS ONE},
  volume = {14},
  number = {10},
  pages = {e0223792},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0223792},
  abstract = {In recent years, the use of a large number of object concepts and naturalistic object images has been growing strongly in cognitive neuroscience research. Classical databases of object concepts are based mostly on a manually curated set of concepts. Further, databases of naturalistic object images typically consist of single images of objects cropped from their background, or a large number of naturalistic images of varying quality, requiring elaborate manual image curation. Here we provide a set of 1,854 diverse object concepts sampled systematically from concrete picturable and nameable nouns in the American English language. Using these object concepts, we conducted a large-scale web image search to compile a database of 26,107 high-quality naturalistic images of those objects, with 12 or more object images per concept and all images cropped to square size. Using crowdsourcing, we provide higher-level category membership for the 27 most common categories and validate them by relating them to representations in a semantic embedding derived from large text corpora. Finally, by feeding images through a deep convolutional neural network, we demonstrate that they exhibit high selectivity for different object concepts, while at the same time preserving variability of different object images within each concept. Together, the THINGS database provides a rich resource of object concepts and object images and offers a tool for both systematic and large-scale naturalistic research in the fields of psychology, neuroscience, and computer science.},
  langid = {english},
  keywords = {Computational neuroscience,Computer and information sciences,Computer imaging,Graphical user interfaces,Neural networks,Semantics,Vision,Visual object recognition},
  file = {/Users/xzfang/Zotero/storage/FCZ3XQVP/Hebart et al. - 2019 - THINGS A database of 1,854 object concepts and mo.pdf;/Users/xzfang/Zotero/storage/WCVAFPRY/article.html}
}

@article{heer_hierarchical_2017,
  title = {The {{Hierarchical Cortical Organization}} of {{Human Speech Processing}}},
  author = {de Heer, Wendy A. and Huth, Alexander G. and Griffiths, Thomas L. and Gallant, Jack L. and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {2017},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {37},
  number = {27},
  pages = {6539--6557},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3267-16.2017},
  abstract = {Speech comprehension requires that the brain extract semantic meaning from the spectral features represented at the cochlea. To investigate this process, we performed an fMRI experiment in which five men and two women passively listened to several hours of natural narrative speech. We then used voxelwise modeling to predict BOLD responses based on three different feature spaces that represent the spectral, articulatory, and semantic properties of speech. The amount of variance explained by each feature space was then assessed using a separate validation dataset. Because some responses might be explained equally well by more than one feature space, we used a variance partitioning analysis to determine the fraction of the variance that was uniquely explained by each feature space. Consistent with previous studies, we found that speech comprehension involves hierarchical representations starting in primary auditory areas and moving laterally on the temporal lobe: spectral features are found in the core of A1, mixtures of spectral and articulatory in STG, mixtures of articulatory and semantic in STS, and semantic in STS and beyond. Our data also show that both hemispheres are equally and actively involved in speech perception and interpretation. Further, responses as early in the auditory hierarchy as in STS are more correlated with semantic than spectral representations. These results illustrate the importance of using natural speech in neurolinguistic research. Our methodology also provides an efficient way to simultaneously test multiple specific hypotheses about the representations of speech without using block designs and segmented or synthetic speech. SIGNIFICANCE STATEMENT To investigate the processing steps performed by the human brain to transform natural speech sound into meaningful language, we used models based on a hierarchical set of speech features to predict BOLD responses of individual voxels recorded in an fMRI experiment while subjects listened to natural speech. Both cerebral hemispheres were actively involved in speech processing in large and equal amounts. Also, the transformation from spectral features to semantic elements occurs early in the cortical speech-processing stream. Our experimental and analytical approaches are important alternatives and complements to standard approaches that use segmented speech and block designs, which report more laterality in speech processing and associated semantic processing to higher levels of cortex than reported here.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2017 the authors 0270-6474/17/376539-19\$15.00/0},
  langid = {english},
  pmid = {28588065},
  keywords = {fMRI,natural stimuli,regression,speech},
  file = {/Users/xzfang/Zotero/storage/97Z9HFMW/Heer et al. - 2017 - The Hierarchical Cortical Organization of Human Sp.pdf;/Users/xzfang/Zotero/storage/EE8WH67U/6539.html}
}

@article{heilbron_great_2018,
  title = {Great {{Expectations}}: {{Is}} There {{Evidence}} for {{Predictive Coding}} in {{Auditory Cortex}}?},
  shorttitle = {Great {{Expectations}}},
  author = {Heilbron, Micha and Chait, Maria},
  year = {2018},
  month = oct,
  journal = {Neuroscience},
  series = {Sensory {{Sequence Processing}} in the {{Brain}}},
  volume = {389},
  pages = {54--73},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2017.07.061},
  abstract = {Predictive coding is possibly one of the most influential, comprehensive, and controversial theories of neural function. While proponents praise its explanatory potential, critics object that key tenets of the theory are untested or even untestable. The present article critically examines existing evidence for predictive coding in the auditory modality. Specifically, we identify five key assumptions of the theory and evaluate each in the light of animal, human and modeling studies of auditory pattern processing. For the first two assumptions \textendash{} that neural responses are shaped by expectations and that these expectations are hierarchically organized \textendash{} animal and human studies provide compelling evidence. The anticipatory, predictive nature of these expectations also enjoys empirical support, especially from studies on unexpected stimulus omission. However, for the existence of separate error and prediction neurons, a key assumption of the theory, evidence is lacking. More work exists on the proposed oscillatory signatures of predictive coding, and on the relation between attention and precision. However, results on these latter two assumptions are mixed or contradictory. Looking to the future, more collaboration between human and animal studies, aided by model-based analyses will be needed to test specific assumptions and implementations of predictive coding \textendash{} and, as such, help determine whether this popular grand theory can fulfill its expectations.},
  langid = {english},
  keywords = {auditory,bayesian brain,MMN,predictive coding,SSA},
  file = {/Users/xzfang/Zotero/storage/XUUDF9LP/Heilbron and Chait - 2018 - Great Expectations Is there Evidence for Predicti.pdf;/Users/xzfang/Zotero/storage/9MPPEBCS/S030645221730547X.html}
}

@techreport{heilbron_hierarchy_2020,
  type = {Preprint},
  title = {A Hierarchy of Linguistic Predictions during Natural Language Comprehension},
  author = {Heilbron, Micha and Armeni, Kristijan and Schoffelen, Jan-Mathijs and Hagoort, Peter and {de Lange}, Floris P.},
  year = {2020},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.12.03.410399},
  abstract = {Understanding spoken language requires transforming ambiguous stimulus streams into a hierarchy of increasingly abstract representations, ranging from speech sounds to meaning. It has been suggested that the brain uses predictive computations to guide the interpretation of incoming information. However, the exact role of prediction in language understanding remains unclear, with widespread disagreement about both the ubiquity of prediction, and the level of representation at which predictions unfold. Here, we address both issues by analysing brain recordings of participants listening to audiobooks, and using a state-ofthe-art deep neural network (GPT-2) to quantify predictions in a fine-grained, contextual fashion. First, we establish clear evidence for predictive processing, confirming that brain responses to words are modulated by probabilistic predictions. Next, we factorised the model-based predictions into distinct linguistic dimensions, revealing dissociable neural signatures of syntactic, phonemic and semantic predictions. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting theories of hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, and demonstrate that linguistic prediction is not implemented by a single system but occurs throughout the language network, forming a hierarchy of linguistic predictions across all levels of analysis.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KTSCA254/Heilbron et al. - 2020 - A hierarchy of linguistic predictions during natur.pdf}
}

@article{heilbron_word_2020,
  title = {Word Contexts Enhance the Neural Representation of Individual Letters in Early Visual Cortex},
  author = {Heilbron, Micha and Richter, David and Ekman, Matthias and Hagoort, Peter and {de Lange}, Floris P.},
  year = {2020},
  month = dec,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {321},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13996-4},
  langid = {english}
}

@article{heittola_contextdependent_2013,
  title = {Context-Dependent Sound Event Detection},
  author = {Heittola, Toni and Mesaros, Annamaria and Eronen, Antti and Virtanen, Tuomas},
  year = {2013},
  month = jan,
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  volume = {2013},
  number = {1},
  pages = {1},
  issn = {1687-4722},
  doi = {10.1186/1687-4722-2013-1},
  abstract = {The work presented in this article studies how the context information can be used in the automatic sound event detection process, and how the detection system can benefit from such information. Humans are using context information to make more accurate predictions about the sound events and ruling out unlikely events given the context. We propose a similar utilization of context information in the automatic sound event detection process. The proposed approach is composed of two stages: automatic context recognition stage and sound event detection stage. Contexts are modeled using Gaussian mixture models and sound events are modeled using three-state left-to-right hidden Markov models. In the first stage, audio context of the tested signal is recognized. Based on the recognized context, a context-specific set of sound event classes is selected for the sound event detection stage. The event detection stage also uses context-dependent acoustic models and count-based event priors. Two alternative event detection approaches are studied. In the first one, a monophonic event sequence is outputted by detecting the most prominent sound event at each time instance using Viterbi decoding. The second approach introduces a new method for producing polyphonic event sequence by detecting multiple overlapping sound events using multiple restricted Viterbi passes. A new metric is introduced to evaluate the sound event detection performance with various level of polyphony. This combines the detection accuracy and coarse time-resolution error into one metric, making the comparison of the performance of detection algorithms simpler. The two-step approach was found to improve the results substantially compared to the context-independent baseline system. In the block-level, the detection accuracy can be almost doubled by using the proposed context-dependent event detection.},
  keywords = {Detection Accuracy,Event Detection,Event Prior,Sound Event,Universal Background Model},
  file = {/Users/xzfang/Zotero/storage/HPBPYGLR/Heittola et al. - 2013 - Context-dependent sound event detection.pdf;/Users/xzfang/Zotero/storage/ABZPFSQQ/1687-4722-2013-1.html}
}

@article{helfrich_neural_2019,
  title = {Neural {{Entrainment}} and {{Network Resonance}} in {{Support}} of {{Top-down}} Guided {{Attention}}},
  author = {Helfrich, Randolph F. and Breska, Assaf and Knight, Robert T.},
  year = {2019},
  month = oct,
  journal = {Current opinion in psychology},
  volume = {29},
  pages = {82--89},
  issn = {2352-250X},
  doi = {10.1016/j.copsyc.2018.12.016},
  abstract = {Which neural mechanisms provide the functional basis of top-down guided cognitive control? Here we review recent evidence that suggest that the neural basis of attention is inherently rhythmic. In particular, we discuss two physical properties of self-sustained networks, namely entrainment and resonance, and how these shape the timescale of attentional control. Several recent findings revealed theta-band (3-8 Hz) dynamics in top-down guided behavior. These reports were paralleled by intracranial recordings, which implicated theta oscillations in the organization of functional attention networks. We discuss how the intrinsic network architecture shapes covert attentional sampling as well as overt behavior. Taken together, we posit that theta rhythmicity is an inherent feature of the attention network in support of top-down guided goal-directed behavior.},
  pmcid = {PMC6606401},
  pmid = {30690228},
  file = {/Users/xzfang/Zotero/storage/5TZ4XJYE/Helfrich et al. - 2019 - Neural Entrainment and Network Resonance in Suppor.pdf}
}

@article{heller_perspectivetaking_2016a,
  title = {Perspective-Taking Behavior as the Probabilistic Weighing of Multiple Domains},
  author = {Heller, Daphna and Parisien, Christopher and Stevenson, Suzanne},
  year = {2016},
  month = apr,
  journal = {Cognition},
  volume = {149},
  pages = {104--120},
  issn = {00100277},
  doi = {10.1016/j.cognition.2015.12.008},
  abstract = {Our starting point is the apparently-contradictory results in the psycholinguistic literature regarding whether, when interpreting a definite referring expressions, listeners process relative to the common ground from the earliest moments of processing. We propose that referring expressions are not interpreted relative solely to the common ground or solely to one's Private (or egocentric) knowledge, but rather reflect the simultaneous integration of the two perspectives. We implement this proposal in a Bayesian model of reference resolution, focusing on the model's predictions for two prior studies: Keysar, Barr, Balin, and Brauner (2000) and Heller, Grodner and Tanenhaus (2008). We test the model's predictions in a visual-world eye-tracking experiment, demonstrating that the original results cannot simply be attributed to different perspective-taking strategies, and showing how they can arise from the same perspective-taking behavior.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NDU2UXQW/Heller et al. - 2016 - Perspective-taking behavior as the probabilistic w.pdf}
}

@inproceedings{henderson_multiscale_2016,
  title = {Multi-Scale Reflection Invariance},
  booktitle = {2016 {{SAI Computing Conference}} ({{SAI}})},
  author = {Henderson, Craig and Izquierdo, Ebroul},
  year = {2016},
  month = jul,
  pages = {420--425},
  publisher = {{IEEE}},
  address = {{London, United Kingdom}},
  doi = {10.1109/SAI.2016.7556016},
  abstract = {In this position paper, we consider the state of computer vision research with respect to invariance to the horizontal orientation of an image \textendash{} what we term reflection invariance. We describe why we consider reflection invariance to be an important property and provide evidence where the absence of this invariance produces surprising inconsistencies in state-of-the-art systems. We demonstrate inconsistencies in methods of object detection and scene classification when they are presented with images and the horizontal mirror of those images. Finally, we examine where some of the invariances are exhibited in feature detection and descriptors, and make a case for future consideration of reflection invariance as a measure of quality in computer vision algorithms.},
  isbn = {978-1-4673-8460-5},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MNYKI6Z6/Henderson and Izquierdo - 2016 - Multi-scale reflection invariance.pdf}
}

@article{hennig_how_2021,
  title = {How Learning Unfolds in the Brain: Toward an Optimization View},
  shorttitle = {How Learning Unfolds in the Brain},
  author = {Hennig, Jay A. and Oby, Emily R. and Losey, Darby M. and Batista, Aaron P. and Yu, Byron M. and Chase, Steven M.},
  year = {2021},
  month = oct,
  journal = {Neuron},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.09.005},
  abstract = {How do changes in the brain lead to learning? To answer this question, consider an artificial neural network (ANN), where learning proceeds by optimizing a given objective or cost function. This ``optimization framework'' may provide new insights into how the brain learns, as many idiosyncratic features of neural activity can be recapitulated by an ANN trained to perform the same task. Nevertheless, there are key features of how neural population activity changes throughout learning that cannot be readily explained in terms of optimization and are not typically features of ANNs. Here we detail three of these features: (1) the inflexibility of neural variability throughout learning, (2) the use of multiple learning processes even during simple tasks, and (3) the presence of large task-nonspecific activity changes. We propose that understanding the role of these features in the brain will be key to describing biological learning using an optimization framework.},
  langid = {english}
}

@article{henrich_weirdest_2010,
  title = {The Weirdest People in the World?},
  author = {Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
  year = {2010},
  month = jun,
  journal = {Behavioral and Brain Sciences},
  volume = {33},
  number = {2-3},
  pages = {61--83},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X0999152X},
  abstract = {Behavioral scientists routinely publish broad claims about human psychology and behavior in the world's top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. Researchers \textendash{} often implicitly \textendash{} assume that either there is little variation across human populations, or that these ``standard subjects'' are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species \textendash{} frequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior \textendash{} hence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EAAAJ33C/Henrich et al. - 2010 - The weirdest people in the world.pdf}
}

@article{henson_neuroimaging_2003,
  title = {Neuroimaging Studies of Priming},
  author = {Henson, R.N.A},
  year = {2003},
  month = may,
  journal = {Progress in Neurobiology},
  volume = {70},
  number = {1},
  pages = {53--81},
  issn = {03010082},
  doi = {10.1016/S0301-0082(03)00086-8},
  abstract = {This article reviews functional neuroimaging studies of priming, a behavioural change associated with the repeated processing of a stimulus. Using the haemodynamic techniques of functional magnetic resonance imaging (fMRI) and positron emission tomography (PET), priming-related effects have been observed in numerous regions of the human brain, with the specific regions depending on the type of stimulus and the manner in which it is processed. The most common finding is a decreased haemodynamic response for primed versus unprimed stimuli, though priming-related response increases have been observed. Attempts have been made to relate these effects to a form of implicit or ``unconscious'' memory. The priming-related decrease has also been used as a tool to map the brain regions associated with different stages of stimulus-processing, a method claimed to offer superior spatial resolution. This decrease has a potential analogue in the stimulus repetition effects measured with single-cell recording in the non-human primate. The paradigms reviewed include word-stem completion, masked priming, repetition priming of visual objects and semantic priming. An attempt is made to relate the findings within a ``component process'' framework, and the relationship between behavioural, haemodynamic and neurophysiological data is discussed. Interpretation of the findings is not always clear-cut, however, given potential confounding factors such as explicit memory, and several recommendations are made for future neuroimaging studies of priming.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TNA2EIJR/Henson - 2003 - Neuroimaging studies of priming.pdf}
}

@article{henson_neuroimaging_2003a,
  title = {Neuroimaging Studies of Priming},
  author = {Henson, R.N.A},
  year = {2003},
  month = may,
  journal = {Progress in Neurobiology},
  volume = {70},
  number = {1},
  pages = {53--81},
  issn = {03010082},
  doi = {10.1016/S0301-0082(03)00086-8},
  abstract = {This article reviews functional neuroimaging studies of priming, a behavioural change associated with the repeated processing of a stimulus. Using the haemodynamic techniques of functional magnetic resonance imaging (fMRI) and positron emission tomography (PET), priming-related effects have been observed in numerous regions of the human brain, with the specific regions depending on the type of stimulus and the manner in which it is processed. The most common finding is a decreased haemodynamic response for primed versus unprimed stimuli, though priming-related response increases have been observed. Attempts have been made to relate these effects to a form of implicit or ``unconscious'' memory. The priming-related decrease has also been used as a tool to map the brain regions associated with different stages of stimulus-processing, a method claimed to offer superior spatial resolution. This decrease has a potential analogue in the stimulus repetition effects measured with single-cell recording in the non-human primate. The paradigms reviewed include word-stem completion, masked priming, repetition priming of visual objects and semantic priming. An attempt is made to relate the findings within a ``component process'' framework, and the relationship between behavioural, haemodynamic and neurophysiological data is discussed. Interpretation of the findings is not always clear-cut, however, given potential confounding factors such as explicit memory, and several recommendations are made for future neuroimaging studies of priming.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/R9P8P554/Henson - 2003 - Neuroimaging studies of priming.pdf}
}

@article{hepner_dual_2020,
  title = {The Dual Origin of Lexical Perseverations in Aphasia: {{Residual}} Activation and Incremental Learning},
  shorttitle = {The Dual Origin of Lexical Perseverations in Aphasia},
  author = {Hepner, Christopher R. and Nozari, Nazbanou},
  year = {2020},
  month = oct,
  journal = {Neuropsychologia},
  volume = {147},
  pages = {107603},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2020.107603},
  abstract = {Lexical perseveration, the inappropriate repetition of a previous response, is common in aphasia. Two underlying mechanisms have been proposed: residual activation and incremental learning. Previous attempts to differentiate the two have relied on experimental paradigms that encourage semantically related errors and analysis techniques designed to detect perseverations over short distances, resulting in a bias towards detecting short-lag, semantically related perseverations that both mechanisms can account for. Two key predictions that differentiate these accounts remain untested: only residual activation can explain short-lag, semantically unrelated perseverations, whereas only incremental learning can explain long-lag, semantically related perseverations. In this paper, we used a large set of picture naming trials and a novel analysis technique to test these key predictions in a multi-session study involving six individuals with aphasia. We found clear evidence for both mechanisms in different individuals, demonstrating that either one is sufficient to cause perseveration. Importantly, perseverations due to residual activation were associated with more severely impaired systems than those due to incremental learning, suggesting that a certain degree of structural and functional integrity was necessary for incremental learning. Finally, the results supported a key prediction of the incremental learning account by showing perseverations over longer lags than have previously been reported.},
  langid = {english},
  keywords = {Aphasia,Incremental learning,Language production,Perseveration,Residual activation},
  file = {/Users/xzfang/Zotero/storage/Q8V2GZMP/Hepner and Nozari - 2020 - The dual origin of lexical perseverations in aphas.pdf;/Users/xzfang/Zotero/storage/V8GUCT99/S002839322030275X.html}
}

@article{hepner_resource_2019,
  title = {Resource Allocation in Phonological Working Memory: {{Same}} or Different Principles from Vision?},
  shorttitle = {Resource Allocation in Phonological Working Memory},
  author = {Hepner, Christopher R. and Nozari, Nazbanou},
  year = {2019},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {106},
  pages = {172--188},
  issn = {0749596X},
  doi = {10.1016/j.jml.2019.03.003},
  abstract = {The nature of working memory resources\textemdash in particular, their quantization (discrete vs. continuous)\textemdash has been studied extensively in the visual domain, with evidence supporting models with flexibly and continuously divisible resources. It remains unclear, however, whether similar mechanisms mediate the division of resources in phonological working memory. In three experiments, we show that, despite representational differences between visual and auditory domains, the principles of resource division are indeed similar in these domains. Exp. 1 tests slot vs. resource models, Exp. 2 gauges the effect of attention on resource division, and Exp. 3 investigates the influence of attention on different stages of working memory. Collectively, the results provide support for a resource model of phonological working memory and, more generally, point to similar computational principles governing the allocation of working memory resources.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J3MK86PU/Hepner and Nozari - 2019 - Resource allocation in phonological working memory.pdf}
}

@article{herman_perception_2003,
  title = {Perception of ``{{Elliptical Speech}}'' {{Following Cochlear Implantation}}: {{Use}} of {{Broad Phonetic Categories}} in {{Speech Perception}}},
  shorttitle = {Perception of ``{{Elliptical Speech}}'' {{Following Cochlear Implantation}}},
  author = {Herman, Rebecca and Pisoni, David B.},
  year = {2003},
  journal = {The Volta review},
  volume = {102},
  number = {4},
  pages = {321--347},
  issn = {0042-8639},
  abstract = {This study investigated the perception of elliptical speech () in an adult cochlear implant patient. A group of 20 adult listeners with normal hearing were used for comparison. Two experiments were conducted using sets of meaningful and anomalous English sentences. Two versions of each set of sentences were constructed: One set contained correct place of articulation cues; the other was transformed into elliptical speech using a procedure in which different places of articulation were all converted to alveolar place of articulation. The patient, ``Mr. S,'' completed a same-different discrimination task and a sentence transcription task. The listeners with normal hearing completed both tasks under masking noise and low-pass filtering. In the same-different task, under both conditions of signal degradation, listeners with normal hearing labeled a sentence with intact place of articulation cues and its elliptical version as the same. Mr. S also showed the same pattern. These findings support the claim by  that under conditions of signal degradation, ellipsis can no longer be detected. In the sentence transcription task, however, subjects with normal hearing showed better transcription performance for sentences with intact place of articulation cues than for elliptical speech sentences, which was unexpected given the findings from the sentence discrimination experiment. Mr. S also showed the same pattern of performance. These new findings on the perception of elliptical speech suggest that cochlear implant users perceive speech and recognize spoken words using broad phonetic categories.},
  pmcid = {PMC3103267},
  pmid = {21625300},
  file = {/Users/xzfang/Zotero/storage/IL8S9HIU/Herman and Pisoni - 2003 - Perception of â€œElliptical Speechâ€ Following Cochle.pdf}
}

@article{hermer-vazquez_sources_1999,
  title = {Sources of {{Flexibility}} in {{Human Cognition}}: {{Dual-Task Studies}} of {{Space}} and {{Language}}},
  shorttitle = {Sources of {{Flexibility}} in {{Human Cognition}}},
  author = {{Hermer-Vazquez}, Linda and Spelke, Elizabeth S. and Katsnelson, Alla S.},
  year = {1999},
  month = aug,
  journal = {Cognitive Psychology},
  volume = {39},
  number = {1},
  pages = {3--36},
  issn = {0010-0285},
  doi = {10.1006/cogp.1998.0713},
  abstract = {Under many circumstances, children and adult rats reorient themselves through a process which operates only on information about the shape of the environment (e.g., Cheng, 1986; Hermer \& Spelke, 1996). In contrast, human adults relocate themselves more flexibly, by conjoining geometric and nongeometric information to specify their position (Hermer \& Spelke, 1994). The present experiments used a dual-task method to investigate the processes that underlie the flexible conjunction of information. In Experiment 1, subjects reoriented themselves flexibly when they performed no secondary task, but they reoriented themselves like children and adult rats when they engaged in verbal shadowing of continuous speech. In Experiment 2, subjects who engaged in nonverbal shadowing of a continuous rhythm reoriented like nonshadowing subjects, suggesting that the interference effect in Experiment 1 did not stem from general limits on working memory or attention but from processes more specific to language. In further experiments, verbally shadowing subjects detected and remembered both nongeometric information (Experiment 3) and geometric information (Experiments 1, 2, and 4), but they failed to conjoin the two types of information to specify the positions of objects (Experiment 4). Together, the experiments suggest that humans' flexible spatial memory depends on the ability to combine diverse information sources rapidly into unitary representations and that this ability, in turn, depends on natural language.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MGPNZDC8/S0010028598907137.html}
}

@article{hershler_first_2005,
  title = {At First Sight: {{A}} High-Level Pop out Effect for Faces},
  shorttitle = {At First Sight},
  author = {Hershler, Orit and Hochstein, Shaul},
  year = {2005},
  month = jun,
  journal = {Vision Research},
  volume = {45},
  number = {13},
  pages = {1707--1724},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2004.12.021},
  abstract = {To determine the nature of face perception, several studies used the visual search paradigm, whereby subjects detect an odd target among distractors. When detection reaction time is set-size independent, the odd element is said to ``pop out'', reflecting a basic mechanism or map for the relevant feature. A number of previous studies suggested that schematic faces do not pop out. We show that natural face stimuli do pop out among assorted non-face objects. Animal faces, on the other hand, do not pop out from among the same assorted non-face objects. In addition, search for a face among distractors of another object category is easier than the reverse search, and face search is mediated by holistic face characteristics, rather than by face parts. Our results indicate that the association of pop out with elementary features and lower cortical areas may be incorrect. Instead, face search, and indeed all feature search, may reflect high-level activity with generalization over spatial and other property details.},
  langid = {english},
  keywords = {Asymmetry,Detection,Face,Search,Visual},
  file = {/Users/xzfang/Zotero/storage/S5WFBYH6/Hershler and Hochstein - 2005 - At first sight A high-level pop out effect for fa.pdf;/Users/xzfang/Zotero/storage/BDELBR7Z/S0042698905000337.html}
}

@article{hervais-adelman_learning_2019,
  title = {Learning to Read Recycles Visual Cortical Networks without Destruction},
  author = {{Hervais-Adelman}, Alexis and Kumar, Uttam and Mishra, Ramesh K. and Tripathi, Viveka N. and Guleria, Anupam and Singh, Jay P. and Eisner, Frank and Huettig, Falk},
  year = {2019},
  month = sep,
  journal = {Science Advances},
  volume = {5},
  number = {9},
  pages = {eaax0262},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aax0262},
  abstract = {Learning to read is associated with the appearance of an orthographically sensitive brain region known as the visual word form area. It has been claimed that development of this area proceeds by impinging upon territory otherwise available for the processing of culturally relevant stimuli such as faces and houses. In a large-scale functional magnetic resonance imaging study of a group of individuals of varying degrees of literacy (from completely illiterate to highly literate), we examined cortical responses to orthographic and nonorthographic visual stimuli. We found that literacy enhances responses to other visual input in early visual areas and enhances representational similarity between text and faces, without reducing the extent of response to nonorthographic input. Thus, acquisition of literacy in childhood recycles existing object representation mechanisms but without destructive competition. Reading co-opts existing cortical visual feature representation without destruction. Reading co-opts existing cortical visual feature representation without destruction.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7872CM2V/Hervais-Adelman et al. - 2019 - Learning to read recycles visual cortical networks.pdf;/Users/xzfang/Zotero/storage/H6I2TKR6/eaax0262.html}
}

@article{hervais-adelman_perceptual_2008,
  title = {Perceptual Learning of Noise Vocoded Words: {{Effects}} of Feedback and Lexicality.},
  shorttitle = {Perceptual Learning of Noise Vocoded Words},
  author = {{Hervais-Adelman}, Alexis and Davis, Matthew H. and Johnsrude, Ingrid S. and Carlyon, Robert P.},
  year = {2008},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {34},
  number = {2},
  pages = {460--474},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.34.2.460},
  abstract = {Speech comprehension is resistant to acoustic distortion in the input, reflecting listeners' ability to adjust perceptual processes to match the speech input. This adjustment is reflected in improved comprehension of distorted speech with experience. For noise vocoding, a manipulation that removes spectral detail from speech, listeners' word report showed a significantly greater improvement over trials for listeners that heard clear speech presentations before rather than after hearing distorted speech (clear-then-distorted compared with distorted-then-clear feedback, in Experiment 1). This perceptual learning generalized to untrained words suggesting a sublexical locus for learning and was equivalent for word and nonword training stimuli (Experiment 2). These findings point to the crucial involvement of phonological short-term memory and top-down processes in the perceptual learning of noise-vocoded speech. Similar processes may facilitate comprehension of speech in an unfamiliar accent or following cochlear implantation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RZ3DPUZP/Hervais-Adelman et al. - 2008 - Perceptual learning of noise vocoded words Effect.pdf}
}

@article{heusser_geometric_2021,
  title = {Geometric Models Reveal Behavioural and Neural Signatures of Transforming Experiences into Memories},
  author = {Heusser, Andrew C. and Fitzpatrick, Paxton C. and Manning, Jeremy R.},
  year = {2021},
  month = feb,
  journal = {Nature Human Behaviour},
  pages = {1--15},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01051-6},
  abstract = {How do we preserve and distort our ongoing experiences when encoding them into episodic memories? The mental contexts in which we interpret experiences are often person-specific, even when the experiences themselves are shared. Here we develop a geometric framework for mathematically characterizing the subjective conceptual content of dynamic naturalistic experiences. We model experiences and memories as trajectories through word-embedding spaces whose coordinates reflect the universe of thoughts under consideration. Memory encoding can then be modelled as geometrically preserving or distorting the `shape' of the original experience. We applied our approach to data collected as participants watched and verbally recounted a television episode while undergoing functional neuroimaging. Participants' recountings preserved coarse spatial properties (essential narrative elements) but not fine spatial scale (low-level) details of the episode's trajectory. We also identified networks of brain structures sensitive to these trajectory shapes.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/E47PDKEZ/Heusser et al. - 2021 - Geometric models reveal behavioural and neural sig.pdf;/Users/xzfang/Zotero/storage/ZRNTAJ4I/s41562-021-01051-6.html}
}

@article{heyes_what_2021,
  title = {{{WHAT HAPPENED TO MIRROR NEURONS}}?},
  author = {Heyes, Cecilia and Catmur, Caroline},
  year = {2021},
  pages = {37},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KLUIZKF4/Heyes and Catmur - WHAT HAPPENED TO MIRROR NEURONS.pdf}
}

@article{hickok_cortical_2007,
  title = {The Cortical Organization of Speech Processing},
  author = {Hickok, Gregory and Poeppel, David},
  year = {2007},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {8},
  number = {5},
  pages = {393--402},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn2113},
  abstract = {Decades of research have not yet succeeded in definitively characterizing the neuroanatomy of speech processing. Hickok and Poeppel describe a dual-stream model of speech processing and discuss how this model can account for some of the field's paradoxical findings.},
  copyright = {2007 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LJ3SIQLR/Hickok and Poeppel - 2007 - The cortical organization of speech processing.pdf;/Users/xzfang/Zotero/storage/574XCJHL/nrn2113.html}
}

@article{hillebrand_direction_2016,
  title = {Direction of Information Flow in Large-Scale Resting-State Networks Is Frequency-Dependent},
  author = {Hillebrand, Arjan and Tewarie, Prejaas and van Dellen, Edwin and Yu, Meichen and Carbo, Ellen W. S. and Douw, Linda and Gouw, Alida A. and van Straaten, Elisabeth C. W. and Stam, Cornelis J.},
  year = {2016},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {14},
  pages = {3867--3872},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1515657113},
  abstract = {Normal brain function requires interactions between spatially separated, and functionally specialized, macroscopic regions, yet the directionality of these interactions in large-scale functional networks is unknown. Magnetoencephalography was used to determine the directionality of these interactions, where directionality was inferred from time series of beamformer-reconstructed estimates of neuronal activation, using a recently proposed measure of phase transfer entropy. We observed well-organized posterior-to-anterior patterns of information flow in the higher-frequency bands (alpha1, alpha2, and beta band), dominated by regions in the visual cortex and posterior default mode network. Opposite patterns of anterior-to-posterior flow were found in the theta band, involving mainly regions in the frontal lobe that were sending information to a more distributed network. Many strong information senders in the theta band were also frequent receivers in the alpha2 band, and vice versa. Our results provide evidence that large-scale resting-state patterns of information flow in the human brain form frequency-dependent reentry loops that are dominated by flow from parieto-occipital cortex to integrative frontal areas in the higher-frequency bands, which is mirrored by a theta band anterior-to-posterior flow.},
  chapter = {Biological Sciences},
  langid = {english},
  pmid = {27001844},
  keywords = {atlas-based beamforming,information flow,magnetoencephalography,phase transfer entropy,resting-state networks},
  file = {/Users/xzfang/Zotero/storage/A2HT9NRE/Hillebrand et al. - 2016 - Direction of information flow in large-scale resti.pdf;/Users/xzfang/Zotero/storage/PW58REBX/3867.html}
}

@article{hillenbrand_acoustic_1995,
  ids = {_acoustic_},
  title = {Acoustic Characteristics of {{American English}} Vowels},
  author = {Hillenbrand, J. and Getty, L. A. and Clark, M. J. and Wheeler, K.},
  year = {1995},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {97},
  number = {5 Pt 1},
  pages = {3099--3111},
  issn = {0001-4966},
  doi = {10.1121/1.411872},
  abstract = {The purpose of this study was to replicate and extend the classic study of vowel acoustics by Peterson and Barney (PB) [J. Acoust. Soc. Am. 24, 175-184 (1952)]. Recordings were made of 45 men, 48 women, and 46 children producing the vowels /i,I,e, epsilon,ae,a, [symbol: see text],O,U,u, lambda,3 iota/ in h-V-d syllables. Formant contours for F1-F4 were measured from LPC spectra using a custom interactive editing tool. For comparison with the PB data, formant patterns were sampled at a time that was judged by visual inspection to be maximally steady. Analysis of the formant data shows numerous differences between the present data and those of PB, both in terms of average frequencies of F1 and F2, and the degree of overlap among adjacent vowels. As with the original study, listening tests showed that the signals were nearly always identified as the vowel intended by the talker. Discriminant analysis showed that the vowels were more poorly separated than the PB data based on a static sample of the formant pattern. However, the vowels can be separated with a high degree of accuracy if duration and spectral change information is included.},
  langid = {english},
  pmid = {7759650},
  keywords = {Adult,Child,Female,Humans,Male,Phonetics,Speech Acoustics},
  file = {/Users/xzfang/Zotero/storage/D78RKIQZ/Acoustic Characteristics of American English Vowel.pdf}
}

@article{hills_optimal_2012,
  title = {Optimal Foraging in Semantic Memory},
  author = {Hills, Thomas T. and Jones, Michael N. and Todd, Peter M.},
  year = {2012},
  month = apr,
  journal = {Psychological Review},
  volume = {119},
  number = {2},
  pages = {431--440},
  issn = {1939-1471},
  doi = {10.1037/a0027373},
  abstract = {Do humans search in memory using dynamic local-to-global search strategies similar to those that animals use to forage between patches in space? If so, do their dynamic memory search policies correspond to optimal foraging strategies seen for spatial foraging? Results from a number of fields suggest these possibilities, including the shared structure of the search problems-searching in patchy environments-and recent evidence supporting a domain-general cognitive search process. To investigate these questions directly, we asked participants to recover from memory as many animal names as they could in 3 min. Memory search was modeled over a representation of the semantic search space generated from the BEAGLE memory model of Jones and Mewhort (2007), via a search process similar to models of associative memory search (e.g., Raaijmakers \& Shiffrin, 1981). We found evidence for local structure (i.e., patches) in memory search and patch depletion preceding dynamic local-to-global transitions between patches. Dynamic models also significantly outperformed nondynamic models. The timing of dynamic local-to-global transitions was consistent with optimal search policies in space, specifically the marginal value theorem (Charnov, 1976), and participants who were more consistent with this policy recalled more items.},
  langid = {english},
  pmid = {22329683},
  keywords = {Animals,Appetitive Behavior,Association Learning,Female,Humans,Male,Memory,Mental Recall,Models; Psychological,Neuropsychological Tests,Problem Solving,Psychological Theory,Semantics,Spatial Behavior,Time Factors},
  file = {/Users/xzfang/Zotero/storage/XNY47SSU/Hills et al. - 2012 - Optimal foraging in semantic memory.pdf}
}

@article{hilton_linguistic_2021,
  title = {Linguistic Syncopation: {{Meter-syntax}} Alignment Affects Sentence Comprehension and Sensorimotor Synchronization},
  shorttitle = {Linguistic Syncopation},
  author = {Hilton, Courtney B. and Goldwater, Micah B.},
  year = {2021},
  month = dec,
  journal = {Cognition},
  volume = {217},
  pages = {104880},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104880},
  abstract = {The hierarchical organization of speech rhythm into meter putatively confers cognitive affordances for perception, memory, and motor coordination. Meter also aligns with phrasal structure in systematic ways. In this paper, we show that this alignment affects the robustness of syntactic comprehension and discuss possible underlying mechanisms. In two experiments, we manipulated meter-syntax alignment while sentences with relative clause structures were either read as text (experiment 1, n~=~40) or listened to as speech (experiment 2, n~=~40). In experiment 2, we also measured the stability with which participants could tap in time with the metrical accents in the sentences they were comprehending. In addition to making more mistakes, sensorimotor synchronization was disrupted when syntactic cues clashed with the metrical context. We suggest that this reflects a tight coordination of top-down linguistic knowledge with the sensorimotor system to optimize comprehension.},
  langid = {english},
  keywords = {Meter,Rhythm,Sensorimotor synchronization,Sentence processing,Syntax},
  file = {/Users/xzfang/Zotero/storage/DFA23HC7/Hilton and Goldwater - 2021 - Linguistic syncopation Meter-syntax alignment aff.pdf;/Users/xzfang/Zotero/storage/SUVALINE/S0010027721003012.html}
}

@article{hilverman_evidence_,
  title = {Evidence of Impaired Naming in Patients with Hippocampal Amnesia},
  author = {Hilverman, Caitlin and Duff, Melissa C.},
  journal = {Hippocampus},
  volume = {n/a},
  number = {n/a},
  issn = {1098-1063},
  doi = {10.1002/hipo.23325},
  abstract = {Object naming involves accessing meaning and retrieving the associated word form from remote semantic memory. Historically, previously acquired semantic knowledge (i.e., remote semantic memory) was thought to be independent of the hippocampus via neocortical consolidation. This view is based on evidence demonstrating a dissociation in behavior in patients with hippocampal amnesia: amnesic patients are impaired in acquiring new vocabulary yet can name and define previously acquired words. More recently, the view that remote semantic memory is hippocampus-independent has been challenged by the documentation of disruptions in aspects of remote semantic memory in patients with hippocampal amnesia, particularly in language use and depth of semantic knowledge. Based on these findings, we hypothesized that the hippocampus plays a long-term role in remote semantic memory. We tested amnesic patients and demographically matched healthy comparison participants in an extensive naming task using photographic images of objects normalized for familiarity, object agreement, and visual complexity. Amnesic patients were less likely to correctly name objects than healthy comparison participants. Further, amnesic patients' performance worsened for words that were less familiar, more visually complex, and had less object agreement. These findings suggest that the hippocampus may play a long-term role in semantic memory processes, rather than a time-limited role in the initial acquisition of semantic information, and that hippocampal damage can disrupt object naming.},
  copyright = {\textcopyright{} 2021 Wiley Periodicals LLC},
  langid = {english},
  keywords = {declarative memory,hippocampal amnesia,naming},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.23325},
  file = {/Users/xzfang/Zotero/storage/LMSI4CAJ/Hilverman and Duff - Evidence of impaired naming in patients with hippo.pdf;/Users/xzfang/Zotero/storage/LKB7PHQF/hipo.html}
}

@article{himberger_principles_2018,
  title = {Principles of {{Temporal Processing Across}} the {{Cortical Hierarchy}}},
  author = {Himberger, Kevin D. and Chien, Hsiang-Yun and Honey, Christopher J.},
  year = {2018},
  month = oct,
  journal = {Neuroscience},
  series = {Sensory {{Sequence Processing}} in the {{Brain}}},
  volume = {389},
  pages = {161--174},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2018.04.030},
  abstract = {The world is richly structured on multiple spatiotemporal scales. In order to represent spatial structure, many machine-learning models repeat a set of basic operations at each layer of a hierarchical architecture. These iterated spatial operations \textendash{} including pooling, normalization and pattern completion \textendash{} enable these systems to recognize and predict spatial structure, while robust to changes in the spatial scale, contrast and noisiness of the input signal. Because our brains also process temporal information that is rich and occurs across multiple time scales, might the brain employ an analogous set of operations for temporal information processing? Here we define a candidate set of temporal operations, and we review evidence that they are implemented in the mammalian cerebral cortex in a hierarchical manner. We conclude that multiple consecutive stages of cortical processing can be understood to perform temporal pooling, temporal normalization and temporal pattern completion.},
  langid = {english},
  keywords = {hierarchy,predictive coding,sequence processing,temporal integration,timescales},
  file = {/Users/xzfang/Zotero/storage/6R4Q452S/Himberger et al. - 2018 - Principles of Temporal Processing Across the Corti.pdf;/Users/xzfang/Zotero/storage/EV6RHT9M/S0306452218302951.html}
}

@article{hinton_deep_2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/W5P2BHY9/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@article{hjortkjaer_cortical_2020,
  title = {Cortical Oscillations and Entrainment in Speech Processing during Working Memory Load},
  author = {Hjortkj{\ae}r, Jens and {M{\"a}rcher-R{\o}rsted}, Jonatan and Fuglsang, S{\o}ren A. and Dau, Torsten},
  year = {2020},
  journal = {European Journal of Neuroscience},
  volume = {51},
  number = {5},
  pages = {1279--1289},
  issn = {1460-9568},
  doi = {10.1111/ejn.13855},
  abstract = {Neuronal oscillations are thought to play an important role in working memory (WM) and speech processing. Listening to speech in real-life situations is often cognitively demanding but it is unknown whether WM load influences how auditory cortical activity synchronizes to speech features. Here, we developed an auditory n-back paradigm to investigate cortical entrainment to speech envelope fluctuations under different degrees of WM load. We measured the electroencephalogram, pupil dilations and behavioural performance from 22 subjects listening to continuous speech with an embedded n-back task. The speech stimuli consisted of long spoken number sequences created to match natural speech in terms of sentence intonation, syllabic rate and phonetic content. To burden different WM functions during speech processing, listeners performed an n-back task on the speech sequences in different levels of background noise. Increasing WM load at higher n-back levels was associated with a decrease in posterior alpha power as well as increased pupil dilations. Frontal theta power increased at the start of the trial and increased additionally with higher n-back level. The observed alpha\textendash theta power changes are consistent with visual n-back paradigms suggesting general oscillatory correlates of WM processing load. Speech entrainment was measured as a linear mapping between the envelope of the speech signal and low-frequency cortical activity ({$<$} 13 Hz). We found that increases in both types of WM load (background noise and n-back level) decreased cortical speech envelope entrainment. Although entrainment persisted under high load, our results suggest a top-down influence of WM processing on cortical speech entrainment.},
  langid = {english},
  keywords = {alpha and theta oscillations,EEG,n-back task,pupillometry,speech entrainment},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.13855},
  file = {/Users/xzfang/Zotero/storage/ECKHAMWW/HjortkjÃ¦r et al. - 2020 - Cortical oscillations and entrainment in speech pr.pdf;/Users/xzfang/Zotero/storage/638XDWK3/ejn.html}
}

@article{hochstein_view_2002,
  title = {View from the {{Top}}: {{Hierarchies}} and {{Reverse Hierarchies}} in the {{Visual System}}},
  shorttitle = {View from the {{Top}}},
  author = {Hochstein, Shaul and Ahissar, Merav},
  year = {2002},
  month = dec,
  journal = {Neuron},
  volume = {36},
  number = {5},
  pages = {791--804},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(02)01091-7},
  langid = {english},
  pmid = {12467584},
  file = {/Users/xzfang/Zotero/storage/LXRTZLI8/Hochstein and Ahissar - 2002 - View from the Top Hierarchies and Reverse Hierarc.pdf;/Users/xzfang/Zotero/storage/MAP7AQMH/S0896-6273(02)01091-7.html}
}

@article{hoffman_broadly_2014,
  title = {Broadly Speaking: {{Vocabulary}} in Semantic Dementia Shifts towards General, Semantically Diverse Words},
  shorttitle = {Broadly Speaking},
  author = {Hoffman, Paul and Meteyard, Lotte and Patterson, Karalyn},
  year = {2014},
  month = jun,
  journal = {Cortex},
  series = {Language, {{Computers}} and {{Cognitive Neuroscience}}},
  volume = {55},
  pages = {30--42},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2012.11.004},
  abstract = {One of the cardinal features of semantic dementia (SD) is a steady reduction in expressive vocabulary. We investigated the nature of this breakdown by assessing the psycholinguistic characteristics of words produced spontaneously by SD patients during an autobiographical memory interview. Speech was analysed with respect to frequency and imageability, and a recently-developed measure called semantic diversity. This measure quantifies the degree to which a word can be used in a broad range of different linguistic contexts. We used this measure in a formal exploration of the tendency for SD patients to replace specific terms with more vague and general words, on the assumption that more specific words are used in a more constrained set of contexts. Relative to healthy controls, patients were less likely to produce low-frequency, high-imageability words, and more likely to produce highly frequent, abstract words. These changes in the lexical-semantic landscape were related to semantic diversity: the highly frequent and abstract words most prevalent in the patients' speech were also the most semantically diverse. In fact, when the speech samples of healthy controls were artificially engineered such that low semantic diversity words (e.g., garage, spanner) were replaced with broader terms (e.g., place, thing), the characteristics of their speech production came to closely resemble that of SD patients. A similar simulation in which low-frequency words were replaced was less successful in replicating the patient data. These findings indicate systematic biases in the deterioration of lexical-semantic space in SD. As conceptual knowledge degrades, speech increasingly consists of general terms that can be applied in a broad range of linguistic contexts and convey less specific information.},
  langid = {english},
  keywords = {Conceptual knowledge,Latent semantic analysis,Semantic diversity,Semantic memory,Spontaneous speech},
  file = {/Users/xzfang/Zotero/storage/T3VXLIWN/Hoffman et al. - 2014 - Broadly speaking Vocabulary in semantic dementia .pdf}
}

@article{holyoak_emergence_2021,
  title = {Emergence of Relational Reasoning},
  author = {Holyoak, Keith J and Lu, Hongjing},
  year = {2021},
  month = feb,
  journal = {Current Opinion in Behavioral Sciences},
  volume = {37},
  pages = {118--124},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2020.11.012},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2BG66W3B/Holyoak and Lu - 2021 - Emergence of relational reasoning.pdf}
}

@article{honey_slow_2012,
  title = {Slow {{Cortical Dynamics}} and the {{Accumulation}} of {{Information}} over {{Long Timescales}}},
  author = {Honey, Christopher~J. and Thesen, Thomas and Donner, Tobias~H. and Silbert, Lauren~J. and Carlson, Chad~E. and Devinsky, Orrin and Doyle, Werner~K. and Rubin, Nava and Heeger, David~J. and Hasson, Uri},
  year = {2012},
  month = oct,
  journal = {Neuron},
  volume = {76},
  number = {2},
  pages = {423--434},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2012.08.011},
  abstract = {Making sense of the world requires us to process information over multiple timescales. We sought to identify brain regions that accumulate information over short and long timescales and to characterize the distinguishing features of their dynamics. We~recorded electrocorticographic (ECoG) signals from individuals watching intact and scrambled movies. Within sensory regions, fluctuations of high-frequency (64\textendash 200~Hz) power reliably tracked instantaneous low-level properties of the intact and scrambled movies. Within higher order regions, the power fluctuations were more reliable for the intact movie than the scrambled movie, indicating that these regions accumulate information over relatively long time periods (several seconds or longer). Slow ({$<$}0.1~Hz) fluctuations of high-frequency power with time courses locked to the movies were observed throughout the cortex. Slow fluctuations were relatively larger in regions that accumulated information over longer time periods, suggesting a connection between slow neuronal population dynamics and temporally extended information processing.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HMLQ7KZ9/Honey et al. - 2012 - Slow Cortical Dynamics and the Accumulation of Inf.pdf;/Users/xzfang/Zotero/storage/2BXI9MFX/S0896627312007179.html}
}

@article{horberg_rational_2021,
  title = {A {{Rational Model}} of {{Incremental Argument Interpretation}}: {{The Comprehension}} of {{Swedish Transitive Clauses}}},
  shorttitle = {A {{Rational Model}} of {{Incremental Argument Interpretation}}},
  author = {H{\"o}rberg, Thomas and Jaeger, T. Florian},
  year = {2021},
  journal = {Frontiers in Psychology},
  volume = {12},
  pages = {4679},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.674202},
  abstract = {A central component of sentence understanding is verb-argument interpretation, determining how the referents in the sentence are related to the events or states expressed by the verb. Previous work has found that comprehenders change their argument interpretations incrementally as the sentence unfolds, based on morphosyntactic (e.g., case, agreement), lexico-semantic (e.g., animacy, verb-argument fit), and discourse cues (e.g., givenness). However, it is still unknown whether these cues have a privileged role in language processing, or whether their effects on argument interpretation originate in implicit expectations based on the joint distribution of these cues with argument assignments experienced in previous language input. We compare the former, linguistic account against the latter, expectation-based account, using data from production and comprehension of transitive clauses in Swedish. Based on a large corpus of Swedish, we develop a rational (Bayesian) model of incremental argument interpretation. This model predicts the processing difficulty experienced at different points in the sentence as a function of the Bayesian surprise associated with changes in expectations over possible argument interpretations. We then test the model against reading times from a self-paced reading experiment on Swedish. We find Bayesian surprise to be a significant predictor of reading times, complementing effects of word surprisal. Bayesian surprise also captures the qualitative effects of morpho-syntactic and lexico-semantic cues. Additional model comparisons find that it\textemdash with a single degree of freedom\textemdash captures much, if not all, of the effects associated with these cues. This suggests that the effects of form- and meaning-based cues to argument interpretation are mediated through expectation-based processing.},
  file = {/Users/xzfang/Zotero/storage/RGYIVXTW/HÃ¶rberg and Jaeger - 2021 - A Rational Model of Incremental Argument Interpret.pdf}
}

@article{huang_why_2021,
  title = {Why Do Readers Fail to Notice Word Transpositions, Omissions, and Repetitions? {{A}} Review of Recent Evidence and Theory},
  shorttitle = {Why Do Readers Fail to Notice Word Transpositions, Omissions, and Repetitions?},
  author = {Huang, Kuan-Jung and Staub, Adrian},
  year = {2021},
  journal = {Language and Linguistics Compass},
  volume = {15},
  number = {7},
  pages = {e12434},
  issn = {1749-818X},
  doi = {10.1111/lnc3.12434},
  abstract = {Most readers have had the experience of initially failing to notice an omission or repetition of a function word, or a transposition of two adjacent words. In the present article, we review recent research investigating this phenomenon. We emphasize that failure to notice such errors is of substantial theoretical interest, given what we have learned about how systematically and incrementally readers inspect and process text. We endorse the idea that a process of rational inference may play a critical role, while we cast doubt on the idea that failure to notice errors arises from parallel processing of multiple words. We review a number of recent studies from our own laboratory that have investigated the relationship between eye movements during reading and noticing, or failing to notice, an error. While the conclusions from these studies are broadly consistent with a rational inference account, we find that when readers fail to notice an error, their eye movements generally show no indication that the error was registered at all. On its surface, this finding may be viewed as inconsistent with the idea that the rational inference process that enables readers to overlook errors is genuinely post-perceptual. We suggest a mechanism by which eye movement control models could account for this finding.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12434},
  file = {/Users/xzfang/Zotero/storage/W7MSZUUZ/Huang and Staub - 2021 - Why do readers fail to notice word transpositions,.pdf;/Users/xzfang/Zotero/storage/8P86CQBP/lnc3.html}
}

@article{hubbard_downstream_2019,
  title = {Downstream {{Behavioral}} and {{Electrophysiological Consequences}} of {{Word Prediction}} on {{Recognition Memory}}},
  author = {Hubbard, Ryan J. and Rommers, Joost and Jacobs, Cassandra L. and Federmeier, Kara D.},
  year = {2019},
  journal = {Frontiers in Human Neuroscience},
  volume = {13},
  pages = {291},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2019.00291},
  abstract = {When people process language, they can use context to predict upcoming information, influencing processing and comprehension as seen in both behavioral and neural measures. Although numerous studies have shown immediate facilitative effects of confirmed predictions, the downstream consequences of prediction have been less explored. In the current study, we examined those consequences by probing participants' recognition memory for words after they read sets of sentences. Participants read strongly and weakly constraining sentences with expected or unexpected endings (``I added my name to the list/basket''), and later were tested on their memory for the sentence endings while EEG was recorded. Critically, the memory test contained words that were predictable (``list'') but were never read (participants saw ``basket''). Behaviorally, participants showed successful discrimination between old and new items, but false alarmed to the expected-item lures more often than to new items, showing that predicted words or concepts can linger, even when predictions are disconfirmed. Although false alarm rates did not differ by constraint, event-related potentials (ERPs) differed between false alarms to strongly and weakly predictable words. Additionally, previously unexpected (compared to previously expected) endings that appeared on the memory test elicited larger N1 and LPC amplitudes, suggesting greater attention and episodic recollection. In contrast, highly predictable sentence endings that had been read elicited reduced LPC amplitudes during the memory test. Thus, prediction can facilitate processing in the moment, but can also lead to false memory and reduced recollection for predictable information.},
  file = {/Users/xzfang/Zotero/storage/RZATXMGJ/Hubbard et al. - 2019 - Downstream Behavioral and Electrophysiological Con.pdf}
}

@article{hubbard_representational_2021,
  title = {Representational {{Pattern Similarity}} of {{Electrical Brain Activity Reveals Rapid}} and {{Specific Prediction}} during {{Language Comprehension}}},
  author = {Hubbard, Ryan J and Federmeier, Kara D},
  year = {2021},
  month = sep,
  journal = {Cerebral Cortex},
  volume = {31},
  number = {9},
  pages = {4300--4313},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab087},
  abstract = {Predicting upcoming events is a critical function of the brain, and language provides a fertile testing ground for studying prediction, as comprehenders use context to predict features of upcoming words. Many aspects of the mechanisms of prediction remain elusive, partly due to a lack of methodological tools to probe prediction formation in the moment. To elucidate what features are neurally preactivated and when, we used representational similarity analysis on previously collected sentence reading data. We compared EEG activity patterns elicited by expected and unexpected sentence final words to patterns from the preceding words of the sentence, in both strongly and weakly constraining sentences. Pattern similarity with the final word was increased in an early time window following the presentation of the pre-final word, and this increase was modulated by both expectancy and constraint. This was not seen at earlier words, suggesting that predictions were precisely timed. Additionally, pre-final word activity\textemdash the predicted representation\textemdash had negative similarity with later final word activity, but only for strongly expected words. These findings shed light on the mechanisms of prediction in the brain: rapid preactivation occurs following certain cues, but the predicted features may receive reduced processing upon confirmation.},
  file = {/Users/xzfang/Zotero/storage/BUHCZQ8A/Hubbard and Federmeier - 2021 - Representational Pattern Similarity of Electrical .pdf;/Users/xzfang/Zotero/storage/U88USKFJ/6249808.html}
}

@article{huda_neural_2019,
  title = {Neural Mechanisms of Sensorimotor Transformation and Action Selection},
  author = {Huda, Rafiq and Goard, Michael J. and Pho, Gerald N. and Sur, Mriganka},
  year = {2019},
  journal = {European Journal of Neuroscience},
  volume = {49},
  number = {8},
  pages = {1055--1060},
  issn = {1460-9568},
  doi = {10.1111/ejn.14069},
  abstract = {Ray Guillery made major contributions to our understanding of the development and function of the brain. One of his principal conceptual insights, developed together with Murray Sherman [S.M. Sherman \& R.W. Guillery (2001) Exploring the Thalamus. Elsevier, Amstrerdam; S. Sherman \& R. Guillery (2006) Exploring the Thalamus and Its Role in Cortical Functioning. Academic Press, New York, NY; S.M. Sherman \& R.W. Guillery (2013) Functional Connections of Cortical Areas: A New View from the Thalamus. MIT Press, Cambridge, MA and then in his last book (R. Guillery (2017) The Brain as a Tool: A Neuroscientist's Account. Oxford University Press, Oxford, UK)], was that the brain is a `tool' to understand the world. In this view, the brain does not passively process sensory information and use the result to inform motor outputs. Rather, sensory and motor signals are widely broadcast and inextricably linked, with ongoing sensorimotor transformations serving as the basis for interaction with the outside world. Here, we describe recent studies from our laboratory and others which demonstrate this astute framing of the link among sensation, perception, and action postulated by Guillery and others [G. Deco \& E.T. Rolls (2005) Prog Neurobiol, 76, 236\textendash 256; P. Cisek \& J.F. Kalaska (2010) Annu Rev Neurosci, 33, 269-298]. Guillery situated his understanding in the deeply intertwined relationship between the thalamus and cortex, and importantly in the feedback from cortex to thalamus which in turn influences feed-forward drive to cortex [S.M. Sherman \& R.W. Guillery (2001) Exploring the Thalamus. Elsevier, Amstrerdam; S. Sherman \& R. Guillery (2006) Exploring the Thalamus and Its Role in Cortical Functioning. Academic Press, New York, NY]. We extend these observations to argue that brain mechanisms for sensorimotor transformations involve cortical and subcortical circuits that create internal models as a substrate for action, that a key role of sensory inputs is to update such models, and that a major function of sensorimotor processing underlying cognition is to enable action selection and execution.},
  copyright = {\textcopyright{} 2018 Federation of European Neuroscience Societies and John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {attention,decision-making,evidence accumulation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.14069},
  file = {/Users/xzfang/Zotero/storage/B2QVDDFS/Huda et al. - 2019 - Neural mechanisms of sensorimotor transformation a.pdf;/Users/xzfang/Zotero/storage/FC9NWP35/ejn.html}
}

@article{huettig_individual_2016,
  title = {Individual Differences in Working Memory and Processing Speed Predict Anticipatory Spoken Language Processing in the Visual World},
  author = {Huettig, Falk and Janse, Esther},
  year = {2016},
  month = jan,
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  pages = {80--93},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2015.1047459},
  abstract = {Several mechanisms of predictive language processing have been proposed. The possible influence of mediating factors such as working memory and processing speed, however, has largely been ignored. We sought to find evidence for such an influence using an individual differences approach. 105 participants from 32\textendash 77 years of age received spoken instructions (e.g. ``Kijk naar deCOM afgebeelde pianoCOM''\textendash{} look at the displayed piano) while viewing 4 objects. Articles (Dutch ``het'' or ``de'') were gender-marked such that the article agreed in gender only with the target. Participants could thus use article gender information to predict the target. Multiple regression analyses showed that enhanced working memory abilities and faster processing speed predicted anticipatory eye movements. Models of predictive language processing therefore must take mediating factors into account. More generally, our results are consistent with the notion that working memory grounds language in space and time, linking linguistic and visual\textendash spatial representations.},
  keywords = {Individual differences,language processing,prediction,processing speed,working memory},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2015.1047459},
  file = {/Users/xzfang/Zotero/storage/8LKVSTTG/Huettig and Janse - 2016 - Individual differences in working memory and proce.pdf;/Users/xzfang/Zotero/storage/YUSUGWP9/23273798.2015.html}
}

@article{huettig_prediction_2016,
  title = {Is Prediction Necessary to Understand Language? {{Probably}} Not},
  shorttitle = {Is Prediction Necessary to Understand Language?},
  author = {Huettig, Falk and Mani, Nivedita},
  year = {2016},
  month = jan,
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  pages = {19--31},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2015.1072223},
  abstract = {Some recent theoretical accounts in the cognitive sciences suggest that prediction is necessary to understand language. Here we evaluate this proposal. We consider arguments that prediction provides a unified theoretical principle of the human mind and that it pervades cortical function. We discuss whether evidence of human abilities to detect statistical regularities is necessarily evidence for predictive processing and evaluate suggestions that prediction is necessary for language learning. We point out that not all language users appear to predict language and that suboptimal input makes prediction often very challenging. Prediction, moreover, is strongly context-dependent and impeded by resource limitations. We also argue that it may be problematic that most experimental evidence for predictive language processing comes from ``prediction-encouraging'' experimental set-ups. We conclude that languages can be learned and understood in the absence of prediction. Claims that all language processing is predictive in nature are premature.},
  keywords = {Cognition,language processing,learning,prediction,predictive coding},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2015.1072223},
  file = {/Users/xzfang/Zotero/storage/42M4TUPJ/Huettig and Mani - 2016 - Is prediction necessary to understand language Pr.pdf;/Users/xzfang/Zotero/storage/3DWJKJP3/23273798.2015.html}
}

@article{huettig_tug_2007,
  title = {The Tug of War between Phonological, Semantic and Shape Information in Language-Mediated Visual Search},
  author = {Huettig, Falk and McQueen, James M.},
  year = {2007},
  month = nov,
  journal = {Journal of Memory and Language},
  series = {Language-{{Vision Interaction}}},
  volume = {57},
  number = {4},
  pages = {460--482},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.02.001},
  abstract = {Experiments 1 and 2 examined the time-course of retrieval of phonological, visual-shape and semantic knowledge as Dutch participants listened to sentences and looked at displays of four pictures. Given a sentence with beker, `beaker', for example, the display contained phonological (a beaver, bever), shape (a bobbin, klos), and semantic (a fork, vork) competitors. When the display appeared at sentence onset, fixations to phonological competitors preceded fixations to shape and semantic competitors. When display onset was 200ms before (e.g.) beker, fixations were directed to shape and then semantic competitors, but not phonological competitors. In Experiments 3 and 4, displays contained the printed names of the previously-pictured entities; only phonological competitors were fixated preferentially. These findings suggest that retrieval of phonological, shape and semantic knowledge in the spoken-word and picture-recognition systems is cascaded, and that visual attention shifts are co-determined by the time-course of retrieval of all three knowledge types and by the nature of the information in the visual environment.},
  langid = {english},
  keywords = {Attention,Eye movements,Phonological representations,Semantic representations,Visual representations},
  file = {/Users/xzfang/Zotero/storage/HMLU2IGP/Huettig and McQueen - 2007 - The tug of war between phonological, semantic and .pdf;/Users/xzfang/Zotero/storage/JGQ5I3I5/S0749596X07000198.html}
}

@article{hurk_development_2017,
  title = {Development of Visual Category Selectivity in Ventral Visual Cortex Does Not Require Visual Experience},
  author = {van den Hurk, Job and Baelen, Marc Van and de Beeck, Hans P. Op},
  year = {2017},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {22},
  pages = {E4501-E4510},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1612862114},
  abstract = {To what extent does functional brain organization rely on sensory input? Here, we show that for the penultimate visual-processing region, ventral-temporal cortex (VTC), visual experience is not the origin of its fundamental organizational property, category selectivity. In the fMRI study reported here, we presented 14 congenitally blind participants with face-, body-, scene-, and object-related natural sounds and presented 20 healthy controls with both auditory and visual stimuli from these categories. Using macroanatomical alignment, response mapping, and surface-based multivoxel pattern analysis, we demonstrated that VTC in blind individuals shows robust discriminatory responses elicited by the four categories and that these patterns of activity in blind subjects could successfully predict the visual categories in sighted controls. These findings were confirmed in a subset of blind participants born without eyes and thus deprived from all light perception since conception. The sounds also could be decoded in primary visual and primary auditory cortex, but these regions did not sustain generalization across modalities. Surprisingly, although not as strong as visual responses, selectivity for auditory stimulation in visual cortex was stronger in blind individuals than in controls. The opposite was observed in primary auditory cortex. Overall, we demonstrated a striking similarity in the cortical response layout of VTC in blind individuals and sighted controls, demonstrating that the overall category-selective map in extrastriate cortex develops independently from visual experience.},
  chapter = {PNAS Plus},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {28507127},
  keywords = {blindness,category perception,functional MRI,pattern analysis,ventral-temporal cortex},
  file = {/Users/xzfang/Zotero/storage/YI2VSK8Z/Hurk et al. - 2017 - Development of visual category selectivity in vent.pdf;/Users/xzfang/Zotero/storage/JQ6S5LY3/E4501.html}
}

@article{huth_continuous_2012,
  title = {A {{Continuous Semantic Space Describes}} the {{Representation}} of {{Thousands}} of {{Object}} and {{Action Categories}} across the {{Human Brain}}},
  author = {Huth, Alexander~G. and Nishimoto, Shinji and Vu, An~T. and Gallant, Jack~L.},
  year = {2012},
  month = dec,
  journal = {Neuron},
  volume = {76},
  number = {6},
  pages = {1210--1224},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2012.10.014},
  abstract = {Humans can see and name thousands of distinct object and action categories, so it is unlikely that each category is represented in a distinct brain area. A more efficient scheme would be to represent categories as locations in a continuous semantic space mapped smoothly across the cortical surface. To search for such a space, we used fMRI to measure human brain activity evoked by natural movies. We then used voxelwise models to examine the cortical representation of 1,705 object and action categories. The first few dimensions of the underlying semantic space were recovered from the fit models by principal components analysis. Projection of the recovered semantic space onto cortical flat maps shows that semantic selectivity is organized into smooth gradients that cover much of visual and nonvisual cortex. Furthermore, both the recovered semantic space and the cortical organization of the space are shared across different individuals. Video Abstract},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PV6GTIH4/Huth et al. - 2012 - A Continuous Semantic Space Describes the Represen.pdf}
}

@article{huth_natural_2016,
  title = {Natural Speech Reveals the Semantic Maps That Tile Human Cerebral Cortex},
  author = {Huth, Alexander G. and {de Heer}, Wendy A. and Griffiths, Thomas L. and Theunissen, Fr{\'e}d{\'e}ric E. and Gallant, Jack L.},
  year = {2016},
  month = apr,
  journal = {Nature},
  volume = {532},
  number = {7600},
  pages = {453--458},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature17637},
  abstract = {The meaning of language is represented in regions of the cerebral cortex collectively known as the `semantic system'. However, little of the semantic system has been mapped comprehensively, and the semantic selectivity of most regions is unknown. Here we systematically map semantic selectivity across the cortex using voxel-wise modelling of functional MRI (fMRI) data collected while subjects listened to hours of narrative stories. We show that the semantic system is organized into intricate patterns that seem to be consistent across individuals. We then use a novel generative model to create a detailed semantic atlas. Our results suggest that most areas within the semantic system represent information about specific semantic domains, or groups of related concepts, and our atlas shows which domains are represented in each area. This study demonstrates that data-driven methods\textemdash commonplace in studies of human neuroanatomy and functional connectivity\textemdash provide a powerful and efficient means for mapping functional representations in the brain.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JF9C94PR/Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf;/Users/xzfang/Zotero/storage/IW5WLVQB/nature17637.html}
}

@article{hyde_brains_2004,
  title = {Brains {{That Are}} out of {{Tune}} but in {{Time}}},
  author = {Hyde, Krista L. and Peretz, Isabelle},
  year = {2004},
  month = may,
  journal = {Psychological Science},
  volume = {15},
  number = {5},
  pages = {356--360},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.0956-7976.2004.00683.x},
  abstract = {It is estimated that about 4\% of the general population may have amusia (or tone deafness). Congenital amusia is a lifelong disability for processing music despite normal intellectual, memory, and language skills. Here we present evidence that the disorder stems from a deficit in fine-grained pitch perception. Amusic and control adults were presented with monotonic and isochronous sequences of five tones (i.e., constant pitch and intertone interval). They were required to detect when the fourth tone was displaced in pitch or time. All amusic participants were impaired in detecting the pitch changes, and showed no sign of improvement with practice. In contrast, they detected time changes as well as control adults and exhibited similar improvements with practice. Thus, the degraded pitch perception seen in the amusic individuals cannot be ascribed to nonspecific problems with the task or to poor hearing in general. Rather, the data point to the presence of a congenital neural anomaly that selectively impairs pitch processing.},
  file = {/Users/xzfang/Zotero/storage/GS42YK9D/Hyde and Peretz - 2004 - Brains That Are out of Tune but in Time.pdf}
}

@article{iacoboni_role_2008,
  title = {The Role of Premotor Cortex in Speech Perception: {{Evidence}} from {{fMRI}} and {{rTMS}}},
  shorttitle = {The Role of Premotor Cortex in Speech Perception},
  author = {Iacoboni, Marco},
  year = {2008},
  month = jan,
  journal = {Journal of Physiology-Paris},
  series = {Links and {{Interactions Between Language}} and {{Motor Systems}} in the {{Brain}}},
  volume = {102},
  number = {1},
  pages = {31--34},
  issn = {0928-4257},
  doi = {10.1016/j.jphysparis.2008.03.003},
  abstract = {This article discusses recent functional magnetic resonance imaging (fMRI) and repetitive Transcranial Magnetic Stimulation (rTMS) data that suggest a direct involvement of premotor cortical areas in speech perception. These new data map well onto psychological theories advocating an active role of motor structures in the perception of speech sounds. It is proposed that the perception of speech is enabled \textendash{} at least in part \textendash{} by a process that simulates speech production.},
  langid = {english},
  keywords = {Mirror neurons,Motor theory of speech perception,Premotor cortex,Speech perception}
}

@article{idemaru_word_2011,
  title = {Word Recognition Reflects Dimension-Based Statistical Learning},
  author = {Idemaru, Kaori and Holt, Lori L.},
  year = {2011},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {37},
  number = {6},
  pages = {1939--1956},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1277},
  doi = {10.1037/a0025641},
  abstract = {Speech processing requires sensitivity to long-term regularities of the native language yet demands listeners to flexibly adapt to perturbations that arise from talker idiosyncrasies such as nonnative accent. The present experiments investigate whether listeners exhibit dimension-based statistical learning of correlations between acoustic dimensions defining perceptual space for a given speech segment. While engaged in a word recognition task guided by a perceptually unambiguous voice-onset time (VOT) acoustics to signal beer, pier, deer, or tear, listeners were exposed incidentally to an artificial ``accent'' deviating from English norms in its correlation of the pitch onset of the following vowel (F0) to VOT. Results across four experiments are indicative of rapid, dimension-based statistical learning; reliance on the F0 dimension in word recognition was rapidly down-weighted in response to the perturbation of the correlation between F0 and VOT dimensions. However, listeners did not simply mirror the short-term input statistics. Instead, response patterns were consistent with a lingering influence of sensitivity to the long-term regularities of English. This suggests that the very acoustic dimensions defining perceptual space are not fixed and, rather, are dynamically and rapidly adjusted to the idiosyncrasies of local experience, such as might arise from nonnative-accent, dialect, or dysarthria. The current findings extend demonstrations of ``object-based'' statistical learning across speech segments to include incidental, online statistical learning of regularities residing within a speech segment. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Learning,Native Language,Perceptual Learning,Speech Characteristics,Speech Perception,Word Recognition},
  file = {/Users/xzfang/Zotero/storage/HVDFF7IP/Idemaru and Holt - 2011 - Word recognition reflects dimension-based statisti.pdf;/Users/xzfang/Zotero/storage/Y4PDI9MH/2011-23770-001.html}
}

@article{intraub_visual_2015,
  title = {Visual, Haptic and Bimodal Scene Perception: {{Evidence}} for a Unitary Representation},
  shorttitle = {Visual, Haptic and Bimodal Scene Perception},
  author = {Intraub, Helene and Morelli, Frank and Gagnier, Kristin M.},
  year = {2015},
  month = may,
  journal = {Cognition},
  volume = {138},
  pages = {132--147},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.01.010},
  abstract = {Participants studied seven meaningful scene-regions bordered by removable boundaries (30s each). In Experiment 1 (N=80) participants used visual or haptic exploration and then minutes later, reconstructed boundary position using the same or the alternate modality. Participants in all groups shifted boundary placement outward (boundary extension), but visual study yielded the greater error. Critically, this modality-specific difference in boundary extension transferred without cost in the cross-modal conditions, suggesting a functionally unitary scene representation. In Experiment 2 (N=20), bimodal study led to boundary extension that did not differ from haptic exploration alone, suggesting that bimodal spatial memory was constrained by the more ``conservative'' haptic modality. In Experiment 3 (N=20), as in picture studies, boundary memory was tested 30s after viewing each scene-region and as with pictures, boundary extension still occurred. Results suggest that scene representation is organized around an amodal spatial core that organizes bottom-up information from multiple modalities in combination with top-down expectations about the surrounding world.},
  langid = {english},
  keywords = {Boundary extension,Cross-modal representation,Multisensory (visual and haptic),Scene representation,Spatial memory},
  file = {/Users/xzfang/Zotero/storage/G9R6FNP2/Intraub et al. - 2015 - Visual, haptic and bimodal scene perception Evide.pdf;/Users/xzfang/Zotero/storage/BYS75M3T/S0010027715000190.html}
}

@article{isik_dynamics_2014,
  title = {The Dynamics of Invariant Object Recognition in the Human Visual System},
  author = {Isik, Leyla and Meyers, Ethan M. and Leibo, Joel Z. and Poggio, Tomaso},
  year = {2014},
  month = jan,
  journal = {Journal of Neurophysiology},
  volume = {111},
  number = {1},
  pages = {91--102},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00394.2013},
  abstract = {The human visual system can rapidly recognize objects despite transformations that alter their appearance. The precise timing of when the brain computes neural representations that are invariant to particular transformations, however, has not been mapped in humans. Here we employ magnetoencephalography decoding analysis to measure the dynamics of size- and position-invariant visual information development in the ventral visual stream. With this method we can read out the identity of objects beginning as early as 60 ms. Size- and position-invariant visual information appear around 125 ms and 150 ms, respectively, and both develop in stages, with invariance to smaller transformations arising before invariance to larger transformations. Additionally, the magnetoencephalography sensor activity localizes to neural sources that are in the most posterior occipital regions at the early decoding times and then move temporally as invariant information develops. These results provide previously unknown latencies for key stages of human-invariant object recognition, as well as new and compelling evidence for a feed-forward hierarchical model of invariant object recognition where invariance increases at each successive visual area along the ventral stream.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9WZ63Z3T/Isik et al. - 2014 - The dynamics of invariant object recognition in th.pdf}
}

@article{isik_perceiving_2017,
  title = {Perceiving Social Interactions in the Posterior Superior Temporal Sulcus},
  author = {Isik, Leyla and Koldewyn, Kami and Beeler, David and Kanwisher, Nancy},
  year = {2017},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {43},
  pages = {E9145-E9152},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1714471114},
  abstract = {Primates are highly attuned not just to social characteristics of individual agents, but also to social interactions between multiple agents. Here we report a neural correlate of the representation of social interactions in the human brain. Specifically, we observe a strong univariate response in the posterior superior temporal sulcus (pSTS) to stimuli depicting social interactions between two agents, compared with (               i               ) pairs of agents not interacting with each other, (               ii               ) physical interactions between inanimate objects, and (               iii               ) individual animate agents pursuing goals and interacting with inanimate objects. We further show that this region contains information about the nature of the social interaction\textemdash specifically, whether one agent is helping or hindering the other. This sensitivity to social interactions is strongest in a specific subregion of the pSTS but extends to a lesser extent into nearby regions previously implicated in theory of mind and dynamic face perception. This sensitivity to the presence and nature of social interactions is not easily explainable in terms of low-level visual features, attention, or the animacy, actions, or goals of individual agents. This region may underlie our ability to understand the structure of our social world and navigate within it.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8HAAWP9K/Isik et al. - 2017 - Perceiving social interactions in the posterior su.pdf}
}

@article{isik_speed_2019,
  title = {The Speed of Human Social Interaction Perception},
  author = {Isik, Leyla and Mynick, Anna and Pantazis, Dimitrios and Kanwisher, Nancy},
  year = {2019},
  month = mar,
  journal = {bioRxiv},
  pages = {579375},
  doi = {10.1101/579375},
  abstract = {{$<$}p{$>$}The ability to detect and understand other people's social interactions is a fundamental part of the human visual experience that develops early in infancy and is shared with other primates. However, the neural computations underlying this ability remain largely unknown. Is the detection of social interactions a rapid perceptual process, or a slower post-perceptual inference? Here we used magnetoencephalography (MEG) decoding and computational modeling to ask whether social interactions can be detected via fast, feedforward processing. Subjects in the MEG viewed snapshots of visually matched real-world scenes containing a pair of people who were either engaged in a social interaction or acting independently. The presence versus absence of a social interaction could be read out from subjects' MEG data spontaneously, even while subjects performed an orthogonal task. This readout generalized across different scenes, revealing abstract representations of social interactions in the human brain. These representations, however, did not come online until quite late, at 300 ms after image onset, well after the time period of feedforward visual processes. In a second experiment, we found that social interaction readout occurred at this same latency even when subjects performed an explicit task detecting social interactions. Consistent with these latency results, a standard feedforward deep neural network did not contain an abstract representation of social interactions at any model layer. We further showed that MEG responses distinguished between different types of social interactions (mutual gaze vs joint attention) even later, around 500 ms after image onset. Taken together, these results suggest that the human brain spontaneously extracts the presence and type of others' social interactions, but does so slowly, likely relying on iterative top-down computations.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZD4LFAXY/Isik et al. - 2019 - The speed of human social interaction perception.pdf;/Users/xzfang/Zotero/storage/X7LRSAUI/579375v1.html}
}

@article{isik_what_2018,
  title = {What Is Changing When: {{Decoding}} Visual Information in Movies from Human Intracranial Recordings},
  shorttitle = {What Is Changing When},
  author = {Isik, Leyla and Singer, Jedediah and Madsen, Joseph R. and Kanwisher, Nancy and Kreiman, Gabriel},
  year = {2018},
  month = oct,
  journal = {NeuroImage},
  volume = {180},
  pages = {147--159},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2017.08.027},
  abstract = {The majority of visual recognition studies have focused on the neural responses to repeated presentations of static stimuli with abrupt and well-defined onset and offset times. In contrast, natural vision involves unique renderings of visual inputs that are continuously changing without explicitly defined temporal transitions. Here we considered commercial movies as a coarse proxy to natural vision. We recorded intracranial field potential signals from 1,284 electrodes implanted in 15 patients with epilepsy while the subjects passively viewed commercial movies. We could rapidly detect large changes in the visual inputs within approximately 100 ms of their occurrence, using exclusively field potential signals from ventral visual cortical areas including the inferior temporal gyrus and inferior occipital gyrus. Furthermore, we could decode the content of those visual changes even in a single movie presentation, generalizing across the wide range of transformations present in a movie. These results present a methodological framework for studying cognition during dynamic and natural vision.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6UKCP7EU/Isik et al. - 2018 - What is changing when Decoding visual information.pdf}
}

@article{ito_how_2017,
  title = {How Robust Are Prediction Effects in Language Comprehension? {{Failure}} to Replicate Article-Elicited {{N400}} Effects},
  shorttitle = {How Robust Are Prediction Effects in Language Comprehension?},
  author = {Ito, Aine and Martin, Andrea E. and Nieuwland, Mante S.},
  year = {2017},
  month = sep,
  journal = {Language, Cognition and Neuroscience},
  volume = {32},
  number = {8},
  pages = {954--965},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2016.1242761},
  abstract = {Current psycholinguistic theory proffers prediction as a central, explanatory mechanism in language processing. However, widely-replicated prediction effects may not mean that prediction is necessary in language processing. As a case in point, C. D. Martin et al. [2013. Bilinguals reading in their second language do not predict upcoming words as native readers do. Journal of Memory and Language, 69(4), 574\textendash 588. doi:10.1016/j.jml.2013.08.001] reported ERP evidence for prediction in native- but not in non-native speakers. Articles mismatching an expected noun elicited larger negativity in the N400 time window compared to articles matching the expected noun in native speakers only. We attempted to replicate these findings, but found no evidence for prediction irrespective of language nativeness. We argue that pre-activation of phonological form of upcoming nouns, as evidenced in article-elicited effects, may not be a robust phenomenon. A view of prediction as a necessary computation in language comprehension must be re-evaluated.},
  keywords = {bilingualism,ERP,language comprehension,N400,Prediction},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2016.1242761},
  file = {/Users/xzfang/Zotero/storage/9V7GZHJC/Ito et al. - 2017 - How robust are prediction effects in language comp.pdf;/Users/xzfang/Zotero/storage/79ND8AA7/23273798.2016.html}
}

@article{ito_predicting_2016,
  title = {Predicting Form and Meaning: {{Evidence}} from Brain Potentials},
  shorttitle = {Predicting Form and Meaning},
  author = {Ito, Aine and Corley, Martin and Pickering, Martin J. and Martin, Andrea E. and Nieuwland, Mante S.},
  year = {2016},
  month = jan,
  journal = {Journal of Memory and Language},
  volume = {86},
  pages = {157--171},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.10.007},
  abstract = {We used ERPs to investigate the pre-activation of form and meaning in language comprehension. Participants read high-cloze sentence contexts (e.g., ``The student is going to the library to borrow a\ldots ''), followed by a word that was predictable (book), form-related (hook) or semantically related (page) to the predictable word, or unrelated (sofa). At a 500ms SOA (Experiment 1), semantically related words, but not form-related words, elicited a reduced N400 compared to unrelated words. At a 700ms SOA (Experiment 2), semantically related words and form-related words elicited reduced N400 effects, but the effect for form-related words occurred in very high-cloze sentences only. At both SOAs, form-related words elicited an enhanced, post-N400 posterior positivity (Late Positive Component effect). The N400 effects suggest that readers can pre-activate meaning and form information for highly predictable words, but form pre-activation is more limited than meaning pre-activation. The post-N400 LPC effect suggests that participants detected the form similarity between expected and encountered input. Pre-activation of word forms crucially depends upon the time that readers have to make predictions, in line with production-based accounts of linguistic prediction.},
  langid = {english},
  keywords = {ERPs,Lexical prediction,Semantic processing,SOA,Word form},
  file = {/Users/xzfang/Zotero/storage/6JWBK9BF/Ito et al. - 2016 - Predicting form and meaning Evidence from brain p.pdf;/Users/xzfang/Zotero/storage/I7ZD622R/S0749596X15001242.html}
}

@article{ito_predicting_2017,
  title = {On Predicting Form and Meaning in a Second Language},
  author = {Ito, Aine and Martin, Andrea E. and Nieuwland, Mante S.},
  year = {2017},
  month = apr,
  journal = {Journal of Experimental Psychology. Learning, Memory, and Cognition},
  volume = {43},
  number = {4},
  pages = {635--652},
  issn = {1939-1285},
  doi = {10.1037/xlm0000315},
  abstract = {We used event-related potentials (ERP) to investigate whether Spanish-English bilinguals preactivate form and meaning of predictable words. Participants read high-cloze sentence contexts (e.g., "The student is going to the library to borrow a . . ."), followed by the predictable word (book), a word that was form-related (hook) or semantically related (page) to the predictable word, or an unrelated word (sofa). Word stimulus onset synchrony (SOA) was 500 ms (Experiment 1) or 700 ms (Experiment 2). In both experiments, all nonpredictable words elicited classic N400 effects. Form-related and unrelated words elicited similar N400 effects. Semantically related words elicited smaller N400s than unrelated words, which however, did not depend on cloze value of the predictable word. Thus, we found no N400 evidence for preactivation of form or meaning at either SOA, unlike native-speaker results (Ito, Corley et al., 2016). However, non-native speakers did show the post-N400 posterior positivity (LPC effect) for form-related words like native speakers, but only at the slower SOA. This LPC effect increased gradually with cloze value of the predictable word. We do not interpret this effect as necessarily demonstrating prediction, but rather as evincing combined effects of top-down activation (contextual meaning) and bottom-up activation (form similarity) that result in activation of unseen words that fit the context well, thereby leading to an interpretation conflict reflected in the LPC. Although there was no evidence that non-native speakers preactivate form or meaning, non-native speakers nonetheless appear to use bottom-up and top-down information to constrain incremental interpretation much like native speakers do. (PsycINFO Database Record},
  langid = {english},
  pmid = {27668483},
  keywords = {Adult,Brain,Brain Mapping,Electroencephalography,Evoked Potentials,Female,Form Perception,Humans,Language,Male,Multilingualism,Photic Stimulation,Reading,Semantics,Time Factors,Vocabulary,Young Adult},
  file = {/Users/xzfang/Zotero/storage/HC8N7RXT/Ito et al. - 2017 - On predicting form and meaning in a second languag.pdf}
}

@article{ivanova_comprehension_2020,
  title = {Comprehension of Computer Code Relies Primarily on Domain-General Executive Brain Regions},
  author = {Ivanova, Anna A and Srikant, Shashank and Sueoka, Yotaro and Kean, Hope H and Dhamala, Riva and O'Reilly, Una-May and Bers, Marina U and Fedorenko, Evelina},
  year = {2020},
  month = dec,
  journal = {eLife},
  volume = {9},
  pages = {e58906},
  issn = {2050-084X},
  doi = {10.7554/eLife.58906},
  abstract = {Computer programming is a novel cognitive tool that has transformed modern society. What cognitive and neural mechanisms support this skill? Here, we used functional magnetic resonance imaging to investigate two candidate brain systems: the multiple demand (MD) system, typically recruited during math, logic, problem solving, and executive tasks, and the language system, typically recruited during linguistic processing. We examined MD and language system responses to code written in Python, a text-based programming language (Experiment 1) and in ScratchJr, a graphical programming language (Experiment 2); for both, we contrasted responses to code problems with responses to content-matched sentence problems. We found that the MD system exhibited strong bilateral responses to code in both experiments, whereas the language system responded strongly to sentence problems, but weakly or not at all to code problems. Thus, the MD system supports the use of novel cognitive tools even when the input is structurally similar to natural language.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8ZYRLAAT/Ivanova et al. - 2020 - Comprehension of computer code relies primarily on.pdf}
}

@article{ivanova_language_2021,
  title = {The {{Language Network Is Recruited}} but {{Not Required}} for {{Nonverbal Event Semantics}}},
  author = {Ivanova, Anna A. and Mineroff, Zachary and Zimmerer, Vitor and Kanwisher, Nancy and Varley, Rosemary and Fedorenko, Evelina},
  year = {2021},
  month = mar,
  journal = {Neurobiology of Language},
  volume = {2},
  number = {2},
  pages = {176--201},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00030},
  abstract = {The ability to combine individual concepts of objects, properties, and actions into complex representations of the world is often associated with language. Yet combinatorial event-level representations can also be constructed from nonverbal input, such as visual scenes. Here, we test whether the language network in the human brain is involved in and necessary for semantic processing of events presented nonverbally. In Experiment 1, we scanned participants with fMRI while they performed a semantic plausibility judgment task versus a difficult perceptual control task on sentences and line drawings that describe/depict simple agent\textendash patient interactions. We found that the language network responded robustly during the semantic task performed on both sentences and pictures (although its response to sentences was stronger). Thus, language regions in healthy adults are engaged during a semantic task performed on pictorial depictions of events. But is this engagement necessary? In Experiment 2, we tested two individuals with global aphasia, who have sustained massive damage to perisylvian language areas and display severe language difficulties, against a group of age-matched control participants. Individuals with aphasia were severely impaired on the task of matching sentences to pictures. However, they performed close to controls in assessing the plausibility of pictorial depictions of agent\textendash patient interactions. Overall, our results indicate that the left frontotemporal language network is recruited but not necessary for semantic processing of nonverbally presented events.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KHGT7SN6/Ivanova et al. - 2021 - The Language Network Is Recruited but Not Required.pdf}
}

@article{jackendoff_morphology_2020,
  title = {Morphology and {{Memory}}: {{Toward}} an {{Integrated Theory}}},
  shorttitle = {Morphology and {{Memory}}},
  author = {Jackendoff, Ray and Audring, Jenny},
  year = {2020},
  journal = {Topics in Cognitive Science},
  volume = {12},
  number = {1},
  pages = {170--196},
  issn = {1756-8765},
  doi = {10.1111/tops.12334},
  abstract = {Framed in psychological terms, the basic question of linguistic theory is what is stored in memory, and in what form. Traditionally, what is stored is divided into grammar and lexicon, where grammar contains the rules and the lexicon is an unstructured list of exceptions. We develop an alternative view in which rules of grammar are simply lexical items that contain variables, and in which rules have two functions. In their generative function, they are used to build novel structures, just as in traditional generative linguistics. In their relational function, they capture generalizations over stored items in the lexicon, a role not seriously explored in traditional linguistic theory. The result is a highly structured lexicon with rich patterns among stored items. We further explore the possibility that this sort of structuring is not specific to language, but appears in other cognitive domains as well, such as the structure of physical objects, of music, and of geographical and social knowledge. The differences among cognitive domains do not lie in this overall texture, but in the materials over which stored relations are defined. The challenge is to develop theories of representation in these other domains comparable to that for language.},
  langid = {english},
  keywords = {Lexicon,Memory,Morphology,Words and rules},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12334},
  file = {/Users/xzfang/Zotero/storage/4MCJCUK5/Jackendoff and Audring - 2020 - Morphology and Memory Toward an Integrated Theory.pdf;/Users/xzfang/Zotero/storage/745J7A44/tops.html}
}

@article{jackendoff_musical_1991,
  title = {Musical {{Parsing}} and {{Musical Affect}}},
  author = {Jackendoff, Ray},
  year = {1991},
  journal = {Music Perception: An Interdisciplinary Journal},
  volume = {9},
  number = {2},
  pages = {199--229},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.2307/40285529},
  abstract = {This paper explores the issue of what is going on in a listener's mind during the real-time processing of music, such that it is possible to account for the listener's understanding of the music. The issue will be approached through evidence internal to music itself, and also by analogy with evidence from the processing of language. I will then examine how processing of the sort I propose provides a basis for considering a particular issue in the theory of musical affect.},
  file = {/Users/xzfang/Zotero/storage/UDQ3XSBT/Jackendoff - 1991 - Musical Parsing and Musical Affect.pdf}
}

@article{jacob_qualitative_2021,
  title = {Qualitative Similarities and Differences in Visual Object Representations between Brains and Deep Networks},
  author = {Jacob, Georgin and Pramod, R. T. and Katti, Harish and Arun, S. P.},
  year = {2021},
  month = mar,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {1872},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22078-3},
  abstract = {Deep neural networks have revolutionized computer vision, and their object representations across layers match coarsely with visual cortical areas in the brain. However, whether these representations exhibit qualitative patterns seen in human perception or brain representations remains unresolved. Here, we recast well-known perceptual and neural phenomena in terms of distance comparisons, and ask whether they are present in feedforward deep neural networks trained for object recognition. Some phenomena were present in randomly initialized networks, such as the global advantage effect, sparseness, and relative size. Many others were present after object recognition training, such as the Thatcher effect, mirror confusion, Weber's law, relative size, multiple object normalization and correlated sparseness. Yet other phenomena were absent in trained networks, such as 3D shape processing, surface invariance, occlusion, natural parts and the global advantage. These findings indicate sufficient conditions for the emergence of these phenomena in brains and deep networks, and offer clues to the properties that could be incorporated to improve deep networks.},
  copyright = {2021 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4DNIKWPL/Jacob et al. - 2021 - Qualitative similarities and differences in visual.pdf;/Users/xzfang/Zotero/storage/FDTMLTZN/s41467-021-22078-3.html}
}

@article{jacobs_selfpriming_2019,
  title = {Self-{{Priming}} in {{Production}}: {{Evidence}} for a {{Hybrid Model}} of {{Syntactic Priming}}},
  shorttitle = {Self-{{Priming}} in {{Production}}},
  author = {Jacobs, Cassandra L. and Cho, Sun-Joo and Watson, Duane G.},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {7},
  pages = {e12749},
  issn = {1551-6709},
  doi = {10.1111/cogs.12749},
  abstract = {Syntactic priming in language production is the increased likelihood of using a recently encountered syntactic structure. In this paper, we examine two theories of why speakers can be primed: error-driven learning accounts (Bock, Dell, Chang, \& Onishi, 2007; Chang, Dell, \& Bock, 2006) and activation-based accounts (Pickering \& Branigan, 1999; Reitter, Keller, \& Moore, 2011). Both theories predict that speakers should be primed by the syntactic choices of others, but only activation-based accounts predict that speakers should be able to prime themselves. Here we test whether speakers can be primed by their own productions in three behavioral experiments and find evidence of structural persistence following both comprehension and speakers' own productions. We also find that comprehension-based priming effects are larger for rarer syntactic structures than for more common ones, which is most consistent with error-driven accounts. Because neither error-driven accounts nor activation-based accounts fully explain the data, we propose a hybrid model.},
  copyright = {\textcopyright{} 2019 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Implicit learning,Language production,Structural persistence,Syntactic priming},
  file = {/Users/xzfang/Zotero/storage/ASHRJDQI/Jacobs et al. - 2019 - Self-Priming in Production Evidence for a Hybrid .pdf;/Users/xzfang/Zotero/storage/BNYVFXGW/cogs.html}
}

@article{jacoby_discourselevel_2020,
  title = {Discourse-Level Comprehension Engages Medial Frontal {{Theory}} of {{Mind}} Brain Regions Even for Expository Texts},
  author = {Jacoby, Nir and Fedorenko, Evelina},
  year = {2020},
  month = jul,
  journal = {Language, Cognition and Neuroscience},
  volume = {35},
  number = {6},
  pages = {780--796},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1525494},
  abstract = {In addition to understanding individual word meanings and processing the syntactic and semantic dependencies among those words within a sentence, language comprehension often requires constructing a higher-order discourse structure based on the relationships among clauses and sentences in the extended context. Prior fMRI studies of discourse-level comprehension have reported greater activation for texts than unconnected sentences in what-appear-to-be regions of the Theory of Mind (ToM) network. However, those studies have generally used narratives rich in mental state content, thus confounding coherence and content. We report an fMRI experiment where ToM regions were defined functionally in each participant, and their responses were examined to texts vs. sentence lists. Critically, we used expository texts to minimise mental state content. Medial frontal but not posterior ToM regions exhibited small but reliable increases in their responses to texts relative to unconnected sentences, suggesting a role for these regions in discourse comprehension independent of content.},
  pmid = {32984430},
  keywords = {Coherence,discourse,fMRI,Theory of Mind network},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2018.1525494},
  file = {/Users/xzfang/Zotero/storage/YNBYTB7T/Jacoby and Fedorenko - 2020 - Discourse-level comprehension engages medial front.pdf;/Users/xzfang/Zotero/storage/5GAWMZAG/23273798.2018.html}
}

@article{jaderberg_spatial_2016,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  journal = {arXiv:1506.02025 [cs]},
  eprint = {1506.02025},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xzfang/Zotero/storage/EZ47GRGB/Jaderberg et al. - 2016 - Spatial Transformer Networks.pdf;/Users/xzfang/Zotero/storage/S62KBP2K/1506.html}
}

@article{jaeger_alignment_2013,
  title = {Alignment as a Consequence of Expectation Adaptation: {{Syntactic}} Priming Is Affected by the Prime's Prediction Error given Both Prior and Recent Experience},
  shorttitle = {Alignment as a Consequence of Expectation Adaptation},
  author = {Jaeger, T. Florian and Snider, Neal E.},
  year = {2013},
  month = apr,
  journal = {Cognition},
  volume = {127},
  number = {1},
  pages = {57--83},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2012.10.013},
  abstract = {Speakers show a remarkable tendency to align their productions with their interlocutors'. Focusing on sentence production, we investigate the cognitive systems underlying such alignment (syntactic priming). Our guiding hypothesis is that syntactic priming is a consequence of a language processing system that is organized to achieve efficient communication in an ever-changing (subjectively non-stationary) environment. We build on recent work suggesting that comprehenders adapt to the statistics of the current environment. If such adaptation is rational or near-rational, the extent to which speakers adapt their expectations for a syntactic structure after processing a prime sentence should be sensitive to the prediction error experienced while processing the prime. This prediction is shared by certain error-based implicit learning accounts, but not by most other accounts of syntactic priming. In three studies, we test this prediction against data from conversational speech, speech during picture description, and written production during sentence completion. All three studies find stronger syntactic priming for primes associated with a larger prediction error (primes with higher syntactic surprisal). We find that the relevant prediction error is sensitive to both prior and recent experience within the experiment. Together with other findings, this supports accounts that attribute syntactic priming to expectation adaptation.},
  langid = {english},
  keywords = {Adaptation,Alignment,Implicit learning,Prediction error,Structural persistence,Surprisal,Syntactic priming},
  file = {/Users/xzfang/Zotero/storage/3L6BPJ9M/S0010027712002636.html}
}

@article{jaeger_redundancy_2010,
  title = {Redundancy and Reduction: {{Speakers}} Manage Syntactic Information Density},
  shorttitle = {Redundancy and Reduction},
  author = {Jaeger, T. Florian},
  year = {2010},
  month = aug,
  journal = {Cognitive Psychology},
  volume = {61},
  number = {1},
  pages = {23--62},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2010.02.002},
  abstract = {A principle of efficient language production based on information theoretic considerations is proposed: Uniform Information Density predicts that language production is affected by a preference to distribute information uniformly across the linguistic signal. This prediction is tested against data from syntactic reduction. A single multilevel logit model analysis of naturally distributed data from a corpus of spontaneous speech is used to assess the effect of information density on complementizer that-mentioning, while simultaneously evaluating the predictions of several influential alternative accounts: availability, ambiguity avoidance, and dependency processing accounts. Information density emerges as an important predictor of speakers' preferences during production. As information is defined in terms of probabilities, it follows that production is probability-sensitive, in that speakers' preferences are affected by the contextual probability of syntactic structures. The merits of a corpus-based approach to the study of language production are discussed as well.},
  langid = {english},
  keywords = {Complementizer -mentioning,Efficient language production,Rational cognition,Syntactic production,Syntactic reduction},
  file = {/Users/xzfang/Zotero/storage/AXEVKVQW/Florian Jaeger - 2010 - Redundancy and reduction Speakers manage syntacti.pdf;/Users/xzfang/Zotero/storage/4T4M867B/S0010028510000083.html}
}

@article{jager_formal_2012a,
  title = {Formal Language Theory: Refining the {{Chomsky}} Hierarchy},
  shorttitle = {Formal Language Theory},
  author = {J{\"a}ger, Gerhard and Rogers, James},
  year = {2012},
  journal = {Philosophical Transactions: Biological Sciences},
  volume = {367},
  number = {1598},
  pages = {1956--1970},
  publisher = {{Royal Society}},
  issn = {0962-8436},
  abstract = {The first part of this article gives a brief overview of the four levels of the Chomsky hierarchy, with a special emphasis on context-free and regular languages. It then recapitulates the arguments why neither regular nor context-free grammar is sufficiently expressive to capture all phenomena in the natural language syntax. In the second part, two refinements of the Chomsky hierarchy are reviewed, which are both relevant to the extant research in cognitive science: the mildly context-sensitive languages (which are located between context-free and context-sensitive languages), and the sub-regular hierarchy (which distinguishes several levels of complexity within the class of regular languages).},
  file = {/Users/xzfang/Zotero/storage/TLCUBVWN/JÃ¤ger and Rogers - 2012 - Formal language theory refining the Chomsky hiera.pdf}
}

@article{jain_incorporating_2018,
  title = {Incorporating {{Context}} into {{Language Encoding Models}} for {{fMRI}}},
  author = {Jain, Shailee and Huth, Alexander G.},
  year = {2018},
  month = nov,
  journal = {bioRxiv},
  pages = {327601},
  doi = {10.1101/327601},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Language encoding models help explain language processing in the human brain by learning functions that predict brain responses from the language stimuli that elicited them. Current word embedding-based approaches treat each stimulus word independently and thus ignore the influence of context on language understanding. In this work, we instead build encoding models using rich contextual representations derived from an LSTM language model. Our models show a significant improvement in encoding performance relative to state-of-the-art embeddings in nearly every brain area. By varying the amount of context used in the models and providing the models with distorted context, we show that this improvement is due to a combination of better word embeddings learned by the LSTM language model and contextual information. We are also able to use our models to map context sensitivity across the cortex. These results suggest that LSTM language models learn high-level representations that are related to representations in the human brain.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Z5UP66LC/Jain and Huth - 2018 - Incorporating Context into Language Encoding Model.pdf;/Users/xzfang/Zotero/storage/B9EI5L5U/327601v2.html}
}

@article{jasmin_understanding_2019,
  title = {Understanding Rostral\textendash Caudal Auditory Cortex Contributions to Auditory Perception},
  author = {Jasmin, Kyle and Lima, C{\'e}sar F. and Scott, Sophie K.},
  year = {2019},
  month = jul,
  journal = {Nature reviews. Neuroscience},
  volume = {20},
  number = {7},
  pages = {425--434},
  issn = {1471-003X},
  doi = {10.1038/s41583-019-0160-2},
  abstract = {There are functional and anatomical distinctions between the neural systems involved in the recognition of sounds in the environment and those involved in the sensorimotor guidance of sound production and the spatial processing of sound. Evidence for the separation of these processes has historically come from disparate literatures on the perception and production of speech, music and other sounds. More recent evidence indicates that there are computational distinctions between the rostral and caudal primate auditory cortex that may underlie functional differences in auditory processing. These functional differences may originate from differences in the response times and temporal profiles of neurons in the rostral and caudal auditory cortex, suggesting that computational accounts of primate auditory pathways should focus on the implications of these temporal response differences.},
  pmcid = {PMC6589138},
  pmid = {30918365},
  file = {/Users/xzfang/Zotero/storage/KXVTUTL6/Jasmin et al. - 2019 - Understanding rostralâ€“caudal auditory cortex contr.pdf}
}

@article{jenkins_variability_2011,
  title = {Variability in Photos of the Same Face},
  author = {Jenkins, Rob and White, David and Van Montfort, Xandra and Mike Burton, A.},
  year = {2011},
  month = dec,
  journal = {Cognition},
  volume = {121},
  number = {3},
  pages = {313--323},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2011.08.001},
  abstract = {Psychological studies of face recognition have typically ignored within-person variation in appearance, instead emphasising differences between individuals. Studies typically assume that a photograph adequately captures a person's appearance, and for that reason most studies use just one, or a small number of photos per person. Here we show that photographs are not consistent indicators of facial appearance because they are blind to within-person variability. Crucially, this within-person variability is often very large compared to the differences between people. To investigate variability in photos of the same face, we collected images from the internet to sample a realistic range for each individual. In Experiments 1 and 2, unfamiliar viewers perceived images of the same person as being different individuals, while familiar viewers perfectly identified the same photos. In Experiment 3, multiple photographs of any individual formed a continuum of good to bad likeness, which was highly sensitive to familiarity. Finally, in Experiment 4, we found that within-person variability exceeded between-person variability in attractiveness. These observations are critical to our understanding of face processing, because they suggest that a key component of face processing has been ignored. As well as its theoretical significance, this scale of variability has important practical implications. For example, our findings suggest that face photographs are unsuitable as proof of identity.},
  langid = {english},
  keywords = {Attractiveness,Face perception,Face recognition,Identity,Photography},
  file = {/Users/xzfang/Zotero/storage/UDZQUXL8/Jenkins et al. - 2011 - Variability in photos of the same face.pdf;/Users/xzfang/Zotero/storage/AVKSMJT8/S0010027711002022.html}
}

@article{jesse_positional_2011,
  title = {Positional Effects in the Lexical Retuning of Speech Perception},
  author = {Jesse, Alexandra and McQueen, James M.},
  year = {2011},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {18},
  number = {5},
  pages = {943--950},
  issn = {1531-5320},
  doi = {10.3758/s13423-011-0129-2},
  abstract = {Listeners use lexical knowledge to adjust to speakers' idiosyncratic pronunciations. Dutch listeners learn to interpret an ambiguous sound between /s/ and /f/ as /f/ if they hear it word-finally in Dutch words normally ending in /f/, but as /s/ if they hear it in normally /s/-final words. Here, we examined two positional effects in lexically guided retuning. In Experiment 1, ambiguous sounds during exposure always appeared in word-initial position (replacing the first sounds of /f/- or /s/-initial words). No retuning was found. In Experiment 2, the same ambiguous sounds always appeared word-finally during exposure. Here, retuning was found. Lexically guided perceptual learning thus appears to emerge reliably only when lexical knowledge is available as the to-be-tuned segment is initially being processed. Under these conditions, however, lexically guided retuning was position independent: It generalized across syllabic positions. Lexical retuning can thus benefit future recognition of particular sounds wherever they appear in words.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YUXZ22YQ/Jesse and McQueen - 2011 - Positional effects in the lexical retuning of spee.pdf}
}

@article{jiang_human_2007,
  title = {Human Visual Cortex Responds to Invisible Chromatic Flicker},
  author = {Jiang, Yi and Zhou, Ke and He, Sheng},
  year = {2007},
  month = may,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {5},
  pages = {657--662},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1879},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/66X9GHHY/Jiang et al. - 2007 - Human visual cortex responds to invisible chromati.pdf}
}

@article{jirovetz_composition_2005,
  title = {Composition, Quality Control and Antimicrobial Activity of the Essential Oil of Cumin ({{Cuminum}} Cyminum {{L}}.) Seeds from {{Bulgaria}} That Had Been Stored for up to 36~Years},
  author = {Jirovetz, Leopold and Buchbauer, Gerhard and Stoyanova, Albena S. and Georgiev, Evgenii V. and Damianova, Stanka T.},
  year = {2005},
  month = mar,
  journal = {International Journal of Food Science \& Technology},
  volume = {40},
  number = {3},
  pages = {305--310},
  issn = {1365-2621},
  doi = {10.1111/j.1365-2621.2004.00915.x},
  abstract = {The essential oil of seeds of cumin (Cuminum cyminum L.) from Bulgaria stored for more than 35 years was analyzed by physicochemical methods, GC, GC-MS and olfactometry and its antimicrobial activity tested using different strains of microorganisms. More than sixty constituents of this cumin oil could be identified as essential volatiles, responsible for the pleasant fresh, clean, spicy (typical cumin-like) odour of a high quality product. Cumin aldehyde (36\%), {$\beta$}-pinene (19.3\%), p-cymene (18.4\%) and {$\gamma$}-terpinene (15.3\%) were the principal compounds found. Antimicrobial testing showed high activity of the essential C. cyminum oil against the mold Aspergillus niger, the Gram (+) bacteria Bacillus subtilis and Staphylococcus epidermidis as well as the yeast Saccharomyces cerevisiae and Candida albicans.},
  langid = {english},
  keywords = {Antimicrobial testings,aroma compounds,quality control,stored seeds},
  file = {/Users/xzfang/Zotero/storage/92GGHPTB/Jirovetz et al. - 2005 - Composition, quality control and antimicrobial act.html}
}

@article{joglekar_inhibition_2014,
  title = {Inhibition of Advanced Glycation End Product Formation by Cymene \textendash{} {{A}} Common Food Constituent},
  author = {Joglekar, Madhav M. and Panaskar, Shrimant N. and Arvindekar, Akalpita U.},
  year = {2014},
  month = jan,
  journal = {Journal of Functional Foods},
  volume = {6},
  pages = {107--115},
  issn = {1756-4646},
  doi = {10.1016/j.jff.2013.09.024},
  abstract = {Protein glycation inhibition is important to prevent secondary complications in diabetes mellitus. p-Cymene, a monoterpene commonly found in Cuminum cyminum was investigated in vivo and in vitro using aminoguanidine as a positive control. Streptozotocin induced diabetic rats were treated with 20 mg cymene kg-1 body weight for 60 days. Cymene treatment improved HbA1c compared with diabetic control and serum fructosamine levels were normalized. Nephropathic parameters like albumin excretion rate, serum creatinine and creatinine clearance rate were improved. The cymene treatment improved collagen solubility profile. In the in vitro studies cymene inhibited total AGEs fluorescence and pentosidine by 56.6\% and 57\%, respectively at 100 {$\mu$}M concentration which was comparable with aminoguanidine (2 mM) concentration. Glycation specific decline in BSA {$\alpha$}-helix content (from 63.8\% to 43.3\%) and increase in {$\beta$}-sheet (from 3.7\% to 17.4\%) was prevented by cymene in vitro, implying its stabilization effect. Electrophoretic mobility of glycated BSA was increased as against cymene treated BSA samples. The results suggest that cymene could have therapeutic potential in the prevention of glycation mediated diabetic complications.},
  keywords = {AGE fluorescence,Cymene,Glycation inhibition,Protein conformation},
  file = {/Users/xzfang/Zotero/storage/Z56KXKTE/Joglekar et al. - 2014 - Inhibition of advanced glycation end product forma.pdf;/Users/xzfang/Zotero/storage/XKUWAIE6/Joglekar et al. - 2014 - Inhibition of advanced glycation end product forma.html}
}

@techreport{johansson_eyemovement_2021,
  type = {Preprint},
  title = {Eye-Movement Replay Supports Episodic Remembering},
  author = {Johansson, Roger and Nystr{\"o}m, Marcus and Dewhurst, Richard and Johansson, Mikael},
  year = {2021},
  month = feb,
  institution = {{In Review}},
  doi = {10.21203/rs.3.rs-275969/v1},
  abstract = {Abstract           When we bring to mind something we have seen before, our eyes spontaneously reproduce a pattern strikingly similar to that made during the original encounter. Eye-movements can then serve the opposite purpose to acquiring new visual information; they can serve as self-generated cues, pointing to memories already stored. By isolating separable properties within the closely bound chain of where and when we look, we demonstrate that specific components of dynamically reinstated eye-movement sequences, facilitate different aspects of episodic remembering. We also show that the fidelity with which a series of connected eye-movements from initial encoding is reproduced during subsequent retrieval, predicts the quality of the recalled memory. Our findings indicate that eye movements are ``replayed'' to assemble visuospatial relations as we remember. Distinct dimensions of these scanpaths differentially contribute depending on the goal-relevant memory.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Q3C2WXBR/Johansson et al. - 2021 - Eye-movement replay supports episodic remembering.pdf}
}

@article{johnson_quiet_2009,
  title = {The Quiet Clam Is Quite Calm: {{Transposed-letter}} Neighborhood Effects on Eye Movements during Reading.},
  shorttitle = {The Quiet Clam Is Quite Calm},
  author = {Johnson, Rebecca L.},
  year = {2009},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {35},
  number = {4},
  pages = {943--969},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/a0015572},
  abstract = {In responses time tasks, inhibitory neighborhood effects have been found for word pairs that differ in a transposition of two adjacent letters (e.g., clam/calm). Here, the author describes two eye-tracking experiments conducted to explore transposed-letter (TL) neighborhood effects within the context of normal silent reading. In Experiment 1, sentences contained a target word that either has a TL neighbor (e.g., angel, which has the TL neighbor angle) or does not (e.g., alien). In Experiment 2, the context was manipulated to examine whether semantic constraints attenuate neighborhood effects. Readers took longer to process words that have a TL neighbor than control words but only when either member of the TL pair was likely. Furthermore, this interference effect occurred very late in processing and was not affected by relative word frequency. These interference effects can be explained either by the spreading of activation from the target word to its TL neighbor or by the misidentification of target words for their TL neighbors. Implications for models of orthographic input coding and models of eye-movement control are discussed.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZMNVMSMU/Johnson - 2009 - The quiet clam is quite calm Transposed-letter ne.pdf}
}

@article{johnson_resonance_2006,
  title = {Resonance in an Exemplar-Based Lexicon: {{The}} Emergence of Social Identity and Phonology},
  shorttitle = {Resonance in an Exemplar-Based Lexicon},
  author = {Johnson, Keith},
  year = {2006},
  month = oct,
  journal = {Journal of Phonetics},
  volume = {34},
  number = {4},
  pages = {485--499},
  issn = {00954470},
  doi = {10.1016/j.wocn.2005.08.004},
  abstract = {Two sets of data are discussed in terms of an exemplar resonance model of the lexicon. First, a cross-linguistic review of vowel formant measurements indicate that phonetic differences between male and female talkers are a function of language, dissociated to a certain extent from vocal tract length. Second, an auditory word recognition study (Strand, 2000) indicates that listeners can process words faster when the talker has a stereotypical sounding voice. An exemplar resonance model of perception derives these effects suggesting that reentrant pathways (Edelman, 1987) between cognitive categories and detailed exemplars of them leads to the emergence of social and linguistic entities.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/A8FPB6IB/Johnson - 2006 - Resonance in an exemplar-based lexicon The emerge.pdf}
}

@incollection{johnson_speaker_2005,
  title = {Speaker {{Normalization}} in {{Speech Perception}}},
  booktitle = {The {{Handbook}} of {{Speech Perception}}},
  author = {Johnson, Keith},
  year = {2005},
  pages = {363--389},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470757024.ch15},
  abstract = {This chapter contains sections titled: Perceiving Vowels in Isolated Syllables Formant Ratio Theories From Auditory Gestalts to Vocal Tract Actions Vocal Tract Normalization Talkers or Vocal Tractsquest; Talker Normalization},
  chapter = {15},
  copyright = {Copyright \textcopyright{} 2005 Blackwell Publishing Ltd},
  isbn = {978-0-470-75702-4},
  langid = {english},
  keywords = {isolated syllables,phonological identity,speaker normalization,speech perception,vowel perception},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470757024.ch15},
  file = {/Users/xzfang/Zotero/storage/5DJYD5ZZ/Johnson - 2005 - Speaker Normalization in Speech Perception.pdf;/Users/xzfang/Zotero/storage/L4JB4DCD/9780470757024.html}
}

@incollection{johnson_speaker_2021,
  title = {Speaker {{Normalization}} in {{Speech Perception}}},
  booktitle = {The {{Handbook}} of {{Speech Perception}}},
  author = {Johnson, Keith and Sjerps, Matthias J.},
  year = {2021},
  pages = {145--176},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119184096.ch6},
  abstract = {People have different acoustic voice signatures because of the unique physiology of the jaw, tongue, lips, and throat. The largest acoustic difference between talkers is the difference between men and women and children. Much of the research done on talker normalization has focused on understanding how listeners must map the acoustic properties of speech produced by men and women to talker-independent linguistic representations. This chapter reviews some practical vowel-normalization methods and discusses the perceptual processes that listeners may use to accomplish speech recognition in the face of talker variation. It focuses on segment-internal talker cues (intrinsic normalization), and provides a general overview of cortical organization for speech perception. An important difference between intrinsic normalization and extrinsic normalization is that the latter requires the system to achieve and maintain a stable representation of the talker and their acoustic voice properties so as to provide a frame of reference for further interpretation.},
  chapter = {6},
  copyright = {\textcopyright{} 2021 John Wiley \& Sons, Inc.},
  isbn = {978-1-119-18409-6},
  langid = {english},
  keywords = {acoustic voice properties,extrinsic normalization,intrinsic normalization,speech perception,talker normalization,talker-independent linguistic representations,vowel-normalization methods},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119184096.ch6},
  file = {/Users/xzfang/Zotero/storage/3JVE8UZ9/Johnson and Sjerps - 2021 - Speaker Normalization in Speech Perception.pdf;/Users/xzfang/Zotero/storage/XC3FQZ4Q/9781119184096.html}
}

@article{johnsrude_swinging_2013,
  title = {Swinging at a Cocktail Party: Voice Familiarity Aids Speech Perception in the Presence of a Competing Voice},
  shorttitle = {Swinging at a Cocktail Party},
  author = {Johnsrude, Ingrid S. and Mackey, Allison and Hakyemez, H{\'e}l{\`e}ne and Alexander, Elizabeth and Trang, Heather P. and Carlyon, Robert P.},
  year = {2013},
  month = oct,
  journal = {Psychological Science},
  volume = {24},
  number = {10},
  pages = {1995--2004},
  issn = {1467-9280},
  doi = {10.1177/0956797613482467},
  abstract = {People often have to listen to someone speak in the presence of competing voices. Much is known about the acoustic cues used to overcome this challenge, but almost nothing is known about the utility of cues derived from experience with particular voices--cues that may be particularly important for older people and others with impaired hearing. Here, we use a version of the coordinate-response-measure procedure to show that people can exploit knowledge of a highly familiar voice (their spouse's) not only to track it better in the presence of an interfering stranger's voice, but also, crucially, to ignore it so as to comprehend a stranger's voice more effectively. Although performance declines with increasing age when the target voice is novel, there is no decline when the target voice belongs to the listener's spouse. This finding indicates that older listeners can exploit their familiarity with a speaker's voice to mitigate the effects of sensory and cognitive decline.},
  langid = {english},
  pmid = {23985575},
  keywords = {Adult,Aged,aging,Aging,attention,Attention,auditory perception,Auditory Perception,auditory perceptual organization,cognitive processes,Cues,Female,hearing,Hearing Loss,Humans,knowledge-based perception,language comprehension,Male,Middle Aged,Pattern Recognition; Physiological,Perceptual Masking,Recognition; Psychology,speech,speech perception,Speech Perception,Spouses,Voice},
  file = {/Users/xzfang/Zotero/storage/834D8YWA/Johnsrude et al. - 2013 - Swinging at a cocktail party voice familiarity ai.pdf}
}

@article{jones_might_2021,
  title = {Might a {{Single Neuron Solve Interesting Machine Learning Problems Through Successive Computations}} on {{Its Dendritic Tree}}?},
  author = {Jones, Ilenna Simone and Kording, Konrad Paul},
  year = {2021},
  month = may,
  journal = {Neural Computation},
  volume = {33},
  number = {6},
  pages = {1554--1571},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01390},
  abstract = {Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. However, it is unclear how aspects of a dendritic tree, such as its branched morphology or its repetition of presynaptic inputs, determine neural computation beyond this apparent nonlinearity. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We manipulate the architecture of this model to investigate the impacts of binary branching constraints and repetition of synaptic inputs on neural computation. We find that models with such manipulations can perform well on machine learning tasks, such as Fashion MNIST or Extended MNIST. We find that model performance on these tasks is limited by binary tree branching and dendritic asymmetry and is improved by the repetition of synaptic inputs to different dendritic branches. These computational experiments further neuroscience theory on how different dendritic properties might determine neural computation of clearly defined tasks.},
  file = {/Users/xzfang/Zotero/storage/WINKFGEC/Jones and Kording - 2021 - Might a Single Neuron Solve Interesting Machine Le.pdf;/Users/xzfang/Zotero/storage/7Y4IVGZS/Might-a-Single-Neuron-Solve-Interesting-Machine.html}
}

@article{jouravlev_processing_2016,
  title = {Processing Temporal Presuppositions: An Event-Related Potential Study},
  shorttitle = {Processing Temporal Presuppositions},
  author = {Jouravlev, Olessia and Stearns, Laura and Bergen, Leon and Eddy, Marianna and Gibson, Edward and Fedorenko, Evelina},
  year = {2016},
  month = nov,
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {10},
  pages = {1245--1256},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2016.1209531},
  abstract = {The ability to efficiently process presuppositions, which contain information that the speaker believes to be in the background to the conversation, is essential for effective communication. To get a deeper understanding of the nature and the time-course of temporal presupposition processing, we examined event-related potential evoked by the word again in two types of sentence contexts. The word again was presented in contexts that supported a presupposition (e.g. Jake had tipped a maid at the hotel once before. Today he tipped a maid at the hotel again \ldots{} ) or violated it (e.g. Jake had never tipped a maid at the hotel before. Today he tipped a maid at the hotel again \ldots{} ). The presupposition violation was associated with increased amplitudes of the P3b/P600 but not the N400 component. We argue for the centrality of the P3b/P600 component for presupposition processing. These findings demonstrate rapid integration of lexical presuppositions with contextual knowledge.},
  keywords = {ERPs,N400,P3b/P600,presupposition violation,Temporal presupposition},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2016.1209531},
  file = {/Users/xzfang/Zotero/storage/THRJXBBJ/Jouravlev et al. - 2016 - Processing temporal presuppositions an event-rela.pdf;/Users/xzfang/Zotero/storage/WTCWIX7U/23273798.2016.html}
}

@article{judd_training_2021,
  title = {Training Spatial Cognition Enhances Mathematical Learning in a Randomized Study of 17,000 Children},
  author = {Judd, Nicholas and Klingberg, Torkel},
  year = {2021},
  month = may,
  journal = {Nature Human Behaviour},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01118-4},
  abstract = {Spatial and mathematical abilities are strongly associated. Here, we analysed data from 17,648 children, aged 6\textendash 8\,years, who performed 7\,weeks of mathematical training together with randomly assigned spatial cognitive training with tasks demanding more spatial manipulation (mental rotation or tangram), maintenance of spatial information (a visuospatial working memory task) or spatial, non-verbal reasoning. We found that the type of cognitive training children performed had a significant impact on mathematical learning, with training of visuospatial working memory and reasoning being the most effective. This large, community-based study shows that spatial cognitive training can result in transfer to academic abilities, and that reasoning ability and maintenance of spatial information is relevant for mathematics learning in young children.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MAQ436KA/Judd and Klingberg - 2021 - Training spatial cognition enhances mathematical l.pdf;/Users/xzfang/Zotero/storage/TUMZW9JM/s41562-021-01118-4.html}
}

@article{julian_remapping_2021,
  title = {Remapping and Realignment in the Human Hippocampal Formation Predict Context-Dependent Spatial Behavior},
  author = {Julian, Joshua B. and Doeller, Christian F.},
  year = {2021},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {24},
  number = {6},
  pages = {863--872},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00835-3},
  abstract = {To guide spatial behavior, the brain must retrieve memories that are appropriately associated with different navigational contexts. Contextual memory might be mediated by cell ensembles in the hippocampal formation that alter their responses to changes in context, processes known as remapping and realignment in the hippocampus and entorhinal cortex, respectively. However, whether remapping and realignment guide context-dependent spatial behavior is unclear. To address this issue, human participants learned object\textendash location associations within two distinct virtual reality environments and subsequently had their memory tested during functional MRI (fMRI) scanning. Entorhinal grid-like representations showed realignment between the two contexts, and coincident changes in fMRI activity patterns consistent with remapping were observed in the hippocampus. Critically, in a third ambiguous context, trial-by-trial remapping and realignment in the hippocampal\textendash entorhinal network predicted context-dependent behavior. These results reveal the hippocampal\textendash entorhinal mechanisms mediating human contextual memory and suggest that the hippocampal formation plays a key role in spatial behavior under uncertainty.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cognitive neuroscience;Spatial memory Subject\_term\_id: cognitive-neuroscience;spatial-memory},
  file = {/Users/xzfang/Zotero/storage/LFINF3N3/Julian and Doeller - 2021 - Remapping and realignment in the human hippocampal.pdf;/Users/xzfang/Zotero/storage/JWVLHRTX/s41593-021-00835-3.html}
}

@book{kahneman_thinking_2011,
  title = {Thinking, {{Fast}} and {{Slow}}},
  author = {Kahneman, Daniel},
  year = {2011},
  month = oct,
  publisher = {{Macmillan}},
  abstract = {Major New York Times bestsellerWinner of the National Academy of Sciences Best Book Award in 2012Selected by the New York Times Book Review as one of the ten best books of 2011A Globe and Mail Best Books of the Year 2011 TitleOne of The Economist's 2011 Books of the Year One of The Wall Street Journal's Best Nonfiction Books of the Year 20112013 Presidential Medal of Freedom RecipientKahneman's work with Amos Tversky is the subject of Michael Lewis's The Undoing Project: A Friendship That Changed Our MindsIn his mega bestseller, Thinking, Fast and Slow, Daniel Kahneman, the renowned psychologist and winner of the Nobel Prize in Economics, takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. The impact of overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the profound effect of cognitive biases on everything from playing the stock market to planning our next vacation\textemdash each of these can be understood only by knowing how the two systems shape our judgments and decisions.Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives\textemdash and how we can use different techniques to guard against the mental glitches that often get us into trouble. Winner of the National Academy of Sciences Best Book Award and the Los Angeles Times Book Prize and selected by The New York Times Book Review as one of the ten best books of 2011, Thinking, Fast and Slow is destined to be a classic.},
  googlebooks = {SHvzzuCnuv8C},
  isbn = {978-0-374-27563-1},
  langid = {english},
  keywords = {Business \& Economics / Decision-Making \& Problem Solving,Psychology / Cognitive Psychology \& Cognition}
}

@misc{kalpadakis-smith_crowding_2021,
  title = {Crowding Changes Appearance Systematically in Peripheral, Amblyopic, and Developing Vision},
  author = {{Kalpadakis-Smith}, A. V. and Tailor, V. K. and {Dahlmann-Noor}, A. H. and Greenwood, J. A.},
  year = {2021},
  month = dec,
  pages = {2021.11.30.470647},
  institution = {{bioRxiv}},
  doi = {10.1101/2021.11.30.470647},
  abstract = {Visual crowding is the disruptive effect of clutter on object recognition. Although most prominent in adult peripheral vision, crowding also disrupts foveal vision in typically-developing children and those with strabismic amblyopia. Do these crowding effects share the same mechanism? Here we exploit observations that crowded errors in peripheral vision are not random: target objects appear either averaged with the flankers (assimilation), or replaced by them (substitution). If amblyopic and developmental crowding share the same mechanism then their errors should be similarly systematic. We tested foveal vision in children aged 3-9 years with typical vision or strabismic amblyopia, and peripheral vision in adults. The perceptual effects of crowding were measured by requiring observers to adjust a reference stimulus to match the perceived orientation of a target `Vac-Man' element. When the target was surrounded by flankers that differed by {$\pm$}30\textdegree, adults and children reported orientations between the target and flankers (assimilation). Errors were reduced with {$\pm$}90\textdegree{} differences, but primarily matched the flanker orientation (substitution) when they did occur. A population pooling model of crowding successfully simulated this pattern of errors in all three groups. We conclude that the perceptual effects of amblyopic and developing crowding are systematic and resemble the near periphery in adults, suggesting a common underlying mechanism.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/P4JZZUZP/Kalpadakis-Smith et al. - 2021 - Crowding changes appearance systematically in peri.pdf;/Users/xzfang/Zotero/storage/U6CEGHXZ/2021.11.30.html}
}

@article{kamide_learning_2012,
  title = {Learning Individual Talkers' Structural Preferences},
  author = {Kamide, Yuki},
  year = {2012},
  month = jul,
  journal = {Cognition},
  volume = {124},
  number = {1},
  pages = {66--71},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2012.03.001},
  abstract = {Listeners are often capable of adjusting to the variability contained in individual talkers' (speakers') speech. The vast majority of findings on talker adaptation are concerned with learning the contingency between phonological characteristics and talker identity. In contrast, the present study investigates representations at a more abstract level \textendash{} the contingency between syntactic attachment style and talker identity. In a `visual-world' experiment, participants were exposed to semi-realistic scenes depicting several objects (e.g., an adult man, a young girl, a motorbike, a carousel, and other objects) accompanied by a spoken sentence with a structurally ambiguous relative clause (e.g., `The uncle of the girl who will ride the motorbike/carousel is from France.' In the context of the scene, `motorbike' suggested the uncle as the agent of the riding, whereas `carousel' suggested the girl as the agent). For half the experimental items, one version of the sentence was read by one talker, who always uttered sentences that resolved, pragmatically, to the high attachment (the uncle as the agent), and the other by another talker, who always uttered sentences resolving to the low attachment (the girl as the agent). For the other half of the experimental items, both versions were read by a third talker who produced both high and low attachments. It was found that, after exposure to these stimuli, and for new sentences not heard previously, participants learnt to anticipate the `appropriate' attachment depending on talker identity (with no attachment preference for the talker who produced both attachment types). The data suggest that listeners can learn the relationship between talker identity and abstract, structural, properties of their speech, and that syntactic attachment decisions in comprehension can reflect sensitivity to talker-specific syntactic style.},
  langid = {english},
  keywords = {Learning,Relative clause attachment ambiguity,Speaker variability,Talker adaptation,Visual-world eye-tracking},
  file = {/Users/xzfang/Zotero/storage/YHRTCEZ5/Kamide - 2012 - Learning individual talkersâ€™ structural preference.pdf;/Users/xzfang/Zotero/storage/CA43Q8NW/S0010027712000510.html}
}

@article{kang_wick_2009,
  title = {The {{Wick}} in the {{Candle}} of {{Learning}}: {{Epistemic Curiosity Activates Reward Circuitry}} and {{Enhances Memory}}},
  shorttitle = {The {{Wick}} in the {{Candle}} of {{Learning}}},
  author = {Kang, Min Jeong and Hsu, Ming and Krajbich, Ian M. and Loewenstein, George and McClure, Samuel M. and Wang, Joseph Tao-yi and Camerer, Colin F.},
  year = {2009},
  month = aug,
  journal = {Psychological Science},
  volume = {20},
  number = {8},
  pages = {963--973},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2009.02402.x},
  abstract = {Curiosity has been described as a desire for learning and knowledge, but its underlying mechanisms are not well understood. We scanned subjects with functional magnetic resonance imaging while they read trivia questions. The level of curiosity when reading questions was correlated with activity in caudate regions previously suggested to be involved in anticipated reward. This finding led to a behavioral study, which showed that subjects spent more scarce resources (either limited tokens or waiting time) to find out answers when they were more curious. The functional imaging also showed that curiosity increased activity in memory areas when subjects guessed incorrectly, which suggests that curiosity may enhance memory for surprising new information. This prediction about memory enhancement was confirmed in a behavioral study: Higher curiosity in an initial session was correlated with better recall of surprising answers 1 to 2 weeks later.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Q45VX8WL/Kang et al. - 2009 - The Wick in the Candle of Learning Epistemic Curi.pdf}
}

@article{kanjlia_absence_2016,
  title = {Absence of Visual Experience Modifies the Neural Basis of Numerical Thinking},
  author = {Kanjlia, Shipra and Lane, Connor and Feigenson, Lisa and Bedny, Marina},
  year = {2016},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {40},
  pages = {11172--11177},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1524982113},
  abstract = {In humans, the ability to reason about mathematical quantities depends on a frontoparietal network that includes the intraparietal sulcus (IPS). How do nature and nurture give rise to the neurobiology of numerical cognition? We asked how visual experience shapes the neural basis of numerical thinking by studying numerical cognition in congenitally blind individuals. Blind (n = 17) and blindfolded sighted (n = 19) participants solved math equations that varied in difficulty (e.g., 27 - 12 = x vs. 7 - 2 = x), and performed a control sentence comprehension task while undergoing fMRI. Whole-cortex analyses revealed that in both blind and sighted participants, the IPS and dorsolateral prefrontal cortices were more active during the math task than the language task, and activity in the IPS increased parametrically with equation difficulty. Thus, the classic frontoparietal number network is preserved in the total absence of visual experience. However, surprisingly, blind but not sighted individuals additionally recruited a subset of early visual areas during symbolic math calculation. The functional profile of these ``visual'' regions was identical to that of the IPS in blind but not sighted individuals. Furthermore, in blindness, number-responsive visual cortices exhibited increased functional connectivity with prefrontal and IPS regions that process numbers. We conclude that the frontoparietal number network develops independently of visual experience. In blindness, this number network colonizes parts of deafferented visual cortex. These results suggest that human cortex is highly functionally flexible early in life, and point to frontoparietal input as a mechanism of cross-modal plasticity in blindness.},
  copyright = {\textcopyright{}  . http://www.pnas.org/site/misc/userlicense.xhtml},
  langid = {english},
  pmid = {27638209},
  keywords = {blindness,development,number,plasticity,vision},
  file = {/Users/xzfang/Zotero/storage/M5CMJ572/Kanjlia et al. - 2016 - Absence of visual experience modifies the neural b.pdf;/Users/xzfang/Zotero/storage/KTASD5B7/11172.html}
}

@article{kanjlia_sensitive_2018,
  title = {Sensitive Period for Cognitive Repurposing of Human Visual Cortex},
  author = {Kanjlia, Shipra and Pant, Rashi and Bedny, Marina},
  year = {2018},
  month = aug,
  journal = {bioRxiv},
  pages = {402321},
  doi = {10.1101/402321},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Studies of sensory loss are a model for understanding the functional flexibility of human cortex. In congenital blindness, subsets of visual cortex are recruited during higher-cognitive tasks, such as language and math tasks. Is such dramatic functional repurposing possible throughout the lifespan or restricted to sensitive periods in development? We compared visual cortex function in individuals who lost their vision as adults (after age 17) to congenitally blind and sighted blindfolded adults. Participants took part in resting-state and task-based fMRI scans during which they solved math equations of varying difficulty and judged the meanings of sentences. Blindness at any age caused ``visual'' cortices to synchronize with specific fronto-parietal networks at rest. However, in task-based data, visual cortices showed regional specialization for math and language and load-dependent activity only in congenital blindness. Thus, despite the presence of long-range functional connectivity, cognitive repurposing of human cortex is limited by sensitive periods.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YI73MDC9/Kanjlia et al. - 2018 - Sensitive period for cognitive repurposing of huma.pdf;/Users/xzfang/Zotero/storage/CV38HQ5B/402321v1.html}
}

@article{kanwisher_fusiform_1997,
  title = {The {{Fusiform Face Area}}: {{A Module}} in {{Human Extrastriate Cortex Specialized}} for {{Face Perception}}},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M},
  year = {1997},
  pages = {10},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QGZWRMAN/Kanwisher et al. - The Fusiform Face Area A Module in Human Extrastr.pdf}
}

@article{kanwisher_quest_2017,
  title = {The {{Quest}} for the {{FFA}} and {{Where It Led}}},
  author = {Kanwisher, Nancy},
  year = {2017},
  month = feb,
  journal = {The Journal of Neuroscience},
  volume = {37},
  number = {5},
  pages = {1056--1061},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1706-16.2016},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3G93GZV4/Kanwisher - 2017 - The Quest for the FFA and Where It Led.pdf}
}

@article{kapadia_selecting_2020,
  title = {Selecting among Competing Models of Talker Adaptation: {{Attention}}, Cognition, and Memory in Speech Processing Efficiency},
  shorttitle = {Selecting among Competing Models of Talker Adaptation},
  author = {Kapadia, Alexandra M. and Perrachione, Tyler K.},
  year = {2020},
  month = nov,
  journal = {Cognition},
  volume = {204},
  pages = {104393},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104393},
  abstract = {Phonetic variability across talkers imposes additional processing costs during speech perception, often measured by performance decrements between single- and mixed-talker conditions. However, models differ in their predictions about whether accommodating greater phonetic variability (i.e., more talkers) imposes greater processing costs. We measured speech processing efficiency in a speeded word identification task, in which we manipulated the number of talkers (1, 2, 4, 8, or 16) listeners heard. Word identification was less efficient in every mixed-talker condition compared to the single-talker condition, but the magnitude of this performance decrement was not affected by the number of talkers. Furthermore, in a condition with uniform transition probabilities between two talkers, word identification was more efficient when the talker was the same as the prior trial compared to trials when the talker switched. These results support an auditory streaming model of talker adaptation, where processing costs associated with changing talkers result from attentional reorientation.},
  langid = {english},
  keywords = {Auditory streaming,Phonetic variability,Processing cost,Speech perception,Talker adaptation},
  file = {/Users/xzfang/Zotero/storage/EWTJ37SQ/Kapadia and Perrachione - 2020 - Selecting among competing models of talker adaptat.pdf;/Users/xzfang/Zotero/storage/L2T5TWMR/S0010027720302122.html}
}

@article{kappenman_effects_2010,
  title = {The {{Effects}} of {{Electrode Impedance}} on {{Data Quality}} and {{Statistical Significance}} in {{ERP Recordings}}},
  author = {Kappenman, Emily S. and Luck, Steven J.},
  year = {2010},
  month = sep,
  journal = {Psychophysiology},
  volume = {47},
  number = {5},
  pages = {888--904},
  issn = {0048-5772},
  doi = {10.1111/j.1469-8986.2010.01009.x},
  abstract = {To determine whether data quality is meaningfully reduced by high electrode impedance, EEG was recorded simultaneously from low- and high-impedance electrode sites during an oddball task. Low-frequency noise was found to be increased at high-impedance sites relative to low-impedance sites, especially when the recording environment was warm and humid. The increased noise at the high-impedance sites caused an increase in the number of trials needed to obtain statistical significance in analyses of P3 amplitude, but this could be partially mitigated by high-pass filtering and artifact rejection. High electrode impedance did not reduce statistical power for the N1 wave unless the recording environment was warm and humid. Thus, high electrode impedance may increase noise and decrease statistical power under some conditions, but these effects can be reduced by using a cool and dry recording environment and appropriate signal processing methods.},
  pmcid = {PMC2902592},
  pmid = {20374541},
  file = {/Users/xzfang/Zotero/storage/KDXDV5JL/Kappenman and Luck - 2010 - The Effects of Electrode Impedance on Data Quality.pdf}
}

@article{karimi-rouzbahani_perceptual_2021,
  title = {Perceptual Difficulty Modulates the Direction of Information Flow in Familiar Face Recognition},
  author = {{Karimi-Rouzbahani}, Hamid and Ramezani, Farzad and Woolgar, Alexandra and Rich, Anina and Ghodrati, Masoud},
  year = {2021},
  month = jun,
  journal = {NeuroImage},
  volume = {233},
  pages = {117896},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2021.117896},
  abstract = {Humans are fast and accurate when they recognize familiar faces. Previous neurophysiological studies have shown enhanced representations for the dichotomy of familiar vs. unfamiliar faces. As familiarity is a spectrum, however, any neural correlate should reflect graded representations for more vs. less familiar faces along the spectrum. By systematically varying familiarity across stimuli, we show a neural familiarity spectrum using electroencephalography. We then evaluated the spatiotemporal dynamics of familiar face recognition across the brain. Specifically, we developed a novel informational connectivity method to test whether peri-frontal brain areas contribute to familiar face recognition. Results showed that feed-forward flow dominates for the most familiar faces and top-down flow was only dominant when sensory evidence was insufficient to support face recognition. These results demonstrate that perceptual difficulty and the level of familiarity influence the neural representation of familiar faces and the degree to which peri-frontal neural networks contribute to familiar face recognition.},
  langid = {english},
  keywords = {Face recognition,Familiar faces,Informational brain connectivity,Multivariate pattern analysis (MVPA),Representational similarity analysis (RSA)},
  file = {/Users/xzfang/Zotero/storage/DRNJIWDG/Karimi-Rouzbahani et al. - 2021 - Perceptual difficulty modulates the direction of i.pdf;/Users/xzfang/Zotero/storage/C5QT2XCA/S1053811921001737.html}
}

@article{karns_altered_2012,
  title = {Altered {{Cross-Modal Processing}} in the {{Primary Auditory Cortex}} of {{Congenitally Deaf Adults}}: {{A Visual-Somatosensory fMRI Study}} with a {{Double-Flash Illusion}}},
  shorttitle = {Altered {{Cross-Modal Processing}} in the {{Primary Auditory Cortex}} of {{Congenitally Deaf Adults}}},
  author = {Karns, Christina M. and Dow, Mark W. and Neville, Helen J.},
  year = {2012},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {32},
  number = {28},
  pages = {9626--9638},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.6488-11.2012},
  abstract = {The developing brain responds to the environment by using statistical correlations in input to guide functional and structural changes\textemdash that is, the brain displays neuroplasticity. Experience shapes brain development throughout life, but neuroplasticity is variable from one brain system to another. How does the early loss of a sensory modality affect this complex process? We examined cross-modal neuroplasticity in anatomically defined subregions of Heschl's gyrus, the site of human primary auditory cortex, in congenitally deaf humans by measuring the fMRI signal change in response to spatially coregistered visual, somatosensory, and bimodal stimuli. In the deaf Heschl's gyrus, signal change was greater for somatosensory and bimodal stimuli than that of hearing participants. Visual responses in Heschl's gyrus, larger in deaf than hearing, were smaller than those elicited by somatosensory stimulation. In contrast to Heschl's gyrus, in the superior-temporal cortex visual signal was comparable to somatosensory signal. In addition, deaf adults perceived bimodal stimuli differently; in contrast to hearing adults, they were susceptible to a double-flash visual illusion induced by two touches to the face. Somatosensory and bimodal signal change in rostrolateral Heschl's gyrus predicted the strength of the visual illusion in the deaf adults in line with the interpretation that the illusion is a functional consequence of the altered cross-modal organization observed in deaf auditory cortex. Our results demonstrate that congenital and profound deafness alters how vision and somatosensation are processed in primary auditory cortex.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2012 the authors 0270-6474/12/329626-13\$15.00/0},
  langid = {english},
  pmid = {22787048},
  file = {/Users/xzfang/Zotero/storage/6DCJHL68/Karns et al. - 2012 - Altered Cross-Modal Processing in the Primary Audi.pdf;/Users/xzfang/Zotero/storage/RBCFDJ5U/9626.html}
}

@article{karuza_neural_2013,
  title = {The Neural Correlates of Statistical Learning in a Word Segmentation Task: {{An fMRI}} Study},
  shorttitle = {The Neural Correlates of Statistical Learning in a Word Segmentation Task},
  author = {Karuza, Elisabeth A. and Newport, Elissa L. and Aslin, Richard N. and Starling, Sarah J. and Tivarus, Madalina E. and Bavelier, Daphne},
  year = {2013},
  month = oct,
  journal = {Brain and Language},
  volume = {127},
  number = {1},
  pages = {46--54},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2012.11.007},
  abstract = {Functional magnetic resonance imaging (fMRI) was used to assess neural activation as participants learned to segment continuous streams of speech containing syllable sequences varying in their transitional probabilities. Speech streams were presented in four runs, each followed by a behavioral test to measure the extent of learning over time. Behavioral performance indicated that participants could discriminate statistically coherent sequences (words) from less coherent sequences (partwords). Individual rates of learning, defined as the difference in ratings for words and partwords, were used as predictors of neural activation to ask which brain areas showed activity associated with these measures. Results showed significant activity in the pars opercularis and pars triangularis regions of the left inferior frontal gyrus (LIFG). The relationship between these findings and prior work on the neural basis of statistical learning is discussed, and parallels to the frontal/subcortical network involved in other forms of implicit sequence learning are considered.},
  langid = {english},
  keywords = {Artificial language,Brocaâ€™s area,fMRI,LIFG,Sequence learning,Statistical learning,Word segmentation},
  file = {/Users/xzfang/Zotero/storage/KL2IJQ2Y/Karuza et al. - 2013 - The neural correlates of statistical learning in a.pdf;/Users/xzfang/Zotero/storage/VVHJYLRQ/S0093934X12002106.html}
}

@article{katsuki_early_2012,
  title = {Early Involvement of Prefrontal Cortex in Visual Bottom-up Attention},
  author = {Katsuki, Fumi and Constantinidis, Christos},
  year = {2012},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {15},
  number = {8},
  pages = {1160--1166},
  issn = {1546-1726},
  doi = {10.1038/nn.3164},
  abstract = {The authors found that, when monkeys detected a salient stimulus defined purely by bottom-up factors, neurons in the dorsolateral prefrontal cortex represented the stimulus no later than those in the posterior parietal cortex. The results suggest an early involvement of the prefrontal cortex in the bottom-up guidance of visual attention.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MDMK6DJX/Katsuki and Constantinidis - 2012 - Early involvement of prefrontal cortex in visual b.pdf;/Users/xzfang/Zotero/storage/3UA78VCX/nn.html}
}

@article{katsuki_early_2012a,
  title = {Early Involvement of Prefrontal Cortex in Visual Bottom-up Attention},
  author = {Katsuki, Fumi and Constantinidis, Christos},
  year = {2012},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {15},
  number = {8},
  pages = {1160--1166},
  issn = {1546-1726},
  doi = {10.1038/nn.3164},
  abstract = {The authors found that, when monkeys detected a salient stimulus defined purely by bottom-up factors, neurons in the dorsolateral prefrontal cortex represented the stimulus no later than those in the posterior parietal cortex. The results suggest an early involvement of the prefrontal cortex in the bottom-up guidance of visual attention.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PGXNL4IJ/Katsuki and Constantinidis - 2012 - Early involvement of prefrontal cortex in visual b.pdf;/Users/xzfang/Zotero/storage/UQEZR3NI/nn.html}
}

@article{katti_are_2019,
  title = {Are You from {{North}} or {{South India}}? {{A}} Hard Face-Classification Task Reveals Systematic Representational Differences between Humans and Machines},
  shorttitle = {Are You from {{North}} or {{South India}}?},
  author = {Katti, Harish and Arun, S. P.},
  year = {2019},
  month = jul,
  journal = {Journal of Vision},
  volume = {19},
  number = {7},
  pages = {1--1},
  publisher = {{The Association for Research in Vision and Ophthalmology}},
  issn = {1534-7362},
  doi = {10.1167/19.7.1},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/F7CWR4VS/Katti and Arun - 2019 - Are you from North or South India A hard face-cla.pdf;/Users/xzfang/Zotero/storage/S5EPXSGY/article.html}
}

@article{katti_machine_2019,
  title = {Machine Vision Benefits from Human Contextual Expectations},
  author = {Katti, Harish and Peelen, Marius V. and Arun, S. P.},
  year = {2019},
  month = feb,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {2112},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-38427-0},
  abstract = {Scene context is known to facilitate object recognition in both machines and humans, suggesting that the underlying representations may be similar. Alternatively, they may be qualitatively different since the training experience of machines and humans are strikingly different. Machines are typically trained on images containing objects and their context, whereas humans frequently experience scenes without objects (such as highways without cars). If these context representations are indeed different, machine vision algorithms will be improved on augmenting them with human context representations, provided these expectations can be measured and are systematic. Here, we developed a paradigm to measure human contextual expectations. We asked human subjects to indicate the scale, location and likelihood at which cars or people might occur in scenes without these objects. This yielded highly systematic expectations that we could then accurately predict using scene features. This allowed us to predict human expectations on novel scenes without requiring explicit measurements. Next we augmented decisions made by deep neural networks with these predicted human expectations and obtained substantial gains in accuracy for detecting cars and people (1\textendash 3\%) as well as on detecting associated objects (3\textendash 20\%). In contrast, augmenting deep network decisions with other conventional computer vision features yielded far smaller gains. Taken together, our results show that augmenting deep neural networks with human-derived contextual expectations improves their performance, suggesting that contextual representations are qualitatively different in humans and deep neural networks.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/G2EH5MW7/Katti et al. - 2019 - Machine vision benefits from human contextual expe.pdf;/Users/xzfang/Zotero/storage/VXZPCQNX/s41598-018-38427-0.html}
}

@inproceedings{kawahara_tandemstraight_2008,
  title = {Tandem-{{STRAIGHT}}: {{A}} Temporally Stable Power Spectral Representation for Periodic Signals and Applications to Interference-Free Spectrum, {{F0}}, and Aperiodicity Estimation},
  shorttitle = {Tandem-{{STRAIGHT}}},
  booktitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Kawahara, H. and Morise, M. and Takahashi, T. and Nisimura, R. and Irino, T. and Banno, H.},
  year = {2008},
  month = mar,
  pages = {3933--3936},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2008.4518514},
  abstract = {A simple new method for estimating temporally stable power spectra is introduced to provide a unified basis for computing an interference-free spectrum, the fundamental frequency (F0), as well as aperiodicity estimation. F0 adaptive spectral smoothing and cepstral liftering based on consistent sampling theory are employed for interference-free spectral estimation. A perturbation spectrum, calculated from temporally stable power and interference-free spectra, provides the basis for both F0 and aperiodicity estimation. The proposed approach eliminates ad-hoc parameter tuning and the heavy demand on computational power, from which STRAIGHT has suffered in the past.},
  keywords = {consistent sampling,Filters,Fourier transforms,Frequency estimation,Interference,periodic signal,periodicity,power spectrum,Sampling methods,Signal sampling,Speech analysis,speech processing,Speech synthesis,Testing,Transfer functions},
  file = {/Users/xzfang/Zotero/storage/GN75JY9M/Kawahara et al. - 2008 - Tandem-STRAIGHT A temporally stable power spectra.pdf}
}

@article{kawahara_technical_2011,
  title = {Technical Foundations of {{TANDEM-STRAIGHT}}, a Speech Analysis, Modification and Synthesis Framework},
  author = {Kawahara, Hideki and Morise, Masanori},
  year = {2011},
  month = oct,
  journal = {Sadhana},
  volume = {36},
  number = {5},
  pages = {713--727},
  issn = {0256-2499, 0973-7677},
  doi = {10.1007/s12046-011-0043-3},
  abstract = {This article presents comprehensive technical information about STRAIGHT and TANDEM-STRAIGHT, a widely used speech modification tool and its successor. They share the same concept: the periodic excitation found in voiced sounds is an efficient mechanism for transmitting underlying smooth time\textendash frequency representation. The tools are also based on the perceptual equivalence of two sets of independent Gaussian random signals. This equivalence makes it possible to discard input phase information intentionally and enables flexible manipulation of parameters.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NNISDEMG/Kawahara and Morise - 2011 - Technical foundations of TANDEM-STRAIGHT, a speech.pdf}
}

@article{kayser_rhythmic_2015,
  title = {Rhythmic {{Auditory Cortex Activity}} at {{Multiple Timescales Shapes Stimulus}}\textendash{{Response Gain}} and {{Background Firing}}},
  author = {Kayser, Christoph and Wilson, Caroline and Safaai, Houman and Sakata, Shuzo and Panzeri, Stefano},
  year = {2015},
  month = may,
  journal = {The Journal of Neuroscience},
  volume = {35},
  number = {20},
  pages = {7750--7762},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.0268-15.2015},
  abstract = {The phase of low-frequency network activity in the auditory cortex captures changes in neural excitability, entrains to the temporal structure of natural sounds, and correlates with the perceptual performance in acoustic tasks. Although these observations suggest a causal link between network rhythms and perception, it remains unknown how precisely they affect the processes by which neural populations encode sounds. We addressed this question by analyzing neural responses in the auditory cortex of anesthetized rats using stimulus\textendash response models. These models included a parametric dependence on the phase of local field potential rhythms in both stimulus-unrelated background activity and the stimulus\textendash response transfer function. We found that phase-dependent models better reproduced the observed responses than static models, during both stimulation with a series of natural sounds and epochs of silence. This was attributable to two factors: (1) phase-dependent variations in background firing (most prominent for delta; 1\textendash 4 Hz); and (2) modulations of response gain that rhythmically amplify and attenuate the responses at specific phases of the rhythm (prominent for frequencies between 2 and 12 Hz). These results provide a quantitative characterization of how slow auditory cortical rhythms shape sound encoding and suggest a differential contribution of network activity at different timescales. In addition, they highlight a putative mechanism that may implement the selective amplification of appropriately timed sound tokens relative to the phase of rhythmic auditory cortex activity.},
  pmcid = {PMC4438125},
  pmid = {25995464},
  file = {/Users/xzfang/Zotero/storage/WJ5C68GI/Kayser et al. - 2015 - Rhythmic Auditory Cortex Activity at Multiple Time.pdf}
}

@article{keetels_phonetic_2016,
  title = {Phonetic Recalibration of Speech by Text},
  author = {Keetels, Mirjam and Schakel, Lemmy and Bonte, Milene and Vroomen, Jean},
  year = {2016},
  month = apr,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {78},
  number = {3},
  pages = {938--945},
  issn = {1943-393X},
  doi = {10.3758/s13414-015-1034-y},
  abstract = {Listeners adjust their phonetic categories to cope with variations in the speech signal (phonetic recalibration). Previous studies have shown that lipread speech (and word knowledge) can adjust the perception of ambiguous speech and can induce phonetic adjustments (Bertelson, Vroomen, \& de Gelder in Psychological Science, 14(6), 592\textendash 597, 2003; Norris, McQueen, \& Cutler in Cognitive Psychology, 47(2), 204\textendash 238, 2003). We examined whether orthographic information (text) also can induce phonetic recalibration. Experiment 1 showed that after exposure to ambiguous speech sounds halfway between /b/ and /d/ that were combined with text (b or d) participants were more likely to categorize auditory-only test sounds in accordance with the exposed letters. Experiment 2 replicated this effect with a very short exposure phase. These results show that listeners adjust their phonetic boundaries in accordance with disambiguating orthographic information and that these adjustments show a rapid build-up.},
  langid = {english},
  keywords = {Letters,Orthographic information,Phonetic recalibration,Rapid recalibration,Speech perception},
  file = {/Users/xzfang/Zotero/storage/JIJRX8U8/Keetels et al. - 2016 - Phonetic recalibration of speech by text.pdf}
}

@article{keitel_auditory_2017,
  title = {Auditory Cortical Delta-Entrainment Interacts with Oscillatory Power in Multiple Fronto-Parietal Networks},
  author = {Keitel, Anne and Ince, Robin A. A. and Gross, Joachim and Kayser, Christoph},
  year = {2017},
  month = feb,
  journal = {NeuroImage},
  volume = {147},
  pages = {32--42},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2016.11.062},
  abstract = {The timing of slow auditory cortical activity aligns to the rhythmic fluctuations in speech. This entrainment is considered to be a marker of the prosodic and syllabic encoding of speech, and has been shown to correlate with intelligibility. Yet, whether and how auditory cortical entrainment is influenced by the activity in other speech\textendash relevant areas remains unknown. Using source-localized MEG data, we quantified the dependency of auditory entrainment on the state of oscillatory activity in fronto-parietal regions. We found that delta band entrainment interacted with the oscillatory activity in three distinct networks. First, entrainment in the left anterior superior temporal gyrus (STG) was modulated by beta power in orbitofrontal areas, possibly reflecting predictive top-down modulations of auditory encoding. Second, entrainment in the left Heschl's Gyrus and anterior STG was dependent on alpha power in central areas, in line with the importance of motor structures for phonological analysis. And third, entrainment in the right posterior STG modulated theta power in parietal areas, consistent with the engagement of semantic memory. These results illustrate the topographical network interactions of auditory delta entrainment and reveal distinct cross-frequency mechanisms by which entrainment can interact with different cognitive processes underlying speech perception.},
  langid = {english},
  keywords = {Auditory entrainment,Delta band,MEG,Prosodic parsing,Speech processing},
  file = {/Users/xzfang/Zotero/storage/57J6MNQQ/Keitel et al. - 2017 - Auditory cortical delta-entrainment interacts with.pdf;/Users/xzfang/Zotero/storage/FJANKYML/S1053811916306814.html}
}

@article{keitel_individual_2016,
  title = {Individual {{Human Brain Areas Can Be Identified}} from {{Their Characteristic Spectral Activation Fingerprints}}},
  author = {Keitel, Anne and Gross, Joachim},
  year = {2016},
  month = jun,
  journal = {PLOS Biology},
  volume = {14},
  number = {6},
  pages = {e1002498},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002498},
  abstract = {The human brain can be parcellated into diverse anatomical areas. We investigated whether rhythmic brain activity in these areas is characteristic and can be used for automatic classification. To this end, resting-state MEG data of 22 healthy adults was analysed. Power spectra of 1-s long data segments for atlas-defined brain areas were clustered into spectral profiles (``fingerprints''), using k-means and Gaussian mixture (GM) modelling. We demonstrate that individual areas can be identified from these spectral profiles with high accuracy. Our results suggest that each brain area engages in different spectral modes that are characteristic for individual areas. Clustering of brain areas according to similarity of spectral profiles reveals well-known brain networks. Furthermore, we demonstrate task-specific modulations of auditory spectral profiles during auditory processing. These findings have important implications for the classification of regional spectral activity and allow for novel approaches in neuroimaging and neurostimulation in health and disease.},
  langid = {english},
  keywords = {Brain,Hierarchical clustering,Linear regression analysis,Neural networks,Neuroscience,Radii,Sensory perception,Statistical models},
  file = {/Users/xzfang/Zotero/storage/9SF9ANCV/Keitel and Gross - 2016 - Individual Human Brain Areas Can Be Identified fro.pdf;/Users/xzfang/Zotero/storage/T9TV4EJG/article.html}
}

@article{keitel_perceptually_2018,
  title = {Perceptually Relevant Speech Tracking in Auditory and Motor Cortex Reflects Distinct Linguistic Features},
  author = {Keitel, Anne and Gross, Joachim and Kayser, Christoph},
  year = {2018},
  month = mar,
  journal = {PLOS Biology},
  volume = {16},
  number = {3},
  pages = {e2004473},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2004473},
  abstract = {During online speech processing, our brain tracks the acoustic fluctuations in speech at different timescales. Previous research has focused on generic timescales (for example, delta or theta bands) that are assumed to map onto linguistic features such as prosody or syllables. However, given the high intersubject variability in speaking patterns, such a generic association between the timescales of brain activity and speech properties can be ambiguous. Here, we analyse speech tracking in source-localised magnetoencephalographic data by directly focusing on timescales extracted from statistical regularities in our speech material. This revealed widespread significant tracking at the timescales of phrases (0.6\textendash 1.3 Hz), words (1.8\textendash 3 Hz), syllables (2.8\textendash 4.8 Hz), and phonemes (8\textendash 12.4 Hz). Importantly, when examining its perceptual relevance, we found stronger tracking for correctly comprehended trials in the left premotor (PM) cortex at the phrasal scale as well as in left middle temporal cortex at the word scale. Control analyses using generic bands confirmed that these effects were specific to the speech regularities in our stimuli. Furthermore, we found that the phase at the phrasal timescale coupled to power at beta frequency (13\textendash 30 Hz) in motor areas. This cross-frequency coupling presumably reflects top-down temporal prediction in ongoing speech perception. Together, our results reveal specific functional and perceptually relevant roles of distinct tracking and cross-frequency processes along the auditory\textendash motor pathway.},
  langid = {english},
  keywords = {Motor cortex,Motor system,Phonemes,Sensory perception,Speech,Speech signal processing,Syllables,Syntax},
  file = {/Users/xzfang/Zotero/storage/PVBAAU2K/Keitel et al. - 2018 - Perceptually relevant speech tracking in auditory .pdf;/Users/xzfang/Zotero/storage/R5H6YH3T/article.html}
}

@article{kell_taskoptimized_2018,
  title = {A {{Task-Optimized Neural Network Replicates Human Auditory Behavior}}, {{Predicts Brain Responses}}, and {{Reveals}} a {{Cortical Processing Hierarchy}}},
  author = {Kell, Alexander J. E. and Yamins, Daniel L. K. and Shook, Erica N. and {Norman-Haignere}, Sam V. and McDermott, Josh H.},
  year = {2018},
  month = may,
  journal = {Neuron},
  volume = {98},
  number = {3},
  pages = {630-644.e16},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.03.044},
  abstract = {A core goal of auditory neuroscience is to build quantitative models that predict cortical responses to natural sounds. Reasoning that a complete model of auditory cortex must solve ecologically relevant tasks, we optimized hierarchical neural networks for speech and music recognition. The best-performing network contained separate music and speech pathways following early shared processing, potentially replicating human cortical organization. The network performed both tasks as well as humans and exhibited human-like errors despite not being optimized to do so, suggesting common constraints on network and human performance. The network predicted fMRI voxel responses substantially better than traditional spectrotemporal filter models throughout auditory cortex. It also provided a quantitative signature of cortical representational hierarchy\textemdash primary and non-primary responses were best predicted by intermediate and late network layers, respectively. The results suggest that task optimization provides a powerful set of tools for modeling sensory systems.},
  langid = {english},
  keywords = {auditory cortex,convolutional neural network,deep learning,deep neural network,encoding models,fMRI,hierarchy,human auditory cortex,natural sounds,word recognition},
  file = {/Users/xzfang/Zotero/storage/27Y2ZNE3/Kell et al. - 2018 - A Task-Optimized Neural Network Replicates Human A.pdf;/Users/xzfang/Zotero/storage/TQU5XE9M/S0896627318302502.html}
}

@article{keshavarzi_transcranial_2020,
  title = {Transcranial Alternating Current Stimulation in the Theta Band but Not in the Delta Band Modulates the Comprehension of Naturalistic Speech in Noise},
  author = {Keshavarzi, Mahmoud and Kegler, Mikolaj and Kadir, Shabnam and Reichenbach, Tobias},
  year = {2020},
  month = apr,
  journal = {NeuroImage},
  volume = {210},
  pages = {116557},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.116557},
  abstract = {Auditory cortical activity entrains to speech rhythms and has been proposed as a mechanism for online speech processing. In particular, neural activity in the theta frequency band (4\textendash 8~\hspace{0pt}Hz) tracks the onset of syllables which may aid the parsing of a speech stream. Similarly, cortical activity in the delta band (1\textendash 4~\hspace{0pt}Hz) entrains to the onset of words in natural speech and has been found to encode both syntactic as well as semantic information. Such neural entrainment to speech rhythms is not merely an epiphenomenon of other neural processes, but plays a functional role in speech processing: modulating the neural entrainment through transcranial alternating current stimulation influences the speech-related neural activity and modulates the comprehension of degraded speech. However, the distinct functional contributions of the delta- and of the theta-band entrainment to the modulation of speech comprehension have not yet been investigated. Here we use transcranial alternating current stimulation with waveforms derived from the speech envelope and filtered in the delta and theta frequency bands to alter cortical entrainment in both bands separately. We find that transcranial alternating current stimulation in the theta band but not in the delta band impacts speech comprehension. Moreover, we find that transcranial alternating current stimulation with the theta-band portion of the speech envelope can improve speech-in-noise comprehension beyond sham stimulation. Our results show a distinct contribution of the theta- but not of the delta-band stimulation to the modulation of speech comprehension. In addition, our findings open up a potential avenue of enhancing the comprehension of speech in noise.},
  langid = {english},
  keywords = {Delta and theta frequency bands,Neural entrainment,Normal hearing,Speech comprehension,Speech envelope,Speech-shaped-noise,Transcranial alternating current stimulation},
  file = {/Users/xzfang/Zotero/storage/VQ877FNA/Keshavarzi et al. - 2020 - Transcranial alternating current stimulation in th.pdf;/Users/xzfang/Zotero/storage/SZEUXYEE/S1053811920300446.html}
}

@misc{keshev_noisy_2020,
  title = {Noisy Is Better than Rare: {{Comprehenders}} Compromise Subject-Verb Agreement to Form More Probable Linguistic Structures},
  shorttitle = {Noisy Is Better than Rare},
  author = {Keshev, Maayan and {Meltzer-Asscher}, Aya},
  year = {2020},
  month = jan,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/6rdw2},
  abstract = {Production and perception errors are common in everyday language use. Recent studies suggest that in order to overcome the flawed speech signal, comprehenders engage in rational noisy-channel processing, which can pull their interpretation towards more probable ``near-neighbor'' analyses, based on the assumption that an error may have occurred in the transmission of the sentence. We investigate this type of processing using subject/object relative clause ambiguity in Hebrew. In four self-paced reading experiments and a sentence completion experiment, we find that during online processing, readers apply elaborate knowledge regarding the distribution of structures in the language, and that they are willing to compromise subject-verb agreement to refrain from (grammatical but) highly improbable structures. The results suggest that the prior probability of alternative analyses modulates the interpretation of agreement.},
  keywords = {Cognitive Psychology,Language,Linguistics,Psycholinguistics and Neurolinguistics,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/X454X5A8/Keshev and Meltzer-Asscher - 2020 - Noisy is better than rare Comprehenders compromis.pdf}
}

@article{keskin_umbelliferae_2016,
  title = {Umbelliferae {{Familyas\i ndan Baz\i{} \"Onemli K\"ult\"ur T\"urlerinin Isparta Ekolojik Ko\c{s}ullar\i nda Tar\i msal}} ve {{Teknolojik \"Ozelliklerinin Belirlenmesi}}},
  author = {Kesk{\.i}N, Sevil and Baydar, Hasan},
  year = {2016},
  month = apr,
  journal = {SD\"U Fen Bilimleri Enstit\"us\"u Dergisi},
  volume = {20},
  number = {1},
  issn = {1308-6529, 1300-7688},
  doi = {10.19113/sdufbed.86008}
}

@article{keysar_foreignlanguage_2012,
  title = {The {{Foreign-Language Effect}}: {{Thinking}} in a {{Foreign Tongue Reduces Decision Biases}}},
  shorttitle = {The {{Foreign-Language Effect}}},
  author = {Keysar, Boaz and Hayakawa, Sayuri L. and An, Sun Gyu},
  year = {2012},
  month = jun,
  journal = {Psychological Science},
  volume = {23},
  number = {6},
  pages = {661--668},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611432178},
  abstract = {Would you make the same decisions in a foreign language as you would in your native tongue? It may be intuitive that people would make the same choices regardless of the language they are using, or that the difficulty of using a foreign language would make decisions less systematic. We discovered, however, that the opposite is true: Using a foreign language reduces decision-making biases. Four experiments show that the framing effect disappears when choices are presented in a foreign tongue. Whereas people were risk averse for gains and risk seeking for losses when choices were presented in their native tongue, they were not influenced by this framing manipulation in a foreign language. Two additional experiments show that using a foreign language reduces loss aversion, increasing the acceptance of both hypothetical and real bets with positive expected value. We propose that these effects arise because a foreign language provides greater cognitive and emotional distance than a native tongue does.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UQHYLFTN/Keysar et al. - 2012 - The Foreign-Language Effect Thinking in a Foreign.pdf}
}

@article{khalighinejad_dynamic_2017,
  title = {Dynamic {{Encoding}} of {{Acoustic Features}} in {{Neural Responses}} to {{Continuous Speech}}},
  author = {Khalighinejad, Bahar and da Silva, Guilherme Cruzatto and Mesgarani, Nima},
  year = {2017},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {37},
  number = {8},
  pages = {2176--2185},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2383-16.2017},
  abstract = {Humans are unique in their ability to communicate using spoken language. However, it remains unclear how the speech signal is transformed and represented in the brain at different stages of the auditory pathway. In this study, we characterized electroencephalography responses to continuous speech by obtaining the time-locked responses to phoneme instances (phoneme-related potential). We showed that responses to different phoneme categories are organized by phonetic features. We found that each instance of a phoneme in continuous speech produces multiple distinguishable neural responses occurring as early as 50 ms and as late as 400 ms after the phoneme onset. Comparing the patterns of phoneme similarity in the neural responses and the acoustic signals confirms a repetitive appearance of acoustic distinctions of phonemes in the neural data. Analysis of the phonetic and speaker information in neural activations revealed that different time intervals jointly encode the acoustic similarity of both phonetic and speaker categories. These findings provide evidence for a dynamic neural transformation of low-level speech features as they propagate along the auditory pathway, and form an empirical framework to study the representational changes in learning, attention, and speech disorders. SIGNIFICANCE STATEMENT We characterized the properties of evoked neural responses to phoneme instances in continuous speech. We show that each instance of a phoneme in continuous speech produces several observable neural responses at different times occurring as early as 50 ms and as late as 400 ms after the phoneme onset. Each temporal event explicitly encodes the acoustic similarity of phonemes, and linguistic and nonlinguistic information are best represented at different time intervals. Finally, we show a joint encoding of phonetic and speaker information, where the neural representation of speakers is dependent on phoneme category. These findings provide compelling new evidence for dynamic processing of speech sounds in the auditory pathway.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2017 Khalighinejad et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {28119400},
  keywords = {EEG,event-related potential,phonemes,speech},
  file = {/Users/xzfang/Zotero/storage/HHYI4NJC/Khalighinejad et al. - 2017 - Dynamic Encoding of Acoustic Features in Neural Re.pdf;/Users/xzfang/Zotero/storage/PA3DGF35/2176.html}
}

@article{khandhadia_audiovisual_2021,
  title = {Audiovisual Integration in Macaque Face Patch Neurons},
  author = {Khandhadia, Amit P. and Murphy, Aidan P. and Romanski, Lizabeth M. and Bizley, Jennifer K. and Leopold, David A.},
  year = {2021},
  month = feb,
  journal = {Current biology: CB},
  issn = {1879-0445},
  doi = {10.1016/j.cub.2021.01.102},
  abstract = {Primate social communication depends on the perceptual integration of visual and auditory cues, reflected in the multimodal mixing of sensory signals in certain cortical areas. The macaque cortical face patch network, identified through visual, face-selective responses measured with fMRI, is assumed to contribute to visual social interactions. However, whether face patch neurons are also influenced by acoustic information, such as the auditory component of a natural vocalization, remains unknown. Here, we recorded single-unit activity in the anterior fundus (AF) face patch, in the superior temporal sulcus, and anterior medial (AM) face patch, on the undersurface of the temporal lobe, in macaques presented with audiovisual, visual-only, and auditory-only renditions of natural movies of macaques vocalizing. The results revealed that 76\% of neurons in face patch AF were significantly influenced by the auditory component of the movie, most often through enhancement of visual responses but sometimes in response to the auditory stimulus alone. By contrast, few neurons in face patch AM exhibited significant auditory responses or modulation. Control experiments in AF used an animated macaque avatar to demonstrate, first, that the structural elements of the face were often essential for audiovisual modulation and, second, that the temporal modulation of the acoustic stimulus was more important than its frequency spectrum. Together, these results identify a striking contrast between two face patches and specifically identify AF as playing a potential role in the integration of audiovisual cues during natural modes of social communication.},
  langid = {english},
  pmid = {33636119},
  keywords = {audition,electrophysiology,face patches,multisensory integration,primate,vision},
  file = {/Users/xzfang/Zotero/storage/4MY8BHI2/Khandhadia et al. - 2021 - Audiovisual integration in macaque face patch neur.pdf}
}

@article{khosla_cortical_2021,
  title = {Cortical Response to Naturalistic Stimuli Is Largely Predictable with Deep Neural Networks},
  author = {Khosla, Meenakshi and Ngo, Gia H. and Jamison, Keith and Kuceyeski, Amy and Sabuncu, Mert R.},
  year = {2021},
  month = may,
  journal = {Science Advances},
  volume = {7},
  number = {22},
  pages = {eabe7547},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abe7547},
  abstract = {Naturalistic stimuli, such as movies, activate a substantial portion of the human brain, invoking a response shared across individuals. Encoding models that predict neural responses to arbitrary stimuli can be very useful for studying brain function. However, existing models focus on limited aspects of naturalistic stimuli, ignoring the dynamic interactions of modalities in this inherently context-rich paradigm. Using movie-watching data from the Human Connectome Project, we build group-level models of neural activity that incorporate several inductive biases about neural information processing, including hierarchical processing, temporal assimilation, and auditory-visual interactions. We demonstrate how incorporating these biases leads to remarkable prediction performance across large areas of the cortex, beyond the sensory-specific cortices into multisensory sites and frontal cortex. Furthermore, we illustrate that encoding models learn high-level concepts that generalize to task-bound paradigms. Together, our findings underscore the potential of encoding models as powerful tools for studying brain function in ecologically valid conditions. Naturalistic stimuli induce a brain response shared across individuals that is largely predictable with deep learning. Naturalistic stimuli induce a brain response shared across individuals that is largely predictable with deep learning.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. https://creativecommons.org/licenses/by-nc/4.0/This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  pmid = {34049888},
  file = {/Users/xzfang/Zotero/storage/CXIYDYXU/Khosla et al. - 2021 - Cortical response to naturalistic stimuli is large.pdf;/Users/xzfang/Zotero/storage/Q9HWYNWJ/tab-article-info.html}
}

@article{kilbourn-ceron_variable_2021,
  title = {Variable Pronunciations Reveal Dynamic Intra-Speaker Variation in Speech Planning},
  author = {{Kilbourn-Ceron}, Oriana and Goldrick, Matthew},
  year = {2021},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {28},
  number = {4},
  pages = {1365--1380},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-01886-0},
  abstract = {In two speech production experiments, we investigated the link between phonetic variation and the scope of advance planning at the word form encoding stage. We examined cases where a word has, in addition to the pronunciation of the word in isolation, a context-specific pronunciation variant that appears only when the following word includes specific sounds. To the extent that the speaker uses the variant specific to the following context, we can infer that the phonological content of the upcoming word is included in the current planning scope. We hypothesize that the time alignment between selection of the phonetic variant in the currently-being-encoded word and retrieval of segmental details of the upcoming word is variable from moment to moment depending on current task demands and the dynamics of lexical access for each word involved. The results showed that the use of a context-sensitive phonetic variant of /t/ (``flapping'') by English speakers reliably increased under conditions which favor advance planning. Our hypothesis was supported by evidence compatible with its three key predictions: an increase in flapping in phrases with a higher frequency following word, more flapping in a procedure with a response delay relative to a speeded response, and an attenuation of the following word frequency effect with delayed responses. This reveals that within speakers, the degree of advance planning varies continuously from moment to moment, reflecting (in part) the accessibility of form properties of individual words in the utterance.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/794VXMPI/Kilbourn-Ceron and Goldrick - 2021 - Variable pronunciations reveal dynamic intra-speak.pdf}
}

@article{kim_independence_2005,
  title = {The Independence of Combinatory Semantic Processing: {{Evidence}} from Event-Related Potentials},
  shorttitle = {The Independence of Combinatory Semantic Processing},
  author = {Kim, Albert and Osterhout, Lee},
  year = {2005},
  month = feb,
  journal = {Journal of Memory and Language},
  volume = {52},
  number = {2},
  pages = {205--225},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2004.10.002},
  abstract = {We recorded event-related brain potentials (ERPs) while participants read sentences, some of which contained an anomalous word. In the critical sentences (e.g., The meal was devouring\ldots ), the syntactic cues unambiguously signaled an Agent interpretation of the subject noun, whereas the semantic cues supported a Theme interpretation. An Agent interpretation would render the main verb semantically anomalous (as meals do not devour things). Conversely, the Theme interpretation would render the main verb syntactically anomalous (as the -ED form, not the -ING form, is syntactically appropriate for this interpretation). We report that the main verbs in such sentences elicit the P600 effect associated with syntactic anomalies, rather than the N400 effect associated with semantic anomalies. We conclude that, at least under certain conditions, semantic information is ``in control'' of how words are combined during sentence processing.},
  langid = {english},
  keywords = {ERPs,N400,P600,Semantics,Sentence processing,Syntax,Thematic roles},
  file = {/Users/xzfang/Zotero/storage/BEXDDVAW/Kim and Osterhout - 2005 - The independence of combinatory semantic processin.pdf;/Users/xzfang/Zotero/storage/SDV39S8U/S0749596X04001159.html}
}

@article{kim_knowledge_2019,
  title = {Knowledge of Animal Appearance among Sighted and Blind Adults},
  author = {Kim, Judy S. and Elli, Giulia V. and Bedny, Marina},
  year = {2019},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {23},
  pages = {11213--11222},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1900952116},
  abstract = {How does first-person sensory experience contribute to knowledge? Contrary to the suppositions of early empiricist philosophers, people who are born blind know about phenomena that cannot be perceived directly, such as color and light. Exactly what is learned and how remains an open question. We compared knowledge of animal appearance across congenitally blind (               n               = 20) and sighted individuals (two groups,               n               = 20 and               n               = 35) using a battery of tasks, including ordering (size and height), sorting (shape, skin texture, and color), odd-one-out (shape), and feature choice (texture). On all tested dimensions apart from color, sighted and blind individuals showed substantial albeit imperfect agreement, suggesting that linguistic communication and visual perception convey partially redundant appearance information. To test the hypothesis that blind individuals learn about appearance primarily by remembering sighted people's descriptions of what they see (e.g., ``elephants are gray''), we measured verbalizability of animal shape, texture, and color in the sighted. Contrary to the learn-from-description hypothesis, blind and sighted groups disagreed most about the appearance dimension that was easiest for sighted people to verbalize: color. Analysis of disagreement patterns across all tasks suggest that blind individuals infer physical features from non-appearance properties of animals such as folk taxonomy and habitat (e.g., bats are textured like mammals but shaped like birds). These findings suggest that in the absence of sensory access, structured appearance knowledge is acquired through inference from ontological kind.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QVPRVT4M/Kim et al. - 2019 - Knowledge of animal appearance among sighted and b.pdf}
}

@article{kim_neural_2013,
  title = {Neural {{Mechanisms}} of {{Rapid Sensitivity}} to {{Syntactic Anomaly}}},
  author = {Kim, Albert E. and Gilley, Phillip M.},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00045},
  abstract = {Recent psycholinguistic models hypothesize that anticipatory processing can speed the response to linguistic input during language comprehension by pre-activating representations necessary for word recognition. We investigated the neurocognitive mechanisms of anticipatory processing by recording event-related brain responses (ERPs) to syntactically anomalous (The thief was caught by for police) and well-formed (e.g., The thief was caught by the police) sentences. One group of participants saw anomalies elicited by the same word in every instance (e.g., for; low-variability stimuli), providing high affordances for predictions about the word-form appearing in the critical position. A second group saw anomalies elicited by seven different prepositions (at, of, on, for, from, over, with; high-variability stimuli) across the study, creating a more difficult prediction task. Syntactic category anomalies enhanced the occipital-temporal N170 component of the ERP, indicating rapid sensitivity\textemdash within 200 ms of word onset\textemdash to syntactic anomaly. For low-variability but not the high-variability stimuli, syntactic anomaly also enhanced the earlier occipital-temporal P1 component, around 130 ms after word-onset, indicating that affordances for prediction engendered earlier sensitivity to syntactic anomaly. Independent components analysis revealed three sources within the ERP signal whose functional dynamics were consistent with predictive processing and early responses to syntactic anomaly. Distributed neural source modeling (sLORETA) of these early-active sources produced a candidate network for early responses to words during reading in the right posterior-occipital, left occipital-temporal, and medial parietal cortex.},
  langid = {english},
  keywords = {anticipatory,N170,occipital temporal cortex,P1,posterior cingulate,prediction,sentence comprehension,syntactic},
  file = {/Users/xzfang/Zotero/storage/8VUPVGKU/Kim and Gilley - 2013 - Neural Mechanisms of Rapid Sensitivity to Syntacti.pdf}
}

@article{kim_neural_2019,
  title = {Neural {{Coding}} for {{Shape}} and {{Texture}} in {{Macaque Area V4}}},
  author = {Kim, Taekjun and Bair, Wyeth and Pasupathy, Anitha},
  year = {2019},
  month = jun,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {24},
  pages = {4760--4774},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3073-18.2019},
  abstract = {The distinct visual sensations of shape and texture have been studied separately in cortex; therefore, it remains unknown whether separate neuronal populations encode each of these properties or one population carries a joint encoding. We directly compared shape and texture selectivity of individual V4 neurons in awake macaques (1 male, 1 female) and found that V4 neurons lie along a continuum from strong tuning for boundary curvature of shapes to strong tuning for perceptual dimensions of texture. Among neurons tuned to both attributes, tuning for shape and texture were largely separable, with the latter delayed by {$\sim$}30 ms. We also found that shape stimuli typically evoked stronger, more selective responses than did texture patches, regardless of whether the latter were contained within or extended beyond the receptive field. These results suggest that there are separate specializations in mid-level cortical processing for visual attributes of shape and texture. SIGNIFICANCE STATEMENT Object recognition depends on our ability to see both the shape of the boundaries of objects and properties of their surfaces. However, neuroscientists have never before examined how shape and texture are linked together in mid-level visual cortex. In this study, we used systematically designed sets of simple shapes and texture patches to probe the responses of individual neurons in the primate visual cortex. Our results provide the first evidence that some cortical neurons specialize in processing shape whereas others specialize in processing textures. Most neurons lie between the ends of this continuum, and in these neurons we find that shape and texture encoding are largely independent.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2019 the authors},
  langid = {english},
  pmid = {30948478},
  keywords = {monkey,object recognition,rhesus macaque,shape,texture,ventral visual pathway},
  file = {/Users/xzfang/Zotero/storage/KKZCCI67/Kim et al. - 2019 - Neural Coding for Shape and Texture in Macaque Are.pdf;/Users/xzfang/Zotero/storage/6RTXX39R/4760.html}
}

@article{kim_notsoclevr_2018,
  title = {Not-{{So-CLEVR}}: Learning Same\textendash Different Relations Strains Feedforward Neural Networks},
  shorttitle = {Not-{{So-CLEVR}}},
  author = {Kim, Junkyung and Ricci, Matthew and Serre, Thomas},
  year = {2018},
  month = aug,
  journal = {Interface Focus},
  volume = {8},
  number = {4},
  pages = {20180011},
  issn = {2042-8898, 2042-8901},
  doi = {10.1098/rsfs.2018.0011},
  abstract = {The advent of deep learning has recently led to great successes in various engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural network, now approach human accuracy on visual recognition tasks like image classification and face recognition. However, here we will show that feedforward neural networks struggle to learn abstract visual relations that are effortlessly recognized by non-human primates, birds, rodents and even insects. We systematically study the ability of feedforward neural networks to learn to recognize a variety of visual relations and demonstrate that same\textendash different visual relations pose a particular strain on these networks. Networks fail to learn same\textendash different visual relations when stimulus variability makes rote memorization difficult. Further, we show that learning same\textendash different problems becomes trivial for a feedforward network that is fed with perceptually grouped stimuli. This demonstration and the comparative success of biological vision in learning visual relations suggests that feedback mechanisms such as attention, working memory and perceptual grouping may be the key components underlying human-level abstract visual reasoning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GSCWPEZS/Kim et al. - 2018 - Not-So-CLEVR learning sameâ€“different relations st.pdf}
}

@article{kim_rapid_2012,
  title = {Rapid {{Interactions}} between {{Lexical Semantic}} and {{Word Form Analysis}} during {{Word Recognition}} in {{Context}}: {{Evidence}} from {{ERPs}}},
  shorttitle = {Rapid {{Interactions}} between {{Lexical Semantic}} and {{Word Form Analysis}} during {{Word Recognition}} in {{Context}}},
  author = {Kim, Albert and Lai, Vicky},
  year = {2012},
  month = may,
  journal = {Journal of Cognitive Neuroscience},
  volume = {24},
  number = {5},
  pages = {1104--1112},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_00148},
  abstract = {We used ERPs to investigate the time course of interactions between lexical semantic and sublexical visual word form processing during word recognition. Participants read sentence-embedded pseudowords that orthographically resembled a contextually supported real word (e.g., ``She measured the flour so she could bake a ceke\ldots '') or did not (e.g., ``She measured the flour so she could bake a tont\ldots '') along with nonword consonant strings (e.g., ``She measured the flour so she could bake a srdt\ldots ''). Pseudowords that resembled a contextually supported real word (``ceke'') elicited an enhanced positivity at 130 msec (P130), relative to real words (e.g., ``She measured the flour so she could bake a cake\ldots ''). Pseudowords that did not resemble a plausible real word (``tont'') enhanced the N170 component, as did nonword consonant strings (``srdt''). The effect pattern shows that the visual word recognition system is, perhaps, counterintuitively, more rapidly sensitive to minor than to flagrant deviations from contextually predicted inputs. The findings are consistent with rapid interactions between lexical and sublexical representations during word recognition, in which rapid lexical access of a contextually supported word (CAKE) provides top\textendash down excitation of form features (``cake''), highlighting the anomaly of an unexpected word ``ceke.''},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EE796W2Y/Kim and Lai - 2012 - Rapid Interactions between Lexical Semantic and Wo.pdf}
}

@article{kim_shared_2021,
  title = {Shared Understanding of Color among Sighted and Blind Adults},
  author = {Kim, Judy Sein and Aheimer, Brianna and Manrara, Ver{\'o}nica Montan{\'e} and Bedny, Marina},
  year = {2021},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {33},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2020192118},
  abstract = {Empiricist philosophers such as Locke famously argued that people born blind might learn arbitrary color facts (e.g., marigolds are yellow) but would lack color understanding. Contrary to this intuition, we find that blind and sighted adults share causal understanding of color, despite not always agreeing about arbitrary color facts. Relative to sighted people, blind individuals are less likely to generate ``yellow'' for banana and ``red'' for stop sign but make similar generative inferences about real and novel objects' colors, and provide similar causal explanations. For example, people infer that two natural kinds (e.g., bananas) and two artifacts with functional colors (e.g., stop signs) are more likely to have the same color than two artifacts with nonfunctional colors (e.g., cars). People develop intuitive and inferentially rich ``theories'' of color regardless of visual experience. Linguistic communication is more effective at aligning intuitive theories than knowledge of arbitrary facts.},
  chapter = {Social Sciences},
  copyright = {\textcopyright{} 2021 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {34385310},
  keywords = {blindness,color,intuitive theories,language}
}

@article{king_backtoback_2020,
  title = {Back-to-Back Regression: {{Disentangling}} the Influence of Correlated Factors from Multivariate Observations},
  shorttitle = {Back-to-Back Regression},
  author = {King, Jean-R{\'e}mi and Charton, Fran{\c c}ois and {Lopez-Paz}, David and Oquab, Maxime},
  year = {2020},
  month = oct,
  journal = {NeuroImage},
  volume = {220},
  pages = {117028},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.117028},
  abstract = {Identifying causes solely from observations can be particularly challenging when i) the factors under investigation are difficult to manipulate independently from one-another and ii) observations are high-dimensional. To address this issue, we introduce ``Back-to-Back'' regression (B2B), a linear method designed to efficiently estimate, from a set of correlated factors, those that most plausibly account for multidimensional observations. First, we prove the consistency of B2B, its links to other linear approaches, and show how it can provide a robust, unbiased and interpretable scalar estimate for each factor. Second, we use a variety of simulated data to show that B2B can outperform forward modeling (``encoding''), backward modeling (``decoding'') as well as cross-decomposition modeling (i.e. canonical correlation analysis and partial least squares) on causal identification when the factors and the observations are not orthogonal. Finally, we apply B2B to a hundred magneto-encephalography recordings and to a hundred functional Magnetic Resonance Imaging recordings acquired while subjects performed a 1~\hspace{0pt}h reading task. B2B successfully disentangles the respective contribution of collinear factors such as word length, word frequency in the early visual and late associative cortical responses respectively. B2B compared favorably to other standard techniques on this disentanglement. We discuss how the speed and the generality of B2B sets promising foundations to help identify the causal contributions of covarying factors from high-dimensional observations.},
  langid = {english},
  keywords = {Cross-decomposition,Decoding,Encoding,Feature discovery,fMRI,MEG,Reading},
  file = {/Users/xzfang/Zotero/storage/YMTW2XD7/King et al. - 2020 - Back-to-back regression Disentangling the influen.pdf;/Users/xzfang/Zotero/storage/5HG7KDKY/S1053811920305140.html}
}

@article{king_characterizing_2014,
  title = {Characterizing the Dynamics of Mental Representations: The Temporal Generalization Method},
  shorttitle = {Characterizing the Dynamics of Mental Representations},
  author = {King, J-R. and Dehaene, S.},
  year = {2014},
  month = apr,
  journal = {Trends in cognitive sciences},
  volume = {18},
  number = {4},
  pages = {203--210},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2014.01.002},
  abstract = {Parsing a cognitive task into a sequence of operations is a central problem in cognitive neuroscience. We argue that a major advance is now possible owing to the application of pattern classifiers to time-resolved recordings of brain activity [electroencephalography (EEG), magnetoencephalography (MEG), or intracranial recordings]. By testing at which moment a specific mental content becomes decodable in brain activity, we can characterize the time course of cognitive codes. Most importantly, the manner in which the trained classifiers generalize across time, and from one experimental condition to another, sheds light on the temporal organization of information-processing stages. A repertoire of canonical dynamical patterns is observed across various experiments and brain regions. This method thus provides a novel way to understand how mental representations are manipulated and transformed.},
  pmcid = {PMC5635958},
  pmid = {24593982},
  file = {/Users/xzfang/Zotero/storage/QQ85V5C3/King and Dehaene - 2014 - Characterizing the dynamics of mental representati.pdf}
}

@article{king_human_2021,
  title = {The {{Human Brain Encodes}} a {{Chronicle}} of {{Visual Events}} at {{Each Instant}} of {{Time Through}} the {{Multiplexing}} of {{Traveling Waves}}},
  author = {King, Jean-R{\'e}mi and Wyart, Valentin},
  year = {2021},
  month = aug,
  journal = {The Journal of Neuroscience},
  volume = {41},
  number = {34},
  pages = {7224--7233},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2098-20.2021},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/S6P2N7KD/King and Wyart - 2021 - The Human Brain Encodes a Chronicle of Visual Even.pdf}
}

@article{king_listening_2020,
  title = {Listening in Complex Acoustic Scenes},
  author = {King, Andrew J and Walker, Kerry MM},
  year = {2020},
  month = dec,
  journal = {Current Opinion in Physiology},
  volume = {18},
  pages = {63--72},
  issn = {2468-8673},
  doi = {10.1016/j.cophys.2020.09.001},
  abstract = {Being able to pick out particular sounds, such as speech, against a background of other sounds represents one of the key tasks performed by the auditory system. Understanding how this happens is important because speech recognition in noise is particularly challenging for older listeners and for people with hearing impairments. Central to this ability is the capacity of neurons to adapt to the statistics of sounds reaching the ears, which helps to generate noise-tolerant representations of sounds in the brain. In more complex auditory scenes, such as a cocktail party \textemdash{} where the background noise comprises other voices, sound features associated with each source have to be grouped together and segregated from those belonging to other sources. This depends on precise temporal coding and modulation of cortical response properties when attending to a particular speaker in a multi-talker environment. Furthermore, the neural processing underlying auditory scene analysis is shaped by experience over multiple timescales.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6MCNZZRF/King and Walker - 2020 - Listening in complex acoustic scenes.pdf;/Users/xzfang/Zotero/storage/6GA5EFWB/S2468867320301061.html}
}

@article{klein_looking_2015,
  title = {Looking and Listening: {{A}} Comparison of Intertrial Repetition Effects in Visual and Auditory Search Tasks},
  shorttitle = {Looking and Listening},
  author = {Klein, Michael D. and Stolz, Jennifer A.},
  year = {2015},
  month = aug,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {77},
  number = {6},
  pages = {1986--1997},
  issn = {1943-393X},
  doi = {10.3758/s13414-015-0908-3},
  abstract = {Previous research shows that performance on pop-out search tasks is facilitated when the target and distractors repeat across trials compared to when they switch. This phenomenon has been shown for many different types of visual stimuli. We tested whether the effect would extend beyond visual stimuli to the auditory modality. Using a temporal search task that has previously been shown to elicit priming of pop-out with visual stimuli (Yashar \& Lamy, Psychological Science, 21(2), 243\textendash 251, 2010), we showed that priming of pop-out does occur with auditory stimuli and has characteristics similar to those of an analogous visual task. These results suggest that either the same or similar mechanisms might underlie priming of pop-out in both modalities.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9QG7KS4M/Klein and Stolz - 2015 - Looking and listening A comparison of intertrial .pdf}
}

@article{kleinschmidt_bayesian_,
  title = {A {{Bayesian}} Model of Memory in a Multi-Context Environment},
  author = {Kleinschmidt, Dave F and Hemmer, Pernille},
  pages = {7},
  abstract = {In a noisy but structured world, memory can be improved by enhancing limited stimulus-specific memory with statistical information about the context. To do this, people have to learn the statistical structure of their current environment. We present a Sequential Monte Carlo (particle filter) model of how people track the statistical properties of the environment across multiple contexts. This model approximates non-parametric Bayesian clustering of percepts over time, capturing how people impute structure in their perceptual experience in order to more efficiently encode that experience in memory. Each trial is treated as a draw from a context-specific distribution, where the number of contexts is unknown (and potentially infinite). The model maintains a finite set of hypotheses about how the percepts encountered thus far are assigned to contexts, updating these in parallel as each new percept comes in. We apply this model to a recall task where subjects had to recall the position of dots (Robbins, Hemmer, \& Tang, 2014). Unbeknownst to subjects, each dot appeared in one of a few pre-defined regions on the screen. Our model captures subjects' ability to learn the inventory of contexts, the statistics of dot positions within each context, and the statistics of transitions between contexts\textemdash as reflected in both recall and prediction.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TAEEPAJ7/Kleinschmidt and Hemmer - A Bayesian model of memory in a multi-context envi.pdf}
}

@article{kleinschmidt_beliefupdating_,
  title = {A Belief-Updating Model of Adaptation and Cue Combination in Syntactic Comprehension},
  author = {Kleinschmidt, Dave F and Fine, Alex B and Jaeger, T Florian},
  journal = {2012},
  pages = {7},
  abstract = {We develop and evaluate a preliminary belief-updating model which links intermediate-term (i.e., over several days) syntactic adaptation to the joint statistics of syntactic structures and lexical cues to those structures. This model shows how subjects differentially depend on different cues to syntactic structure following changes in the reliability of those cues, as shown by Fine and Jaeger (2011). By relating syntactic adaptation and cue combination to rational inference under uncertainty, this work links learning and adaptation in sentence processing with adaptation in speech perception and non-linguistic domains.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/S2GVJTVX/Kleinschmidt et al. - A belief-updating model of adaptation and cue comb.pdf}
}

@techreport{kleinschmidt_learning_2018,
  type = {Preprint},
  title = {Learning Distributions as They Come: {{Particle}} Filter Models for Online Distributional Learning of Phonetic Categories},
  shorttitle = {Learning Distributions as They Come},
  author = {Kleinschmidt, Dave F},
  year = {2018},
  month = feb,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/dymc8},
  abstract = {Human infants have the remarkable ability to learn any human language. One proposed mechanism for this ability is distributional learning, where learners infer the underlying cluster structure from unlabeled input. Computational models of distributional learning have historically been principled but psychologically-implausible computational-level models, or ad hoc but psychologically plausible algorithmic-level models. Approximate rational models like particle filters can potentially bridge this divide, and allow principled, but psychologically plausible models of distributional learning to be specified and evaluated. As a proof of concept, I evaluate one such particle filter model, applied to learning English voicing categories from distributions of voice-onset times (VOTs). I find that this model learns well, but behaves somewhat differently from the standard, unconstrained Gibbs sampler implementation of the underlying rational model.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KC4MGRR8/Kleinschmidt - 2018 - Learning distributions as they come Particle filt.pdf}
}

@article{kleinschmidt_reexamining_2016,
  title = {Re-Examining Selective Adaptation: {{Fatiguing}} Feature Detectors, or Distributional Learning?},
  shorttitle = {Re-Examining Selective Adaptation},
  author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
  year = {2016},
  month = jun,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {3},
  pages = {678--691},
  issn = {1531-5320},
  doi = {10.3758/s13423-015-0943-z},
  abstract = {When a listener hears many good examples of a /b/ in a row, they are less likely to classify other sounds on, e.g., a /b/-to-/d/ continuum as /b/. This phenomenon is known as selective adaptation and is a well-studied property of speech perception. Traditionally, selective adaptation is seen as a mechanistic property of the speech perception system, and attributed to fatigue in acoustic-phonetic feature detectors. However, recent developments in our understanding of non-linguistic sensory adaptation and higher-level adaptive plasticity in speech perception and language comprehension suggest that it is time to re-visit the phenomenon of selective adaptation. We argue that selective adaptation is better thought of as a computational property of the speech perception system. Drawing on a common thread in recent work on both non-linguistic sensory adaptation and plasticity in language comprehension, we furthermore propose that selective adaptation can be seen as a consequence of distributional learning across multiple levels of representation. This proposal opens up new questions for research on selective adaptation itself, and also suggests that selective adaptation can be an important bridge between work on adaptation in low-level sensory systems and the complicated plasticity of the adult language comprehension system.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FP4A8YHF/Kleinschmidt and Jaeger - 2016 - Re-examining selective adaptation Fatiguing featu.pdf}
}

@article{kleinschmidt_robust_2015,
  title = {Robust Speech Perception: {{Recognize}} the Familiar, Generalize to the Similar, and Adapt to the Novel},
  shorttitle = {Robust Speech Perception},
  author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
  year = {2015},
  month = apr,
  journal = {Psychological review},
  volume = {122},
  number = {2},
  pages = {148--203},
  issn = {0033-295X},
  doi = {10.1037/a0038695},
  abstract = {Successful speech perception requires that listeners map the acoustic signal to linguistic categories. These mappings are not only probabilistic, but change depending on the situation. For example, one talker's /p/ might be physically indistinguishable from another talker's /b/ (cf. lack of invariance). We characterize the computational problem posed by such a subjectively non-stationary world and propose that the speech perception system overcomes this challenge by (1) recognizing previously encountered situations, (2) generalizing to other situations based on previous similar experience, and (3) adapting to novel situations. We formalize this proposal in the ideal adapter framework: (1) to (3) can be understood as inference under uncertainty about the appropriate generative model for the current talker, thereby facilitating robust speech perception despite the lack of invariance. We focus on two critical aspects of the ideal adapter. First, in situations that clearly deviate from previous experience, listeners need to adapt. We develop a distributional (belief-updating) learning model of incremental adaptation. The model provides a good fit against known and novel phonetic adaptation data, including perceptual recalibration and selective adaptation. Second, robust speech recognition requires listeners learn to represent the structured component of cross-situation variability in the speech signal. We discuss how these two aspects of the ideal adapter provide a unifying explanation for adaptation, talker-specificity, and generalization across talkers and groups of talkers (e.g., accents and dialects). The ideal adapter provides a guiding framework for future investigations into speech perception and adaptation, and more broadly language comprehension.},
  pmcid = {PMC4744792},
  pmid = {25844873},
  file = {/Users/xzfang/Zotero/storage/QKFJES3U/Kleinschmidt and Jaeger - 2015 - Robust speech perception Recognize the familiar, .pdf}
}

@article{kleinschmidt_sociolinguistic_2018,
  title = {Sociolinguistic {{Perception}} as {{Inference Under Uncertainty}}},
  author = {Kleinschmidt, Dave F. and Weatherholtz, Kodi and Jaeger, T. Florian},
  year = {2018},
  journal = {Topics in Cognitive Science},
  volume = {10},
  number = {4},
  pages = {818--834},
  issn = {1756-8765},
  doi = {10.1111/tops.12331},
  abstract = {Social and linguistic perceptions are linked. On one hand, talker identity affects speech perception. On the other hand, speech itself provides information about a talker's identity. Here, we propose that the same probabilistic knowledge might underlie both socially conditioned linguistic inferences and linguistically conditioned social inferences. Our computational\textendash level approach\textemdash the ideal adapter\textemdash starts from the idea that listeners use probabilistic knowledge of covariation between social, linguistic, and acoustic cues in order to infer the most likely explanation of the speech signals they hear. As a first step toward understanding social inferences in this framework, we use a simple ideal observer model to show that it would be possible to infer aspects of a talker's identity using cue distributions based on actual speech production data. This suggests the possibility of a single formal framework for social and linguistic inferences and the interactions between them.},
  copyright = {\textcopyright{} 2018 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Computational modeling,Social perception,Speech perception},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12331},
  file = {/Users/xzfang/Zotero/storage/36WN3PHC/Kleinschmidt et al. - 2018 - Sociolinguistic Perception as Inference Under Unce.pdf;/Users/xzfang/Zotero/storage/JBFGHWDQ/tops.html}
}

@article{kleinschmidt_structure_2019,
  title = {Structure in Talker Variability: {{How}} Much Is There and How Much Can It Help?},
  shorttitle = {Structure in Talker Variability},
  author = {Kleinschmidt, Dave F.},
  year = {2019},
  month = jan,
  journal = {Language, Cognition and Neuroscience},
  volume = {34},
  number = {1},
  pages = {43--68},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1500698},
  abstract = {One of the persistent puzzles in understanding human speech perception is how listeners cope with talker variability. One thing that might help listeners is structure in talker variability: rather than varying randomly, talkers of the same gender, dialect, age, etc. tend to produce language in similar ways. Listeners are sensitive to this covariation between linguistic variation and socio-indexical variables. In this paper I present new techniques based on ideal observer models to quantify (1) the amount and type of structure in talker variation (informativity of a grouping variable), and (2) how useful such structure can be for robust speech recognition in the face of talker variability (the utility of a grouping variable). I demonstrate these techniques in two phonetic domains\textemdash word-initial stop voicing and vowel identity\textemdash and show that these domains have different amounts and types of talker variability, consistent with previous, impressionistic findings. An R package (phondisttools) accompanies this paper, and the source and data are available from osf.io/zv6e3.},
  keywords = {computational modelling,Erratum,Speech perception,variability},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2018.1500698},
  file = {/Users/xzfang/Zotero/storage/UN7EKZMQ/Kleinschmidt - 2019 - Structure in talker variability How much is there.pdf;/Users/xzfang/Zotero/storage/8CSPUQNU/23273798.2018.html;/Users/xzfang/Zotero/storage/9L8C7NIS/23273798.2018.html}
}

@article{kleinschmidt_supervised_,
  title = {Supervised and Unsupervised Learning in Phonetic Adaptation},
  author = {Kleinschmidt, Dave F and Raizada, Rajeev and Jaeger, T Florian},
  pages = {6},
  abstract = {Speech perception requires ongoing perceptual category learning. Each talker speaks differently, and listeners need to learn each talker's particular acoustic cue distributions in order to comprehend speech robustly from multiple talkers. This phonetic adaptation is a semi-supervised learning problem, because sometimes a particular cue value occurs with information that labels the talker's intended category for the listener, but other times no such labels are available. Previous work has shown that adaptation can occur in both purely supervised (all labeled) and purely unsupervised (all unlabeled) settings, but the interaction between them has not been investigated. We compare unsupervised with (semi-) supervised phonetic adaptation and find, surprisingly, that adult listeners do not take advantage of labeling information to adapt more quickly or effectively, even though the labels affect their categorization. This suggests that, like language acquisition, phonetic adaptation in adults is dominated by unsupervised, distributional learning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NSCAPLYY/Kleinschmidt et al. - Supervised and unsupervised learning in phonetic a.pdf}
}

@article{kleinschmidt_what_2016,
  title = {What Do You Expect from an Unfamiliar Talker?},
  author = {Kleinschmidt, Dave F and Jaeger, T Florian},
  year = {2016},
  pages = {6},
  abstract = {Speech perception is made much harder by variability between talkers. As a result, listeners need to adapt to each different talker's particular acoustic cue distributions. Thinking of this adaptation as a form of statistical inference, we explore the role that listeners' prior expectations play in adapting to an unfamiliar talker. Specifically, we test the hypothesis that listeners will have a harder time adapting to talkers whose cue distributions fall outside the range of normal variation across talkers. We also show that it is possible to infer listeners' shared prior expectations based on patterns of adaptation to different cue distributions. This provides a potentially powerful tool for directly probing listeners' prior expectations about talkers that does not rely on speech produced by many different talkers, which is costly to collect and annotate, and only indirectly related to listeners' subjective expectations.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2EPHXS4J/Kleinschmidt and Jaeger - What do you expect from an unfamiliar talker.pdf}
}

@misc{kleinschmidt_what_2020,
  title = {What Constrains Distributional Learning in Adults?},
  author = {Kleinschmidt, Dave},
  year = {2020},
  month = jun,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/6yhbe},
  abstract = {One of the many remarkable features of human language is it's flexibility: during acquisition, any normally-developing human infant can acquire any human language, and during adulthood, language users quickly and flexibly adapt to a wide range of talker variation.  Both language acquisition in infants and adaptation in adults have been hypothesized to be forms of distributional learning, where flexibility is driven by sensitivity to statistical properties of sensory stimuli and the corresponding underlying linguistic structures.  Despite the similarities between these forms of linguistic flexibility, there are obvious differences as well, chief among them being that adults have a much harder time acquiring the same unfamiliar languages that they would have picked up naturally during infancy.  This suggests that there are strong constraints on distributional learning during adulthood.  This paper provides further, direct evidence for these constraints, by showing that American English listeners struggle to learn voice-onset time (VOT) distributions that are atypical of American English.  Moreover, computational modeling shows that the pattern of distributional learning (or lack thereof) across different VOT distributions is consistent with Bayesian belief-updating, starting from prior beliefs that are very similar to the VOT distributions produced by a typical talker of American English.  Together, this suggests that distributional learning in adults is constrained by prior experience with other talkers, and that distributional learning may be a computational principle of human language that operates throughout the lifespan.},
  keywords = {Cognitive Psychology,Language,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/B6RZW4FW/Kleinschmidt - 2020 - What constrains distributional learning in adults.pdf}
}

@article{klimovich-gray_one_2021,
  title = {One {{Way}} or {{Another}}: {{Cortical Language Areas Flexibly Adapt Processing Strategies}} to {{Perceptual And Contextual Properties}} of {{Speech}}},
  shorttitle = {One {{Way}} or {{Another}}},
  author = {{Klimovich-Gray}, Anastasia and Barrena, Ander and Agirre, Eneko and Molinaro, Nicola},
  year = {2021},
  month = sep,
  journal = {Cerebral Cortex},
  volume = {31},
  number = {9},
  pages = {4092--4103},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab071},
  abstract = {Cortical circuits rely on the temporal regularities of speech to optimize signal parsing for sound-to-meaning mapping. Bottom-up speech analysis is accelerated by top\textendash down predictions about upcoming words. In everyday communications, however, listeners are regularly presented with challenging input\textemdash fluctuations of speech rate or semantic content. In this study, we asked how reducing speech temporal regularity affects its processing\textemdash parsing, phonological analysis, and ability to generate context-based predictions. To ensure that spoken sentences were natural and approximated semantic constraints of spontaneous speech we built a neural network to select stimuli from large corpora. We analyzed brain activity recorded with magnetoencephalography during sentence listening using evoked responses, speech-to-brain synchronization and representational similarity analysis. For normal speech theta band (6.5\textendash 8~Hz) speech-to-brain synchronization was increased and the left fronto-temporal areas generated stronger contextual predictions. The reverse was true for temporally irregular speech\textemdash weaker theta synchronization and reduced top\textendash down effects. Interestingly, delta-band (0.5 Hz) speech tracking was greater when contextual/semantic predictions were lower or if speech was temporally jittered. We conclude that speech temporal regularity is relevant for (theta) syllabic tracking and robust semantic predictions while the joint support of temporal and contextual predictability reduces word and phrase-level cortical tracking (delta).},
  file = {/Users/xzfang/Zotero/storage/5MCM8BKA/Klimovich-Gray et al. - 2021 - One Way or Another Cortical Language Areas Flexib.pdf;/Users/xzfang/Zotero/storage/YWQYIHCQ/6213404.html}
}

@article{knill_bayesian_2004,
  title = {The {{Bayesian}} Brain: The Role of Uncertainty in Neural Coding and Computation},
  shorttitle = {The {{Bayesian}} Brain},
  author = {Knill, David C. and Pouget, Alexandre},
  year = {2004},
  month = dec,
  journal = {Trends in Neurosciences},
  volume = {27},
  number = {12},
  pages = {712--719},
  issn = {01662236},
  doi = {10.1016/j.tins.2004.10.007},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IEMIH3JB/Knill and Pouget - 2004 - The Bayesian brain the role of uncertainty in neu.pdf}
}

@article{kocagoncu_decoding_2017,
  title = {Decoding the {{Cortical Dynamics}} of {{Sound-Meaning Mapping}}},
  author = {Kocagoncu, Ece and Clarke, Alex and Devereux, Barry J. and Tyler, Lorraine K.},
  year = {2017},
  month = feb,
  journal = {The Journal of Neuroscience},
  volume = {37},
  number = {5},
  pages = {1312--1319},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.2858-16.2016},
  abstract = {Comprehending speech involves the rapid and optimally efficient mapping from sound to meaning. Influential cognitive models of spoken word recognition () propose that the onset of a spoken word initiates a continuous process of activation of the lexical and semantic properties of the word candidates matching the speech input and competition between them, which continues until the point at which the word is differentiated from all other cohort candidates (the uniqueness point, UP). At this point, the word is recognized uniquely and only the target word's semantics are active. Although it is well established that spoken word recognition engages the superior (), middle, and inferior () temporal cortices, little is known about the real-time brain activity that underpins the computations and representations that evolve over time during the transformation from speech to meaning. Here, we test for the first time the spatiotemporal dynamics of these processes by collecting MEG data while human participants listened to spoken words. By constructing quantitative models of competition and access to meaning in combination with spatiotemporal searchlight representational similarity analysis () in source space, we were able to test where and when these models produced significant effects. We found early transient effects {$\sim$}400 ms before the UP of lexical competition in left supramarginal gyrus, left superior temporal gyrus, left middle temporal gyrus (MTG), and left inferior frontal gyrus (IFG) and of semantic competition in MTG, left angular gyrus, and IFG. After the UP, there were no competitive effects, only target-specific semantic effects in angular gyrus and MTG., SIGNIFICANCE STATEMENT Understanding spoken words involves complex processes that transform the auditory input into a meaningful interpretation. This effortless transition occurs on millisecond timescales, with remarkable speed and accuracy and without any awareness of the complex computations involved. Here, we reveal the real-time neural dynamics of these processes by collecting data about listeners' brain activity as they hear spoken words. Using novel statistical models of different aspects of the recognition process, we can locate directly which parts of the brain are accessing the stored form and meaning of words and how the competition between different word candidates is resolved neurally in real time. This gives us a uniquely differentiated picture of the neural substrate for the first 500 ms of word recognition.},
  pmcid = {PMC6596862},
  pmid = {28028201},
  file = {/Users/xzfang/Zotero/storage/LJVJALMI/Kocagoncu et al. - 2017 - Decoding the Cortical Dynamics of Sound-Meaning Ma.pdf}
}

@article{koch_switching_2011,
  title = {Switching in the Cocktail Party: {{Exploring}} Intentional Control of Auditory Selective Attention.},
  shorttitle = {Switching in the Cocktail Party},
  author = {Koch, Iring and Lawo, Vera and Fels, Janina and Vorl{\"a}nder, Michael},
  year = {2011},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {37},
  number = {4},
  pages = {1140--1147},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/a0022189},
  abstract = {Using a novel variant of dichotic selective listening, we examined the control of auditory selective attention. In our task, subjects had to respond selectively to one of two simultaneously presented auditory stimuli (number words), always spoken by a female and a male speaker, by performing a numerical size categorization. The gender of the task-relevant speaker could change, as indicated by a visual cue prior to auditory stimulus onset. Three experiments show clear performance costs with instructed attention switches. Experiment 2 varied the cuing interval to examine advance preparation for an attention switch. Experiment 3 additionally isolated auditory switch costs from visual cue priming by using two cues for each gender, so that gender repetition could be indicated by a changed cue. Experiment 2 showed that switch costs decreased with prolonged cuing intervals, but Experiment 3 revealed that preparation did not affect auditory switch costs but only visual cue priming. Moreover, incongruent numerical categories in competing auditory stimuli produced interference and substantially increased error rates, suggesting continued processing of task-relevant information that often leads to responding to the incorrect auditory source. Together, the data show clear limitations in advance preparation of auditory attention switches and suggest a considerable degree of inertia in intentional control of auditory selection criteria.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ERWX24GH/Koch et al. - 2011 - Switching in the cocktail party Exploring intenti.pdf}
}

@article{koelsch_music_2004,
  title = {Music, Language and Meaning: Brain Signatures of Semantic Processing},
  shorttitle = {Music, Language and Meaning},
  author = {Koelsch, Stefan and Kasper, Elisabeth and Sammler, Daniela and Schulze, Katrin and Gunter, Thomas and Friederici, Angela D.},
  year = {2004},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {7},
  number = {3},
  pages = {302--307},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1197},
  abstract = {Semantics is a key feature of language, but whether or not music can activate brain mechanisms related to the processing of semantic meaning is not known. We compared processing of semantic meaning in language and music, investigating the semantic priming effect as indexed by behavioral measures and by the N400 component of the event-related brain potential (ERP) measured by electroencephalography (EEG). Human subjects were presented visually with target words after hearing either a spoken sentence or a musical excerpt. Target words that were semantically unrelated to prime sentences elicited a larger N400 than did target words that were preceded by semantically related sentences. In addition, target words that were preceded by semantically unrelated musical primes showed a similar N400 effect, as compared to target words preceded by related musical primes. The N400 priming effect did not differ between language and music with respect to time course, strength or neural generators. Our results indicate that both music and language can prime the meaning of a word, and that music can, as language, determine physiological indices of semantic processing.},
  copyright = {2004 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/Users/xzfang/Zotero/storage/38HSRCZX/Koelsch et al. - 2004 - Music, language and meaning brain signatures of s.pdf;/Users/xzfang/Zotero/storage/PZ3UNHEY/nn1197.html}
}

@article{koelsch_predictive_2019,
  title = {Predictive {{Processes}} and the {{Peculiar Case}} of {{Music}}},
  author = {Koelsch, Stefan and Vuust, Peter and Friston, Karl},
  year = {2019},
  month = jan,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {1},
  pages = {63--77},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2018.10.006},
  abstract = {We suggest that music perception is an active act of listening, providing an irresistible epistemic offering. When listening to music we constantly generate plausible hypotheses about what could happen next, while actively attending to music resolves the ensuing uncertainty. Within the predictive coding framework, we present a novel formulation of precision filtering and attentional selection, which explains why some lower-level auditory, and even higher-level music-syntactic processes elicited by irregular events are relatively exempt from top-down predictive processes. We review findings providing unique evidence for the attentional selection of salient auditory features. This formulation suggests that `listening' is a more active process than traditionally conceived in models of perception.},
  langid = {english},
  keywords = {active inference,auditory processing,embodiment,ERAN,MMN,music perception,predictive coding},
  file = {/Users/xzfang/Zotero/storage/TVZZBCTJ/Koelsch et al. - 2019 - Predictive Processes and the Peculiar Case of Musi.pdf;/Users/xzfang/Zotero/storage/HLIH8YYU/S1364661318302547.html}
}

@article{kok_prior_2014,
  title = {Prior {{Expectations Evoke Stimulus Templates}} in the {{Primary Visual Cortex}}},
  author = {Kok, Peter and Failing, Michel F. and {de Lange}, Floris P.},
  year = {2014},
  month = jul,
  journal = {Journal of Cognitive Neuroscience},
  volume = {26},
  number = {7},
  pages = {1546--1554},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_00562},
  abstract = {Sensory processing is strongly influenced by prior expectations. Valid expectations have been shown to lead to improvements in perception as well as in the quality of sensory representations in primary visual cortex. However, very little is known about the neural correlates of the expectations themselves. Previous studies have demonstrated increased activity in sensory cortex following the omission of an expected stimulus, yet it is unclear whether this increased activity constitutes a general surprise signal or rather has representational content. One intriguing possibility is that top\textendash down expectation leads to the formation of a template of the expected stimulus in visual cortex, which can then be compared with subsequent bottom\textendash up input. To test this hypothesis, we used fMRI to noninvasively measure neural activity patterns in early visual cortex of human participants during expected but omitted visual stimuli. Our results show that prior expectation of a specific visual stimulus evokes a feature-specific pattern of activity in the primary visual cortex (V1) similar to that evoked by the corresponding actual stimulus. These results are in line with the notion that prior expectation triggers the formation of specific stimulus templates to efficiently process expected sensory inputs.},
  file = {/Users/xzfang/Zotero/storage/LCN84FM6/Kok et al. - 2014 - Prior Expectations Evoke Stimulus Templates in the.pdf;/Users/xzfang/Zotero/storage/6NMN57JM/Prior-Expectations-Evoke-Stimulus-Templates-in-the.html}
}

@article{kok_prior_2017,
  title = {Prior Expectations Induce Prestimulus Sensory Templates},
  author = {Kok, Peter and Mostert, Pim and de Lange, Floris P.},
  year = {2017},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {39},
  pages = {10473--10478},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1705652114},
  abstract = {Perception can be described as a process of inference, integrating bottom-up sensory inputs and top-down expectations. However, it is unclear how this process is neurally implemented. It has been proposed that expectations lead to prestimulus baseline increases in sensory neurons tuned to the expected stimulus, which in turn, affect the processing of subsequent stimuli. Recent fMRI studies have revealed stimulus-specific patterns of activation in sensory cortex as a result of expectation, but this method lacks the temporal resolution necessary to distinguish pre- from poststimulus processes. Here, we combined human magnetoencephalography (MEG) with multivariate decoding techniques to probe the representational content of neural signals in a time-resolved manner. We observed a representation of expected stimuli in the neural signal shortly before they were presented, showing that expectations indeed induce a preactivation of stimulus templates. The strength of these prestimulus expectation templates correlated with participants' behavioral improvement when the expected feature was task-relevant. These results suggest a mechanism for how predictive perception can be neurally implemented.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{}  . http://www.pnas.org/site/misc/userlicense.xhtml},
  langid = {english},
  pmid = {28900010},
  keywords = {feature-based attention,feature-based expectation,perceptual inference,prediction,predictive coding},
  file = {/Users/xzfang/Zotero/storage/CEJ3JE4V/Kok et al. - 2017 - Prior expectations induce prestimulus sensory temp.pdf;/Users/xzfang/Zotero/storage/27QG85ZG/10473.html}
}

@misc{kondapaneni_number_2021,
  title = {A {{Number Sense}} as an {{Emergent Property}} of the {{Manipulating Brain}}},
  author = {Kondapaneni, Neehar and Perona, Pietro},
  year = {2021},
  month = feb,
  number = {arXiv:2012.04132},
  eprint = {2012.04132},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  institution = {{arXiv}},
  abstract = {The ability to understand and manipulate numbers and quantities emerges during childhood, but the mechanism through which this ability is developed is still poorly understood. In particular, it is not known whether acquiring such a \{\textbackslash em number sense\} is possible without supervision from a teacher. To explore this question, we propose a model in which spontaneous and undirected manipulation of small objects trains perception to predict the resulting scene changes. We find that, from this task, an image representation emerges that exhibits regularities that foreshadow numbers and quantity. These include distinct categories for zero and the first few natural numbers, a notion of order, and a signal that correlates with numerical quantity. As a result, our model acquires the ability to estimate the number of objects in the scene, as well as \{\textbackslash em subitization\}, i.e. the ability to recognize at a glance the exact number of objects in small scenes. We conclude that important aspects of a facility with numbers and quantities may be learned without explicit teacher supervision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,Quantitative Biology - Neurons and Cognition},
  file = {/Users/xzfang/Zotero/storage/TJZLD8J8/Kondapaneni and Perona - 2021 - A Number Sense as an Emergent Property of the Mani.pdf;/Users/xzfang/Zotero/storage/KKZG7YXK/2012.html}
}

@article{konkle_scene_2010,
  title = {Scene {{Memory Is More Detailed Than You Think}}: {{The Role}} of {{Categories}} in {{Visual Long-Term Memory}}},
  shorttitle = {Scene {{Memory Is More Detailed Than You Think}}},
  author = {Konkle, Talia and Brady, Timothy F. and Alvarez, George A. and Oliva, Aude},
  year = {2010},
  month = nov,
  journal = {Psychological Science},
  volume = {21},
  number = {11},
  pages = {1551--1556},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797610385359},
  abstract = {Observers can store thousands of object images in visual long-term memory with high fidelity, but the fidelity of scene representations in long-term memory is not known. Here, we probed scene-representation fidelity by varying the number of studied exemplars in different scene categories and testing memory using exemplar-level foils. Observers viewed thousands of scenes over 5.5 hr and then completed a series of forced-choice tests. Memory performance was high, even with up to 64 scenes from the same category in memory. Moreover, there was only a 2\% decrease in accuracy for each doubling of the number of studied scene exemplars. Surprisingly, this degree of categorical interference was similar to the degree previously demonstrated for object memory. Thus, although scenes have often been defined as a superset of objects, our results suggest that scenes and objects may be entities at a similar level of abstraction in visual long-term memory.},
  langid = {english},
  keywords = {memory capacity,object categories,scene categories,visual memory},
  file = {/Users/xzfang/Zotero/storage/46DQEFY2/Konkle et al. - 2010 - Scene Memory Is More Detailed Than You Think The .pdf}
}

@article{kosakowski_selective_2022,
  title = {Selective Responses to Faces, Scenes, and Bodies in the Ventral Visual Pathway of Infants},
  author = {Kosakowski, Heather L. and Cohen, Michael A. and Takahashi, Atsushi and Keil, Boris and Kanwisher, Nancy and Saxe, Rebecca},
  year = {2022},
  month = jan,
  journal = {Current Biology},
  volume = {32},
  number = {2},
  pages = {265-274.e5},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2021.10.064},
  abstract = {Three of the most robust functional landmarks in the human brain are the selective responses to faces in the fusiform face area (FFA), scenes in the parahippocampal place area (PPA), and bodies in the extrastriate body area (EBA). Are the selective responses of these regions present early in development or do they require many years to develop? Prior evidence leaves this question unresolved. We designed a new 32-channel infant magnetic resonance imaging (MRI) coil and collected high-quality functional MRI (fMRI) data from infants (2\textendash 9~months of age) while they viewed stimuli from four conditions\textemdash faces, bodies, objects, and scenes. We find that infants have face-, scene-, and body-selective responses in the location of the adult FFA, PPA, and EBA, respectively, powerfully constraining accounts of cortical development.},
  langid = {english},
  keywords = {bodies,category selectivity,development,EBA,faces,FFA,high-level vision,infancy,infant cortex,PPA,scenes},
  file = {/Users/xzfang/Zotero/storage/E8HBRQKR/Kosakowski et al. - 2022 - Selective responses to faces, scenes, and bodies i.pdf;/Users/xzfang/Zotero/storage/MT2EHSEJ/S0960982221015086.html}
}

@article{kourtzi_activation_2000,
  title = {Activation in {{Human MT}}/{{MST}} by {{Static Images}} with {{Implied Motion}}},
  author = {Kourtzi, Zoe and Kanwisher, Nancy},
  year = {2000},
  month = jan,
  journal = {Journal of Cognitive Neuroscience},
  volume = {12},
  number = {1},
  pages = {48--55},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/08989290051137594},
  abstract = {A still photograph of an object in motion may convey dynamic information about the position of the object immediately before and after the photograph was taken (implied motion). Medial temporal/medial superior temporal cortex (MT/MST) is one of the main brain regions engaged in the perceptual analysis of visual motion. In two experiments we examined whether MT/MST is also involved in representing implied motion from static images. We found stronger functional magnetic resonance imaging (fMRI) activation within MT/MST during viewing of static photographs with implied motion compared to viewing of photographs without implied motion. These results suggest that brain regions involved in the visual analysis of motion are also engaged in processing implied dynamic information from static images.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/394P6HYP/Kourtzi and Kanwisher - 2000 - Activation in Human MTMST by Static Images with I.pdf}
}

@article{kragel_decoding_2016,
  title = {Decoding the {{Nature}} of {{Emotion}} in the {{Brain}}},
  author = {Kragel, Philip A. and LaBar, Kevin S.},
  year = {2016},
  month = jun,
  journal = {Trends in cognitive sciences},
  volume = {20},
  number = {6},
  pages = {444--455},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2016.03.011},
  abstract = {A central, unresolved problem in affective neuroscience is understanding how emotions are represented in nervous system activity. After prior localization approaches largely failed, researchers began applying multivariate statistical tools to reconceptualize how emotion constructs might be embedded in large-scale brain networks. Findings from pattern analyses of neuroimaging data show that affective dimensions and emotion categories are uniquely represented in the activity of distributed neural systems that span cortical and subcortical regions. Results from multiple-category decoding studies are incompatible with theories postulating that specific emotions emerge from the neural coding of valence and arousal. This `new look' into emotion representation promises to improve and reformulate neurobiological models of affect.},
  pmcid = {PMC4875847},
  pmid = {27133227},
  file = {/Users/xzfang/Zotero/storage/JSX4CBLL/Kragel and LaBar - 2016 - Decoding the Nature of Emotion in the Brain.pdf}
}

@article{kraljic_first_2008,
  title = {First {{Impressions}} and {{Last Resorts}}: {{How Listeners Adjust}} to {{Speaker Variability}}},
  shorttitle = {First {{Impressions}} and {{Last Resorts}}},
  author = {Kraljic, Tanya and Samuel, Arthur G. and Brennan, Susan E.},
  year = {2008},
  month = apr,
  journal = {Psychological Science},
  volume = {19},
  number = {4},
  pages = {332--338},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2008.02090.x},
  abstract = {Perceptual theories must explain how perceivers extract meaningful information from a continuously variable physical signal. In the case of speech, the puzzle is that little reliable acoustic invariance seems to exist. We tested the hypothesis that speech-perception processes recover invariants not about the signal, but rather about the source that produced the signal. Findings from two manipulations suggest that the system learns those properties of speech that result from idiosyncratic characteristics of the speaker; the same properties are not learned when they can be attributed to incidental factors. We also found evidence for how the system determines what is characteristic: In the absence of other information about the speaker, the system relies on episodic order, representing those properties present during early experience as characteristic of the speaker. This ``first-impressions'' bias can be overridden, however, when variation is an incidental consequence of a temporary state (a pen in the speaker's mouth), rather than characteristic of the speaker.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7AR4UH9F/Kraljic et al. - 2008 - First Impressions and Last Resorts How Listeners .pdf}
}

@article{kraljic_generalization_2006,
  title = {Generalization in Perceptual Learning for Speech},
  author = {Kraljic, Tanya and Samuel, Arthur G.},
  year = {2006},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {13},
  number = {2},
  pages = {262--268},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03193841},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YNV8B6KZ/Kraljic and Samuel - 2006 - Generalization in perceptual learning for speech.pdf}
}

@article{kraljic_perceptual_2005,
  title = {Perceptual Learning for Speech: {{Is}} There a Return to Normal?},
  shorttitle = {Perceptual Learning for Speech},
  author = {Kraljic, Tanya and Samuel, Arthur G.},
  year = {2005},
  journal = {Cognitive Psychology},
  volume = {51},
  number = {2},
  pages = {141--178},
  publisher = {{Elsevier Science}},
  address = {{Netherlands}},
  issn = {1095-5623(Electronic),0010-0285(Print)},
  doi = {10.1016/j.cogpsych.2005.05.001},
  abstract = {Recent work on perceptual learning shows that listeners' phonemic representations dynamically adjust to reflect the speech they hear (Norris, McQueen, \& Cutler, 2003). We investigate how the perceptual system makes such adjustments, and what (if anything) causes the representations to return to their pre-perceptual learning settings. Listeners are exposed to a speaker whose pronunciation of a particular sound (either /s/ or /{$\int$}/) is ambiguous (e.g., halfway between /s/ and /{$\int$}/). After exposure, participants are tested for perceptual learning on two continua that range from /s/ to /{$\int$}/, one in the Same voice they heard during exposure, and one in a Different voice. To assess how representations revert to their prior settings, half of Experiment 1's participants were tested immediately after exposure; the other half performed a 25-min silent intervening task. The perceptual learning effect was actually larger after such a delay, indicating that simply allowing time to pass does not cause learning to fade. The remaining experiments investigate different ways that the system might unlearn a person's pronunciations: listeners hear the Same or a Different speaker for 25 min with either: no relevant (i.e., 'good') /s/ or /{$\int$}/ input (Experiment 2), one of the relevant inputs (Experiment 3), or both relevant inputs (Experiment 4). The results support a view of phonemic representations as dynamic and flexible, and suggest that they interact with both higher- (e.g., lexical) and lower-level (e.g., acoustic) information in important ways. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Adjustment,Auditory Stimulation,Learning,Perceptual Learning,Speech Perception},
  file = {/Users/xzfang/Zotero/storage/CIE3XJ3P/Kraljic and Samuel - 2005 - Perceptual learning for speech Is there a return .pdf;/Users/xzfang/Zotero/storage/9X8RF8C8/2005-12192-002.html}
}

@article{kraljic_perceptual_2007,
  title = {Perceptual Adjustments to Multiple Speakers},
  author = {Kraljic, Tanya and Samuel, Arthur G.},
  year = {2007},
  month = jan,
  journal = {Journal of Memory and Language},
  volume = {56},
  number = {1},
  pages = {1--15},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2006.07.010},
  abstract = {Different speakers may pronounce the same sounds very differently, yet listeners have little difficulty perceiving speech accurately. Recent research suggests that listeners adjust their preexisting phonemic categories to accommodate speakers' pronunciations (perceptual learning). In some cases, these adjustments appear to reflect general changes to phonemic categories, rather than speaker-specific adjustments. But what happens when listeners encounter multiple speakers with different pronunciations? We exposed listeners to two speakers who varied in their pronunciation of a particular phoneme (Experiment 1: /d/ or /t/; Experiment 2: /s/ or /{$\Elzesh$}/). Listeners then categorized sounds on /d/-/t/ or /s/-/{$\Elzesh$}/ continua, in the same two voices. The results suggest that perceptual experience leads to very different learning for different types of phonemic contrasts. For fricatives, perceptual learning was speaker-specific: The system was able to maintain multiple different representations simultaneously. In contrast, perceptual learning for stop consonants resulted in more general changes that required the system to re-adjust when a new pronunciation was encountered.},
  langid = {english},
  keywords = {Adjustments,Multiple speakers,Partner effects,Perceptual learning,Priming,Speech perception},
  file = {/Users/xzfang/Zotero/storage/DKS2H5K3/Kraljic and Samuel - 2007 - Perceptual adjustments to multiple speakers.pdf;/Users/xzfang/Zotero/storage/KAIYWK64/S0749596X06000842.html}
}

@article{kreitewolf_perceptual_2018,
  title = {Perceptual Grouping in the Cocktail Party: {{Contributions}} of Voice-Feature Continuity},
  shorttitle = {Perceptual Grouping in the Cocktail Party},
  author = {Kreitewolf, Jens and Mathias, Samuel R. and Trapeau, R{\'e}gis and Obleser, Jonas and Sch{\"o}nwiesner, Marc},
  year = {2018},
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {4},
  pages = {2178--2188},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.5058684},
  abstract = {Cocktail parties pose a difficult yet solvable problem for the auditory system. Previous work has shown that the cocktail-party problem is considerably easier when all sounds in the target stream are spoken by the same talker (the voice-continuity benefit). The present study investigated the contributions of two of the most salient voice features\textemdash glottal-pulse rate (GPR) and vocal-tract length (VTL)\textemdash to the voice-continuity benefit. Twenty young, normal-hearing listeners participated in two experiments. On each trial, listeners heard concurrent sequences of spoken digits from three different spatial locations and reported the digits coming from a target location. Critically, across conditions, GPR and VTL either remained constant or varied across target digits. Additionally, across experiments, the target location either remained constant (Experiment 1) or varied (Experiment 2) within a trial. In Experiment 1, listeners benefited from continuity in either voice feature, but VTL continuity was more helpful than GPR continuity. In Experiment 2, spatial discontinuity greatly hindered listeners' abilities to exploit continuity in GPR and VTL. The present results suggest that selective attention benefits from continuity in target voice features and that VTL and GPR play different roles for perceptual grouping and stream segregation in the cocktail party.},
  file = {/Users/xzfang/Zotero/storage/CTGES65F/Kreitewolf et al. - 2018 - Perceptual grouping in the cocktail party Contrib.pdf}
}

@article{kriegeskorte_grid_2016,
  title = {Grid {{Cells}} for {{Conceptual Spaces}}?},
  author = {Kriegeskorte, Nikolaus and Storrs, Katherine R.},
  year = {2016},
  month = oct,
  journal = {Neuron},
  volume = {92},
  number = {2},
  pages = {280--284},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.10.006},
  abstract = {``Grid cells'' encode an animal's location and direction of movement in 2D physical environments via regularly repeating receptive fields. Constantinescu et~al. (2016) report the first evidence of grid cells for 2D conceptual spaces. The work has exciting implications for mental representation and shows how detailed neural-coding hypotheses can be tested with bulk population-activity measures.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5P45NIF3/Kriegeskorte and Storrs - 2016 - Grid Cells for Conceptual Spaces.pdf}
}

@article{kriegstein_how_2010,
  title = {How the {{Human Brain Recognizes Speech}} in the {{Context}} of {{Changing Speakers}}},
  author = {von Kriegstein, Katharina and Smith, David R. R. and Patterson, Roy D. and Kiebel, Stefan J. and Griffiths, Timothy D.},
  year = {2010},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {2},
  pages = {629--638},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2742-09.2010},
  abstract = {We understand speech from different speakers with ease, whereas artificial speech recognition systems struggle with this task. It is unclear how the human brain solves this problem. The conventional view is that speech message recognition and speaker identification are two separate functions and that message processing takes place predominantly in the left hemisphere, whereas processing of speaker-specific information is located in the right hemisphere. Here, we distinguish the contribution of specific cortical regions, to speech recognition and speaker information processing, by controlled manipulation of task and resynthesized speaker parameters. Two functional magnetic resonance imaging studies provide evidence for a dynamic speech-processing network that questions the conventional view. We found that speech recognition regions in left posterior superior temporal gyrus/superior temporal sulcus (STG/STS) also encode speaker-related vocal tract parameters, which are reflected in the amplitude peaks of the speech spectrum, along with the speech message. Right posterior STG/STS activated specifically more to a speaker-related vocal tract parameter change during a speech recognition task compared with a voice recognition task. Left and right posterior STG/STS were functionally connected. Additionally, we found that speaker-related glottal fold parameters (e.g., pitch), which are not reflected in the amplitude peaks of the speech spectrum, are processed in areas immediately adjacent to primary auditory cortex, i.e., in areas in the auditory hierarchy earlier than STG/STS. Our results point to a network account of speech recognition, in which information about the speech message and the speaker's vocal tract are combined to solve the difficult task of understanding speech from different speakers.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2010 the authors 0270-6474/10/300629-10\$15.00/0},
  langid = {english},
  pmid = {20071527},
  file = {/Users/xzfang/Zotero/storage/UYAYTZDX/Kriegstein et al. - 2010 - How the Human Brain Recognizes Speech in the Conte.pdf;/Users/xzfang/Zotero/storage/2ANLT7JI/629.html}
}

@article{kristjansson_attentional_2019,
  title = {Attentional Priming: Recent Insights and Current Controversies},
  shorttitle = {Attentional Priming},
  author = {Kristj{\'a}nsson, {\'A}rni and {\'A}sgeirsson, {\'A}rni Gunnar},
  year = {2019},
  month = oct,
  journal = {Current Opinion in Psychology},
  volume = {29},
  pages = {71--75},
  issn = {2352250X},
  doi = {10.1016/j.copsyc.2018.11.013},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HASLLJ64/KristjÃ¡nsson and Ãsgeirsson - 2019 - Attentional priming recent insights and current c.pdf}
}

@article{kristjansson_rapid_2006,
  title = {Rapid Learning in Attention Shifts: {{A}} Review},
  shorttitle = {Rapid Learning in Attention Shifts},
  author = {Kristj{\'a}nsson, {\'A}rni},
  year = {2006},
  month = feb,
  journal = {Visual Cognition},
  volume = {13},
  number = {3},
  pages = {324--362},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/13506280544000039},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LLJ8ULVM/KristjÃ¡nsson - 2006 - Rapid learning in attention shifts A review.pdf}
}

@article{kristjansson_simultaneous_2006,
  title = {Simultaneous Priming along Multiple Feature Dimensions in a Visual Search Task},
  author = {Kristj{\'a}nsson, {\'A}rni},
  year = {2006},
  month = aug,
  journal = {Vision Research},
  volume = {46},
  number = {16},
  pages = {2554--2570},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2006.01.015},
  abstract = {What we have recently seen generally has a large effect on how we consequently perceive our visual environment. Such priming effects play a surprisingly large role in visual search tasks, for example. It is unclear, however, whether different features of an object show independent but simultaneous priming. For example, if the color and orientation of a target item are the same as on a previous trial, is performance better than if only one of those features is repeated? In other words this paper presents an attempt at assessing the capacity of priming for different feature dimensions. Observers searched for a three featured object (a gabor patch that was either redscale or greenscale, oriented either to the left or right of vertical and of high or low spatial frequency) among distractors with different values along these feature dimensions. Which feature was the target defining feature; which was the response defining feature and which was the irrelevant feature, was varied between the different experiments. Task relevant features (target defining, or response defining) always resulted in priming effects, while when spatial frequency or orientation were task irrelevant neither resulted in priming, but color always did, even when task irrelevant. Further experiments showed that priming from spatial frequency and orientation could occur when they were task irrelevant but only when the other feature of the two was kept constant across all display items. The results show that simultaneous priming for different features can occur simultaneously, but also that task relevance has a strong modulatory effect on the priming.},
  langid = {english},
  keywords = {Awareness,Neural mechanisms of attention,Priming,Visual attention,Visual search},
  file = {/Users/xzfang/Zotero/storage/QZATNEBC/KristjÃ¡nsson - 2006 - Simultaneous priming along multiple feature dimens.pdf;/Users/xzfang/Zotero/storage/VML9NL2D/S0042698906000514.html}
}

@article{kristjansson_where_2010,
  title = {Where Perception Meets Memory: {{A}} Review of Repetition Priming in Visual Search Tasks},
  shorttitle = {Where Perception Meets Memory},
  author = {Kristj{\'a}nsson, {\'A}rni and Campana, Gianluca},
  year = {2010},
  month = jan,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {72},
  number = {1},
  pages = {5--18},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/APP.72.1.5},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LR9SSRLV/KristjÃ¡nsson and Campana - 2010 - Where perception meets memory A review of repetit.pdf}
}

@article{kroczek_communicative_2017,
  title = {Communicative Predictions Can Overrule Linguistic Priors},
  author = {Kroczek, Leon O. H. and Gunter, Thomas C.},
  year = {2017},
  month = dec,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {17581},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-17907-9},
  abstract = {Predictions allow for efficient human communication. To be efficient, listeners' predictions need to be adapted to the communicative context. Here we show that during speech processing this adaptation is a highly flexible and selective process that is able to fine-tune itself to individual language styles of specific interlocutors. In a newly developed paradigm, speakers differed in the probabilities by which they used particular sentence structures. Probe trials were applied to infer participants' syntactic expectations for a given speaker and to track changes of these expectations over time. The results show that listeners fine-tune their linguistic expectations according to the individual language style of a speaker. Strikingly, nine months after the initial experiment these highly specific expectations could be rapidly reactivated when confronted with the particular language style of a speaker but not merely on the basis of an association with speaker identity per se. These findings highlight that communicative interaction fine-tunes and consolidates interlocutor specific communicative predictions which can overrule strong linguistic priors.},
  copyright = {2017 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FZXEVPXQ/Kroczek and Gunter - 2017 - Communicative predictions can overrule linguistic .pdf;/Users/xzfang/Zotero/storage/AWYCUZ92/s41598-017-17907-9.html}
}

@article{kroll_category_1994,
  title = {Category {{Interference}} in {{Translation}} and {{Picture Naming}}: {{Evidence}} for {{Asymmetric Connections Between Bilingual Memory Representations}}},
  shorttitle = {Category {{Interference}} in {{Translation}} and {{Picture Naming}}},
  author = {Kroll, J. F. and Stewart, E.},
  year = {1994},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {33},
  number = {2},
  pages = {149--174},
  issn = {0749-596X},
  doi = {10.1006/jmla.1994.1008},
  abstract = {Three experiments are reported in which picture naming and bilingual translation were performed in the context of semantically categorized or randomized lists. In Experiments 1 and 3 picture naming and bilingual translation were slower in the categorized than randomized conditions. In Experiment 2 this category interference effect in picture naming was eliminated when picture naming alternated with word naming. Taken together, the results of the three experiments suggest that in both picture naming and bilingual translation a conceptual representation of the word or picture is used to retrieve a lexical entry in one of the speaker{${'}$}s languages. When conceptual activity is sufficiently great to activate a multiple set of corresponding lexical representations, interference is produced in the process of retrieving a single best lexical candidate as the name or translation. The results of Experiment 3 showed further that category interference in bilingual translation occurred only when translation was performed from the first language to the second language, suggesting that the two directions of translation engage different interlanguage connections. A model to account for the asymmetric mappings of words to concepts in bilingual memory is described.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/73KFN9LV/Kroll and Stewart - 1994 - Category Interference in Translation and Picture N.pdf;/Users/xzfang/Zotero/storage/M865RPPR/S0749596X84710084.html}
}

@article{kroll_visual_1985,
  title = {Visual {{Priming Effects}} as a {{Measure}} of {{Short-Term Visual Memory}}},
  author = {Kroll, Neal E. A. and Schepeler, Eva M.},
  year = {1985},
  journal = {The American Journal of Psychology},
  volume = {98},
  number = {3},
  pages = {449--468},
  publisher = {{University of Illinois Press}},
  issn = {0002-9556},
  doi = {10.2307/1422629},
  abstract = {On each trial, subjects classified one of four letters as belonging to one of two categories. Visual priming occurs when the classification response is faster to a stimulus visually identical to a previous stimulus than to one identical only in name. Earlier experiments found no visual priming effects between stimuli separated by a stimulus of the same task but from the opposite classification category. Two of the five conditions in the present experiment varied the stimulus-response (S-R) contingencies in such a way that the penultimate but not the immediately preceding trial had the same contingencies. Only these two conditions gave evidence of the above type of visual priming. Visual priming was found, however, in almost all conditions when the intervening stimulus was from the same task and the same classification category. It is argued that a similarity of S-R contingency, and not simply stimulus similarity, is an important component of the visual priming effect.},
  file = {/Users/xzfang/Zotero/storage/CU9I4IN4/Kroll and Schepeler - 1985 - Visual Priming Effects as a Measure of Short-Term .pdf}
}

@article{kronrod_unified_2016,
  title = {A Unified Account of Categorical Effects in Phonetic Perception},
  author = {Kronrod, Yakov and Coppess, Emily and Feldman, Naomi H.},
  year = {2016},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {6},
  pages = {1681--1712},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1049-y},
  abstract = {Categorical effects are found across speech sound categories, with the degree of these effects ranging from extremely strong categorical perception in consonants to nearly continuous perception in vowels. We show that both strong and weak categorical effects can be captured by a unified model. We treat speech perception as a statistical inference problem, assuming that listeners use their knowledge of categories as well as the acoustics of the signal to infer the intended productions of the speaker. Simulations show that the model provides close fits to empirical data, unifying past findings of categorical effects in consonants and vowels and capturing differences in the degree of categorical effects through a single parameter.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6TD2SDVT/Kronrod et al. - 2016 - A unified account of categorical effects in phonet.pdf}
}

@article{kuhl_early_2004,
  title = {Early Language Acquisition: Cracking the Speech Code},
  shorttitle = {Early Language Acquisition},
  author = {Kuhl, Patricia K.},
  year = {2004},
  month = nov,
  journal = {Nature Reviews Neuroscience},
  volume = {5},
  number = {11},
  pages = {831--843},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1533},
  abstract = {Infants learn language with remarkable speed, but how they do it remains a mystery. New data show that infants use computational strategies to detect the statistical and prosodic patterns in language input, and that this leads to the discovery of phonemes and words. Social interaction with another human being affects speech learning in a way that resembles communicative learning in songbirds. The brain's commitment to the statistical and prosodic patterns that are experienced early in life might help to explain the long-standing puzzle of why infants are better language learners than adults. Successful learning by infants, as well as constraints on that learning, are changing theories of language acquisition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JZMTY3IY/Kuhl - 2004 - Early language acquisition cracking the speech co.pdf}
}

@article{kuhl_foreignlanguage_2003,
  title = {Foreign-Language Experience in Infancy: {{Effects}} of Short-Term Exposure and Social Interaction on Phonetic Learning},
  shorttitle = {Foreign-Language Experience in Infancy},
  author = {Kuhl, Patricia K. and Tsao, Feng-Ming and Liu, Huei-Mei},
  year = {2003},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {100},
  number = {15},
  pages = {9096--9101},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1532872100},
  abstract = {Infants acquire language with remarkable speed, although little is known about the mechanisms that underlie the acquisition process. Studies of the phonetic units of language have shown that early in life, infants are capable of discerning differences among the phonetic units of all languages, including native- and foreign-language sounds. Between 6 and 12 mo of age, the ability to discriminate foreign-language phonetic units sharply declines. In two studies, we investigate the necessary and sufficient conditions for reversing this decline in foreign-language phonetic perception. In Experiment 1, 9-mo-old American infants were exposed to native Mandarin Chinese speakers in 12 laboratory sessions. A control group also participated in 12 language sessions but heard only English. Subsequent tests of Mandarin speech perception demonstrated that exposure to Mandarin reversed the decline seen in the English control group. In Experiment 2, infants were exposed to the same foreign-language speakers and materials via audiovisual or audio-only recordings. The results demonstrated that exposure to recorded Mandarin, without interpersonal interaction, had no effect. Between 9 and 10 mo of age, infants show phonetic learning from live, but not prerecorded, exposure to a foreign language, suggesting a learning process that does not require long-term listening and is enhanced by social interaction.},
  chapter = {Social Sciences},
  copyright = {Copyright \textcopyright{} 2003, The National Academy of  Sciences},
  isbn = {9781532872105},
  langid = {english},
  pmid = {12861072},
  file = {/Users/xzfang/Zotero/storage/L6N84RPE/Kuhl et al. - 2003 - Foreign-language experience in infancy Effects of.pdf;/Users/xzfang/Zotero/storage/JFTIAH4W/9096.html}
}

@article{kumar_searching_2020,
  title = {Searching through Functional Space Reveals Distributed Visual, Auditory, and Semantic Coding in the Human Brain},
  author = {Kumar, Sreejan and Ellis, Cameron T. and O'Connell, Thomas P. and Chun, Marvin M. and {Turk-Browne}, Nicholas B.},
  year = {2020},
  month = dec,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {12},
  pages = {e1008457},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008457},
  abstract = {The extent to which brain functions are localized or distributed is a foundational question in neuroscience. In the human brain, common fMRI methods such as cluster correction, atlas parcellation, and anatomical searchlight are biased by design toward finding localized representations. Here we introduce the functional searchlight approach as an alternative to anatomical searchlight analysis, the most commonly used exploratory multivariate fMRI technique. Functional searchlight removes any anatomical bias by grouping voxels based only on functional similarity and ignoring anatomical proximity. We report evidence that visual and auditory features from deep neural networks and semantic features from a natural language processing model, as well as object representations, are more widely distributed across the brain than previously acknowledged and that functional searchlight can improve model-based similarity and decoding accuracy. This approach provides a new way to evaluate and constrain computational models with brain activity and pushes our understanding of human brain function further along the spectrum from strict modularity toward distributed representation.},
  langid = {english},
  keywords = {Acoustic signals,Brain,Functional magnetic resonance imaging,Neural networks,Preprocessing,Principal component analysis,Vision,Word recognition},
  file = {/Users/xzfang/Zotero/storage/MBRX97TR/Kumar et al. - 2020 - Searching through functional space reveals distrib.pdf;/Users/xzfang/Zotero/storage/V8MTT3V4/article.html}
}

@article{kuperberg_distinct_2003,
  title = {Distinct {{Patterns}} of {{Neural Modulation}} during the {{Processing}} of {{Conceptual}} and {{Syntactic Anomalies}}},
  author = {Kuperberg, Gina R. and Holcomb, Phillip J. and Sitnikova, Tatiana and Greve, Douglas and Dale, Anders M. and Caplan, David},
  year = {2003},
  month = feb,
  journal = {Journal of Cognitive Neuroscience},
  volume = {15},
  number = {2},
  pages = {272--293},
  issn = {0898-929X},
  doi = {10.1162/089892903321208204},
  abstract = {The aim of this study was to gain further insights into how the brain distinguishes between meaning and syntax during language comprehension. Participants read and made plausibility judgments on sentences that were plausible, morpho-syntactically anomalous, or pragmatically anomalous. In an event-related potential (ERP) experiment, morphosyntactic and pragmatic violations elicited significant P600 and N400 effects, respectively, replicating previous ERP studies that have established qualitative differences in processing conceptually and syntactic anomalies. Our main focus was a functional magnetic resonance imaging (fMRI) study in which the same subjects read the same sentences presented in the same pseudorandomized sequence while performing the same task as in the ERP experiment. Rapid-presentation event-related fMRI methods allowed us to estimate the hemodynamic response at successive temporal windows as the sentences unfolded word by word, without assumptions about the shape of the underlying response function. Relative to nonviolated sentences, the pragmatic anomalies were associated with an increased hemodynamic response in left temporal and inferior frontal regions and a decreased response in the right medial parietal cortex. Relative to nonviolated sentences, the morphosyntactic anomalies were associated with an increased response in bilateral medial and lateral parietal regions and a decreased response in left temporal and inferior frontal regions. Thus, overlapping neural networks were modulated in opposite directions to the two types of anomaly. These fMRI findings document both qualitative and quantitative differences in how the brain distinguishes between these two types of anomalies. This suggests that morphosyntactic and pragmatic information can be processed in different ways but by the same neural systems.},
  file = {/Users/xzfang/Zotero/storage/JFUYRUZ9/Kuperberg et al. - 2003 - Distinct Patterns of Neural Modulation during the .pdf;/Users/xzfang/Zotero/storage/4KDH7C3E/Distinct-Patterns-of-Neural-Modulation-during-the.html}
}

@article{kuperberg_electrophysiological_2003,
  title = {Electrophysiological Distinctions in Processing Conceptual Relationships within Simple Sentences},
  author = {Kuperberg, Gina R and Sitnikova, Tatiana and Caplan, David and Holcomb, Phillip J},
  year = {2003},
  month = jun,
  journal = {Cognitive Brain Research},
  volume = {17},
  number = {1},
  pages = {117--129},
  issn = {0926-6410},
  doi = {10.1016/S0926-6410(03)00086-7},
  abstract = {The aim of this study was to determine whether or not the brain distinguishes between two types of conceptual relationships between noun-phrases (NPs) and verbs during online processing of simple, unambiguous English sentences. A total of 15 participants read and made plausibility judgments on sentences that were presented word-by-word. Event-related potentials elicited by critical verbs were measured. In all cases, the critical verb assigned a thematic role of `agent' to its subject NP. In non-violated sentences (e.g. ``For breakfast the boys would only eat\ldots ''), the preceding NP was animate (``boys'') and was a likely agent for a given verb (``eat'') given its preceding context (``For breakfast''). In both types of conceptually violated sentences, the NPs were unlikely agents for the verbs given their preceding contexts. In `thematic role animacy violations' (e.g. ``For breakfast the eggs would only eat\ldots ''), the NP was inanimate (``eggs'') and was therefore more likely to occupy the role of `theme' than `agent', i.e. eggs, being inanimate, cannot eat but they can be eaten. In `non-thematic role pragmatic violations' (e.g. ``For breakfast the boys would only bury\ldots ''), the thematic role of agent assigned by the verb (``bury'') to its preceding NP (``boys'') is inherently acceptable (boys can bury), but the sentence is still pragmatically incongruous given the preceding context (``At breakfast''). As expected, the non-thematic role pragmatic violations elicited a significant N400 effect. The thematic role animacy violations elicited a smaller N400 effect that only approached significance across all participants. The thematic role animacy violations, however, elicited a significant P600 effect\textemdash an ERP component that is most commonly associated with processing syntactic information during language comprehension. We discuss the possibility that the P600 was elicited by the thematic role animacy violations (but not by the non-thematic role pragmatic violations) because, in the former but not the latter, there was an online attempt to structurally repair and make sense of the sentences by reassigning the thematic role of the NP that preceded the critical verb from `agent' to `theme'. Our findings suggest a qualitative neural distinction in processing these two types of conceptual anomalies within simple, unambiguous sentences.},
  langid = {english},
  keywords = {Animacy,Context,ERP,Event related potentials,Language,N400,P600,Pragmatics,Proposition,Semantics,Sentences,Syntax,Thematic roles,Verb-argument structure},
  file = {/Users/xzfang/Zotero/storage/8IQQY4F7/Kuperberg et al. - 2003 - Electrophysiological distinctions in processing co.pdf;/Users/xzfang/Zotero/storage/TXF2XHJN/S0926641003000867.html}
}

@article{kuperberg_electrophysiological_2010,
  title = {Electrophysiological {{Correlates}} of {{Complement Coercion}}},
  author = {Kuperberg, Gina R. and Choi, Arim and Cohn, Neil and Paczynski, Martin and Jackendoff, Ray},
  year = {2010},
  month = dec,
  journal = {Journal of Cognitive Neuroscience},
  volume = {22},
  number = {12},
  pages = {2685--2701},
  issn = {0898-929X},
  doi = {10.1162/jocn.2009.21333},
  abstract = {This study examined the electrophysiological correlates of complement coercion. ERPs were measured as participants read and made acceptability judgments about plausible coerced sentences, plausible noncoerced sentences, and highly implausible animacy-violated sentences (``The journalist began/wrote/astonished the article before his coffee break''). Relative to noncoerced complement nouns, the coerced nouns evoked an N400 effect. This effect was not modulated by the number of possible activities implied by the coerced nouns (e.g., began reading the article; began writing the article) and did not differ either in magnitude or scalp distribution from the N400 effect evoked by the animacy-violated complement nouns. We suggest that the N400 modulation to both coerced and animacy-violated complement nouns reflected different types of mismatches between the semantic restrictions of the verb and the semantic properties of the incoming complement noun. This is consistent with models holding that a verb's semantic argument structure is represented and stored at a distinct level from its syntactic argument structure. Unlike the coerced complement noun, the animacy-violated nouns also evoked a robust P600 effect, which may have been triggered by the judgments of the highly implausible (syntactically determined) meanings of the animacy-violated propositions. No additional ERP effects were seen in the coerced sentences until the sentence-final word that, relative to sentence-final words in the noncoerced sentences, evoked a sustained anteriorly distributed positivity. We suggest that this effect reflected delayed attempts to retrieve the specific event(s) implied by coerced complement nouns.},
  file = {/Users/xzfang/Zotero/storage/DVUKRV7L/Kuperberg et al. - 2010 - Electrophysiological Correlates of Complement Coer.pdf;/Users/xzfang/Zotero/storage/QFNFI5NS/Electrophysiological-Correlates-of-Complement.html}
}

@article{kuperberg_neural_2006,
  title = {Neural Correlates of Processing Syntactic, Semantic, and Thematic Relationships in Sentences},
  author = {Kuperberg, Gina R. and Caplan, David and Sitnikova, Tatiana and Eddy, Marianna and Holcomb, Phillip J.},
  year = {2006},
  month = aug,
  journal = {Language and Cognitive Processes},
  volume = {21},
  number = {5},
  pages = {489--530},
  publisher = {{Routledge}},
  issn = {0169-0965},
  doi = {10.1080/01690960500094279},
  abstract = {Event-related potentials were measured as subjects read sentences presented word by word. A small N400 and a robust P600 effect were elicited by verbs that assigned the thematic role of Agent to their preceding noun-phrase argument when this argument was inanimate in nature. The amplitude of the P600, but not the N400, was modulated by the transitivity of the critical verbs and by plausibility ratings of passivised versions of these sentences (reflecting the fit between the critical verb and the inanimate noun-phrase as the verb's Theme). The P600 was similar in scalp distribution although smaller in amplitude, than that elicited by verbs with morphosyntactic violations. Pragmatically unlikely verbs that did not violate thematic constraints elicited a larger N400 but no P600 effect. These findings support the theory that the cost of syntactic processing on a verb is influenced by the precise thematic relationships between that verb and its preceding arguments.},
  annotation = {\_eprint: https://doi.org/10.1080/01690960500094279},
  file = {/Users/xzfang/Zotero/storage/85A9HWIC/Kuperberg et al. - 2006 - Neural correlates of processing syntactic, semanti.pdf;/Users/xzfang/Zotero/storage/KQ4D97EJ/01690960500094279.html}
}

@article{kuperberg_separate_2016,
  title = {Separate Streams or Probabilistic Inference? {{What}} the {{N400}} Can Tell Us about the Comprehension of Events},
  shorttitle = {Separate Streams or Probabilistic Inference?},
  author = {Kuperberg, Gina R.},
  year = {2016},
  month = may,
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {5},
  pages = {602--616},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2015.1130233},
  abstract = {Since the early 2000s, several event-related potential studies have challenged the assumption that we always use syntactic contextual information to influence semantic processing of incoming words, as reflected by the N400 component. One approach for explaining these findings is to posit distinct semantic and syntactic processing mechanisms, each with distinct time courses. While this approach can explain specific datasets, it cannot account for the wider body of findings. I propose an alternative explanation: a dynamic generative framework in which our goal is to infer the underlying event that best explains the set of inputs encountered at any given time. Within this framework, combinations of semantic and syntactic cues with varying reliabilities are used as evidence to weight probabilistic hypotheses about this event. I further argue that the computational principles of this framework can be extended to understand how we infer situation models during discourse comprehension, and intended messages during spoken communication.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HATJAWTD/Kuperberg - 2016 - Separate streams or probabilistic inference What .pdf}
}

@article{kuperberg_tale_2019,
  title = {A {{Tale}} of {{Two Positivities}} and the {{N400}}: {{Distinct Neural Signatures Are Evoked}} by {{Confirmed}} and {{Violated Predictions}} at {{Different Levels}} of {{Representation}}},
  shorttitle = {A {{Tale}} of {{Two Positivities}} and the {{N400}}},
  author = {Kuperberg, Gina R. and Brothers, Trevor and Wlotko, Edward W.},
  year = {2019},
  month = sep,
  journal = {Journal of Cognitive Neuroscience},
  volume = {32},
  number = {1},
  pages = {12--35},
  publisher = {{MIT Press}},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01465},
  abstract = {It has been proposed that hierarchical prediction is a fundamental computational principle underlying neurocognitive processing. Here, we ask whether the brain engages distinct neurocognitive mechanisms in response to inputs that fulfill versus violate strong predictions at different levels of representation during language comprehension. Participants read three-sentence scenarios in which the third sentence constrained for a broad event structure, for example, \{Agent caution animate\textendash Patient\}. High constraint contexts additionally constrained for a specific event/lexical item, for example, a two-sentence context about a beach, lifeguards, and sharks constrained for the event, \{Lifeguards cautioned Swimmers\}, and the specific lexical item swimmers. Low constraint contexts did not constrain for any specific event/lexical item. We measured ERPs on critical nouns that fulfilled and/or violated each of these constraints. We found clear, dissociable effects to fulfilled semantic predictions (a reduced N400), to event/lexical prediction violations (an increased late frontal positivity), and to event structure/animacy prediction violations (an increased late posterior positivity/P600). We argue that the late frontal positivity reflects a large change in activity associated with successfully updating the comprehender's current situation model with new unpredicted information. We suggest that the late posterior positivity/P600 is triggered when the comprehender detects a conflict between the input and her model of the communicator and communicative environment. This leads to an initial failure to incorporate the unpredicted input into the situation model, which may be followed by second-pass attempts to make sense of the discourse through reanalysis, repair, or reinterpretation. Together, these findings provide strong evidence that confirmed and violated predictions at different levels of representation manifest as distinct spatiotemporal neural signatures.},
  file = {/Users/xzfang/Zotero/storage/2DHHUM4P/Kuperberg et al. - 2019 - A Tale of Two Positivities and the N400 Distinct .pdf;/Users/xzfang/Zotero/storage/DKA64L9G/jocn_a_01465.html;/Users/xzfang/Zotero/storage/FAMZ8MKP/jocn_a_01465.html}
}

@article{kuperberg_tea_2021,
  title = {Tea {{With Milk}}? {{A Hierarchical Generative Framework}} of {{Sequential Event Comprehension}}},
  shorttitle = {Tea {{With Milk}}?},
  author = {Kuperberg, Gina R.},
  year = {2021},
  month = jan,
  journal = {Topics in Cognitive Science},
  volume = {13},
  number = {1},
  pages = {256--298},
  issn = {1756-8757, 1756-8765},
  doi = {10.1111/tops.12518},
  abstract = {To make sense of the world around us, we must be able to segment a continual stream of sensory inputs into discrete events. In this review, I propose that in order to comprehend events, we engage hierarchical generative models that ``reverse engineer'' the intentions of other agents as they produce sequential action in real time. By generating probabilistic predictions for upcoming events, generative models ensure that we are able to keep up with the rapid pace at which perceptual inputs unfold. By tracking our certainty about other agents' goals and the magnitude of prediction errors at multiple temporal scales, generative models enable us to detect event boundaries by inferring when a goal has changed. Moreover, by adapting flexibly to the broader dynamics of the environment and our own comprehension goals, generative models allow us to optimally allocate limited resources. Finally, I argue that we use generative models not only to comprehend events but also to produce events (carry out goal-relevant sequential action) and to continually learn about new events from our surroundings. Taken together, this hierarchical generative framework provides new insights into how the human brain processes events so effortlessly while highlighting the fundamental links between event comprehension, production, and learning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3ELNULEJ/Kuperberg - 2021 - Tea With Milk A Hierarchical Generative Framework.pdf}
}

@article{kuperberg_what_2016,
  title = {What Do We Mean by Prediction in Language Comprehension?},
  author = {Kuperberg, Gina R. and Jaeger, T. Florian},
  year = {2016},
  month = jan,
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  pages = {32--59},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2015.1102299},
  abstract = {We consider several key aspects of prediction in language comprehension: its computational nature, the representational level(s) at which we predict, whether we use higher-level representations to predictively pre-activate lower level representations, and whether we ``commit'' in any way to our predictions, beyond pre-activation. We argue that the bulk of behavioural and neural evidence suggests that we predict probabilistically and at multiple levels and grains of representation. We also argue that we can, in principle, use higher-level inferences to predictively pre-activate information at multiple lower representational levels. We suggest that the degree and level of predictive pre-activation might be a function of its expected utility, which, in turn, may depend on comprehenders' goals and their estimates of the relative reliability of their prior knowledge and the bottom-up input. Finally, we argue that all these properties of language understanding can be naturally explained and productively explored within a multi-representational hierarchical actively generative architecture whose goal is to infer the message intended by the producer, and in which predictions play a crucial role in explaining the bottom-up input.},
  pmid = {27135040},
  keywords = {generative model,Language comprehension,prediction error,probabilistic,surprisal},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2015.1102299},
  file = {/Users/xzfang/Zotero/storage/VEX6SU4K/Kuperberg and Jaeger - 2016 - What do we mean by prediction in language comprehe.pdf;/Users/xzfang/Zotero/storage/D6BK5J36/23273798.2015.html}
}

@article{kurczek_hippocampal_2013,
  title = {Hippocampal Contributions to Language: {{Evidence}} of Referential Processing Deficits in Amnesia},
  shorttitle = {Hippocampal Contributions to Language},
  author = {Kurczek, Jake and {Brown-Schmidt}, Sarah and Duff, Melissa C.},
  year = {2013},
  month = nov,
  journal = {Journal of experimental psychology. General},
  volume = {142},
  number = {4},
  pages = {1346--1354},
  issn = {0096-3445},
  doi = {10.1037/a0034026},
  abstract = {A growing body of work suggests the hippocampus contributes to a variety of cognitive domains beyond its traditional role in memory. We propose that the hippocampus, in its capacity for relational binding, representational flexibility, and on-line maintenance and integration of multimodal relational representations, is a key contributor to language processing. Here we test the hypothesis that the on-line interpretation of pronouns is hippocampus-dependent. We combined eye-tracking with neuropsychological methods, where participants (4 patients with bilateral hippocampal damage and severe declarative memory impairment, 4 patients with ventromedial prefrontal cortex (vmPFC) damage, and healthy comparison participants) viewed a scene while listening to short dialogs introducing two characters; e.g., ``Melissa'' is playing violin for ``Debbie''/''Danny'' as the sun is shining overhead. She is wearing a blue/purple dress. Consistent with previous work, analysis of eye-gaze showed that younger and older healthy comparison participants and the vmPFC patients rapidly identified the intended referent of the pronoun when gender uniquely identified the referent, and when it did not, they showed a preference to interpret the pronoun as referring to the first-mentioned character. By contrast, hippocampal patients, while exhibiting a similar gender effect, exhibited significant disruptions in their ability to use information about which character had been mentioned first to interpret the pronoun. This finding suggests that the hippocampus plays a role in maintaining and integrating information even over a very short discourse history. These observed disruptions in referential processing demonstrate how promiscuously the hallmark processing features of the hippocampus are used in service of a variety of cognitive domains including language.},
  pmcid = {PMC3974972},
  pmid = {23937178},
  file = {/Users/xzfang/Zotero/storage/XAWJGWFT/Kurczek et al. - 2013 - Hippocampal contributions to language Evidence of.pdf}
}

@article{kurumada_effects_2018,
  title = {Effects of Distributional Information on Categorization of Prosodic Contours},
  author = {Kurumada, Chigusa and Brown, Meredith and Tanenhaus, Michael K.},
  year = {2018},
  month = jun,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {3},
  pages = {1153--1160},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1332-6},
  abstract = {Although prosody clearly affects the interpretation of utterances, the mapping between prosodic representations and acoustic features is highly variable. Listeners may in part cope with this variability by adapting to distributions of acoustic features in the input. We examined whether listeners adapt to distributional changes using the construction It looks like an X. When pronounced with an H* pitch accent on the final noun and a low boundary tone, the construction supports an affirmative interpretation (e.g., It looks like a ZEBRA [and I think it is one]). Conversely, when pronounced with a L+H* pitch accent and a rising boundary tone, it suggests a negative interpretation (e.g., It LOOKS like a zebra.... [but it is not]). Experiment 1 elicited pragmatic interpretations of resynthesized 12-step continua with these two contours as the end points. In Experiment 2, one group of listeners heard items sampled from the most ambiguous region along the continua followed by affirmative continuations (e.g., It looks like a zebra because it has stripes all over its body) and items near the contrastive endpoint followed by negative continuations (e.g., It looks like a zebra but it is actually something else). Another group heard the reverse (i.e., ambiguous items with negative continuations and non-contrastive items with affirmative continuations). The two groups of participants subsequently derived diverging interpretations for novel ambiguous items, suggesting that prosodic processing involves flexible mappings between acoustic features and prosodic representations that are meaningful in interpretation of speech.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/453HFVGN/Kurumada et al. - 2018 - Effects of distributional information on categoriz.pdf}
}

@article{kurumada_it_2014,
  title = {Is It or Isn't It: {{Listeners}} Make Rapid Use of Prosody to Infer Speaker Meanings},
  shorttitle = {Is It or Isn't It},
  author = {Kurumada, Chigusa and Brown, Meredith and Bibyk, Sarah and Pontillo, Daniel F. and Tanenhaus, Michael K.},
  year = {2014},
  month = nov,
  journal = {Cognition},
  volume = {133},
  number = {2},
  pages = {335--342},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2014.05.017},
  abstract = {A visual world experiment examined the time course for pragmatic inferences derived from visual context and contrastive intonation contours. We used the construction It looks like an X pronounced with either (a) a H* pitch accent on the final noun and a low boundary tone, or (b) a contrastive L+H* pitch accent and a rising boundary tone, a contour that can support contrastive inference (e.g., It LOOKSL+H* like a zebra L-H\%...(but it is not)). When the visual display contained a single related set of contrasting pictures (e.g. a zebra vs. a zebra-like animal), effects of LOOKSL+H* emerged prior to the processing of phonemic information from the target noun. The results indicate that the prosodic processing is incremental and guided by contextually-supported expectations. Additional analyses ruled out explanations based on context-independent heuristics that might substitute for online computation of contrast.},
  pmcid = {PMC4163505},
  pmid = {25128792},
  file = {/Users/xzfang/Zotero/storage/L6B7DS5R/Kurumada et al. - 2014 - Is it or isn't it Listeners make rapid use of pro.pdf}
}

@article{kutas_brain_1984,
  title = {Brain Potentials during Reading Reflect Word Expectancy and Semantic Association},
  author = {Kutas, Marta and Hillyard, Steven A.},
  year = {1984},
  month = jan,
  journal = {Nature},
  volume = {307},
  number = {5947},
  pages = {161--163},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/307161a0},
  abstract = {The neuroelectric activity of the human brain that accompanies linguistic processing can be studied through recordings of event-related potentials (e.r.p. components) from the scalp. The e.r.ps triggered by verbal stimuli have been related to several different aspects of language processing1. For example, the N400 component, peaking around 400 ms post-stimulus, appears to be a sensitive indicator of the semantic relationship between a word and the context in which it occurs. Words that complete sentences in a nonsensical fashion elicit much larger N400 waves than do semantically appropriate words or non-semantic irregularities in a text2,3. In the present study, e.r.ps were recorded in response to words that completed meaningful sentences. The amplitude of the N400 component of the e.r.p. was found to be an inverse function of the subject's expectancy for the terminal word as measured by its `Cloze probability'. In addition, unexpected words that were semantically related to highly expected words elicited lower N400 amplitudes. These findings suggest N400 may reflect processes of semantic priming or activation.},
  copyright = {1984 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TS8GR3IC/Kutas and Hillyard - 1984 - Brain potentials during reading reflect word expec.pdf}
}

@article{kutas_thirty_2011,
  title = {Thirty {{Years}} and {{Counting}}: {{Finding Meaning}} in the {{N400 Component}} of the {{Event-Related Brain Potential}} ({{ERP}})},
  shorttitle = {Thirty {{Years}} and {{Counting}}},
  author = {Kutas, Marta and Federmeier, Kara D.},
  year = {2011},
  journal = {Annual Review of Psychology},
  volume = {62},
  number = {1},
  pages = {621--647},
  doi = {10.1146/annurev.psych.093008.131123},
  abstract = {We review the discovery, characterization, and evolving use of the N400, an event-related brain potential response linked to meaning processing. We describe the elicitation of N400s by an impressive range of stimulus types\textemdash including written, spoken, and signed words or pseudowords; drawings, photos, and videos of faces, objects, and actions; sounds; and mathematical symbols\textemdash and outline the sensitivity of N400 amplitude (as its latency is remarkably constant) to linguistic and nonlinguistic manipulations. We emphasize the effectiveness of the N400 as a dependent variable for examining almost every aspect of language processing and highlight its expanding use to probe semantic memory and to determine how the neurocognitive system dynamically and flexibly uses bottom-up and top-down information to make sense of the world. We conclude with different theories of the N400's functional significance and offer an N400-inspired reconceptualization of how meaning processing might unfold.},
  pmid = {20809790},
  annotation = {\_eprint: https://doi.org/10.1146/annurev.psych.093008.131123},
  file = {/Users/xzfang/Zotero/storage/ASTGCJMV/Kutas and Federmeier - 2011 - Thirty Years and Counting Finding Meaning in the .pdf}
}

@article{ladefoged_information_1957,
  title = {Information {{Conveyed}} by {{Vowels}}},
  author = {Ladefoged, Peter and Broadbent, D. E.},
  year = {1957},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {29},
  number = {1},
  pages = {98--104},
  issn = {0001-4966},
  doi = {10.1121/1.1908694},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BDQJRGDW/Ladefoged and Broadbent - 1957 - Information Conveyed by Vowels.pdf}
}

@article{ladefoged_modeling_1989,
  title = {Modeling Articulatory\textendash Acoustics Relations: A Comment on {{Stevens}}' ``{{On}} the Quantal Nature of Speech''},
  shorttitle = {Modeling Articulatory\textendash Acoustics Relations},
  author = {Ladefoged, Peter and Lindau, Mona},
  year = {1989},
  month = jan,
  journal = {Journal of Phonetics},
  volume = {17},
  number = {1},
  pages = {99--106},
  issn = {0095-4470},
  doi = {10.1016/S0095-4470(19)31515-3},
  abstract = {This paper attempts to validate Stevens' quantal theory of vowel articulation by studying the acoustic effects of a wider and considerably more realistic and natural range of articulations than the simple tube model used by Stevens. Our model is implemented on a Macintosh computer and uses a vocal tract shape that is fairly similar to that of a human speaker. We studied the acoustic effects of a large number of articulatory vowel gestures. The results do not show much support for a quantal theory of vowel production. When articulatory gestures are varied in a simple way, the acoustic effects tend to vary monotonically with the articulatory variation, thus not producing stable plateau areas in the formant patterns.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8A3IRBFQ/Ladefoged and Lindau - 1989 - Modeling articulatoryâ€“acoustics relations a comme.pdf;/Users/xzfang/Zotero/storage/P65KHIIN/S0095447019315153.html}
}

@article{lakatos_entrainment_2008,
  title = {Entrainment of {{Neuronal Oscillations}} as a {{Mechanism}} of {{Attentional Selection}}},
  author = {Lakatos, Peter and Karmos, George and Mehta, Ashesh D. and Ulbert, Istvan and Schroeder, Charles E.},
  year = {2008},
  month = apr,
  journal = {Science},
  volume = {320},
  number = {5872},
  pages = {110--113},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1154735},
  file = {/Users/xzfang/Zotero/storage/A35HRRAH/Lakatos et al. - 2008 - Entrainment of Neuronal Oscillations as a Mechanis.pdf}
}

@article{lakatos_neuronal_2007,
  title = {Neuronal {{Oscillations}} and {{Multisensory Interaction}} in {{Primary Auditory Cortex}}},
  author = {Lakatos, Peter and Chen, Chi-Ming and O'Connell, Monica N. and Mills, Aimee and Schroeder, Charles E.},
  year = {2007},
  month = jan,
  journal = {Neuron},
  volume = {53},
  number = {2},
  pages = {279--292},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2006.12.011},
  abstract = {Recent anatomical, physiological, and neuroimaging findings indicate multisensory convergence at early, putatively unisensory stages of cortical processing. The objective of this study was to confirm somatosensory-auditory interaction in A1 and to define both its physiological mechanisms and its consequences for auditory information processing. Laminar current source density and multiunit activity sampled during multielectrode penetrations of primary auditory area A1 in awake macaques revealed clear somatosensory-auditory interactions, with a novel mechanism: somatosensory inputs appear to reset the phase of ongoing neuronal oscillations, so that accompanying auditory inputs arrive during an ideal, high-excitability phase, and produce amplified neuronal responses. In contrast, responses to auditory inputs arriving during the opposing low-excitability phase tend to~be suppressed. Our findings underscore the instrumental role of neuronal oscillations in cortical operations. The timing and laminar profile of the multisensory interactions in A1 indicate that nonspecific thalamic systems may play~a key role in the effect.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/Users/xzfang/Zotero/storage/XTFRBJSP/Lakatos et al. - 2007 - Neuronal Oscillations and Multisensory Interaction.pdf}
}

@article{lakatos_new_2019,
  title = {A {{New Unifying Account}} of the {{Roles}} of {{Neuronal Entrainment}}},
  author = {Lakatos, Peter and Gross, Joachim and Thut, Gregor},
  year = {2019},
  month = sep,
  journal = {Current Biology},
  volume = {29},
  number = {18},
  pages = {R890-R905},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2019.07.075},
  abstract = {Rhythms are a fundamental and defining feature of neuronal activity in animals including humans. This rhythmic brain activity interacts in complex ways with rhythms in the internal and external environment through the phenomenon of `neuronal entrainment', which is attracting increasing attention due to its suggested role in a multitude of sensory and cognitive processes. Some senses, such as touch and vision, sample the environment rhythmically, while others, like audition, are faced with mostly rhythmic inputs. Entrainment couples rhythmic brain activity to external and internal rhythmic events, serving fine-grained routing and modulation of external and internal signals across multiple spatial and temporal hierarchies. This interaction between a brain and its environment can be experimentally investigated and even modified by rhythmic sensory stimuli or invasive and non-invasive neuromodulation techniques. We provide a comprehensive overview of the topic and propose a theoretical framework of how neuronal entrainment dynamically structures information from incoming neuronal, bodily and environmental sources. We discuss the different types of neuronal entrainment, the conceptual advances in the field, and converging evidence for general principles.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WT6XMQ4D/Lakatos et al. - 2019 - A New Unifying Account of the Roles of Neuronal En.pdf}
}

@article{lakatos_spectrotemporal_2013,
  title = {The {{Spectrotemporal Filter Mechanism}} of {{Auditory Selective Attention}}},
  author = {Lakatos, Peter and Musacchia, Gabriella and O'Connel, Monica N. and Falchier, Arnaud Y. and Javitt, Daniel C. and Schroeder, Charles E.},
  year = {2013},
  month = feb,
  journal = {Neuron},
  volume = {77},
  number = {4},
  pages = {750--761},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2012.11.034},
  abstract = {Although we have convincing evidence that attention to auditory stimuli modulates neuronal responses at or before the level of primary auditory cortex (A1), the underlying physiological mechanisms are unknown. We found that attending to rhythmic auditory streams resulted in the entrainment of ongoing oscillatory activity reflecting rhythmic excitability fluctuations in A1. Strikingly, although the rhythm of the entrained oscillations in A1 neuronal ensembles reflected the temporal structure of the attended stream, the phase depended on the attended frequency content. Counter-phase entrainment across differently tuned A1 regions resulted in both the amplification and sharpening of responses at attended time points, in essence acting as a spectrotemporal filter mechanism. Our data suggest that selective attention generates a dynamically evolving model of attended auditory stimulus streams in the form of modulatory subthreshold oscillations across tonotopically organized neuronal ensembles in A1 that enhances the representation of attended stimuli.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HTYXSLRQ/Lakatos et al. - 2013 - The Spectrotemporal Filter Mechanism of Auditory S.pdf}
}

@article{lake_building_,
  title = {Building Machines That Learn and Think like People},
  author = {Lake, Brenden M},
  pages = {72},
  abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EIEXGY5Y/Lake - Building machines that learn and think like people.pdf}
}

@article{lally_shaping_2020,
  title = {Shaping the Precision of Letter Position Coding by Varying Properties of a Writing System},
  author = {Lally, Clare and Taylor, J. S. H. and Lee, Chang H. and Rastle, Kathleen},
  year = {2020},
  month = apr,
  journal = {Language, Cognition and Neuroscience},
  volume = {35},
  number = {3},
  pages = {374--382},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2019.1663222},
  abstract = {There is substantial debate around the nature of letter position coding in reading. Research on a variety of Indo-European languages suggests uncertainty in position coding; for example, readers perceive transposed-letter stimuli (jugde) as similar to their base words (judge). However, these effects are not apparent for all languages. We developed a powerful new method to delineate how specific properties of a writing system shape the representation of letter position. Two groups of 24 adults learned to read novel words printed in artificial scripts. One group learned a dense orthography (i.e. with many anagrams) and one group learned a sparse orthography (i.e. no anagrams). Following four days of training, participants showed a larger transposed-letter effect in the sparse orthography than in the dense orthography. These results challenge existing models of orthographic processing in reading, and support the claim that orthographic representations are shaped by the nature of the writing system.},
  keywords = {artificial language learning,letter coding,orthography,transposed letter,writing systems},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2019.1663222},
  file = {/Users/xzfang/Zotero/storage/BPYDPEBU/Lally et al. - 2020 - Shaping the precision of letter position coding by.pdf;/Users/xzfang/Zotero/storage/ZBJVBU5K/23273798.2019.html}
}

@article{lalor_neural_2010,
  title = {Neural Responses to Uninterrupted Natural Speech Can Be Extracted with Precise Temporal Resolution},
  author = {Lalor, Edmund C. and Foxe, John J.},
  year = {2010},
  journal = {European Journal of Neuroscience},
  volume = {31},
  number = {1},
  pages = {189--193},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.2009.07055.x},
  abstract = {The human auditory system has evolved to efficiently process individual streams of speech. However, obtaining temporally detailed responses to distinct continuous natural speech streams has hitherto been impracticable using standard neurophysiological techniques. Here a method is described which provides for the estimation of a temporally precise electrophysiological response to uninterrupted natural speech. We have termed this response AESPA (Auditory Evoked Spread Spectrum Analysis) and it represents an estimate of the impulse response of the auditory system. It is obtained by assuming that the recorded electrophysiological function represents a convolution of the amplitude envelope of a continuous speech stream with the to-be-estimated impulse response. We present examples of these responses using both scalp and intracranially recorded human EEG, which were obtained while subjects listened to a binaurally presented recording of a male speaker reading naturally from a classic work of fiction. This method expands the arsenal of stimulation types that can now be effectively used to derive auditory evoked responses and allows for the use of considerably more ecologically valid stimulation parameters. Some implications for future research efforts are presented.},
  langid = {english},
  keywords = {auditory evoked potential,EEG,impulse response,speech},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-9568.2009.07055.x},
  file = {/Users/xzfang/Zotero/storage/RIQF3J6K/Lalor and Foxe - 2010 - Neural responses to uninterrupted natural speech c.pdf;/Users/xzfang/Zotero/storage/AYZLI7LB/j.1460-9568.2009.07055.html}
}

@article{lam_repetition_2014,
  title = {Repetition Reduction: {{Lexical}} Repetition in the Absence of Referent Repetition},
  shorttitle = {Repetition Reduction},
  author = {Lam, Tuan Q. and Watson, Duane G.},
  year = {2014},
  month = may,
  journal = {Journal of experimental psychology. Learning, memory, and cognition},
  volume = {40},
  number = {3},
  pages = {829--843},
  issn = {0278-7393},
  doi = {10.1037/a0035780},
  abstract = {Repeated words are produced with reduced acoustic prominence compared to words that are new to a discourse. Although these effects are often attributed to priming in the production system, the locus of the effect within the production system remains unresolved because in natural speech, repetition often involves repetition of referents and lexical items simultaneously. Therefore, repetition reduction could be due to repeated mention of a referent, or repetition of a word or referring expression. In our study, we test whether repetition reduction is due to repetition of lexical items or repeated mention of referents using an event description task. The results show that repeated lexical items lead to reduced duration and intensity even in the absence of referent repetition whereas repeated referents lead to reduced intensity alone. The general pattern suggests that repetition reduction is due most strongly to repetition of the lexical item, rather than repeated mention of the referent.},
  pmcid = {PMC4104601},
  pmid = {24548320},
  file = {/Users/xzfang/Zotero/storage/TN2PI8WT/Lam and Watson - 2014 - Repetition reduction Lexical repetition in the ab.pdf}
}

@article{landau_learning_2020,
  title = {Learning {{Simple Spatial Terms}}: {{Core}} and {{More}}},
  shorttitle = {Learning {{Simple Spatial Terms}}},
  author = {Landau, Barbara},
  year = {2020},
  journal = {Topics in Cognitive Science},
  volume = {12},
  number = {1},
  pages = {91--114},
  issn = {1756-8765},
  doi = {10.1111/tops.12394},
  abstract = {How do children learn the meanings of simple spatial prepositions like in and on? In this paper, I argue that children come to spatial term learning with an a priori conceptual distinction between core versus non-core concepts of containment and support, and that they learn how language maps onto this distinction by considering both the simple prepositions and the company they keep\textemdash that is, the distributions of their co-occurrences with particular verbs. Core types of containment and support are largely expressed by in/on together with the light verb BE; non-core types are expressed by lexical verbs such as insert, hang, stick, and so on, which represent the specific mechanical means by which containment or support is achieved. These latter types arguably depend on extensive learning about the particular mechanisms of containment and support, many of which are invented by humans, as well as learning the specific lexical verbs that encode these mechanisms. The core versus non-core distinction is reflected in young children's and adults' linguistic descriptions of different spatial configurations, via different distributions of expression types across different configurations. Differences between children and adults are not likely to be rooted in either conceptual or semantic differences, but rather, in the probabilistic nature of available expressions, along with early limits on children's vocabulary of lexical verbs that express complex mechanical relationships between objects.},
  copyright = {\textcopyright{} 2018 Cognitive Science Society, Inc},
  langid = {english},
  keywords = {Containment,Language learning,Spatial language,Support,Syntax and semantics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12394},
  file = {/Users/xzfang/Zotero/storage/AI68888E/Landau - 2020 - Learning Simple Spatial Terms Core and More.pdf;/Users/xzfang/Zotero/storage/HM4DJKAW/tops.html}
}

@techreport{landemard_distinct_2020,
  type = {Preprint},
  title = {Distinct Higher-Order Representations of Natural Sounds in Human and Ferret Auditory Cortex},
  author = {Landemard, Agn{\`e}s and Bimbard, C{\'e}lian and Demen{\'e}, Charlie and Shamma, Shihab and {Norman-Haignere}, Sam and Boubenec, Yves},
  year = {2020},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.30.321695},
  abstract = {Abstract           Little is known about how neural representations of natural sounds differ across species. For example, speech and music play a unique role in human hearing, yet it is unclear how auditory representations of speech and music differ between humans and other animals. Using functional Ultrasound imaging, we measured responses in ferrets to a set of natural and spectrotemporally-matched synthetic sounds previously tested in humans. Ferrets showed similar lower-level frequency and modulation tuning to that observed in humans. But while humans showed prominent selectivity for natural vs. synthetic speech and music in non-primary regions, ferret responses to natural and synthetic sounds were closely matched throughout primary and non-primary auditory cortex, even when tested with ferret vocalizations. This finding reveals that auditory representations in humans and ferrets diverge sharply at late stages of cortical processing, potentially driven by higher-order processing demands in speech and music.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QGL2QI6K/Landemard et al. - 2020 - Distinct higher-order representations of natural s.pdf}
}

@article{landemard_distinct_2021,
  title = {Distinct Higher-Order Representations of Natural Sounds in Human and Ferret Auditory Cortex},
  author = {Landemard, Agn{\`e}s and Bimbard, C{\'e}lian and Demen{\'e}, Charlie and Shamma, Shihab and {Norman-Haignere}, Sam and Boubenec, Yves},
  editor = {Groh, Jennifer M and King, Andrew J and Cogan, Greg and Overath, Tobias},
  year = {2021},
  month = nov,
  journal = {eLife},
  volume = {10},
  pages = {e65566},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.65566},
  abstract = {Little is known about how neural representations of natural sounds differ across species. For example, speech and music play a unique role in human hearing, yet it is unclear how auditory representations of speech and music differ between humans and other animals. Using functional ultrasound imaging, we measured responses in ferrets to a set of natural and spectrotemporally matched synthetic sounds previously tested in humans. Ferrets showed similar lower-level frequency and modulation tuning to that observed in humans. But while humans showed substantially larger responses to natural vs. synthetic speech and music in non-primary regions, ferret responses to natural and synthetic sounds were closely matched throughout primary and non-primary auditory cortex, even when tested with ferret vocalizations. This finding reveals that auditory representations in humans and ferrets diverge sharply at late stages of cortical processing, potentially driven by higher-order processing demands in speech and music.},
  keywords = {auditory cortex,functional ultrasound imaging,natural sounds,sensory coding,vocalizations},
  file = {/Users/xzfang/Zotero/storage/UKNU5WB8/Landemard et al. - 2021 - Distinct higher-order representations of natural s.pdf}
}

@article{landi_fast_2021,
  title = {A Fast Link between Face Perception and Memory in the Temporal Pole},
  author = {Landi, Sofia M. and Viswanathan, Pooja and Serene, Stephen and Freiwald, Winrich A.},
  year = {2021},
  month = jul,
  journal = {Science},
  volume = {373},
  number = {6554},
  pages = {581--585},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abi6671},
  abstract = {What makes familiar faces so special? Explicit semantic information in the brain is generated by gradually stripping off the specific context in which the item is embedded. A particularly striking example of such explicit representations are face-specific neurons. Landi et al. report the properties of neurons in a small region of the monkey anterior temporal cortex that respond to the sight of familiar faces. These cells respond to the internal features of familiar faces but not unknown faces. Some of these responses are very highly selective, reliably responding to only one face out of a vast number of other stimuli. These findings will advance our understanding about where and how semantic memories are stored in the brain. Science, abi6671, this issue p. 581 The question of how the brain recognizes the faces of familiar individuals has been important throughout the history of neuroscience. Cells linking visual processing to person memory have been proposed but not found. Here, we report the discovery of such cells through recordings from an area in the macaque temporal pole identified with functional magnetic resonance imaging. These cells responded to faces that were personally familiar. They responded nonlinearly to stepwise changes in face visibility and detail and holistically to face parts, reflecting key signatures of familiar face recognition. They discriminated between familiar identities, as fast as a general face identity area. The discovery of these cells establishes a new pathway for the fast recognition of familiar individuals. The temporal pole region of the brain temporal lobe contains neurons that link face perception to person memory. The temporal pole region of the brain temporal lobe contains neurons that link face perception to person memory.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. https://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {34210891},
  file = {/Users/xzfang/Zotero/storage/9GPXGVVG/Landi et al. - 2021 - A fast link between face perception and memory in .pdf;/Users/xzfang/Zotero/storage/IQYXD7MU/581.html}
}

@article{lane_visual_2015,
  title = {"{{Visual}}" {{Cortex}} of {{Congenitally Blind Adults Responds}} to {{Syntactic Movement}}},
  author = {Lane, C. and Kanjlia, S. and Omaki, A. and Bedny, M.},
  year = {2015},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {35},
  number = {37},
  pages = {12859--12868},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1256-15.2015},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3Q58HPDC/Lane et al. - 2015 - Visual Cortex of Congenitally Blind Adults Respo.pdf}
}

@article{lang_bayesian_2004,
  title = {Bayesian {{P-Splines}}},
  author = {Lang, Stefan and Brezger, Andreas},
  year = {2004},
  month = mar,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {13},
  number = {1},
  pages = {183--212},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/1061860043010},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3J3KUSEX/Lang and Brezger - 2004 - Bayesian P-Splines.pdf}
}

@article{lange_early_2003,
  title = {Early Processing Stages Are Modulated When Auditory Stimuli Are Presented at an Attended Moment in Time: {{An}} Event-Related Potential Study},
  shorttitle = {Early Processing Stages Are Modulated When Auditory Stimuli Are Presented at an Attended Moment in Time},
  author = {Lange, Kathrin and R{\"o}sler, Frank and R{\"o}der, Brigitte},
  year = {2003},
  journal = {Psychophysiology},
  volume = {40},
  number = {5},
  pages = {806--817},
  issn = {1469-8986},
  doi = {10.1111/1469-8986.00081},
  abstract = {The present study investigated with event-related potentials whether attending to a moment in time modulates the processing of auditory stimuli at a similar early, perceptual level as attending to a location in space. The participants listened to short (600 ms) and long (1,200 ms) intervals marked by white noise bursts. The task was to attend in alternating runs either to the short or to the long intervals and to respond to rare offset markers that differed in intensity from the frequent standard offset markers. Prior to the to-be-attended moment, a slow negative potential developed over the frontal scalp. Stimuli presented at the attended compared to the unattended moments in time elicited an enhanced N1 and an enhanced posteriorly distributed positivity (300\textendash 370 ms). The results show that attention can be flexibly controlled in time and that not only late but also early perceptual processing stages are modulated by attending to a moment in time.},
  langid = {english},
  keywords = {Attention,Event-related potential,N1,Slow potential,Time},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1469-8986.00081},
  file = {/Users/xzfang/Zotero/storage/GPQN8US4/Lange et al. - 2003 - Early processing stages are modulated when auditor.pdf;/Users/xzfang/Zotero/storage/LY7ATGKF/1469-8986.html}
}

@article{langlois_serial_2021,
  title = {Serial Reproduction Reveals the Geometry of Visuospatial Representations},
  author = {Langlois, Thomas A. and Jacoby, Nori and Suchow, Jordan W. and Griffiths, Thomas L.},
  year = {2021},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {13},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2012938118},
  abstract = {An essential function of the human visual system is to locate objects in space and navigate the environment. Due to limited resources, the visual system achieves this by combining imperfect sensory information with a belief state about locations in a scene, resulting in systematic distortions and biases. These biases can be captured by a Bayesian model in which internal beliefs are expressed in a prior probability distribution over locations in a scene. We introduce a paradigm that enables us to measure these priors by iterating a simple memory task where the response of one participant becomes the stimulus for the next. This approach reveals an unprecedented richness and level of detail in these priors, suggesting a different way to think about biases in spatial memory. A prior distribution on locations in a visual scene can reflect the selective allocation of coding resources to different visual regions during encoding (``efficient encoding''). This selective allocation predicts that locations in the scene will be encoded with variable precision, in contrast to previous work that has assumed fixed encoding precision regardless of location. We demonstrate that perceptual biases covary with variations in discrimination accuracy, a finding that is aligned with simulations of our efficient encoding model but not the traditional fixed encoding view. This work demonstrates the promise of using nonparametric data-driven approaches that combine crowdsourcing with the careful curation of information transmission within social networks to reveal the hidden structure of shared visual representations.},
  chapter = {Social Sciences},
  copyright = {Copyright \textcopyright{} 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {33771919},
  keywords = {Bayesian statistics,iterated learning,spatial memory,visual perception},
  file = {/Users/xzfang/Zotero/storage/X924AGED/Langlois et al. - 2021 - Serial reproduction reveals the geometry of visuos.pdf;/Users/xzfang/Zotero/storage/VBY48EDM/e2012938118.html}
}

@article{lanning_natural_2020,
  title = {Natural Music Context Biases Musical Instrument Categorization},
  author = {Lanning, Joshua M. and Stilp, Christian},
  year = {2020},
  month = jul,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {5},
  pages = {2209--2214},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-020-01980-w},
  abstract = {Perception of sounds occurs in the context of surrounding sounds. When spectral properties differ between earlier (context) and later (target) sounds, categorization of later sounds becomes biased through spectral contrast effects (SCEs). Past research has shown SCEs to bias categorization of speech and music alike. Recent studies have extended SCEs to naturalistic listening conditions when the inherent spectral composition of (unfiltered) sentences biased speech categorization. Here, we tested whether natural (unfiltered) music would similarly bias categorization of French horn and tenor saxophone targets. Preceding contexts were either solo performances of the French horn or tenor saxophone (unfiltered; 1 second duration in Experiment 1, or 3 seconds duration in Experiment 2) or a string quintet processed to emphasize frequencies in the horn or saxophone (filtered; 1 second duration). Both approaches produced SCEs, producing more ``saxophone'' responses following horn / horn-like contexts and vice versa. One-second filtered contexts produced SCEs as in previous studies, but 1-second unfiltered contexts did not. Three-second unfiltered contexts biased perception, but to a lesser degree than filtered contexts did. These results extend SCEs in musical instrument categorization to everyday listening conditions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/88CW7QAB/Lanning and Stilp - 2020 - Natural music context biases musical instrument ca.pdf}
}

@article{laszlo_beautiful_2009,
  title = {A Beautiful Day in the Neighborhood: {{An}} Event-Related Potential Study of Lexical Relationships and Prediction in Context},
  shorttitle = {A Beautiful Day in the Neighborhood},
  author = {Laszlo, Sarah and Federmeier, Kara D.},
  year = {2009},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {61},
  number = {3},
  pages = {326--338},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2009.06.004},
  abstract = {Two related questions critical to understanding the predictive processes that come online during sentence comprehension are (1) what information is included in the representation created through prediction and (2) at what functional stage does top-down, predicted information begin to affect bottom-up word processing? We investigated these questions by recording event-related potentials (ERPs) as participants read sentences that ended with expected words or with unexpected items (words, pseudowords, or illegal strings) that were either orthographically unrelated to the expected word or were one of its orthographic neighbors. The data show that, regardless of lexical status, attempts at semantic access (N400) for orthographic neighbors of expected words are facilitated relative to the processing of orthographically unrelated items. Our findings support a view of sentence processing wherein orthographically organized information is brought online by prediction and interacts with input prior to any filter on lexical status.},
  langid = {english},
  keywords = {ERPs,N400,Orthographic neighborhood,Sentence comprehension},
  file = {/Users/xzfang/Zotero/storage/49DF8S2F/Laszlo and Federmeier - 2009 - A beautiful day in the neighborhood An event-rela.pdf;/Users/xzfang/Zotero/storage/PFBMBCUP/S0749596X09000692.html}
}

@article{laszlo_neurally_2012,
  title = {A Neurally Plausible {{Parallel Distributed Processing}} Model of {{Event-Related Potential}} Word Reading Data},
  author = {Laszlo, Sarah and Plaut, David C.},
  year = {2012},
  month = mar,
  journal = {Brain and Language},
  volume = {120},
  number = {3},
  pages = {271--281},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2011.09.001},
  abstract = {The Parallel Distributed Processing (PDP) framework has significant potential for producing models of cognitive tasks that approximate how the brain performs the same tasks. To date, however, there has been relatively little contact between PDP modeling and data from cognitive neuroscience. In an attempt to advance the relationship between explicit, computational models and physiological data collected during the performance of cognitive tasks, we developed a PDP model of visual word recognition which simulates key results from the ERP reading literature, while simultaneously being able to successfully perform lexical decision\textemdash a benchmark task for reading models. Simulations reveal that the model's success depends on the implementation of several neurally plausible features in its architecture which are sufficiently domain-general to be relevant to cognitive modeling more generally.},
  langid = {english},
  keywords = {Computational modeling,Event-Related Potentials,N400,Parallel Distributed Processing,Visual word recognition},
  file = {/Users/xzfang/Zotero/storage/5QWNE9SD/Laszlo and Plaut - 2012 - A neurally plausible Parallel Distributed Processi.pdf;/Users/xzfang/Zotero/storage/M25WVLNE/S0093934X11001544.html}
}

@article{lau_dissociating_2013,
  title = {Dissociating {{N400 Effects}} of {{Prediction}} from {{Association}} in {{Single-word Contexts}}},
  author = {Lau, Ellen F. and Holcomb, Phillip J. and Kuperberg, Gina R.},
  year = {2013},
  month = mar,
  journal = {Journal of Cognitive Neuroscience},
  volume = {25},
  number = {3},
  pages = {484--502},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_00328},
  abstract = {When a word is preceded by a supportive context such as a semantically associated word or a strongly constraining sentence frame, the N400 component of the ERP is reduced in amplitude. An ongoing debate is the degree to which this reduction reflects a passive spread of activation across long-term semantic memory representations as opposed to specific predictions about upcoming input. We addressed this question by embedding semantically associated prime\textendash target pairs within an experimental context that encouraged prediction to a greater or lesser degree. The proportion of related items was used to manipulate the predictive validity of the prime for the target while holding semantic association constant. A semantic category probe detection task was used to encourage semantic processing and to preclude the need for a motor response on the trials of interest. A larger N400 reduction to associated targets was observed in the high than the low relatedness proportion condition, consistent with the hypothesis that predictions about upcoming stimuli make a substantial contribution to the N400 effect. We also observed an earlier priming effect (205\textendash 240 msec) in the high-proportion condition, which may reflect facilitation because of form-based prediction. In summary, the results suggest that predictability modulates N400 amplitude to a greater degree than the semantic content of the context.},
  file = {/Users/xzfang/Zotero/storage/FN929LSH/Lau et al. - 2013 - Dissociating N400 Effects of Prediction from Assoc.pdf;/Users/xzfang/Zotero/storage/5WCSYYVT/Dissociating-N400-Effects-of-Prediction-from.html}
}

@article{lavan_flexible_2019,
  title = {Flexible Voices: {{Identity}} Perception from Variable Vocal Signals},
  shorttitle = {Flexible Voices},
  author = {Lavan, Nadine and Burton, A. Mike and Scott, Sophie K. and McGettigan, Carolyn},
  year = {2019},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {26},
  number = {1},
  pages = {90--102},
  issn = {1531-5320},
  doi = {10.3758/s13423-018-1497-7},
  abstract = {Human voices are extremely variable: The same person can sound very different depending on whether they are speaking, laughing, shouting or whispering. In order to successfully recognise someone from their voice, a listener needs to be able to generalize across these different vocal signals (`telling people together'). However, in most studies of voice-identity processing to date, the substantial within-person variability has been eliminated through the use of highly controlled stimuli, thus focussing on how we tell people apart. We argue that this obscures our understanding of voice-identity processing by controlling away an essential feature of vocal stimuli that may include diagnostic information. In this paper, we propose that we need to extend the focus of voice-identity research to account for both ``telling people together'' as well as ``telling people apart.'' That is, we must account for whether, and to what extent, listeners can overcome within-person variability to obtain a stable percept of person identity from vocal cues. To do this, our theoretical and methodological frameworks need to be adjusted to explicitly include the study of within-person variability.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Y955PEFT/Lavan et al. - 2019 - Flexible voices Identity perception from variable.pdf}
}

@article{lebel_falsifiability_2017,
  title = {Falsifiability Is Not Optional},
  author = {LeBel, Etienne P. and Berger, Derek and Campbell, Lorne and Loving, Timothy J.},
  year = {2017},
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {2},
  pages = {254--261},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1315},
  doi = {10.1037/pspi0000106},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 113(5) of Journal of Personality and Social Psychology (see record 2017-45261-001). In the reply, there were two errors in the References list. The publishing year for the 14th and 21st articles was cited incorrectly as 2016. The in-text acronym associated with these citations should read instead as FER2017 and LCL2017. The correct References list citations should read as follows, respectively: Finkel, E. J., Eastwick, P. W., \& Reis, H. T. (2017). Replicability and other features of a high-quality science: Toward a balanced and empirical approach. Journal of Personality and Social Psychology, 113, 244\textendash 253. http://dx.doi.org/10.1037/pspi0000075 LeBel, E. P., Campbell, L., \& Loving, T. J. (2017). Benefits of open and high-powered research outweigh costs. Journal of Personality and Social Psychology, 113, 230\textendash 243. http://dx.doi.org/10 .1037/pspi0000049. The online version of this article has been corrected.] Finkel, Eastwick, and Reis (2016; FER2016) argued the post-2011 methodological reform movement has focused narrowly on replicability, neglecting other essential goals of research. We agree multiple scientific goals are essential, but argue, however, a more fine-grained language, conceptualization, and approach to replication is needed to accomplish these goals. Replication is the general empirical mechanism for testing and falsifying theory. Sufficiently methodologically similar replications, also known as direct replications, test the basic existence of phenomena and ensure cumulative progress is possible a priori. In contrast, increasingly methodologically dissimilar replications, also known as conceptual replications, test the relevance of auxiliary hypotheses (e.g., manipulation and measurement issues, contextual factors) required to productively investigate validity and generalizability. Without prioritizing replicability, a field is not empirically falsifiable. We also disagree with FER2016's position that ``bigger samples are generally better, but . . . that very large samples could have the downside of commandeering resources that would have been better invested in other studies'' (abstract). We identify problematic assumptions involved in FER2016's modifications of our original research-economic model, and present an improved model that quantifies when (and whether) it is reasonable to worry that increasing statistical power will engender potential trade-offs. Sufficiently powering studies (i.e., {$>$}80\%) maximizes both research efficiency and confidence in the literature (research quality). Given that we are in agreement with FER2016 on all key open science points, we are eager to start seeing the accelerated rate of cumulative knowledge development of social psychological phenomena such a sufficiently transparent, powered, and falsifiable approach will generate. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Experimental Design,Experimental Methods,Experimental Replication,Social Psychology,Statistical Power,Test Validity},
  file = {/Users/xzfang/Zotero/storage/QKWS6CRD/LeBel et al. - 2017 - Falsifiability is not optional.pdf;/Users/xzfang/Zotero/storage/V2AW5HXS/2017-30567-003.html}
}

@article{leckey_p3b_2020,
  title = {The {{P3b}} and {{P600}}(s): {{Positive}} Contributions to Language Comprehension},
  shorttitle = {The {{P3b}} and {{P600}}(s)},
  author = {Leckey, Michelle and Federmeier, Kara D.},
  year = {2020},
  journal = {Psychophysiology},
  volume = {57},
  number = {7},
  pages = {e13351},
  issn = {1469-8986},
  doi = {10.1111/psyp.13351},
  abstract = {Since its discovery in the 1960s, the P300 has been contributing both directly and indirectly to language research. Perhaps most notably, it has been suggested that the P600, an ERP component that was first characterized in the context of syntactic processing, could be a variant of the P3b subcomponent of the P300. Here, we review studies on both sides of the debate. We also review the ``semantic P600,'' a positivity with a similar time course and distribution to the P600 seen for syntactic manipulations but that is obtained in response to some types of semantic anomalies. Because most current theories of the P600 try to account for both the syntactic and the semantic variant, linking the syntactic P600 to the P3b might also imply a similar link for the semantic P600. However, we describe emerging research in our lab that casts doubt on the idea that the syntactic P600 and the semantic P600 are the same effect. We argue that grouping ERP responses primarily by domain (language vs. nonlanguage) is likely to be misleading and suggest alternative ways of determining whether ERP effects reflect similar or different processing mechanisms.},
  langid = {english},
  keywords = {ERPs,language/speech,P3b,P600},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.13351},
  file = {/Users/xzfang/Zotero/storage/CX3MJZTI/psyp.html}
}

@article{lecun_handwritten_,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  author = {LeCun, Yann and Boser, Bernhard E and Denker, John S and Henderson, Donnie and Howard, R E and Hubbard, Wayne E and Jackel, Lawrence D},
  pages = {9},
  abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1\% error rate and about a 9\% reject rate on zipcode digits provided by the U.S. Postal Service.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8SACZWSJ/LeCun et al. - Handwritten Digit Recognition with a Back-Propagat.pdf}
}

@article{lee_anticipation_2021,
  title = {Anticipation of Temporally Structured Events in the Brain},
  author = {Lee, Caroline S and Aly, Mariam and Baldassano, Christopher},
  editor = {Peelen, Marius V},
  year = {2021},
  month = apr,
  journal = {eLife},
  volume = {10},
  pages = {e64972},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.64972},
  abstract = {Learning about temporal structure is adaptive because it enables the generation of expectations. We examined how the brain uses experience in structured environments to anticipate upcoming events. During fMRI, individuals watched a 90-second movie clip six times. Using a Hidden Markov Model applied to searchlights across the whole brain, we identified temporal shifts between activity patterns evoked by the first vs. repeated viewings of the movie clip. In many regions throughout the cortex, neural activity patterns for repeated viewings shifted to precede those of initial viewing by up to 15 seconds. This anticipation varied hierarchically in a posterior (less anticipation) to anterior (more anticipation) fashion. We also identified specific regions in which the timing of the brain's event boundaries were related to those of human-labeled event boundaries, with the timing of this relationship shifting on repeated viewings. With repeated viewing, the brain's event boundaries came to precede human-annotated boundaries by 1-4 seconds on average. Together, these results demonstrate a hierarchy of anticipatory signals in the human brain and link them to subjective experiences of events.},
  file = {/Users/xzfang/Zotero/storage/5VPVT8E2/Lee et al. - 2021 - Anticipation of temporally structured events in th.pdf}
}

@article{lee_implicit_2022,
  title = {Implicit and Explicit Learning in Talker Identification},
  author = {Lee, Jayden J. and Perrachione, Tyler K.},
  year = {2022},
  month = may,
  journal = {Attention, Perception \& Psychophysics},
  issn = {1943-393X},
  doi = {10.3758/s13414-022-02500-8},
  abstract = {In the real world, listeners seem to implicitly learn talkers' vocal identities during interactions that prioritize attending to the content of talkers' speech. In contrast, most laboratory experiments of talker identification employ training paradigms that require listeners to explicitly practice identifying voices. Here, we investigated whether listeners become familiar with talkers' vocal identities during initial exposures that do not involve explicit talker identification. Participants were assigned to one of three exposure tasks, in which they heard identical stimuli but were differentially required to attend to the talkers' vocal identity or to the verbal content of their speech: (1) matching the talker to a concurrent visual cue (talker-matching); (2) discriminating whether the talker was the same as the prior trial (talker 1-back); or (3) discriminating whether speech content matched the previous trial (verbal 1-back). All participants were then tested on their ability to learn to identify talkers from novel speech content. Critically, we manipulated whether the talkers during this post-test differed from those heard during training. Compared to learning to identify novel talkers, listeners were significantly more accurate learning to identify the talkers they had previously been exposed to in the talker-matching and verbal 1-back tasks, but not the talker 1-back task. The correlation between talker identification test performance and exposure task performance was also greater when the talkers were the same in both tasks. These results suggest that listeners learn talkers' vocal identity implicitly during speech perception, even if they are not explicitly attending to the talkers' identity.},
  langid = {english},
  pmid = {35534783},
  keywords = {Attention in learning,Perceptual categorization and identification,Psycholinguistics},
  file = {/Users/xzfang/Zotero/storage/JPFPKEXU/Lee and Perrachione - 2022 - Implicit and explicit learning in talker identific.pdf}
}

@article{legge_mr_1997,
  title = {Mr. {{Chips}}: {{An}} Ideal-Observer Model of Reading.},
  shorttitle = {Mr. {{Chips}}},
  author = {Legge, Gordon E. and Klitz, Timothy S. and Tjan, Bosco S.},
  year = {1997},
  journal = {Psychological Review},
  volume = {104},
  number = {3},
  pages = {524},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.104.3.524},
  file = {/Users/xzfang/Zotero/storage/7L2J3A3F/Legge et al. - 1997 - Mr. Chips An ideal-observer model of reading..pdf;/Users/xzfang/Zotero/storage/C74JCFU4/1997-04806-004.html}
}

@article{leibo_viewtolerant_2017,
  title = {View-{{Tolerant Face Recognition}} and {{Hebbian Learning Imply Mirror-Symmetric Neural Tuning}} to {{Head Orientation}}},
  author = {Leibo, Joel Z. and Liao, Qianli and Anselmi, Fabio and Freiwald, Winrich A. and Poggio, Tomaso},
  year = {2017},
  month = jan,
  journal = {Current Biology},
  volume = {27},
  number = {1},
  pages = {62--67},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.10.015},
  abstract = {The primate brain contains a hierarchy of visual areas, dubbed the ventral stream, which rapidly computes object representations that are both specific for object identity and robust against identity-preserving transformations, like depth rotations [1, 2]. Current computational models of object recognition, including recent deep-learning networks, generate these properties through a hierarchy of alternating selectivity-increasing filtering and tolerance-increasing pooling operations, similar to simple-complex cells operations [3\textendash 6]. Here, we prove that a class of hierarchical architectures and a broad set of biologically plausible learning rules generate approximate invariance to identity-preserving transformations at the top level of the processing hierarchy. However, all past models tested failed to reproduce the most salient property of an intermediate representation of a three-level face-processing hierarchy in the brain: mirror-symmetric tuning to head orientation [7]. Here, we demonstrate that one specific biologically plausible Hebb-type learning rule generates mirror-symmetric tuning to bilaterally symmetric stimuli, like faces, at intermediate levels of the architecture and show why it does so. Thus, the tuning properties of individual cells inside the visual stream appear to result from group properties of the stimuli they encode and to reflect the learning rules that sculpted the information-processing system within which they reside.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2T5295Y7/Leibo et al. - 2017 - View-Tolerant Face Recognition and Hebbian Learnin.pdf}
}

@article{leonard_perceptual_2016,
  title = {Perceptual Restoration of Masked Speech in Human Cortex},
  author = {Leonard, Matthew K. and Baud, Maxime O. and Sjerps, Matthias J. and Chang, Edward F.},
  year = {2016},
  month = dec,
  journal = {Nature Communications},
  volume = {7},
  number = {1},
  pages = {13619},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/ncomms13619},
  abstract = {Humans are adept at understanding speech despite the fact that our natural listening environment is often filled with interference. An example of this capacity is phoneme restoration, in which part of a word is completely replaced by noise, yet listeners report hearing the whole word. The neurological basis for this unconscious fill-in phenomenon is unknown, despite being a fundamental characteristic of human hearing. Here, using direct cortical recordings in humans, we demonstrate that missing speech is restored at the acoustic-phonetic level in bilateral auditory cortex, in real-time. This restoration is preceded by specific neural activity patterns in a separate language area, left frontal cortex, which predicts the word that participants later report hearing. These results demonstrate that during speech perception, missing acoustic content is synthesized online from the integration of incoming sensory cues and the internal neural dynamics that bias word-level expectation and prediction.},
  copyright = {2016 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cortex;Language Subject\_term\_id: cortex;language},
  file = {/Users/xzfang/Zotero/storage/92H52JSE/Leonard et al. - 2016 - Perceptual restoration of masked speech in human c.pdf;/Users/xzfang/Zotero/storage/2BPAUKFQ/ncomms13619.html}
}

@article{leopold_normbased_2006,
  title = {Norm-Based Face Encoding by Single Neurons in the Monkey Inferotemporal Cortex},
  author = {Leopold, David A. and Bondar, Igor V. and Giese, Martin A.},
  year = {2006},
  month = aug,
  journal = {Nature},
  volume = {442},
  number = {7102},
  pages = {572--575},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature04951},
  abstract = {More than 30 years ago, the discovery of face-selective neurons was reported in the inferotemporal cortex of monkeys. Despite dozens of subsequent reports of such 'face cells', and a general assumption that such neurons were likely to be involved in face recognition, little has been done since to compare spiking responses with the vast human psychophysical literature on face perception. But now a new set of experiments links face-selective neurons in the monkey inferotemporal cortex with a well-established psychological dimension of face perception. The study, in macaque monkeys trained to recognize computer-generated human faces, showed that the monkeys recognized a face by comparison to an average or 'norm' stored in their brains, not by memorizing what every individual looks like. A similar mechanism operating in humans might explain how we can recognize a face in a fraction of a second.},
  copyright = {2006 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9C4XFN6W/Leopold et al. - 2006 - Norm-based face encoding by single neurons in the .pdf;/Users/xzfang/Zotero/storage/L8XYNIEQ/nature04951.html}
}

@article{lerner_topographic_2011,
  title = {Topographic {{Mapping}} of a {{Hierarchy}} of {{Temporal Receptive Windows Using}} a {{Narrated Story}}},
  author = {Lerner, Y. and Honey, C. J. and Silbert, L. J. and Hasson, U.},
  year = {2011},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {31},
  number = {8},
  pages = {2906--2915},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3684-10.2011},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MRXS8E7K/Lerner et al. - 2011 - Topographic Mapping of a Hierarchy of Temporal Rec.pdf}
}

@article{lescroart_human_2019,
  title = {Human {{Scene-Selective Areas Represent 3D Configurations}} of {{Surfaces}}},
  author = {Lescroart, Mark D. and Gallant, Jack L.},
  year = {2019},
  month = jan,
  journal = {Neuron},
  volume = {101},
  number = {1},
  pages = {178-192.e7},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.11.004},
  abstract = {It has been argued that scene-selective areas in the human brain represent both the 3D structure of the local visual environment and low-level 2D features (such as spatial frequency) that provide cues for 3D structure. To evaluate the degree to which each of these hypotheses explains variance in scene-selective areas, we develop an encoding model of 3D scene structure and test it against a model of low-level 2D features. We fit the models to fMRI data recorded while subjects viewed visual scenes. The fit models reveal that scene-selective areas represent the distance to and orientation of large surfaces, at least partly independent of low-level features. Principal component analysis of the model weights reveals that the most important dimensions of 3D structure are distance and openness. Finally, reconstructions of the stimuli based on the model weights demonstrate that our model captures unprecedented detail about the local visual environment from scene-selective areas.},
  langid = {english},
  keywords = {encoding model,fMRI,occipital place area,OPA,parahippocampal place area,PPA,retrosplenial complex,RSC,scene perception,scene representation},
  file = {/Users/xzfang/Zotero/storage/9X98CZIU/Lescroart and Gallant - 2019 - Human Scene-Selective Areas Represent 3D Configura.pdf}
}

@article{lescroart_human_2019a,
  title = {Human {{Scene-Selective Areas Represent 3D Configurations}} of {{Surfaces}}},
  author = {Lescroart, Mark D. and Gallant, Jack L.},
  year = {2019},
  month = jan,
  journal = {Neuron},
  volume = {101},
  number = {1},
  pages = {178-192.e7},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.11.004},
  abstract = {It has been argued that scene-selective areas in the human brain represent both the 3D structure of the local visual environment and low-level 2D features (such as spatial frequency) that provide cues for 3D structure. To evaluate the degree to which each of these hypotheses explains variance in scene-selective areas, we develop an encoding model of 3D scene structure and test it against a model of low-level 2D features. We fit the models to fMRI data recorded while subjects viewed visual scenes. The fit models reveal that scene-selective areas represent the distance to and orientation of large surfaces, at least partly independent of low-level features. Principal component analysis of the model weights reveals that the most important dimensions of 3D structure are distance and openness. Finally, reconstructions of the stimuli based on the model weights demonstrate that our model captures unprecedented detail about the local visual environment from scene-selective areas.},
  langid = {english},
  keywords = {encoding model,fMRI,occipital place area,OPA,parahippocampal place area,PPA,retrosplenial complex,RSC,scene perception,scene representation},
  file = {/Users/xzfang/Zotero/storage/QQGDB6YQ/Lescroart and Gallant - 2019 - Human Scene-Selective Areas Represent 3D Configura.pdf;/Users/xzfang/Zotero/storage/5M5K2BRJ/S0896627318309954.html}
}

@article{lesenfants_predicting_2019,
  title = {Predicting Individual Speech Intelligibility from the Cortical Tracking of Acoustic- and Phonetic-Level Speech Representations},
  author = {Lesenfants, D. and Vanthornhout, J. and Verschueren, E. and Decruy, L. and Francart, T.},
  year = {2019},
  month = sep,
  journal = {Hearing Research},
  volume = {380},
  pages = {1--9},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2019.05.006},
  abstract = {Objective To objectively measure speech intelligibility of individual subjects from the EEG, based on cortical tracking of different representations of speech: low-level acoustical, higher-level discrete, or a combination. To compare each model's prediction of the speech reception threshold (SRT) for each individual with the behaviorally measured SRT. Methods Nineteen participants listened to Flemish Matrix sentences presented at different signal-to-noise ratios (SNRs), corresponding to different levels of speech understanding. For different EEG frequency bands (delta, theta, alpha, beta or low-gamma), a model was built to predict the EEG signal from various speech representations: envelope, spectrogram, phonemes, phonetic features or a combination of phonetic Features and Spectrogram (FS). The same model was used for all subjects. The model predictions were then compared to the actual EEG of each subject for the different SNRs, and the prediction accuracy in function of SNR was used to predict the SRT. Results The model based on the FS speech representation and the theta EEG band yielded the best SRT predictions, with a difference between the behavioral and objective SRT below 1 decibel for 53\% and below 2 decibels for 89\% of the subjects. Conclusion A model including low- and higher-level speech features allows to predict the speech reception threshold from the EEG of people listening to natural speech. It has potential applications in diagnostics of the auditory system.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SHTD9FI3/Lesenfants et al. - 2019 - Predicting individual speech intelligibility from .pdf;/Users/xzfang/Zotero/storage/Y3BUEQIY/S0378595518305306.html}
}

@article{leung_parents_2021,
  title = {Parents {{Fine-Tune Their Speech}} to {{Children}}'s {{Vocabulary Knowledge}}},
  author = {Leung, Ashley and Tunkel, Alexandra and Yurovsky, Daniel},
  year = {2021},
  month = jul,
  journal = {Psychological Science},
  pages = {0956797621993104},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797621993104},
  abstract = {Young children learn language at an incredible rate. Although children come prepared with powerful statistical-learning mechanisms, the statistics they encounter are also prepared for them: Children learn from caregivers motivated to communicate with them. How precisely do parents tune their speech to their children's individual language knowledge? To answer this question, we asked parent\textendash child pairs (N = 41) to play a reference game in which the parents' goal was to guide their child to select a target animal from a set of three. Parents fine-tuned their referring expressions to their children's knowledge at the lexical level, producing more informative references for animals they thought their children did not know. Further, parents learned about their children's knowledge over the course of the game and tuned their referring expressions accordingly. Child-directed speech may thus support children's learning not because it is uniformly simplified but because it is tuned to individual children's language development.},
  langid = {english},
  keywords = {communication,language development,open data,open materials,parentâ€“child interaction},
  file = {/Users/xzfang/Zotero/storage/7XRA53I5/Leung et al. - 2021 - Parents Fine-Tune Their Speech to Childrenâ€™s Vocab.pdf}
}

@article{levi_visual_2011,
  title = {Visual Crowding},
  author = {Levi, Dennis M.},
  year = {2011},
  month = sep,
  journal = {Current Biology},
  volume = {21},
  number = {18},
  pages = {R678-R679},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2011.07.025},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KXAYA4G6/Levi - 2011 - Visual crowding.pdf;/Users/xzfang/Zotero/storage/JTJWIYYS/S0960982211008244.html}
}

@article{levshina_efficiency_2021,
  title = {Efficiency in Human Languages: {{Corpus}} Evidence for Universal Principles},
  shorttitle = {Efficiency in Human Languages},
  author = {Levshina, Natalia and Moran, Steven},
  year = {2021},
  month = may,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {s3},
  publisher = {{De Gruyter Mouton}},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2020-0081},
  abstract = {Over the last few years, there has been a growing interest in communicative efficiency. It has been argued that language users act efficiently, saving effort for processing and articulation, and that language structure and use reflect this tendency. The emergence of new corpus data has brought to life numerous studies on efficient language use in the lexicon, in morphosyntax, and in discourse and phonology in different languages. In this introductory paper, we discuss communicative efficiency in human languages, focusing on evidence of efficient language use found in multilingual corpora. The evidence suggests that efficiency is a universal feature of human language. We provide an overview of different manifestations of efficiency on different levels of language structure, and we discuss the major questions and findings so far, some of which are addressed for the first time in the contributions in this special collection.},
  chapter = {Linguistics Vanguard},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZVEP2CPU/Levshina and Moran - 2021 - Efficiency in human languages Corpus evidence for.pdf;/Users/xzfang/Zotero/storage/RLIP5HE7/html.html}
}

@article{levy_expectationbased_2008,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  year = {2008},
  month = mar,
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159\textendash 166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  langid = {english},
  keywords = {Frequency,Information theory,Parsing,Prediction,Sentence processing,Syntactic complexity,Syntax,Word order},
  file = {/Users/xzfang/Zotero/storage/J3N4DXJR/Levy - 2008 - Expectation-based syntactic comprehension.pdf}
}

@article{levy_eye_2009,
  title = {Eye Movement Evidence That Readers Maintain and Act on Uncertainty about Past Linguistic Input},
  author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
  year = {2009},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {106},
  number = {50},
  pages = {21086--21090},
  issn = {0027-8424},
  doi = {10.1073/pnas.0907664106},
  abstract = {In prevailing approaches to human sentence comprehension, the outcome of the word recognition process is assumed to be a categorical representation with no residual uncertainty. Yet perception is inevitably uncertain, and a system making optimal use of available information might retain this uncertainty and interactively recruit grammatical analysis and subsequent perceptual input to help resolve it. To test for the possibility of such an interaction, we tracked readers' eye movements as they read sentences constructed to vary in (i) whether an early word had near neighbors of a different grammatical category, and (ii) how strongly another word further downstream cohered grammatically with these potential near neighbors. Eye movements indicated that readers maintain uncertain beliefs about previously read word identities, revise these beliefs on the basis of relative grammatical consistency with subsequent input, and use these changing beliefs to guide saccadic behavior in ways consistent with principles of rational probabilistic inference.},
  pmcid = {PMC2795489},
  pmid = {19965371},
  file = {/Users/xzfang/Zotero/storage/FSYWZLN9/Levy et al. - 2009 - Eye movement evidence that readers maintain and ac.pdf}
}

@inproceedings{levy_noisychannel_2008,
  title = {A Noisy-Channel Model of Rational Human Sentence Comprehension under Uncertain Input},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} - {{EMNLP}} '08},
  author = {Levy, Roger},
  year = {2008},
  pages = {234},
  publisher = {{Association for Computational Linguistics}},
  address = {{Honolulu, Hawaii}},
  doi = {10.3115/1613715.1613749},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5Q557ZAX/Levy - 2008 - A noisy-channel model of rational human sentence c.pdf}
}

@article{levy_speakers_2007,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  author = {Levy, Roger and T. Florian, Jaeger},
  year = {2007},
  journal = {Advances in Neural Information Processing Systems 19},
  doi = {10.7551/mitpress/7503.003.0111},
  abstract = {If language users are rational, they might choose to structure their utterances so as to optimize communicative properties. In particular, information-theoretic and psycholinguistic considerations suggest that this may include maximizing the uniformity of information density in an utterance. We investigate this possibility in the context of syntactic reduction, where the speaker has the option of either marking a higher-order unit (a phrase) with an extra word, or leaving it unmarked. We demonstrate that speakers are more likely to reduce less information-dense phrases. In a second step, we combine a stochastic model of structured utterance production with a logistic-regression model of syntactic reduction to study which types of cues speakers employ when estimating the predictability of upcoming elements. We demonstrate that the trend toward predictability-sensitive syntactic reduction (Jaeger, 2006) is robust in the face of a wide variety of control variables, and present evidence that speakers use both surface and structural cues for predictability estimation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/H4TDZDM7/SchÃ¶lkopf et al. - 2007 - Speakers optimize information density through synt.pdf}
}

@article{levy_syntactic_2013,
  title = {The Syntactic Complexity of {{Russian}} Relative Clauses},
  author = {Levy, Roger and Fedorenko, Evelina and Gibson, Edward},
  year = {2013},
  month = nov,
  journal = {Journal of memory and language},
  volume = {69},
  number = {4},
  pages = {461--496},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.10.005},
  abstract = {Although syntactic complexity has been investigated across dozens of studies, the available data still greatly underdetermine relevant theories of processing difficulty. Memory-based and expectation-based theories make opposite predictions regarding fine-grained time course of processing difficulty in syntactically constrained contexts, and each class of theory receives support from results on some constructions in some languages. Here we report four self-paced reading experiments on the online comprehension of Russian relative clauses together with related corpus studies, taking advantage of Russian's flexible word order to disentangle predictions of competing theories. We find support for key predictions of memory-based theories in reading times at RC verbs, and for key predictions of expectation-based theories in processing difficulty at RC-initial accusative noun phrase (NP) objects, which corpus data suggest should be highly unexpected. These results suggest that a complete theory of syntactic complexity must integrate insights from both expectation-based and memory-based theories.},
  pmcid = {PMC3975271},
  pmid = {24711687},
  file = {/Users/xzfang/Zotero/storage/V7CRRY96/Levy et al. - 2013 - The syntactic complexity of Russian relative claus.pdf}
}

@article{lewendon_phonological_2020,
  title = {The {{Phonological Mapping}} ({{Mismatch}}) {{Negativity}}: {{History}}, {{Inconsistency}}, and {{Future Direction}}},
  shorttitle = {The {{Phonological Mapping}} ({{Mismatch}}) {{Negativity}}},
  author = {Lewendon, Jennifer and Mortimore, Laurie and Egan, Ciara},
  year = {2020},
  journal = {Frontiers in Psychology},
  volume = {11},
  pages = {1967},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.01967},
  file = {/Users/xzfang/Zotero/storage/5CRGBY8K/Lewendon et al. - 2020 - The Phonological Mapping (Mismatch) Negativity Hi.pdf}
}

@article{lewis_distributional_2019,
  title = {Distributional Semantics as a Source of Visual Knowledge},
  author = {Lewis, Molly and Zettersten, Martin and Lupyan, Gary},
  year = {2019},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {39},
  pages = {19237--19238},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1910148116},
  abstract = {In PNAS Kim et al. (1) detail congenitally blind individuals' extensive knowledge of the visual appearance of animals. This is exciting and important work speaking directly to long-standing questions about the role of direct perceptual experience in semantic knowledge. Despite lacking visual input, blind people show substantial alignment with one another and with sighted people in judging animal shape, skin texture, size, and, to a much lesser extent, color. Where does this knowledge come from? One possibility, advanced by the authors, is inferential reasoning. Knowing that birds have feathers and that ostriches are birds allows blind people to infer that ostriches have feathers despite never having seen an ostrich (or feathers). Another possibility is the distributional structure of language. The authors reject the ``obvious idea \ldots{} that blind individuals learn from sighted people's verbal descriptions'' on the grounds that the lowest \ldots{}  [{$\carriagereturn$}][1]1To whom correspondence may be addressed. Email: mollyllewis\{at\}gmail.com.  [1]: \#xref-corresp-1-1},
  chapter = {Letter},
  copyright = {\textcopyright{} 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {31488726},
  file = {/Users/xzfang/Zotero/storage/RAP7AKBJ/Lewis et al. - 2019 - Distributional semantics as a source of visual kno.pdf;/Users/xzfang/Zotero/storage/UUN3SY39/19237.html}
}

@article{li_confidence_2020,
  title = {Confidence Reports in Decision-Making with Multiple Alternatives Violate the {{Bayesian}} Confidence Hypothesis},
  author = {Li, Hsin-Hung and Ma, Wei Ji},
  year = {2020},
  month = apr,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {2004},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-15581-6},
  abstract = {Decision confidence reflects our ability to evaluate the quality of decisions and guides subsequent behavior. Experiments on confidence reports have almost exclusively focused on two-alternative decision-making. In this realm, the leading theory is that confidence reflects the probability that a decision is correct (the posterior probability of the chosen option). There is, however, another possibility, namely that people are less confident if the best two options are closer to each other in posterior probability, regardless of how probable they are in absolute terms. This possibility has not previously been considered because in two-alternative decisions, it reduces to the leading theory. Here, we test this alternative theory in a three-alternative visual categorization task. We found that confidence reports are best explained by the difference between the posterior probabilities of the best and the next-best options, rather than by the posterior probability of the chosen (best) option alone, or by the overall uncertainty (entropy) of the posterior distribution. Our results upend the leading notion of decision confidence and instead suggest that confidence reflects the observer's subjective probability that they made the best possible decision.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Decision,Human behaviour},
  file = {/Users/xzfang/Zotero/storage/CTJN8SUT/Li and Ma - 2020 - Confidence reports in decision-making with multipl.pdf;/Users/xzfang/Zotero/storage/C34YPKUD/s41467-020-15581-6.html}
}

@article{li_human_2021,
  title = {Human Cortical Encoding of Pitch in Tonal and Non-Tonal Languages},
  author = {Li, Yuanning and Tang, Claire and Lu, Junfeng and Wu, Jinsong and Chang, Edward F.},
  year = {2021},
  month = feb,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {1161},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-21430-x},
  abstract = {Languages can use a common repertoire of vocal sounds to signify distinct meanings. In tonal languages, such as Mandarin Chinese, pitch contours of syllables distinguish one word from another, whereas in non-tonal languages, such as English, pitch is used to convey intonation. The neural computations underlying language specialization in speech perception are unknown. Here, we use a cross-linguistic approach to address this. Native Mandarin- and English- speaking participants each listened to both Mandarin and English speech, while neural activity was directly recorded from the non-primary auditory cortex. Both groups show language-general coding of speaker-invariant pitch at the single electrode level. At the electrode population level, we find language-specific distribution of cortical tuning parameters in Mandarin speakers only, with enhanced sensitivity to Mandarin tone categories. Our results show that speech perception relies upon a shared cortical auditory feature processing mechanism, which may be tuned to the statistics of a given language.},
  copyright = {2021 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LPTR6FZG/Li et al. - 2021 - Human cortical encoding of pitch in tonal and non-.pdf;/Users/xzfang/Zotero/storage/W76M7RAH/s41467-021-21430-x.html}
}

@article{li_lexfindr_2021,
  title = {{{LexFindR}}: {{A}} Fast, Simple, and Extensible {{R}} Package for Finding Similar Words in a Lexicon},
  author = {Li, ZhaoBin and Crinnion, Anne Marie and Magnuson, James S},
  year = {2021},
  pages = {27},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QGBSWEZF/Li et al. - LexFindR A fast, simple, and extensible R package.pdf}
}

@phdthesis{li_noisy_2021,
  title = {A {{Noisy Channel Model}} of {{N400}} and {{P600 Effects}} in {{Sentence Comprehension}}},
  author = {Li, Jiaxuan},
  year = {2021},
  doi = {10.6082/uchicago.2901},
  abstract = {N400 and P600 event-related potential (ERP) components have long been the object of study in psycholinguistics. Traditional accounts have associated N400 effects with semantic violations, and P600 effects with syntactic violations. However, this picture is complicated by P600 effects\textemdash without N400 effects\textemdash in response to animacy and thematic- role violations, as well as biphasic N400/P600 effects for conventional semantic violations. Building on explanations involving interplay of plausibility- driven and syntax-driven interpretations, we present a computational model that accounts for these complicating observations via a noisy channel modeling framework. Our model assumes early-stage sentence interpretations determined by noisy channel computation (influenced by plausibility), with these heuristic interpretations driving the N400 amplitude. The P600 then reflects a reconciliation with the true (syntax-driven) interpretation, impacted by the extent to which heuristic interpretations deviate from the true input. Running this model on original experimental stimuli, we successfully simulate N400 and P600 effects reported by eight studies in the literature that we examined. To our knowledge this is the first fully-specified computational formalization of plausibility/syntax interplay, the first implemented noisy channel model to carry out direct prediction of both N400 and P600 components},
  langid = {english},
  school = {University of Chicago},
  keywords = {Cognitive modeling,ERPs,N400,Noisy channel,semantic P600,Semantic processing},
  file = {/Users/xzfang/Zotero/storage/5AL4NNXI/Li - 2021 - A Noisy Channel Model of N400 and P600 Effects in .pdf}
}

@article{li_oblique_2003,
  title = {Oblique {{Effect}}: {{A Neural Basis}} in the {{Visual Cortex}}},
  shorttitle = {Oblique {{Effect}}},
  author = {Li, Baowang and Peterson, Matthew R. and Freeman, Ralph D.},
  year = {2003},
  month = jul,
  journal = {Journal of Neurophysiology},
  volume = {90},
  number = {1},
  pages = {204--217},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00954.2002},
  abstract = {The details of oriented visual stimuli are better resolved when they are horizontal or vertical rather than oblique. This ``oblique effect'' has been confirmed in numerous behavioral studies in humans and to some extent in animals. However, investigations of its neural basis have produced mixed and inconclusive results, presumably due in part to limited sample sizes. We have used a database to analyze a population of 4,418 cells in the cat's striate cortex to determine possible differences as a function of orientation. We find that both the numbers of cells and the widths of orientation tuning vary as a function of preferred orientation. Specifically, more cells prefer horizontal and vertical orientations compared with oblique angles. The largest population of cells is activated by orientations close to horizontal. In addition, orientation tuning widths are most narrow for cells preferring horizontal orientations. These findings are most prominent for simple cells tuned to high spatial frequencies. Complex cells and simple cells tuned to low spatial frequencies do not exhibit these anisotropies. For a subset of simple cells from our population (n = 104), we examined the relative contributions of linear and nonlinear mechanisms in shaping orientation tuning curves. We find that linear contributions alone do not account for the narrower tuning widths at horizontal orientations. By modeling simple cells as linear filters followed by static expansive nonlinearities, our analysis indicates that horizontally tuned cells have a greater nonlinear component than those tuned to other orientations. This suggests that intracortical mechanisms play a major role in shaping the oblique effect.},
  file = {/Users/xzfang/Zotero/storage/QSA7IBKH/Li et al. - 2003 - Oblique Effect A Neural Basis in the Visual Corte.pdf}
}

@article{li_perceptual_2004,
  title = {Perceptual Learning and Top-down Influences in Primary Visual Cortex},
  author = {Li, Wu and Pi{\"e}ch, Valentin and Gilbert, Charles D},
  year = {2004},
  month = jun,
  journal = {Nature neuroscience},
  volume = {7},
  number = {6},
  pages = {651--657},
  issn = {1097-6256},
  doi = {10.1038/nn1255},
  abstract = {Neuronal responses at early stages in visual cortical processing, including those in primary visual cortex (V1), are subject to the influences of visual context, experience and attention. Here we show that for monkeys trained in a shape discrimination task, V1 neurons took on novel functional properties related to the attributes of the trained shapes. Furthermore, these properties depended on the perceptual task being performed; neurons responded very differently to an identical visual stimulus under different visual discrimination tasks. These top-down influences were seen from the very beginning and throughout the entire time course of the neural responses. Information theoretic analysis showed that neurons carried more information about a stimulus attribute when the animals were performing a task related to that attribute. Our findings suggest that the output from V1 reflects both sensory and behavioral context.},
  pmcid = {PMC1440483},
  pmid = {15156149},
  file = {/Users/xzfang/Zotero/storage/ZQP9Z9B5/Li et al. - 2004 - Perceptual learning and top-down influences in pri.pdf}
}

@misc{li_tuning_2021,
  title = {Tuning in Scene-Preferring Cortex for Mid-Level Visual Features Gives Rise to Selectivity across Multiple Levels of Stimulus Complexity},
  author = {Li, Shi Pui Donald and Bonner, Michael F.},
  year = {2021},
  month = sep,
  pages = {2021.09.24.461733},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.09.24.461733},
  abstract = {The scene-preferring portion of the human ventral visual stream, known as the parahippocampal place area (PPA), responds to scenes and landmark objects, which tend to be large in real-world size, fixed in location, and inanimate. However, the PPA also exhibits preferences for low-level contour statistics, including rectilinearity and cardinal orientations, that are not directly predicted by theories of scene- and landmark-selectivity. It is unknown whether these divergent findings of both low- and high-level selectivity in the PPA can be explained by a unified computational theory. To address this issue, we fit hierarchical computational models of mid-level tuning to the image-evoked fMRI responses of the PPA, and we performed a series of high-throughput experiments on these models. Our findings show that hierarchical encoding models of the PPA exhibit emergent selectivity across multiple levels of complexity, giving rise to high-level preferences along dimensions of real-world size, fixedness, and naturalness/animacy as well as low-level preferences for rectilinear shapes and cardinal orientations. These results reconcile disparate theories of PPA function in a unified model of mid-level visual representation, and they demonstrate how multifaceted selectivity profiles naturally emerge from the hierarchical computations of visual cortex and the natural statistics of images. SIGNIFICANCE STATEMENT Visual neuroscientists characterize cortical selectivity by identifying stimuli that drive regional responses. A perplexing finding is that many higher-order visual regions exhibit selectivity spanning multiple levels of complexity: they respond to highly complex categories, such as scenes and landmarks, but also to surprisingly simplistic features, such as specific contour orientations. Using large-scale computational analyses and human brain imaging, we show how multifaceted selectivity in scene-preferring cortex can emerge from the coding of mid-level visual features, whose complexity is neither as simple as local contours nor as complex as scenes or objects. Our work reconciles seemingly divergent findings of selectivity in scene-preferring cortex and suggests that mid-level features may be central to understanding the category-selective organization of the human visual system.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/X2EIBEA4/Li and Bonner - 2021 - Tuning in scene-preferring cortex for mid-level vi.pdf;/Users/xzfang/Zotero/storage/PF7RCCPY/2021.09.24.461733v1.html}
}

@article{liebal_infants_2009,
  title = {Infants Use Shared Experience to Interpret Pointing Gestures},
  author = {Liebal, Kristin and Behne, Tanya and Carpenter, Malinda and Tomasello, Michael},
  year = {2009},
  month = mar,
  journal = {Developmental Science},
  volume = {12},
  number = {2},
  pages = {264--271},
  issn = {1363755X, 14677687},
  doi = {10.1111/j.1467-7687.2008.00758.x},
  abstract = {We investigated whether 1-year-old infants use their shared experience with an adult to determine the meaning of a pointing gesture. In the first study, after two adults had each shared a different activity with the infant, one of the adults pointed to a target object. Eighteen- but not 14-month-olds responded appropriately to the pointing gesture based on the particular activity they had previously shared with that particular adult. In the second study, 14-month-olds were successful in a simpler procedure in which the pointing adult either had or had not shared a relevant activity with the infant prior to the pointing. Infants just beginning to learn language thus already show a complex understanding of the pragmatics of cooperative communication in which shared experience with particular individuals plays a crucial role.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FM43F2FW/Liebal et al. - 2009 - Infants use shared experience to interpret pointin.pdf}
}

@article{lievers_linguistic_2021,
  title = {The Linguistic Dimensions of Concrete and Abstract Concepts: Lexical Category, Morphological Structure, Countability, and Etymology},
  shorttitle = {The Linguistic Dimensions of Concrete and Abstract Concepts},
  author = {Lievers, Francesca Strik and Bolognesi, Marianna and Winter, Bodo},
  year = {2021},
  month = oct,
  journal = {Cognitive Linguistics},
  publisher = {{De Gruyter Mouton}},
  issn = {1613-3641},
  doi = {10.1515/cog-2021-0007},
  abstract = {The distinction between abstract and concrete concepts is fundamental to cognitive linguistics and cognitive science. This distinction is commonly operationalized through concreteness ratings based on the aggregated judgments of many people. What is often overlooked in experimental studies using this operationalization is that ratings are attributed to words , not to concepts directly. In this paper we explore the relationship between the linguistic properties of English words and conceptual abstractness/concreteness. Based on hypotheses stated in the existing linguistic literature we select a set of variables (part of speech, morphological structure, countability, etymology) and verify whether they are statistically associated with concreteness ratings. We show that English nouns are rated as more concrete compared to other parts of speech, but mass nouns are rated as less concrete than count nouns. Furthermore, a more complex morphological structure is associated with abstractness, and as for etymology, French- and Latin-derived words are more abstract than words of other origin. This shows that linguistic properties of words are indeed associated with the degree of concreteness that we attribute to the underlying concepts, and we discuss the implications that these findings have for linguistic theory and for empirical investigations in the cognitive sciences.},
  langid = {english},
  keywords = {concreteness,count nouns,etymology,mass nouns,part of speech},
  file = {/Users/xzfang/Zotero/storage/Q9QUTQFV/Lievers et al. - 2021 - The linguistic dimensions of concrete and abstract.pdf}
}

@article{lim_effects_2019,
  title = {Effects of Talker Continuity and Speech Rate on Auditory Working Memory},
  author = {Lim, Sung-Joo and {Shinn-Cunningham}, Barbara G. and Perrachione, Tyler K.},
  year = {2019},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {4},
  pages = {1167--1177},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01684-w},
  abstract = {Speech processing is slower and less accurate when listeners encounter speech from multiple talkers compared to one continuous talker. However, interference from multiple talkers has been investigated only using immediate speech recognition or long-term memory recognition tasks. These tasks reveal opposite effects of speech processing time on speech recognition \textendash{} while fast processing of multi-talker speech impedes immediate recognition, it also results in more abstract and less talker-specific long-term memories for speech. Here, we investigated whether and how processing multi-talker speech disrupts working memory maintenance, an intermediate stage between perceptual recognition and long-term memory. In a digit sequence recall task, listeners encoded seven-digit sequences and recalled them after a 5-s delay. Sequences were spoken by either a single talker or multiple talkers at one of three presentation rates (0-, 200-, and 500-ms inter-digit intervals). Listeners' recall was slower and less accurate for sequences spoken by multiple talkers than a single talker. Especially for the fastest presentation rate, listeners were less efficient when recalling sequences spoken by multiple talkers. Our results reveal that talker-specificity effects for speech working memory are most prominent when listeners must rapidly encode speech. These results suggest that, like immediate speech recognition, working memory for speech is susceptible to interference from variability across talkers. While many studies ascribe effects of talker variability to the need to calibrate perception to talker-specific acoustics, these results are also consistent with the idea that a sudden change of talkers disrupts attentional focus, interfering with efficient working-memory processing.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/X5ZT3FTN/Lim et al. - 2019 - Effects of talker continuity and speech rate on au.pdf}
}

@misc{lim_talker_2021,
  title = {Talker Discontinuity Disrupts Attention to Speech: {{Evidence}} from {{EEG}} and Pupillometry},
  shorttitle = {Talker Discontinuity Disrupts Attention to Speech},
  author = {Lim, Sung-Joo and Carter, Yaminah D. and Njoroge, J. Michelle and {Shinn-Cunningham}, Barbara G. and Perrachione, Tyler K.},
  year = {2021},
  month = jan,
  pages = {2021.01.28.428718},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.01.28.428718},
  abstract = {Speech is processed less efficiently from discontinuous, mixed talkers than one consistent talker, but little is known about the neural mechanisms for processing talker variability. Here, we measured psychophysiological responses to talker variability using electroencephalography (EEG) and pupillometry while listeners performed a delayed recall of digit span task. Listeners heard and recalled seven-digit sequences with both talker (single- vs. mixed-talker digits) and temporal (0- vs. 500-ms inter-digit intervals) discontinuities. Talker discontinuity reduced serial recall accuracy. Both talker and temporal discontinuities elicited P3a-like neural evoked response, while rapid processing of mixed-talkers' speech led to increased phasic pupil dilation. Furthermore, mixed-talkers' speech produced less alpha oscillatory power during working memory maintenance, but not during speech encoding. Overall, these results are consistent with an auditory attention and streaming framework in which talker discontinuity leads to involuntary, stimulus-driven attentional reorientation to novel speech sources, resulting in the processing interference classically associated with talker variability.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TC2Y8VTI/Lim et al. - 2021 - Talker discontinuity disrupts attention to speech.pdf;/Users/xzfang/Zotero/storage/6RQPAS7I/2021.01.28.428718v1.html}
}

@article{lindborg_speechspecific_2019,
  title = {Speech-Specific Audiovisual Integration Modulates Induced Theta-Band Oscillations},
  author = {Lindborg, Alma and Baart, Martijn and Stekelenburg, Jeroen J. and Vroomen, Jean and Andersen, Tobias S.},
  year = {2019},
  month = jul,
  journal = {PLOS ONE},
  volume = {14},
  number = {7},
  pages = {e0219744},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0219744},
  abstract = {Speech perception is influenced by vision through a process of audiovisual integration. This is demonstrated by the McGurk illusion where visual speech (for example /ga/) dubbed with incongruent auditory speech (such as /ba/) leads to a modified auditory percept (/da/). Recent studies have indicated that perception of the incongruent speech stimuli used in McGurk paradigms involves mechanisms of both general and audiovisual speech specific mismatch processing and that general mismatch processing modulates induced theta-band (4\textendash 8 Hz) oscillations. Here, we investigated whether the theta modulation merely reflects mismatch processing or, alternatively, audiovisual integration of speech. We used electroencephalographic recordings from two previously published studies using audiovisual sine-wave speech (SWS), a spectrally degraded speech signal sounding nonsensical to na\"ive perceivers but perceived as speech by informed subjects. Earlier studies have shown that informed, but not na\"ive subjects integrate SWS phonetically with visual speech. In an N1/P2 event-related potential paradigm, we found a significant difference in theta-band activity between informed and na\"ive perceivers of audiovisual speech, suggesting that audiovisual integration modulates induced theta-band oscillations. In a McGurk mismatch negativity paradigm (MMN) where infrequent McGurk stimuli were embedded in a sequence of frequent audio-visually congruent stimuli we found no difference between congruent and McGurk stimuli. The infrequent stimuli in this paradigm are violating both the general prediction of stimulus content, and that of audiovisual congruence. Hence, we found no support for the hypothesis that audiovisual mismatch modulates induced theta-band oscillations. We also did not find any effects of audiovisual integration in the MMN paradigm, possibly due to the experimental design.},
  langid = {english},
  keywords = {Audio signal processing,Electroencephalography,Permutation,Phonology,Sensory perception,Speech,Speech signal processing,Vision},
  file = {/Users/xzfang/Zotero/storage/7L9YT59H/Lindborg et al. - 2019 - Speech-specific audiovisual integration modulates .pdf;/Users/xzfang/Zotero/storage/ETLRTY9C/article.html}
}

@article{lindquist_modeling_2009,
  title = {Modeling the {{Hemodynamic Response Function}} in {{fMRI}}: {{Efficiency}}, {{Bias}} and {{Mis-modeling}}},
  shorttitle = {Modeling the {{Hemodynamic Response Function}} in {{fMRI}}},
  author = {Lindquist, Martin A. and Loh, Ji Meng and Atlas, Lauren Y. and Wager, Tor D.},
  year = {2009},
  month = mar,
  journal = {Neuroimage},
  volume = {45},
  number = {1 Suppl},
  pages = {S187-S198},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2008.10.065},
  abstract = {Most brain research to date has focused on studying the amplitude of evoked fMRI responses, though there has recently been an increased interest in measuring onset, peak latency and duration of the responses as well. A number of modeling procedures provide measures of the latency and duration of fMRI responses. In this work we compare several techniques that vary in their assumptions, model complexity, and interpretation. For each method, we introduce methods for estimating amplitude, peak latency, and duration and for performing inference in a multi-subject fMRI setting. We then assess the techniques' relative sensitivity and their propensity for mis-attributing task effects on one parameter (e.g., duration) to another (e.g., amplitude). Finally, we introduce methods for quantifying model misspecification and assessing bias and power-loss related to the choice of model. Overall, the results show that it is surprisingly difficult to accurately recover true task-evoked changes in BOLD signal and that there are substantial differences among models in terms of power, bias and parameter confusability. Because virtually all fMRI studies in cognitive and affective neuroscience employ these models, the results bear on the interpretation of hemodynamic response estimates across a wide variety of psychological and neuroscientific studies.},
  pmcid = {PMC3318970},
  pmid = {19084070},
  file = {/Users/xzfang/Zotero/storage/TD2XFXNS/Lindquist et al. - 2009 - Modeling the Hemodynamic Response Function in fMRI.pdf}
}

@article{lisman_thetagamma_2013,
  title = {The {{Theta-Gamma Neural Code}}},
  author = {Lisman, John E. and Jensen, Ole},
  year = {2013},
  month = mar,
  journal = {Neuron},
  volume = {77},
  number = {6},
  pages = {1002--1016},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.03.007},
  abstract = {Theta and gamma frequency oscillations occur in the same brain regions and interact with each other, a process called cross-frequency coupling. Here, we review evidence for the following hypothesis: that the dual oscillations form a code for representing multiple items in an ordered way. This form of coding has been most clearly demonstrated in the hippocampus, where different spatial information is represented in different gamma subcycles of a theta cycle. Other experiments have tested the functional importance of oscillations and their coupling. These involve correlation of oscillatory properties with memory states, correlation with memory performance, and effects of disrupting oscillations on memory. Recent work suggests that this coding scheme coordinates communication between brain regions and is involved in sensory as well as memory processes.},
  pmcid = {PMC3648857},
  pmid = {23522038},
  file = {/Users/xzfang/Zotero/storage/5C3F97HS/Lisman and Jensen - 2013 - The Theta-Gamma Neural Code.pdf}
}

@article{liszkowski_12_2006,
  title = {12- and 18-{{Month-Olds Point}} to {{Provide Information}} for {{Others}}},
  author = {Liszkowski, Ulf and Carpenter, Malinda and Striano, Tricia and Tomasello, Michael},
  year = {2006},
  month = apr,
  journal = {Journal of Cognition and Development},
  volume = {7},
  number = {2},
  pages = {173--187},
  issn = {1524-8372, 1532-7647},
  doi = {10.1207/s15327647jcd0702_2},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Y6EUTKAY/Liszkowski et al. - 2006 - 12- and 18-Month-Olds Point to Provide Information.pdf}
}

@article{little_physically_2021,
  title = {Physically {{Implied Surfaces}}},
  author = {Little, Patrick C. and Firestone, Chaz},
  year = {2021},
  month = apr,
  journal = {Psychological Science},
  pages = {0956797620939942},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797620939942},
  abstract = {In addition to seeing objects that are directly in view, we also represent objects that are merely implied (e.g., by occlusion, motion, and other cues). What can imply the presence of an object? Here, we explored (in three preregistered experiments; N = 360 adults) the role of physical interaction in creating impressions of objects that are not actually present. After seeing an actor collide with an invisible wall or step onto an invisible box, participants gave facilitated responses to actual, visible surfaces that appeared where the implied wall or box had been\textemdash a Stroop-like pattern of facilitation and interference that suggested automatic inferences about the relevant implied surfaces. Follow-up experiments ruled out confounding geometric cues and anticipatory responses. We suggest that physical interactions can trigger representations of the participating surfaces such that we automatically infer the presence of objects implied only by their physical consequences.},
  langid = {english},
  keywords = {amodal perception,causal perception,intuitive physics,magic,mime,open data,open materials,preregistered},
  file = {/Users/xzfang/Zotero/storage/LEKEZKRG/Little and Firestone - 2021 - Physically Implied Surfaces.pdf}
}

@article{liu_dimensionbased_2015,
  title = {{{DIMENSION-BASED STATISTICAL LEARNING OF VOWELS}}},
  author = {Liu, Ran and Holt, Lori L.},
  year = {2015},
  month = dec,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {41},
  number = {6},
  pages = {1783--1798},
  issn = {0096-1523},
  doi = {10.1037/xhp0000092},
  abstract = {Speech perception depends on long-term representations that reflect regularities of the native language. However, listeners rapidly adapt when speech acoustics deviate from these regularities due to talker idiosyncrasies such as foreign accents and dialects. To better understand these dual aspects of speech perception, we probe native English listeners' baseline perceptual weighting of two acoustic dimensions (spectral quality and vowel duration) towards vowel categorization and examine how they subsequently adapt to an ``artificial accent'' that deviates from English norms in the correlation between the two dimensions. At baseline, listeners rely relatively more on spectral quality than vowel duration to signal vowel category, but duration nonetheless contributes. Upon encountering an ``artificial accent'' in which the spectral-duration correlation is perturbed relative to English language norms, listeners rapidly down-weight reliance on duration. Listeners exhibit this type of short-term statistical learning even in the context of nonwords, confirming that lexical information is not necessary to this form of adaptive plasticity in speech perception. Moreover, learning generalizes to both novel lexical contexts and acoustically-distinct altered voices. These findings are discussed in the context of a mechanistic proposal for how supervised learning may contribute to this type of adaptive plasticity in speech perception.},
  pmcid = {PMC4666748},
  pmid = {26280268},
  file = {/Users/xzfang/Zotero/storage/M3EP4KRS/Liu and Holt - 2015 - DIMENSION-BASED STATISTICAL LEARNING OF VOWELS.pdf}
}

@article{liu_emotional_2018,
  title = {Emotional {{Connotations}} of {{Musical Instrument Timbre}} in {{Comparison With Emotional Speech Prosody}}: {{Evidence From Acoustics}} and {{Event-Related Potentials}}},
  shorttitle = {Emotional {{Connotations}} of {{Musical Instrument Timbre}} in {{Comparison With Emotional Speech Prosody}}},
  author = {Liu, Xiaoluan and Xu, Yi and Alter, Kai and Tuomainen, Jyrki},
  year = {2018},
  journal = {Frontiers in Psychology},
  volume = {9},
  pages = {737},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.00737},
  abstract = {Music and speech both communicate emotional meanings in addition to their domain-specific contents. But it is not clear whether and how the two kinds of emotional meanings are linked. The present study is focused on exploring the emotional connotations of musical timbre of isolated instrument sounds through the perspective of emotional speech prosody. The stimuli were isolated instrument sounds and emotional speech prosody categorized by listeners into anger, happiness and sadness, respectively. We first analyzed the timbral features of the stimuli, which showed that relations between the three emotions were relatively consistent in those features for speech and music. The results further echo the size-code hypothesis in which different sound timbre indicates different body size projections. Then we conducted an ERP experiment using a priming paradigm with isolated instrument sounds as primes and emotional speech prosody as targets. The results showed that emotionally incongruent instrument-speech pairs triggered a larger N400 response than emotionally congruent pairs. Taken together, this is the first study to provide evidence that the timbre of simple and isolated musical instrument sounds can convey emotion in a way similar to emotional speech prosody.},
  file = {/Users/xzfang/Zotero/storage/EYYT3HMH/Liu et al. - 2018 - Emotional Connotations of Musical Instrument Timbr.pdf}
}

@article{liu_failure_2017,
  title = {Failure to Replicate Talker-Specific Syntactic Adaptation},
  author = {Liu, Linda and Burchill, Zachary and Tanenhaus, Michael K and Jaeger, T Florian},
  year = {2017},
  pages = {6},
  abstract = {Sentence understanding is affected by recent experience. An important open question is whether this reflects adaptation to the statistics of the input. Support for this hypothesis comes from the recent finding that listeners can simultaneously learn and maintain the syntactic statistics of multiple talkers (Kamide, 2012). We attempt\textemdash and fail\textemdash to replicate this finding. This calls into questions whether recency effects in sentence processing originate in the same adaptive mechanisms operating during speech perception (for which talker-specific adaptation is well-established).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/R2KD4DLE/Liu et al. - Failure to replicate talker-speciï¬c syntactic adap.pdf}
}

@article{liu_music_2015,
  title = {A Music Perception Disorder (Congenital Amusia) Influences Speech Comprehension},
  author = {Liu, Fang and Jiang, Cunmei and Wang, Bei and Xu, Yi and Patel, Aniruddh D.},
  year = {2015},
  month = jan,
  journal = {Neuropsychologia},
  volume = {66},
  pages = {111--118},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2014.11.001},
  abstract = {This study investigated the underlying link between speech and music by examining whether and to what extent congenital amusia, a musical disorder characterized by degraded pitch processing, would impact spoken sentence comprehension for speakers of Mandarin, a tone language. Sixteen Mandarin-speaking amusics and 16 matched controls were tested on the intelligibility of news-like Mandarin sentences with natural and flat fundamental frequency (F0) contours (created via speech resynthesis) under four signal-to-noise (SNR) conditions (no noise, +5, 0, and -5dB SNR). While speech intelligibility in quiet and extremely noisy conditions (SNR=-5dB) was not significantly compromised by flattened F0, both amusic and control groups achieved better performance with natural-F0 sentences than flat-F0 sentences under moderately noisy conditions (SNR=+5 and 0dB). Relative to normal listeners, amusics demonstrated reduced speech intelligibility in both quiet and noise, regardless of whether the F0 contours of the sentences were natural or flattened. This deficit in speech intelligibility was not associated with impaired pitch perception in amusia. These findings provide evidence for impaired speech comprehension in congenital amusia, suggesting that the deficit of amusics extends beyond pitch processing and includes segmental processing.},
  langid = {english},
  keywords = {Congenital amusia,Music perception,Noise,Pitch,Segment,Speech comprehension},
  file = {/Users/xzfang/Zotero/storage/UAAPS5SP/Liu et al. - 2015 - A music perception disorder (congenital amusia) in.pdf;/Users/xzfang/Zotero/storage/5C6A6IFN/S0028393214004138.html}
}

@article{liu_sixmonthold_2017,
  title = {Six-Month-Old Infants Expect Agents to Minimize the Cost of Their Actions},
  author = {Liu, Shari and Spelke, Elizabeth S.},
  year = {2017},
  month = mar,
  journal = {Cognition},
  volume = {160},
  pages = {35--42},
  issn = {00100277},
  doi = {10.1016/j.cognition.2016.12.007},
  abstract = {Substantial evidence indicates that infants expect agents to move directly to their goals when no obstacles block their paths, but the representations that articulate this expectation and its robustness have not been characterized. Across three experiments (total N = 60), 6-month-old infants responded to a novel, curvilinear action trajectory on the basis of its efficiency, in accord with the expectation that an agent will move to its goal on the least costly path that the environment affords. Infants expected minimally costly action when presented with a novel constraint, and extended this expectation to agents who had previously acted inefficiently. Infants' understanding of goal-directed action cannot be explained alone by sensitivity to specific features of agent's actions (e.g. agents tend to move on straight paths, along supporting surfaces, when facing their goals directly) or extrapolations of agents' past actions to their future ones (e.g. if an agent took the shortest path to an object in the past, it will continue to do so in the future). Instead, infants' reasoning about efficiency accords with the overhypothesis that agents minimize the cost of their actions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9AIVSTXY/Liu and Spelke - 2017 - Six-month-old infants expect agents to minimize th.pdf}
}

@article{liu_stages_2002,
  title = {Stages of Processing in Face Perception: An {{MEG}} Study},
  shorttitle = {Stages of Processing in Face Perception},
  author = {Liu, Jia and Harris, Alison and Kanwisher, Nancy},
  year = {2002},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {5},
  number = {9},
  pages = {910--916},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn909},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/G9INF6BP/Liu et al. - 2002 - Stages of processing in face perception an MEG st.pdf}
}

@article{liu_talkerspecific_2019a,
  title = {Talker-Specific Pronunciation or Speech Error? {{Discounting}} (or Not) Atypical Pronunciations during Speech Perception.},
  shorttitle = {Talker-Specific Pronunciation or Speech Error?},
  author = {Liu, Linda and Jaeger, T. Florian},
  year = {2019},
  month = dec,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {45},
  number = {12},
  pages = {1562--1588},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/xhp0000693},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Y4DZD4D5/Liu and Jaeger - 2019 - Talker-specific pronunciation or speech error Dis.pdf}
}

@article{liu_transposedword_2020,
  title = {A Transposed-Word Effect in {{Chinese}} Reading},
  author = {Liu, Zhiwei and Li, Yan and Paterson, Kevin B. and Wang, Jingxin},
  year = {2020},
  month = nov,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {8},
  pages = {3788--3794},
  issn = {1943-393X},
  doi = {10.3758/s13414-020-02114-y},
  abstract = {Studies using a grammaticality decision task suggest surprising flexibility in the processing of the relative order of words in sentences when reading alphabetic scripts like French. In these studies, participants made rapid grammaticality decisions for ungrammatical stimuli created by transposing two adjacent words in either a grammatical or an ungrammatical base sentence, which were intermixed with equal numbers of grammatically correct stimuli. The key finding was that participants made more errors and were slower to reject transposed-word stimuli created from grammatical than ungrammatical base sentences. This suggested that flexibility in the processing of word order allowed participants to access representations of the base grammatical sentences, interfering with their decisions to correctly reject transposed-word stimuli. With the present research, we investigated if a similar transposed-word effect is observed for a non-alphabetic script (Chinese) that uses few grammatical markers and primarily conveys grammatical structure via word order. Such scripts may require stricter processing of word order during reading and so provide a strong test of the cross-linguistic generality of the transposed-word effect. We report three experiments using the same design and procedure as previous research, while varying the length of the transposed words across experiments. In all three experiments, participants made more errors and were slower to reject transposed-word stimuli derived from grammatical than ungrammatical base sentences. This replicates previous findings with alphabetic scripts and provides novel evidence for a transposed-word effect in Chinese reading. We consider the implications for models of reading in alphabetic and non-alphabetic scripts.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QUGVVNQA/Liu et al. - 2020 - A transposed-word effect in Chinese reading.pdf}
}

@article{liu_visual_2008,
  title = {The {{Visual Word Form Area}}: {{Evidence}} from an {{fMRI}} Study of Implicit Processing of {{Chinese}} Characters},
  shorttitle = {The {{Visual Word Form Area}}},
  author = {Liu, Chao and Zhang, Wu-Tian and Tang, Yi-Yuan and Mai, Xiao-Qin and Chen, Hsuan-Chih and Tardif, Twila and Luo, Yue-Jia},
  year = {2008},
  month = apr,
  journal = {NeuroImage},
  volume = {40},
  number = {3},
  pages = {1350--1361},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2007.10.014},
  abstract = {A notable controversy in neurolinguistics is whether there is a particular brain area specialized for visual word recognition within the visual ventral stream. We investigated this question via implicit processing of Chinese characters. Implicit processing of four types of stimuli \textendash{} real characters, pseudo characters, artificial characters, and checkerboard \textendash{} in two different sizes, were compared in 14 normal participants using functional MRI (fMRI) with a size judgment task. The results showed that when the three character types were contrasted to one another, there was significantly greater activation in the left middle fusiform gyrus during real and pseudo character processing compared to artificial characters. Moreover, individual analysis revealed that the coordinates were consistent with the Visual Word Form Area (VWFA) reported for alphabetic scripts. Results also showed a consistent activation in the left middle frontal gyrus (BA 9) for real and pseudo characters. The relation between this region and the VWFA in Characters processing still needs further investigation.},
  langid = {english},
  keywords = {Chinese character,fMRI,Implicit processing,Orthographic processing,Visual Word Form Area},
  file = {/Users/xzfang/Zotero/storage/SECFQFAN/Liu et al. - 2008 - The Visual Word Form Area Evidence from an fMRI s.pdf;/Users/xzfang/Zotero/storage/RUBWHLJW/S1053811907009378.html}
}

@article{lizarazu_language_2021,
  title = {Language {{Proficiency Entails Tuning Cortical Activity}} to {{Second Language Speech}}},
  author = {Lizarazu, Mikel and Carreiras, Manuel and Bourguignon, Mathieu and Zarraga, Asier and Molinaro, Nicola},
  year = {2021},
  month = aug,
  journal = {Cerebral Cortex},
  volume = {31},
  number = {8},
  pages = {3820--3831},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab051},
  abstract = {Cortical tracking of linguistic structures in speech, such as phrases (\&lt;3~Hz, delta band) and syllables (3\textendash 8~Hz, theta band), is known to be crucial for speech comprehension. However, it has not been established whether this effect is related to language proficiency. Here, we investigate how auditory cortical activity in second language (L2) learners tracked L2 speech. Using magnetoencephalography, we recorded brain activity from participants listening to Spanish and Basque. Participants were Spanish native (L1) language speakers studying Basque (L2) at the same language center at three different levels: beginner (Grade 1), intermediate (Grade 2), and advanced (Grade 3). We found that 1) both delta and theta tracking to L2 speech in the auditory cortex were related to L2 learning proficiency and that 2) top-down modulations of activity in the left auditory regions during L2 speech listening\textemdash by the left inferior frontal and motor regions in delta band and by the left middle temporal regions in theta band\textemdash were also related to L2 proficiency. Altogether, these results indicate that the ability to learn an L2 is related to successful cortical tracking of L2 speech and its modulation by neuronal oscillations in higher-order cortical regions.},
  file = {/Users/xzfang/Zotero/storage/GP9ESK85/Lizarazu et al. - 2021 - Language Proficiency Entails Tuning Cortical Activ.pdf}
}

@article{lo_transform_2015,
  title = {To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data},
  shorttitle = {To Transform or Not to Transform},
  author = {Lo, Steson and Andrews, Sally},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  pages = {1171},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.01171},
  abstract = {Linear mixed-effect models (LMMs) are being increasingly widely used in psychology to analyse multi-level research designs. This feature allows LMMs to address some of the problems identified by Speelman and McGann (2013) about the use of mean data, because they do not average across individual responses. However, recent guidelines for using LMM to analyse skewed reaction time (RT) data collected in many cognitive psychological studies recommend the application of non-linear transformations to satisfy assumptions of normality. Uncritical adoption of this recommendation has important theoretical implications which can yield misleading conclusions. For example, Balota et al. (2013) showed that analyses of raw RT produced additive effects of word frequency and stimulus quality on word identification, which conflicted with the interactive effects observed in analyses of transformed RT. Generalized linear mixed-effect models (GLMM) provide a solution to this problem by satisfying normality assumptions without the need for transformation. This allows differences between individuals to be properly assessed, using the metric most appropriate to the researcher's theoretical context. We outline the major theoretical decisions involved in specifying a GLMM, and illustrate them by reanalysing Balota et al.'s datasets. We then consider the broader benefits of using GLMM to investigate individual differences.},
  file = {/Users/xzfang/Zotero/storage/P6YAFVPJ/Lo and Andrews - 2015 - To transform or not to transform using generalize.pdf}
}

@article{lobier_phase_2013,
  title = {Phase {{Transfer Entropy}}: {{A}} Novel Phase-Based Measure for Directed Connectivity in Networks Coupled by Oscillatory Interactions.},
  shorttitle = {Phase {{Transfer Entropy}}},
  author = {Lobier, Muriel and Siebenh{\"u}hner, Felix and Palva, Satu and Palva, J. Matias},
  year = {2013},
  month = sep,
  journal = {NeuroImage},
  volume = {85},
  doi = {10.1016/j.neuroimage.2013.08.056},
  abstract = {We introduce here Phase Transfer Entropy (Phase TE) as a measure of directed connectivity among neuronal oscillations. Phase TE quantifies the transfer entropy between phase time-series extracted from neuronal signals by filtering for instance. To validate the measure, we used coupled Neuronal Mass Models to both evaluate the characteristics of Phase TE and compare its performance with that of a real-valued TE implementation. We showed that Phase TE detects the strength and direction of connectivity even in the presence of such amounts of noise and linear mixing that typically characterize MEG and EEG recordings. Phase TE performed well across a wide range of analysis lags and sample sizes. Comparisons between Phase TE and real-valued TE estimates showed that Phase TE is more robust to nuisance parameters and considerably more efficient computationally. In addition, Phase TE accurately untangled bi-directional frequency-band-specific interaction patterns that confounded real-valued TE. Finally, we found that surrogate data can be used to construct appropriate null-hypothesis distributions and to estimate statistical significance of Phase TE. These results hence suggest that Phase TE is well suited for the estimation of directed phase-based connectivity in large-scale investigations of the human functional connectome.}
}

@article{logothetis_what_2008,
  title = {What We Can Do and What We Cannot Do with {{fMRI}}},
  author = {Logothetis, Nikos K.},
  year = {2008},
  month = jun,
  journal = {Nature},
  volume = {453},
  number = {7197},
  pages = {869--878},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature06976},
  abstract = {Functional magnetic resonance imaging (fMRI) has become the mainstay of neuroimaging in the neural and cognitive sciences. It measures haemodynamic changes following neural activity and has the potential, eventually, to reveal the intimate details of brain organization. But in a Review Article, Nikos Logothetis strikes a note of caution: the conclusions drawn from fMRI data often ignore the actual limitations of the methodology. Logothetis gives an overview of current fMRI technology and outlines our understanding of the haemodynamic signals and the constraints they impose on the interpretation of neuroimaging data.},
  copyright = {2008 Macmillan Publishers Limited. All rights reserved},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/75Z5UWSA/Logothetis - 2008 - What we can do and what we cannot do with fMRI.pdf;/Users/xzfang/Zotero/storage/TJLAXPBY/nature06976.html}
}

@article{loiotile_naturalistic_2019,
  title = {Naturalistic {{Audio-Movies}} and {{Narrative Synchronize}} ``{{Visual}}'' {{Cortices}} across {{Congenitally Blind But Not Sighted Individuals}}},
  author = {Loiotile, Rita E. and Cusack, Rhodri and Bedny, Marina},
  year = {2019},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {45},
  pages = {8940--8948},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0298-19.2019},
  abstract = {How does developmental experience, as opposed to intrinsic physiology, shape cortical function? Naturalistic stimuli were used to elicit neural synchrony in individuals blind from birth (n = 18) and those who grew up with sight (n = 18). Blind and blindfolded sighted participants passively listened to three audio-movie clips, an auditory narrative, a sentence shuffled version of the narrative (maintaining language but lacking a plotline), and a version of the narrative backward (lacking both language and plot). For both groups, early auditory cortices were synchronized to a similar degree across stimulus types, whereas higher-cognitive temporoparietal and prefrontal areas were more synchronized by meaningful, temporally extended stimuli (i.e., audio movies and narrative). ``Visual'' cortices were more synchronized across blind than sighted individuals, but only for audio-movies and narrative. In the blind group, visual cortex synchrony was low for backward speech and intermediate for sentence shuffle. Meaningful auditory stimuli synchronize visual cortices of people born blind. SIGNIFICANCE STATEMENT Naturalistic stimuli engage cognitive processing at many levels. Here, we harnessed this richness to investigate the effect of experience on cortical function. We find that listening to naturalistic audio movies and narrative drives synchronized activity across ``visual'' cortices of blind, more so than sighted, individuals. Visual cortex synchronization varies with meaningfulness and cognitive complexity. Higher synchrony is observed for temporally extended meaningful stimuli (e.g., movies/narrative), intermediate for shuffled sentences, lowest for time varying complex noise. By contrast, auditory cortex was synchronized equally by meaningful and meaningless stimuli. In congenitally blind individuals most of visual cortex is engaged by meaningful naturalistic stimuli.},
  copyright = {Copyright \textcopyright{} 2019 the authors},
  langid = {english},
  pmid = {31548238},
  keywords = {blindness,narrative,naturalistic,plasticity,synchrony,visual cortex},
  file = {/Users/xzfang/Zotero/storage/AW45A9JZ/Loiotile et al. - 2019 - Naturalistic Audio-Movies and Narrative Synchroniz.pdf;/Users/xzfang/Zotero/storage/D3HXFIA8/8940.html}
}

@article{lollo_featurebinding_2012,
  title = {The Feature-Binding Problem Is an Ill-Posed Problem},
  author = {Lollo, Vincent Di},
  year = {2012},
  month = jun,
  journal = {Trends in Cognitive Sciences},
  volume = {16},
  number = {6},
  pages = {317--321},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2012.04.007},
  langid = {english},
  pmid = {22595013},
  file = {/Users/xzfang/Zotero/storage/TMBJGRJD/Lollo - 2012 - The feature-binding problem is an ill-posed proble.pdf;/Users/xzfang/Zotero/storage/EKN3JBFV/S1364-6613(12)00098-8.html}
}

@article{long_functional_2016,
  title = {Functional {{Segregation}} of {{Cortical Regions Underlying Speech Timing}} and {{Articulation}}},
  author = {Long, Michael~A. and Katlowitz, Kalman~A. and Svirsky, Mario~A. and Clary, Rachel~C. and Byun, Tara~McAllister and Majaj, Najib and Oya, Hiroyuki and Howard, Matthew~A. and Greenlee, Jeremy~D. W.},
  year = {2016},
  month = mar,
  journal = {Neuron},
  volume = {89},
  number = {6},
  pages = {1187--1193},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.01.032},
  abstract = {Spoken language is a central part of our everyday lives, but the precise roles that individual cortical regions play in the production of speech are often poorly understood. To address this issue, we focally lowered the temperature of distinct cortical regions in awake neurosurgical patients, and we relate this perturbation to changes in produced speech sequences. Using this method, we confirm that speech is highly lateralized, with the vast majority of behavioral effects seen on the left hemisphere. We then use this approach to demonstrate a clear functional dissociation between nearby cortical speech sites. Focal cooling of pars triangularis/pars opercularis (Broca's region) and the ventral portion of the precentral gyrus (speech motor cortex) resulted in the manipulation of speech timing and articulation, respectively. Our results support a class of models that have proposed distinct processing centers underlying motor sequencing and execution for speech.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/66UPZSS6/Long et al. - 2016 - Functional Segregation of Cortical Regions Underly.pdf;/Users/xzfang/Zotero/storage/QHBZQ7QZ/S089662731600057X.html}
}

@article{longcamp_visual_2003,
  title = {Visual Presentation of Single Letters Activates a Premotor Area Involved in Writing},
  author = {Longcamp, Marieke and Anton, Jean-Luc and Roth, Muriel and Velay, Jean-Luc},
  year = {2003},
  month = aug,
  journal = {NeuroImage},
  volume = {19},
  number = {4},
  pages = {1492--1500},
  issn = {1053-8119},
  doi = {10.1016/S1053-8119(03)00088-0},
  abstract = {In the present fMRI study, we addressed the question as to whether motor\textendash perceptual interactions might be involved in reading. Recognizing the letters encountered when reading is generally assumed to be a purely visual process, yet because we know how to write, we also possess a sensorimotor representation of the letters. Does simply viewing a letter suffice to activate the corresponding motor representation? To answer this question, we asked right-handed subjects first to look at and then to copy single letters or pseudoletters. We established that the visual presentation of letters activated a part of the left premotor cortex (BA6) that was also activated when the letters were being written by the subjects. This premotor zone resembles Exner's area, which is thought to contain the motor programs necessary for producing letters. Visually presented pseudoletters, which had never been written before by the subjects, did not activate this region. These results indicate that the writing motor processes are implicitly evoked when passively observing letters. The cerebral representation of letters is therefore presumably not strictly visual, but based on a multicomponent neural network built up while learning concomitantly to read and write. One of the components might be a sensorimotor one associated with handwriting. This finding shows the existence of close functional relations between the reading and writing processes, and suggests that our reading abilities might be somehow dependent on the way we write.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XVEATNCR/Longcamp et al. - 2003 - Visual presentation of single letters activates a .pdf;/Users/xzfang/Zotero/storage/J7P4MTGD/S1053811903000880.html}
}

@article{longcamp_visual_2003a,
  title = {Visual Presentation of Single Letters Activates a Premotor Area Involved in Writing},
  author = {Longcamp, Marieke and Anton, Jean-Luc and Roth, Muriel and Velay, Jean-Luc},
  year = {2003},
  month = aug,
  journal = {NeuroImage},
  volume = {19},
  number = {4},
  pages = {1492--1500},
  issn = {1053-8119},
  doi = {10.1016/S1053-8119(03)00088-0},
  abstract = {In the present fMRI study, we addressed the question as to whether motor\textendash perceptual interactions might be involved in reading. Recognizing the letters encountered when reading is generally assumed to be a purely visual process, yet because we know how to write, we also possess a sensorimotor representation of the letters. Does simply viewing a letter suffice to activate the corresponding motor representation? To answer this question, we asked right-handed subjects first to look at and then to copy single letters or pseudoletters. We established that the visual presentation of letters activated a part of the left premotor cortex (BA6) that was also activated when the letters were being written by the subjects. This premotor zone resembles Exner's area, which is thought to contain the motor programs necessary for producing letters. Visually presented pseudoletters, which had never been written before by the subjects, did not activate this region. These results indicate that the writing motor processes are implicitly evoked when passively observing letters. The cerebral representation of letters is therefore presumably not strictly visual, but based on a multicomponent neural network built up while learning concomitantly to read and write. One of the components might be a sensorimotor one associated with handwriting. This finding shows the existence of close functional relations between the reading and writing processes, and suggests that our reading abilities might be somehow dependent on the way we write.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WNESEAQC/Longcamp et al. - 2003 - Visual presentation of single letters activates a .pdf;/Users/xzfang/Zotero/storage/DR4GCQYN/S1053811903000880.html}
}

@article{lonnqvist_comparative_2021,
  title = {A Comparative Biology Approach to {{DNN}} Modeling of Vision: {{A}} Focus on Differences, Not Similarities},
  shorttitle = {A Comparative Biology Approach to {{DNN}} Modeling of Vision},
  author = {Lonnqvist, Ben and Bornet, Alban and Doerig, Adrien and Herzog, Michael H.},
  year = {2021},
  month = sep,
  journal = {Journal of Vision},
  volume = {21},
  number = {10},
  pages = {17},
  issn = {1534-7362},
  doi = {10.1167/jov.21.10.17},
  abstract = {Deep neural networks (DNNs) have revolutionized computer science and are now widely used for neuroscientific research. A hot debate has ensued about the usefulness of DNNs as neuroscientific models of the human visual system; the debate centers on to what extent certain shortcomings of DNNs are real failures and to what extent they are redeemable. Here, we argue that the main problem is that we often do not understand which human functions need to be modeled and, thus, what counts as a falsification. Hence, not only is there a problem on the DNN side, but there is also one on the brain side (i.e., with the explanandum\textemdash the thing to be explained). For example, should DNNs reproduce illusions? We posit that we can make better use of DNNs by adopting an approach of comparative biology by focusing on the differences, rather than the similarities, between DNNs and humans to improve our understanding of visual information processing in general.},
  file = {/Users/xzfang/Zotero/storage/AJI5I3D3/Lonnqvist et al. - 2021 - A comparative biology approach to DNN modeling of .pdf;/Users/xzfang/Zotero/storage/A4P3LGXY/article.html}
}

@article{lonnqvist_crowding_2019,
  title = {Crowding in Humans Is Unlike That in Convolutional Neural Networks},
  author = {Lonnqvist, Ben and Clarke, Alasdair D. F. and Chakravarthi, Ramakrishna},
  year = {2019},
  month = nov,
  journal = {arXiv:1903.00258 [cs]},
  eprint = {1903.00258},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Object recognition is a primary function of the human visual system. It has recently been claimed that the highly successful ability to recognise objects in a set of emergent computer vision systems\textemdash Deep Convolutional Neural Networks (DCNNs)\textemdash can form a useful guide to recognition in humans. To test this assertion, we systematically evaluated visual crowding, a dramatic breakdown of recognition in clutter, in DCNNs and compared their performance to extant research in humans. We examined crowding in three architectures of DCNNs with the same methodology as that used among humans. We manipulated multiple stimulus factors including inter-letter spacing, letter colour, size, and flanker location to assess the extent and shape of crowding in DCNNs. We found that crowding followed a predictable pattern across architectures that was different from that in humans. Some characteristic hallmarks of human crowding, such as invariance to size, the effect of target-flanker similarity, and confusions between target and flanker identities, were completely missing, minimised or even reversed. These data show that DCNNs, while proficient in object recognition, likely achieve this competence through a set of mechanisms that are distinct from those in humans. They are not necessarily equivalent models of human or primate object recognition and caution must be exercised when inferring mechanisms derived from their operation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xzfang/Zotero/storage/8KMANUQQ/Lonnqvist et al. - 2019 - Crowding in humans is unlike that in convolutional.pdf}
}

@article{lotter_neural_2020,
  title = {A Neural Network Trained for Prediction Mimics Diverse Features of Biological Neurons and Perception},
  author = {Lotter, William and Kreiman, Gabriel and Cox, David},
  year = {2020},
  month = apr,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {4},
  pages = {210--219},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0170-9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WDXESPPH/Lotter et al. - 2020 - A neural network trained for prediction mimics div.pdf}
}

@article{loui_enhanced_2011,
  title = {Enhanced {{Cortical Connectivity}} in {{Absolute Pitch Musicians}}: {{A Model}} for {{Local Hyperconnectivity}}},
  shorttitle = {Enhanced {{Cortical Connectivity}} in {{Absolute Pitch Musicians}}},
  author = {Loui, Psyche and Charles Li, Hui C. and Hohmann, Anja and Schlaug, Gottfried},
  year = {2011},
  month = apr,
  journal = {Journal of cognitive neuroscience},
  volume = {23},
  number = {4},
  pages = {1015--1026},
  issn = {0898-929X},
  doi = {10.1162/jocn.2010.21500},
  abstract = {Connectivity in the human brain has received increased scientific interest in recent years. Although connection disorders can affect perception, production, learning, and memory, few studies have associated brain connectivity with graded variations in human behavior, especially among normal individuals. One group of normal individuals who possess unique characteristics in both behavior and brain structure is absolute pitch (AP) musicians, who can name the appropriate pitch class of any given tone without a reference. Using diffusion tensor imaging and tractography, we observed hyperconnectivity in bilateral superior temporal lobe structures linked to AP possession. Furthermore, volume of tracts connecting left superior temporal gyrus to left middle temporal gyrus predicted AP performance. These findings extend previous reports of exaggerated temporal lobe asymmetry, may explain the higher incidence of AP in developmental disorders, and may provide a model for understanding the heightened connectivity that is thought to underlie savant skills and cases of exceptional creativity.},
  pmcid = {PMC3012137},
  pmid = {20515408},
  file = {/Users/xzfang/Zotero/storage/YSKP58RY/Loui et al. - 2011 - Enhanced Cortical Connectivity in Absolute Pitch M.pdf}
}

@article{loui_enhanced_2012,
  title = {Enhanced Functional Networks in Absolute Pitch},
  author = {Loui, Psyche and Zamm, Anna and Schlaug, Gottfried},
  year = {2012},
  month = nov,
  journal = {NeuroImage},
  volume = {63},
  number = {2},
  pages = {632--640},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2012.07.030},
  abstract = {Functional networks in the human brain give rise to complex cognitive and perceptual abilities. While the decrease of functional connectivity is linked to neurological and psychiatric disorders, less is known about the consequences of increased functional connectivity. One population that has exceptionally enhanced perceptual abilities is people with absolute pitch (AP) \textemdash{} an ability to categorize tones into pitch classes without reference. AP has been linked to exceptional talent as well as to psychiatric and neurological conditions. Here we show that AP possessors have increased functional activation during music listening, as well as increased degrees, clustering, and local efficiency of functional correlations, with the difference being highest around the left superior temporal gyrus. Our results provide the first evidence that increased functional connectivity in a small-world brain network is related to exceptional perceptual abilities in a healthy population.},
  pmcid = {PMC3666350},
  pmid = {22836173},
  file = {/Users/xzfang/Zotero/storage/FJNW8784/Loui et al. - 2012 - Enhanced functional networks in absolute pitch.pdf}
}

@article{lowder_see_2019,
  title = {I {{See What You Meant To Say}}: {{Anticipating Speech Errors During Online Sentence Processing}}},
  shorttitle = {I {{See What You Meant To Say}}},
  author = {Lowder, Matthew W. and Ferreira, Fernanda},
  year = {2019},
  month = oct,
  journal = {Journal of experimental psychology. General},
  volume = {148},
  number = {10},
  pages = {1849--1858},
  issn = {0096-3445},
  doi = {10.1037/xge0000544},
  abstract = {Everyday speech is rife with errors and disfluencies, yet processing what we hear usually feels effortless. How does the language comprehension system accomplish such an impressive feat? The current experiment tests the hypothesis that listeners draw on relevant contextual and linguistic cues to anticipate speech errors and mentally correct them even before receiving an explicit correction from the speaker. In the current visual-world eyetracking experiment, we monitored participants' eye movements to objects in a display while they listened to utterances containing reparandum-repair speech errors (e.g., \ldots his cat, uh I mean his dog\ldots ). The contextual plausibility of the misspoken word, as well as the certainty with which the speaker uttered this word, were systematically manipulated. Results showed that listeners immediately exploited these cues to generate top-down expectations regarding the speaker's communicative intention. Crucially, listeners used these expectations to constrain the bottom-up speech input and mentally correct perceived speech errors even before the speaker initiated the correction. The results provide powerful evidence regarding the joint process of correcting speech errors that involves both the speaker and the listener.},
  pmcid = {PMC6579724},
  pmid = {30556724},
  file = {/Users/xzfang/Zotero/storage/8K4Z27YN/Lowder and Ferreira - 2019 - I See What You Meant To Say Anticipating Speech E.pdf}
}

@article{lu_syntactic_2021a,
  title = {Syntactic Satiation Is Driven by Speaker-Specific Adaptation},
  author = {Lu, Jiayi and Lassiter, Daniel and Degen, Judith},
  year = {2021},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {43},
  number = {43},
  abstract = {Listeners adapt to variability in language use by updating their expectations over variants, often in speaker-specific ways. We propose that adaptation of this sort contributes to satiation, the phenomenon whereby the acceptability of unacceptable sentences increases after repeated exposure. We provide support for an adaptation account of satiation by showing that the satiation of purportedly unacceptable island-violating constructions demonstrates speaker-specificity, a key property of adaptation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BCI2EHHX/Lu et al. - 2021 - Syntactic satiation is driven by speaker-specific .pdf;/Users/xzfang/Zotero/storage/CGKRR94V/1dn4v3b4.html}
}

@article{lubinus_datadriven_2019,
  title = {Data-Driven Classification of Spectral Profiles Reveals Brain Region-Specific Plasticity},
  author = {Lubinus, Christina and {Gudi-Mindermann}, Helene and Keitel, Anne and Engel, Andreas K. and R{\"o}der, Brigitte and Rimmele, Johanna M.},
  year = {2019},
  month = sep,
  journal = {bioRxiv},
  pages = {782979},
  doi = {10.1101/782979},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The human brain exhibits rhythms that are characteristic for anatomical areas and presumably involved in diverse perceptual and cognitive processes. Visual deprivation results in behavioral adaptation and cortical reorganization, particularly affecting sensory cortices. Whether these plasticity-related changes are accompanied by altered spectral properties of neural signals and whether certain brain areas are particularly targeted by these changes is unknown. With a recently introduced approach, we analyzed MEG resting state data of a group of congenitally blind and matched sighted individuals. First, using clustering procedures (k-means and Gaussian Mixture Models) we identified brain region-specific spectral clusters. Second, a classifier was employed to test the specificity of the spectral profiles within and the differences between groups. We replicated the previously reported finding of area-specific spectral profiles, indicated by high classification performance in the sighted. Additionally, we found high classification performance in the blind, suggesting that after deprivation-related restructuring, area-specific spectral profiles can be consistently identified. Crucially, in the cross-group classification (sighted vs. blind), several sensory (visual and auditory) and right frontal brain areas were classified significantly worse compared to the control condition. Overall the spectral profiles of those brain areas showed increased neuronal power in higher frequency-bands, possibly reflecting acceleration of the regionally prevalent brain rhythms in the blind compared to the sighted. We provide evidence that visual deprivation-related plasticity selectively alters the spectral profiles of right frontal and sensory brain areas, possibly reflecting increased temporal processing capabilities (auditory, frontal cortices) and changes in the visual inhibitory-excitatory circuits in the blind.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J6PGE5CI/Lubinus et al. - 2019 - Data-driven classification of spectral profiles re.pdf;/Users/xzfang/Zotero/storage/PX7LE9X2/782979v1.html}
}

@article{luck_electrophysiological_1990,
  title = {Electrophysiological Evidence for Parallel and Serial Processing during Visual Search},
  author = {Luck, Steven J. and Hillyard, Steven A.},
  year = {1990},
  month = nov,
  journal = {Perception \& Psychophysics},
  volume = {48},
  number = {6},
  pages = {603--617},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03211606},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3CYG2K2X/Luck and Hillyard - 1990 - Electrophysiological evidence for parallel and ser.pdf}
}

@article{luck_how_2017,
  title = {How to {{Get Statistically Significant Effects}} in {{Any ERP Experiment}} (and {{Why You Shouldn}}'t)},
  author = {Luck, Steven J. and Gaspelin, Nicholas},
  year = {2017},
  month = jan,
  journal = {Psychophysiology},
  volume = {54},
  number = {1},
  pages = {146--157},
  issn = {0048-5772},
  doi = {10.1111/psyp.12639},
  abstract = {Event-related potential (ERP) experiments generate massive data sets, often containing thousands of values for each participant, even after averaging. The richness of these data sets can be very useful in testing sophisticated hypotheses, but this richness also creates many opportunities to obtain effects that are statistically significant but do not reflect true differences among groups or conditions (bogus effects). The purpose of this paper is to demonstrate how common and seemingly innocuous methods for quantifying and analyzing ERP effects can lead to very high rates of significant-but-bogus effects, with the likelihood of obtaining at least one such bogus effect exceeding 50\% in many experiments. We focus on two specific problems: using the grand average data to select the time windows and electrode sites for quantifying component amplitudes and latencies, and using one or more multi-factor statistical analyses. Re-analyses of prior data and simulations of typical experimental designs are used to show how these problems can greatly increase the likelihood of significant-but-bogus results. Several strategies are described for avoiding these problems and for increasing the likelihood that significant effects actually reflect true differences among groups or conditions.},
  pmcid = {PMC5178877},
  pmid = {28000253},
  file = {/Users/xzfang/Zotero/storage/J5BZY6AE/Luck and Gaspelin - 2017 - How to Get Statistically Significant Effects in An.pdf}
}

@article{luke_semantic_2012,
  title = {Semantic Predictability Eliminates the Transposed-Letter Effect},
  author = {Luke, Steven G. and Christianson, Kiel},
  year = {2012},
  month = may,
  journal = {Memory \& Cognition},
  volume = {40},
  number = {4},
  pages = {628--641},
  issn = {1532-5946},
  doi = {10.3758/s13421-011-0170-4},
  abstract = {Semantic predictability facilitates word recognition during language processing. One possible explanation for this facilitation is that highly specific predictions generated online during language processing preactivate some features of upcoming words. To explore whether, how, and when these predictions affect visual word recognition, in the two experiments reported here we investigated the influence of semantic predictability on transposed-letter priming. In order to do so, a paradigm that combines self-paced word-by-word reading with masked priming was developed. Transposed-letter priming occurred in nonconstraining contexts but not in constraining contexts, indicating that readers use context to make predictions about both letter identity and position in upcoming words, and that these predictions have an early influence on visual word recognition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/M245ES3D/Luke and Christianson - 2012 - Semantic predictability eliminates the transposed-.pdf}
}

@article{lukic_dissociating_2021,
  title = {Dissociating Nouns and Verbs in Temporal and Perisylvian Networks: {{Evidence}} from Neurodegenerative Diseases},
  shorttitle = {Dissociating Nouns and Verbs in Temporal and Perisylvian Networks},
  author = {Lukic, Sladjana and Borghesani, Valentina and Weis, Elizabeth and Welch, Ariane and Bogley, Rian and Neuhaus, John and Deleon, Jessica and Miller, Zachary A. and Kramer, Joel H. and Miller, Bruce L. and Dronkers, Nina F. and {Gorno-Tempini}, Maria L.},
  year = {2021},
  month = sep,
  journal = {Cortex},
  volume = {142},
  pages = {47--61},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2021.05.006},
  abstract = {Naming of nouns and verbs can be selectively impaired in neurological disorders, but the specificity of the neural and cognitive correlates of such dissociation remains unclear. Functional imaging and stroke research sought to identify cortical regions selectively recruited for nouns versus verbs, yet findings are inconsistent. The present study investigated this issue in neurodegenerative diseases known to selectively affect different brain networks, thus providing new critical evidence of network specificity. We examined naming performances on nouns and verbs in 146 patients with different neurodegenerative syndromes (Primary Progressive Aphasia \textendash{} PPA, Alzheimer's disease \textendash{} AD, and behavioral variant Frontotemporal Dementia \textendash{} FTD) and 30 healthy adults. We then correlated naming scores with MRI-derived cortical thickness values as well as with performances in semantic and syntactic tasks, across all subjects. Results indicated that patients with the semantic variant PPA named significantly fewer nouns than verbs. Instead, nonfluent/agrammatic PPA patients named fewer verbs than nouns. Across all subjects, performance on nouns (adjusted for verbs) specifically correlated with cortical atrophy in left anterior temporal regions, and performance on verbs (adjusted for nouns) with atrophy in left inferior and middle frontal, inferior parietal and posterior temporal regions. Furthermore, lower lexical-semantic abilities correlated with deficits in naming both nouns and verbs, while lower syntactic abilities only correlated with naming verbs. Our results show that different neural and cognitive mechanisms underlie naming of specific grammatical categories in neurodegenerative diseases. Importantly, our findings showed that verb processing depends on a widespread perisylvian networks, suggesting that some regions might be involved in processing different types of action knowledge. These findings have important implications for early differential diagnosis of neurodegenerative disorders.},
  langid = {english},
  keywords = {Cortical atrophy,Lexical-semantics,Neurodegenerative diseases,Nouns,Syntax,Verbs},
  file = {/Users/xzfang/Zotero/storage/E29FBALM/Lukic et al. - 2021 - Dissociating nouns and verbs in temporal and peris.pdf;/Users/xzfang/Zotero/storage/SFEQEMGB/S0010945221001945.html}
}

@article{lundqvist_gamma_2016,
  title = {Gamma and {{Beta Bursts Underlie Working Memory}}},
  author = {Lundqvist, Mikael and Rose, Jonas and Herman, Pawel and Brincat, Scott~L. and Buschman, Timothy~J. and Miller, Earl~K.},
  year = {2016},
  month = apr,
  journal = {Neuron},
  volume = {90},
  number = {1},
  pages = {152--164},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.02.028},
  abstract = {Working memory is thought to result from sustained neuron spiking. However, computational models suggest complex dynamics with discrete oscillatory bursts. We analyzed local field potential (LFP) and spiking from the prefrontal cortex (PFC) of monkeys performing a working memory task. There were brief~bursts of narrow-band gamma oscillations (45\textendash 100~Hz), varied in time and frequency, accompanying encoding and re-activation of sensory information. They appeared at a minority of recording sites associated with spiking reflecting the to-be-remembered items. Beta oscillations (20\textendash 35~Hz) also occurred in brief, variable bursts but reflected a default state interrupted by encoding and decoding. Only activity of neurons reflecting encoding/decoding correlated with changes in gamma burst rate. Thus, gamma bursts could gate access to, and prevent sensory interference with, working memory. This supports the hypothesis that working memory is manifested by discrete oscillatory dynamics and spiking, not sustained activity.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2YIQ9JSR/Lundqvist et al. - 2016 - Gamma and Beta Bursts Underlie Working Memory.pdf;/Users/xzfang/Zotero/storage/92B6798P/S0896627316001458.html}
}

@article{luo_phase_2007,
  title = {Phase {{Patterns}} of {{Neuronal Responses Reliably Discriminate Speech}} in {{Human Auditory Cortex}}},
  author = {Luo, Huan and Poeppel, David},
  year = {2007},
  month = jun,
  journal = {Neuron},
  volume = {54},
  number = {6},
  pages = {1001--1010},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2007.06.004},
  abstract = {How natural speech is represented in the auditory cortex constitutes a major challenge for cognitive neuroscience. Although many single-unit and neuroimaging studies have yielded valuable insights about the processing of speech and matched complex sounds, the mechanisms underlying the analysis of speech dynamics in human auditory cortex remain largely unknown. Here, we show that the phase pattern of theta band (4\textendash 8 Hz) responses recorded from human auditory cortex with magnetoencephalography (MEG) reliably tracks and discriminates spoken sentences and that this discrimination ability is correlated with speech intelligibility. The findings suggest that an {$\sim$}200 ms temporal window (period of theta oscillation) segments the incoming speech signal, resetting and sliding to track speech dynamics. This hypothesized mechanism for cortical speech analysis is based on the stimulus-induced modulation of inherent cortical rhythms and provides further evidence implicating the syllable as a computational primitive for the representation of spoken language.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/Users/xzfang/Zotero/storage/98VH3NJC/Luo and Poeppel - 2007 - Phase Patterns of Neuronal Responses Reliably Disc.pdf;/Users/xzfang/Zotero/storage/53SPAFCH/S0896627307004138.html}
}

@article{luongo_mice_2021,
  title = {Mice and Primates Use Distinct Strategies for Visual Segmentation},
  author = {Luongo, Francisco J. and Liu, Lu and Ho, Chun Lum Andy and Hesse, Janis K. and Wekselblatt, Joseph B. and Lanfranchi, Francesco and Huber, Daniel and Tsao, Doris Y.},
  year = {2021},
  month = jul,
  journal = {bioRxiv},
  pages = {2021.07.04.451059},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.07.04.451059},
  abstract = {{$<$}p{$>$}The rodent visual system has attracted great interest in recent years due to its experimental tractability, but the fundamental mechanisms used by the mouse to represent the visual world remain unclear. In the primate, researchers have argued from both behavioral and neural evidence that a key step in visual representation is "figure-ground segmentation," the delineation of figures as distinct from backgrounds [1-4]. To determine if mice also show behavioral and neural signatures of figure-ground segmentation, we trained mice on a figure-ground segmentation task where figures were defined by gratings and naturalistic textures moving counterphase to the background. Unlike primates, mice were severely limited in their ability to segment figure from ground using the opponent motion cue, with segmentation behavior strongly dependent on the specific carrier pattern. Remarkably, when mice were forced to localize naturalistic patterns defined by opponent motion, they adopted a strategy of brute force memorization of texture patterns. In contrast, primates, including humans, macaques, and mouse lemurs, could readily segment figures independent of carrier pattern using the opponent motion cue. Consistent with mouse behavior, neural responses to the same stimuli recorded in mouse visual areas V1, RL, and LM also did not support texture-invariant segmentation of figures using opponent motion. Modeling revealed that the texture dependence of both the mouse9s behavior and neural responses could be explained by a feedforward neural network lacking explicit segmentation capabilities. These findings reveal a fundamental limitation in the ability of mice to segment visual objects compared to primates.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GFEYMTBR/Luongo et al. - 2021 - Mice and primates use distinct strategies for visu.pdf;/Users/xzfang/Zotero/storage/JPNIF9RR/2021.07.04.451059v1.html}
}

@article{lupyan_effects_2020,
  title = {Effects of {{Language}} on {{Visual Perception}}},
  author = {Lupyan, Gary and Rahman, Rasha Abdel and Boroditsky, Lera and Clark, Andy},
  year = {2020},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {11},
  pages = {930--944},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2020.08.005},
  langid = {english},
  pmid = {33012687},
  keywords = {categorization,language,perception,prediction,top-down effects,vision},
  file = {/Users/xzfang/Zotero/storage/45JPN5H7/Lupyan et al. - 2020 - Effects of Language on Visual Perception.pdf;/Users/xzfang/Zotero/storage/2KN9M5V3/S1364-6613(20)30213-8.html}
}

@article{lupyan_words_2015,
  title = {Words and the {{World}}: {{Predictive Coding}} and the {{Language-Perception-Cognition Interface}}},
  shorttitle = {Words and the {{World}}},
  author = {Lupyan, Gary and Clark, Andy},
  year = {2015},
  month = aug,
  journal = {Current Directions in Psychological Science},
  volume = {24},
  number = {4},
  pages = {279--284},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/0963721415570732},
  abstract = {Can what we know change what we see? Does language affect cognition and perception? The last few years have seen increased attention to these seemingly disparate questions, but with little theoretical advance. We argue that substantial clarity can be gained by considering these questions through the lens of predictive processing, a framework in which mental representations\textemdash from the perceptual to the cognitive\textemdash reflect an interplay between downward-flowing predictions and upward-flowing sensory signals. This framework provides a parsimonious account of how (and when) what we know ought to change what we see and helps us understand how a putatively high-level trait such as language can impact putatively low-level processes such as perception. Within this framework, language begins to take on a surprisingly central role in cognition by providing a uniquely focused and flexible means of constructing predictions against which sensory signals can be evaluated. Predictive processing thus provides a plausible mechanism for many of the reported effects of language on perception, thought, and action, and new insights on how and when speakers of different languages construct the same ``reality'' in alternate ways.},
  langid = {english},
  keywords = {attention,language,language and thought,perception,predictive coding,top-down effects},
  file = {/Users/xzfang/Zotero/storage/HZBVCMPW/Lupyan and Clark - 2015 - Words and the World Predictive Coding and the Lan.pdf}
}

@article{lupyan_wordsasmappings_2019,
  title = {From Words-as-Mappings to Words-as-Cues: The Role of Language in Semantic Knowledge},
  shorttitle = {From Words-as-Mappings to Words-as-Cues},
  author = {Lupyan, Gary and Lewis, Molly},
  year = {2019},
  month = nov,
  journal = {Language, Cognition and Neuroscience},
  volume = {34},
  number = {10},
  pages = {1319--1337},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2017.1404114},
  abstract = {Semantic knowledge (or semantic memory) is knowledge we have about the world. For example, we know that knives are typically sharp, made of metal, and that they are tools used for cutting. To what kinds of experiences do we owe such knowledge? Most work has stressed the role of direct sensory and motor experiences. Another kind of experience, considerably less well understood, is our experience with language. We review two ways of thinking about the relationship between language and semantic knowledge: (i) language as mapping onto independently-acquired concepts, and (ii) language as a set of cues to meaning. We highlight some problems with the words-as-mappings view, and argue in favour of the words-as-cues alternative. We then review some surprising ways that language impacts semantic knowledge, and discuss how distributional semantics models can help us better understand its role. We argue that language has an abstracting effect on knowledge, helping to go beyond concrete experiences which are more characteristic of perception and action. We conclude by describing several promising directions for future research.},
  keywords = {Concepts,knowledge,language,semantics,words},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2017.1404114},
  file = {/Users/xzfang/Zotero/storage/Z6P97QDJ/Lupyan and Lewis - 2019 - From words-as-mappings to words-as-cues the role .pdf;/Users/xzfang/Zotero/storage/DTHUL9BA/23273798.2017.html}
}

@article{luthra_boosting_2021,
  title = {Boosting Lexical Support Does Not Enhance Lexically Guided Perceptual Learning.},
  author = {Luthra, Sahil and Magnuson, James S. and Myers, Emily B.},
  year = {2021},
  month = apr,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {47},
  number = {4},
  pages = {685--704},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000945},
  abstract = {A challenge for listeners is to learn the appropriate mapping between acoustics and phonetic categories for an individual talker. Lexically guided perceptual learning (LGPL) studies have shown that listeners can leverage lexical knowledge to guide this process. For instance, listeners learn to interpret ambiguous /s/-/Í/ blends as /s/ if they have previously encountered them in /s/-biased contexts like epi?ode. Here, we examined whether the degree of preceding lexical support might modulate the extent of perceptual learning. In Experiment 1, we first demonstrated that perceptual learning could be obtained in a modified LGPL paradigm where listeners were first biased to interpret ambiguous tokens as one phoneme (e.g., /s/) and then later as another (e.g., /Í/). In subsequent experiments, we tested whether the extent of learning differed depending on whether targets encountered predictive contexts or neutral contexts prior to the auditory target (e.g., epi?ode). Experiment 2 used auditory sentence contexts (e.g., ``I love The Walking Dead and eagerly await every new . . .''), whereas Experiment 3 used written sentence contexts. In Experiment 4, participants did not receive sentence contexts but rather saw the written form of the target word (episode) or filler text (\#\#\#\#\#\#\#\#) prior to hearing the critical auditory token. While we consistently observed effects of context on in-the-moment processing of critical words, the size of the learning effect was not modulated by the type of context. We hypothesize that boosting lexical support through preceding context may not strongly influence perceptual learning when ambiguous speech sounds can be identified solely from lexical information.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/M2KUMDVT/Luthra et al. - 2021 - Boosting lexical support does not enhance lexicall.pdf}
}

@article{luthra_lexical_2020,
  title = {Lexical {{Information Guides Retuning}} of {{Neural Patterns}} in {{Perceptual Learning}} for {{Speech}}},
  author = {Luthra, Sahil and Correia, Jo{\~a}o M. and Kleinschmidt, Dave F. and Mesite, Laura and Myers, Emily B.},
  year = {2020},
  month = oct,
  journal = {Journal of Cognitive Neuroscience},
  volume = {32},
  number = {10},
  pages = {2001--2012},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_01612},
  abstract = {Abstract             A listener's interpretation of a given speech sound can vary probabilistically from moment to moment. Previous experience (i.e., the contexts in which one has encountered an ambiguous sound) can further influence the interpretation of speech, a phenomenon known as perceptual learning for speech. This study used multivoxel pattern analysis to query how neural patterns reflect perceptual learning, leveraging archival fMRI data from a lexically guided perceptual learning study conducted by Myers and Mesite [Myers, E. B., \& Mesite, L. M. Neural systems underlying perceptual adjustment to non-standard speech tokens. Journal of Memory and Language, 76, 80\textendash 93, 2014]. In that study, participants first heard ambiguous /s/\textendash/{$\int$}/ blends in either /s/-biased lexical contexts (epi\_ode) or /{$\int$}/-biased contexts (refre\_ing); subsequently, they performed a phonetic categorization task on tokens from an /asi/\textendash/a{$\int$}i/ continuum. In the current work, a classifier was trained to distinguish between phonetic categorization trials in which participants heard unambiguous productions of /s/ and those in which they heard unambiguous productions of /{$\int$}/. The classifier was able to generalize this training to ambiguous tokens from the middle of the continuum on the basis of individual participants' trial-by-trial perception. We take these findings as evidence that perceptual learning for speech involves neural recalibration, such that the pattern of activation approximates the perceived category. Exploratory analyses showed that left parietal regions (supramarginal and angular gyri) and right temporal regions (superior, middle, and transverse temporal gyri) were most informative for categorization. Overall, our results inform an understanding of how moment-to-moment variability in speech perception is encoded in the brain.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YMTQNBLK/Luthra et al. - 2020 - Lexical Information Guides Retuning of Neural Patt.pdf}
}

@article{luthra_listener_2021,
  title = {Listener Expectations and the Perceptual Accommodation of Talker Variability: {{A}} Pre-Registered Replication},
  shorttitle = {Listener Expectations and the Perceptual Accommodation of Talker Variability},
  author = {Luthra, Sahil and Saltzman, David and Myers, Emily B. and Magnuson, James S.},
  year = {2021},
  month = aug,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {83},
  number = {6},
  pages = {2367--2376},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-021-02317-x},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XR8N4VZ8/Luthra et al. - 2021 - Listener expectations and the perceptual accommoda.pdf}
}

@article{luthra_neural_2019,
  title = {Neural Substrates of Subphonemic Variation and Lexical Competition in Spoken Word Recognition},
  author = {Luthra, Sahil and Guediche, Sara and Blumstein, Sheila E. and Myers, Emily B.},
  year = {2019},
  journal = {Language, cognition and neuroscience},
  volume = {34},
  number = {2},
  pages = {151--169},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1531140},
  abstract = {In spoken word recognition, subphonemic variation influences lexical activation, with sounds near a category boundary increasing phonetic competition as well as lexical competition. The current study investigated the interplay of these factors using a visual world task in which participants were instructed to look at a picture of an auditory target (e.g., peacock). Eyetracking data indicated that participants were slowed when a voiced onset competitor (e.g., beaker) was also displayed, and this effect was amplified when acoustic-phonetic competition was increased. Simultaneously-collected fMRI data showed that several brain regions were sensitive to the presence of the onset competitor, including the supramarginal, middle temporal, and inferior frontal gyri, and functional connectivity analyses revealed that the coordinated activity of left frontal regions depends on both acoustic-phonetic and lexical factors. Taken together, results suggest a role for frontal brain structures in resolving lexical competition, particularly as atypical acoustic-phonetic information maps on to the lexicon.},
  pmcid = {PMC6516505},
  pmid = {31106225},
  file = {/Users/xzfang/Zotero/storage/JGMJ9CEX/Luthra et al. - 2019 - Neural substrates of subphonemic variation and lex.pdf}
}

@article{luthra_perceptual_2021,
  title = {Perceptual Learning of Multiple Talkers Requires Additional Exposure},
  author = {Luthra, Sahil and Mechtenberg, Hannah and Myers, Emily B.},
  year = {2021},
  month = mar,
  journal = {Attention, Perception, \& Psychophysics},
  issn = {1943-393X},
  doi = {10.3758/s13414-021-02261-w},
  abstract = {Because different talkers produce their speech sounds differently, listeners benefit from maintaining distinct generative models (sets of beliefs) about the correspondence between acoustic information and phonetic categories for different talkers. A robust literature on phonetic recalibration indicates that when listeners encounter a talker who produces their speech sounds idiosyncratically (e.g., a talker who produces their /s/ sound atypically), they can update their generative model for that talker. Such recalibration has been shown to occur in a relatively talker-specific way. Because listeners in ecological situations often meet several new talkers at once, the present study considered how the process of simultaneously updating two distinct generative models compares to updating one model at a time. Listeners were exposed to two talkers, one who produced /s/ atypically and one who produced /{$\int$}/ atypically. Critically, these talkers only produced these sounds in contexts where lexical information disambiguated the phoneme's identity (e.g., epi\_ode, flouri\_ing). When initial exposure to the two talkers was blocked by voice (Experiment 1), listeners recalibrated to these talkers after relatively little exposure to each talker (32 instances per talker, of which 16 contained ambiguous fricatives). However, when the talkers were intermixed during learning (Experiment 2), listeners required more exposure trials before they were able to adapt to the idiosyncratic productions of these talkers (64 instances per talker, of which 32 contained ambiguous fricatives). Results suggest that there is a perceptual cost to simultaneously updating multiple distinct generative models, potentially because listeners must first select which generative model to update.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/L6XQDJRI/Luthra et al. - 2021 - Perceptual learning of multiple talkers requires a.pdf}
}

@article{luthra_robust_2021,
  title = {Robust {{Lexically Mediated Compensation}} for {{Coarticulation}}: {{Christmash Time Is Here Again}}},
  shorttitle = {Robust {{Lexically Mediated Compensation}} for {{Coarticulation}}},
  author = {Luthra, Sahil and {Peraza-Santiago}, Giovanni and Beeson, Keia'na and Saltzman, David and Crinnion, Anne Marie and Magnuson, James S.},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {4},
  pages = {e12962},
  issn = {1551-6709},
  doi = {10.1111/cogs.12962},
  abstract = {A long-standing question in cognitive science is how high-level knowledge is integrated with sensory input. For example, listeners can leverage lexical knowledge to interpret an ambiguous speech sound, but do such effects reflect direct top-down influences on perception or merely postperceptual biases? A critical test case in the domain of spoken word recognition is lexically mediated compensation for coarticulation (LCfC). Previous LCfC studies have shown that a lexically restored context phoneme (e.g., /s/ in Christma\#) can alter the perceived place of articulation of a subsequent target phoneme (e.g., the initial phoneme of a stimulus from a tapes-capes continuum), consistent with the influence of an unambiguous context phoneme in the same position. Because this phoneme-to-phoneme compensation for coarticulation is considered sublexical, scientists agree that evidence for LCfC would constitute strong support for top\textendash down interaction. However, results from previous LCfC studies have been inconsistent, and positive effects have often been small. Here, we conducted extensive piloting of stimuli prior to testing for LCfC. Specifically, we ensured that context items elicited robust phoneme restoration (e.g., that the final phoneme of Christma\# was reliably identified as /s/) and that unambiguous context-final segments (e.g., a clear /s/ at the end of Christmas) drove reliable compensation for coarticulation for a subsequent target phoneme. We observed robust LCfC in a well-powered, preregistered experiment with these pretested items (N = 40) as well as in a direct replication study (N = 40). These results provide strong evidence in favor of computational models of spoken word recognition that include top\textendash down feedback.},
  langid = {english},
  keywords = {Cognitive penetrability,Computational model,Feedback,Language,Speech perception,Top-down effects},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12962},
  file = {/Users/xzfang/Zotero/storage/GV3HXCRG/Luthra et al. - 2021 - Robust Lexically Mediated Compensation for Coartic.pdf;/Users/xzfang/Zotero/storage/5ACXFNTF/cogs.html}
}

@article{lyu_neural_2019,
  title = {Neural Dynamics of Semantic Composition},
  author = {Lyu, Bingjiang and Choi, Hun S. and {Marslen-Wilson}, William D. and Clarke, Alex and Randall, Billi and Tyler, Lorraine K.},
  year = {2019},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {42},
  pages = {21318--21327},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1903402116},
  abstract = {Human speech comprehension is remarkable for its immediacy and rapidity. The listener interprets an incrementally delivered auditory input, millisecond by millisecond as it is heard, in terms of complex multilevel representations of relevant linguistic and nonlinguistic knowledge. Central to this process are the neural computations involved in semantic combination, whereby the meanings of words are combined into more complex representations, as in the combination of a verb and its following direct object (DO) noun (e.g., ``eat the apple''). These combinatorial processes form the backbone for incremental interpretation, enabling listeners to integrate the meaning of each word as it is heard into their dynamic interpretation of the current utterance. Focusing on the verb-DO noun relationship in simple spoken sentences, we applied multivariate pattern analysis and computational semantic modeling to source-localized electro/magnetoencephalographic data to map out the specific representational constraints that are constructed as each word is heard, and to determine how these constraints guide the interpretation of subsequent words in the utterance. Comparing context-independent semantic models of the DO noun with contextually constrained noun models reflecting the semantic properties of the preceding verb, we found that only the contextually constrained model showed a significant fit to the brain data. Pattern-based measures of directed connectivity across the left hemisphere language network revealed a continuous information flow among temporal, inferior frontal, and inferior parietal regions, underpinning the verb's modification of the DO noun's activated semantics. These results provide a plausible neural substrate for seamless real-time incremental interpretation on the observed millisecond time scales.},
  chapter = {PNAS Plus},
  copyright = {Copyright \textcopyright{} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {31570590},
  keywords = {computational modelling,directed connectivity,EEG/MEG,RSA,speech},
  file = {/Users/xzfang/Zotero/storage/DIS6LQ8V/Lyu et al. - 2019 - Neural dynamics of semantic composition.pdf;/Users/xzfang/Zotero/storage/42HJGL5E/21318.html}
}

@article{ma_behavior_2011,
  title = {Behavior and Neural Basis of Near-Optimal Visual Search},
  author = {Ma, Wei Ji and Navalpakkam, Vidhya and Beck, Jeffrey M. and van den Berg, Ronald and Pouget, Alexandre},
  year = {2011},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {14},
  number = {6},
  pages = {783--790},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2814},
  abstract = {When making a decision, we have to take into account not only the information that is available, but also the reliability of this information. This behavioral study finds that, while searching for a visual target, people weigh up cue reliability in an almost identical fashion to a mathematical ideal observer, and a neural network model can explain how this behavior is produced.},
  copyright = {2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Network models;Perception;Visual system Subject\_term\_id: network-models;perception;visual-system},
  file = {/Users/xzfang/Zotero/storage/2J8SNZ9L/Ma et al. - 2011 - Behavior and neural basis of near-optimal visual s.pdf;/Users/xzfang/Zotero/storage/3SB4JPQL/nn.html}
}

@article{maddox_influence_2012,
  title = {Influence of {{Task-Relevant}} and {{Task-Irrelevant Feature Continuity}} on {{Selective Auditory Attention}}},
  author = {Maddox, Ross K. and {Shinn-Cunningham}, Barbara G.},
  year = {2012},
  month = feb,
  journal = {JARO: Journal of the Association for Research in Otolaryngology},
  volume = {13},
  number = {1},
  pages = {119--129},
  issn = {1525-3961},
  doi = {10.1007/s10162-011-0299-7},
  abstract = {Past studies have explored the relative strengths of auditory features in a selective attention task by pitting features against one another and asking listeners to report the words perceived in a given sentence. While these studies show that the continuity of competing features affects streaming, they did not address whether the influence of specific features is modulated by volitionally directed attention. Here, we explored whether the continuity of a task-irrelevant feature affects the ability to selectively report one of two competing speech streams when attention is specifically directed to a different feature. Sequences of simultaneous pairs of spoken digits were presented in which exactly one digit of each pair matched a primer phrase in pitch and exactly one digit of each pair matched the primer location. Within a trial, location and pitch were randomly paired; they either were consistent with each other from digit to digit or were switched (e.g., the sequence from the primer's location changed pitch across digits). In otherwise identical blocks, listeners were instructed to report digits matching the primer either in location or in pitch. Listeners were told to ignore the irrelevant feature, if possible, in order to perform well. Listener responses depended on task instructions, proving that top\textendash down attention alters how a subject performs the task. Performance improved when the separation of the target and masker in the task-relevant feature increased. Importantly, the values of the task-irrelevant feature also influenced performance in some cases. Specifically, when instructed to attend location, listeners performed worse as the separation between target and masker pitch increased, especially when the spatial separation between digits was small. These results indicate that task-relevant and task-irrelevant features are perceptually bound together: continuity of task-irrelevant features influences selective attention in an automatic, obligatory manner, consistent with the idea that auditory attention operates on objects.},
  pmcid = {PMC3254717},
  pmid = {22124889},
  file = {/Users/xzfang/Zotero/storage/HGVTQ4AU/Maddox and Shinn-Cunningham - 2012 - Influence of Task-Relevant and Task-Irrelevant Fea.pdf}
}

@article{magnuson_acoustic_2007,
  title = {Acoustic Differences, Listener Expectations, and the Perceptual Accommodation of Talker Variability.},
  author = {Magnuson, James S. and Nusbaum, Howard C.},
  year = {2007},
  month = apr,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  number = {2},
  pages = {391},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1277},
  doi = {10.1037/0096-1523.33.2.391},
  file = {/Users/xzfang/Zotero/storage/SST4VPRJ/Magnuson and Nusbaum - Acoustic differences, listener expectations, and t.pdf;/Users/xzfang/Zotero/storage/BBS5P9CD/2007-04962-009.html}
}

@article{magnuson_dynamics_2007,
  title = {The {{Dynamics}} of {{Lexical Competition During Spoken Word Recognition}}},
  author = {Magnuson, James S. and Dixon, James A. and Tanenhaus, Michael K. and Aslin, Richard N.},
  year = {2007},
  journal = {Cognitive Science},
  volume = {31},
  number = {1},
  pages = {133--156},
  issn = {1551-6709},
  doi = {10.1080/03640210709336987},
  abstract = {The sounds that make up spoken words are heard in a series and must be mapped rapidly onto words in memory because their elements, unlike those of visual words, cannot simultaneously exist or persist in time. Although theories agree that the dynamics of spoken word recognition are important, they differ in how they treat the nature of the competitor set\textemdash precisely which words are activated as an auditory word form unfolds in real time. This study used eye tracking to measure the impact over time of word frequency and 2 partially overlapping competitor set definitions: onset density and neighborhood density. Time course measures revealed early and continuous effects of frequency (facilitatory) and on set based similarity (inhibitory). Neighborhood density appears to have early facilitatory effects and late inhibitory effects. The late inhibitory effects are due to differences in the temporal distribution of similarity within neighborhoods. The early facilitatory effects are due to subphonemic cues that inform the listener about word length before the entire word is heard. The results support a new conception of lexical competition neighborhoods in which recognition occurs against a background of activated competitors that changes over time based on fine-grained goodness-of-fit and competition dynamics.},
  copyright = {2007 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Eye tracking,Psycholinguistics,Spoken word recognition},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1080/03640210709336987},
  file = {/Users/xzfang/Zotero/storage/GARQFHFZ/Magnuson et al. - 2007 - The Dynamics of Lexical Competition During Spoken .pdf;/Users/xzfang/Zotero/storage/BZL2L5N3/03640210709336987.html}
}

@article{magnuson_earshot_2020,
  title = {{{EARSHOT}}: {{A Minimal Neural Network Model}} of {{Incremental Human Speech Recognition}}},
  shorttitle = {{{EARSHOT}}},
  author = {Magnuson, James S. and You, Heejo and Luthra, Sahil and Li, Monica and Nam, Hosung and Escab{\'i}, Monty and Brown, Kevin and Allopenna, Paul D. and Theodore, Rachel M. and Monto, Nicholas and Rueckl, Jay G.},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {4},
  pages = {e12823},
  issn = {1551-6709},
  doi = {10.1111/cogs.12823},
  abstract = {Despite the lack of invariance problem (the many-to-many mapping between acoustics and percepts), human listeners experience phonetic constancy and typically perceive what a speaker intends. Most models of human speech recognition (HSR) have side-stepped this problem, working with abstract, idealized inputs and deferring the challenge of working with real speech. In contrast, carefully engineered deep learning networks allow robust, real-world automatic speech recognition (ASR). However, the complexities of deep learning architectures and training regimens make it difficult to use them to provide direct insights into mechanisms that may support HSR. In this brief article, we report preliminary results from a two-layer network that borrows one element from ASR, long short-term memory nodes, which provide dynamic memory for a range of temporal spans. This allows the model to learn to map real speech from multiple talkers to semantic targets with high accuracy, with human-like timecourse of lexical access and phonological competition. Internal representations emerge that resemble phonetically organized responses in human superior temporal gyrus, suggesting that the model develops a distributed phonological code despite no explicit training on phonetic or phonemic targets. The ability to work with real speech is a major advance for cognitive models of HSR.},
  langid = {english},
  keywords = {Computational modeling,Human speech recognition,Neurobiology of language},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12823},
  file = {/Users/xzfang/Zotero/storage/6G8V4YYR/Magnuson et al. - 2020 - EARSHOT A Minimal Neural Network Model of Increme.pdf;/Users/xzfang/Zotero/storage/U66M9SLS/cogs.html}
}

@inbook{magnuson_spoken_1999,
  title = {Spoken {{Word Recognition}} in the {{Visual World Paradigm Reflects}} the {{Structure}} of the {{Entire Lexicon}}},
  booktitle = {Proceedings of the {{Twenty First Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Magnuson, James S. and Tanenhaus, Michael K. and Aslin, Richard N. and Dahan, Delphine},
  year = {1999},
  edition = {First},
  pages = {331--336},
  publisher = {{Psychology Press}},
  address = {{New York}},
  doi = {10.4324/9781410603494-63},
  abstract = {When subjects are asked to move items in a visual display in response to spoken instructions, their eye movements are closely time-locked to the unfolding speech signal. A recently developed eye-tracking method, the ``visual world paradigm'', exploits this phenomenon to provide a sensitive, continuous measure of ambiguity resolution in language processing phenomena, including competition effects in spoken word recognition (Tanenhaus, SpiveyKnowlton, Eberhard, \& Sedivy, 1995). With this method, competition is typically measured between names of objects which are simultaneously displayed in front of the subject. This means that fixation probabilities may not reflect competition within the entire lexicon, but only that among items which become active because they are displayed simultaneously. To test this, we created a small, artificial lexicon with specific lexical similarity characteristics. Subjects learned novel names for 16 novel geometric objects. Objects were presented with high, medium or low frequency during training. Each lexical item had two potential competitors. The crucial comparison was between high-frequency items which had either high- or low-frequency competitors. In spoken word recognition, performance is correlated with the number of frequencyweighted neighbors (phonologically similar words) a word has, suggesting that neighbors compete for recognition as a function of frequency and similarity (e.g., Luce \& Pisoni, 1998). We found that in the visual world paradigm, fixation probabilities for items with high-frequency neighbors were delayed compared to those for items with low-frequency neighbors, even when the items were presented with unrelated items. This indicates that fixation probabilities reflect the internal structure of the lexicon, and not just the characteristics of displayed items.},
  collaborator = {Hahn, Martin and Stoness, Scott C.},
  isbn = {978-1-4106-0349-4},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZYZEINWC/Magnuson et al. - 2020 - Spoken Word Recognition in the Visual World Paradi.pdf}
}

@article{magnuson_talker_2021,
  title = {Talker Familiarity and the Accommodation of Talker Variability},
  author = {Magnuson, James S. and Nusbaum, Howard C. and {Akahane-Yamada}, Reiko and Saltzman, David},
  year = {2021},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {83},
  number = {4},
  pages = {1842--1860},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-020-02203-y},
  abstract = {A fundamental problem in speech perception is how (or whether) listeners accommodate variability in the way talkers produce speech. One view of the way listeners cope with this variability is that talker differences are normalized \textendash{} a mapping between talker-specific characteristics and phonetic categories is computed such that speech is recognized in the context of the talker's vocal characteristics. Consistent with this view, listeners process speech more slowly when the talker changes randomly than when the talker remains constant. An alternative view is that speech perception is based on talker-specific auditory exemplars in memory clustered around linguistic categories that allow talker-independent perception. Consistent with this view, listeners become more efficient at talker-specific phonetic processing after voice identification training. We asked whether phonetic efficiency would increase with talker familiarity by testing listeners with extremely familiar talkers (family members), newly familiar talkers (based on laboratory training), and unfamiliar talkers. We also asked whether familiarity would reduce the need for normalization. As predicted, phonetic efficiency (word recognition in noise) increased with familiarity (unfamiliar {$<$} trainedon {$<$} family). However, we observed a constant processing cost for talker changes even for pairs of family members. We discuss how normalization and exemplar theories might account for these results, and constraints the results impose on theoretical accounts of phonetic constancy.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ASSWUQQZ/Magnuson et al. - 2021 - Talker familiarity and the accommodation of talker.pdf}
}

@article{magnuson_time_2003,
  title = {The Time Course of Spoken Word Learning and Recognition: {{Studies}} with Artificial Lexicons.},
  shorttitle = {The Time Course of Spoken Word Learning and Recognition},
  author = {Magnuson, James S. and Tanenhaus, Michael K. and Aslin, Richard N. and Dahan, Delphine},
  year = {2003},
  journal = {Journal of Experimental Psychology: General},
  volume = {132},
  number = {2},
  pages = {202--227},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/0096-3445.132.2.202},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HNI62HVJ/Magnuson et al. - 2003 - The time course of spoken word learning and recogn.pdf}
}

@article{mahowald_grammatical_2022,
  title = {Grammatical Cues Are Largely, but Not Completely, Redundant with Word Meanings in Natural Language},
  author = {Mahowald, Kyle and Diachek, Evgeniia and Gibson, Edward and Fedorenko, Evelina and Futrell, Richard},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.12911 [cs]},
  eprint = {2201.12911},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The combinatorial power of language has historically been argued to be enabled by syntax: rules that allow words to combine hierarchically to convey complex meanings. But how important are these rules in practice? We performed a broad-coverage cross-linguistic investigation of the importance of grammatical cues for interpretation. First, English and Russian speakers (n=484) were presented with subjects, verbs, and objects (in random order and with morphological markings removed) extracted from naturally occurring sentences, and were asked to identify which noun is the agent of the action. Accuracy was high in both languages (\textasciitilde 89\% in English, \textasciitilde 87\% in Russian), suggesting that word meanings strongly constrain who is doing what to whom. Next, we trained a neural network machine classifier on a similar task: predicting which nominal in a subject-verb-object triad is the subject. Across 30 languages from eight language families, performance was consistently high: a median accuracy of 87\%, comparable to the accuracy observed in the human experiments. These results have ramifications for any theory of why languages look the way that they do, and seemingly pose a challenge for efficiency-based theories: why have grammatical cues for argument role if they only have utility in 10-15\% of sentences? We suggest that although grammatical cues are not usually necessary, they are useful in the rare cases when the intended meaning cannot be inferred from the words alone, including descriptions of human interactions, where roles are often reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing non-canonical meanings (e.g., the man bit the dog). Importantly, for such cues to be useful, they have to be reliable, which means being ubiquitously used, including when they are not needed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/MBWBDUFV/Mahowald et al. - 2022 - Grammatical cues are largely, but not completely, .pdf;/Users/xzfang/Zotero/storage/JYKTS56A/2201.html}
}

@article{makeig_erp_2012,
  title = {{{ERP}} Features and {{EEG}} Dynamics: {{An ICA}} Perspective},
  shorttitle = {{{ERP}} Features and {{EEG}} Dynamics},
  author = {Makeig, Scott and Onton, Julie},
  year = {2012},
  month = jan,
  journal = {The Oxford Handbook of Event-Related Potential Components},
  doi = {10.1093/oxfordhb/9780195374148.013.0035},
  abstract = {This chapter considers the relationship between ongoing electroencephalographic (EEG) activity as recorded in event-related paradigms and trial averages time locked to some class of experimental events, known as eventrelated potentials (ERPs). It first discusses the concept of the ERP as averaging potentials generated by spatially coherent activity within a number of cortical EEG source areas as well as nonbrain sources typically treated as data artifacts. Event-related potential image (ERP-image) plotting is used to visualize variability in EEG dynamics across trials associated with events of interest using an example data set. The concept and use of independent component analysis (ICA) is then introduced to undo the effects of source signal mixing at scalp electrodes and to identify EEG sources contributing to the averaged ERP. After presenting some basic time-frequency measures useful for studying trial-to-trial variability, the chapter takes another look at trial-to-trial variability, now focusing on the contributions of selected independent component processes to the recorded scalp signals.}
}

@article{malik-moraleda_domaingeneral_2021,
  title = {The {{Domain-General Multiple Demand}} Network Is {{More Active}} in {{Early Balanced Bilinguals}} than {{Monolinguals During Executive Processing}}},
  author = {{Malik-Moraleda}, Saima and Cucu, Theodor and Lipkin, Benjamin and Fedorenko, Evelina},
  year = {2021},
  month = oct,
  journal = {Neurobiology of Language},
  pages = {1--36},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00058},
  abstract = {The bilingual experience may place special cognitive demands on speakers and has been argued to lead to improvements in domain-general executive abilities, like cognitive control and working memory. Such improvements have been argued for based on both behavioral and brain imaging evidence. However, the empirical landscape is complex and ridden by controversy. Here we attempt to shed light on this question through an fMRI investigation of relatively large, relatively homogeneous, and carefully matched samples of early balanced bilinguals (n=55) and monolinguals (n=54) using robust, previously validated individual-level markers of neural activity in the domain-general Multiple Demand (MD) network, which supports executive functions. We find that the bilinguals, compared to the monolinguals, show significantly stronger neural responses to an executive (spatial working memory) task, and a larger difference between a harder and an easier condition of the task, across the MD network. These stronger neural responses are accompanied by better behavioral performance on the working memory task. We further show that the bilingual-vs.-monolingual difference in neural responses is not ubiquitous across the brain as no group difference in magnitude is observed in primary visual areas, which also respond to the task. Although the neural group difference in the MD network appears robust, it remains difficult to causally link it to bilingual experience specifically.Dedication: We would like to dedicate this paper to the memory of Albert Costa, who we both knew well and loved as a mentor and a friend. Saima will always be grateful that Albert let her spend her senior year in his lab despite not even being from the same university; his support, mentorship and guidance helped her not stray away from academia when things got tough. And Ev will forever remember the weekly Friday night partying with Albert and the rest of the ``crew'' in The Cellar and The People's Republik during her undergrad years in the Caramazza Lab in the late 1990s and early 2000s.},
  file = {/Users/xzfang/Zotero/storage/EJVH33GE/Malik-Moraleda et al. - 2021 - The Domain-General Multiple Demand network is More.pdf;/Users/xzfang/Zotero/storage/5RJA2JZE/The-Domain-General-Multiple-Demand-network-is-More.html}
}

@article{maljkovic_priming_1994,
  title = {Priming of Pop-out: {{I}}. {{Role}} of Features},
  shorttitle = {Priming of Pop-Out},
  author = {Maljkovic, V. and Nakayama, K.},
  year = {1994},
  month = nov,
  journal = {Memory \& Cognition},
  volume = {22},
  number = {6},
  pages = {657--672},
  issn = {0090-502X},
  doi = {10.3758/bf03209251},
  abstract = {We examined a visual search task, in which observers responded to the high-acuity aspect of a pop-out target (shape of an odd-colored diamond or vernier offset of an odd spatial-frequency patch). Repetition of the attention-driving feature (color or spatial frequency) in this task primes the pop-out; repetition of the high-acuity aspect (shape, vernier offset) does not. Priming of pop-out is due to a decaying memory trace of the attention-focusing feature laid down with each trial. The trace exerts a diminishing effect over the following five to eight trials (approximately 30 sec), and its influence over this time is cumulative. Observers cannot wilfully overcome the priming, which suggests that it is passive and autonomous. Both target facilitation and distractor inhibition are evident; the former has a greater effect. The phenomenon shows complete binocular transfer.},
  langid = {english},
  pmid = {7808275},
  keywords = {Attention,Color Perception,Form Perception,Humans,Memory; Short-Term,Reaction Time,Visual Acuity,Visual Perception},
  file = {/Users/xzfang/Zotero/storage/HKK9X55W/Maljkovic and Nakayama - 1994 - Priming of pop-out I. Role of features.pdf}
}

@article{manilow_simultaneous_2020,
  title = {Simultaneous {{Separation}} and {{Transcription}} of {{Mixtures}} with {{Multiple Polyphonic}} and {{Percussive Instruments}}},
  author = {Manilow, Ethan and Seetharaman, Prem and Pardo, Bryan},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.12621 [cs, eess]},
  eprint = {1910.12621},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We present a single deep learning architecture that can both separate an audio recording of a musical mixture into constituent single-instrument recordings and transcribe these instruments into a human-readable format at the same time, learning a shared musical representation for both tasks. This novel architecture, which we call Cerberus, builds on the Chimera network for source separation by adding a third "head" for transcription. By training each head with different losses, we are able to jointly learn how to separate and transcribe up to 5 instruments in our experiments with a single network. We show that the two tasks are highly complementary with one another and when learned jointly, lead to Cerberus networks that are better at both separation and transcription and generalize better to unseen mixtures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/xzfang/Zotero/storage/LE82TUL6/Manilow et al. - 2020 - Simultaneous Separation and Transcription of Mixtu.pdf}
}

@article{mann_influence_,
  title = {Influence of Preceding Liquid on Stop-Consonant Perception},
  author = {Mann, Virginia A},
  pages = {6},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SH4MTYQ2/Mann - Influence of preceding liquid on stop-consonant pe.pdf}
}

@article{mann_influence_a,
  title = {Influence of Vocalic Context on Perception of the [{{J}}]-[s] Distinction},
  author = {Mann, Virginia A and Repp, Bruno H},
  pages = {16},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/X9FNW329/Mann and Repp - Influence of vocalic context on perception of the .pdf}
}

@article{mansimov_generating_2016,
  title = {Generating {{Images}} from {{Captions}} with {{Attention}}},
  author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  year = {2016},
  month = feb,
  journal = {arXiv:1511.02793 [cs]},
  eprint = {1511.02793},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/3AW57RYS/Mansimov et al. - 2016 - Generating Images from Captions with Attention.pdf;/Users/xzfang/Zotero/storage/RSYWQB4C/1511.html}
}

@article{mansimov_generating_2016a,
  title = {Generating {{Images}} from {{Captions}} with {{Attention}}},
  author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  year = {2016},
  month = feb,
  journal = {arXiv:1511.02793 [cs]},
  eprint = {1511.02793},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/2LNHS6Z6/Mansimov et al. - 2016 - Generating Images from Captions with Attention.pdf}
}

@article{mao_spatial_2021,
  title = {Spatial Modulation of Hippocampal Activity in Freely Moving Macaques},
  author = {Mao, Dun and Avila, Eric and Caziot, Baptiste and Laurens, Jean and Dickman, J. David and Angelaki, Dora E.},
  year = {2021},
  month = oct,
  journal = {Neuron},
  volume = {0},
  number = {0},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.09.032},
  langid = {english},
  pmid = {34644546},
  keywords = {motion tracking,non-human primate},
  file = {/Users/xzfang/Zotero/storage/J4CWGHFW/Mao et al. - 2021 - Spatial modulation of hippocampal activity in free.pdf;/Users/xzfang/Zotero/storage/5D64MNER/S0896-6273(21)00705-4.html}
}

@article{marcet_can_2018,
  title = {Can {{I}} Order a Burger at Rnacdonalds.Com? {{Visual}} Similarity Effects of Multi-Letter Combinations at the Early Stages of Word Recognition.},
  shorttitle = {Can {{I}} Order a Burger at Rnacdonalds.Com?},
  author = {Marcet, Ana and Perea, Manuel},
  year = {2018},
  month = may,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {44},
  number = {5},
  pages = {699--706},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000477},
  abstract = {Previous research has shown that early in the word recognition process, there is some degree of uncertainty concerning letter identity and letter position. Here, we examined whether this uncertainty also extends to the mapping of letter features onto letters, as predicted by the Bayesian Reader (Norris \& Kinoshita, 2012). Indeed, anecdotal evidence suggests that nonwords containing multi-letter homoglyphs (e.g., rn\textexclamdown m), such as docurnent, can be confusable with their base word. We conducted 2 masked priming lexical decision experiments in which the words/nonwords contained a middle letter that was visually similar to a multi-letter homoglyph (e.g., docurnent [rn\textendash m], presiclent [cl\textendash{} d]). Three types of primes were employed: identity, multi-letter homoglyph, and orthographic control. We used 2 commonly used fonts: Tahoma in Experiment 1 and Calibri in Experiment 2. Results in both experiments showed faster word identification times in the homoglyph condition than in the control condition (e.g., docurnento\textendash DOCUMENTO faster than docusnento\textendash DOCUMENTO). Furthermore, the homoglyph condition produced nearly the same latencies as the identity condition. These findings have important implications not only at a theoretical level (models of printed word recognition) but also at an applied level (Internet administrators/users).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TE38VRVS/Marcet and Perea - 2018 - Can I order a burger at rnacdonalds.com Visual si.pdf}
}

@article{marcet_nevtral_2017,
  title = {Is Nevtral {{NEUTRAL}}? {{Visual}} Similarity Effects in the Early Phases of Written-Word Recognition},
  shorttitle = {Is Nevtral {{NEUTRAL}}?},
  author = {Marcet, Ana and Perea, Manuel},
  year = {2017},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {24},
  number = {4},
  pages = {1180--1185},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1180-9},
  abstract = {For simplicity, contemporary models of written-word recognition and reading have unspecified feature/letter levels\textemdash they predict that the visually similar substituted-letter nonword PEQPLE is as effective at activating the word PEOPLE as the visually dissimilar substituted-letter nonword PEYPLE. Previous empirical evidence on the effects of visual similarly across letters during written-word recognition is scarce and nonconclusive. To examine whether visual similarity across letters plays a role early in word processing, we conducted two masked priming lexical decision experiments (stimulus-onset asynchrony = 50~ms). The substituted-letter primes were visually very similar to the target letters (u/v in Experiment 1 and i/j in Experiment 2; e.g., nevtral\textendash NEUTRAL). For comparison purposes, we included an identity prime condition (neutral\textendash NEUTRAL) and a dissimilar-letter prime condition (neztral-NEUTRAL). Results showed that the similar-letter prime condition produced faster word identification times than the dissimilar-letter prime condition. We discuss how models of written-word recognition should be amended to capture visual similarity effects across letters.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/P476JXHB/Marcet and Perea - 2017 - Is nevtral NEUTRAL Visual similarity effects in t.pdf}
}

@article{marion_music_2021,
  title = {The {{Music}} of {{Silence}}. {{Part I}}: {{Responses}} to {{Musical Imagery Encode Melodic Expectations}} and {{Acoustics}}},
  shorttitle = {The {{Music}} of {{Silence}}. {{Part I}}},
  author = {Marion, Guilhem and Di Liberto, Giovanni M. and Shamma, Shihab A.},
  year = {2021},
  month = aug,
  journal = {The Journal of Neuroscience},
  pages = {JN-RM-0183-21},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0183-21.2021},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/USLU4Y3C/Marion et al. - 2021 - The Music of Silence. Part I Responses to Musical.pdf}
}

@article{maris_nonparametric_2007,
  title = {Nonparametric Statistical Testing of {{EEG-}} and {{MEG-data}}},
  author = {Maris, Eric and Oostenveld, Robert},
  year = {2007},
  month = aug,
  journal = {Journal of Neuroscience Methods},
  volume = {164},
  number = {1},
  pages = {177--190},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2007.03.024},
  abstract = {In this paper, we show how ElectroEncephaloGraphic (EEG) and MagnetoEncephaloGraphic (MEG) data can be analyzed statistically using nonparametric techniques. Nonparametric statistical tests offer complete freedom to the user with respect to the test statistic by means of which the experimental conditions are compared. This freedom provides a straightforward way to solve the multiple comparisons problem (MCP) and it allows to incorporate biophysically motivated constraints in the test statistic, which may drastically increase the sensitivity of the statistical test. The paper is written for two audiences: (1) empirical neuroscientists looking for the most appropriate data analysis method, and (2) methodologists interested in the theoretical concepts behind nonparametric statistical tests. For the empirical neuroscientist, a large part of the paper is written in a tutorial-like fashion, enabling neuroscientists to construct their own statistical test, maximizing the sensitivity to the expected effect. And for the methodologist, it is explained why the nonparametric test is formally correct. This means that we formulate a null hypothesis (identical probability distribution in the different experimental conditions) and show that the nonparametric test controls the false alarm rate under this null hypothesis.},
  langid = {english},
  keywords = {EEG,Hypothesis testing,MEG,Multiple comparisons problem,Nonparametric statistical testing}
}

@article{maris_nonparametric_2007a,
  title = {Nonparametric Statistical Testing of Coherence Differences},
  author = {Maris, Eric and Schoffelen, Jan-Mathijs and Fries, Pascal},
  year = {2007},
  month = jun,
  journal = {Journal of Neuroscience Methods},
  volume = {163},
  number = {1},
  pages = {161--175},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2007.02.011},
  abstract = {Many important questions in neuroscience are about interactions between neurons or neuronal groups. These interactions are often quantified by coherence, which is a frequency-indexed measure that quantifies the extent to which two signals exhibit a consistent phase relation. In this paper, we consider the statistical testing of the difference between coherence values observed in two experimental conditions. We pay special attention to problems induced by (1) unequal sample sizes and (2) the fact that coherence is typically evaluated at a large number of frequency bins and between large numbers of pairs of neurons or neuronal groups (the multiple comparisons problem). We show that nonparametric statistical tests provide convincing and elegant solutions for both problems. We also show that these tests allow to incorporate biophysically motivated constraints in the test statistic, which may drastically increase the sensitivity of the test. Finally, we explain why the nonparametric test is formally correct. This means that we formulate a null hypothesis (identical probability distribution in the different experimental conditions) and show that the nonparametric test controls the false alarm rate under this null hypothesis. The proposed methodology is illustrated by analyses of data collected in a study on cortico-spinal coherence [Schoffelen JM, Oostenveld R, Fries P. Neuronal coherence as a mechanism of effective corticospinal interaction. Science 2005;308(5718):111-3].},
  langid = {english},
  keywords = {Coherence,Cortico-muscular coherence,Cortico-spinal coherence,Multiple comparisons problem,Nonparametric statistical testing}
}

@article{maris_statistical_2012,
  title = {Statistical Testing in Electrophysiological Studies},
  author = {Maris, Eric},
  year = {2012},
  journal = {Psychophysiology},
  volume = {49},
  number = {4},
  pages = {549--565},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.2011.01320.x},
  abstract = {This article describes the mechanics and rationale of four different approaches to the statistical testing of electrophysiological data: (1) the Neyman-Pearson approach, (2) the permutation-based approach, (3), the bootstrap-based approach, and (4) the Bayesian approach. These approaches are evaluated from the perspective of electrophysiological studies, which involve multivariate (i.e., spatiotemporal) observations in which source-level signals are picked up to a certain extent by all sensors. Besides formal statistical techniques, there are also techniques that do not involve probability calculations but are very useful in dealing with multivariate data (i.e., verification of data-based predictions, cross-validation, and localizers). Moreover, data-based decision making can also be informed by mechanistic evidence that is provided by the structure in the data.},
  langid = {english},
  keywords = {Bayesian inference,Bootstrap,Common pick-up,Electrophysiology,Mechanistic evidence,Multiple comparisons problem,Neurobiological data,Neyman-Pearson,Permutation test,Statistical test},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2011.01320.x},
  file = {/Users/xzfang/Zotero/storage/JHXUXA3J/Maris - 2012 - Statistical testing in electrophysiological studie.pdf;/Users/xzfang/Zotero/storage/79HQVXE3/j.1469-8986.2011.01320.html}
}

@article{marslen-wilson_levels_1994,
  title = {Levels of Perceptual Representation and Process in Lexical Access: {{Words}}, Phonemes, and Features},
  shorttitle = {Levels of Perceptual Representation and Process in Lexical Access},
  author = {{Marslen-Wilson}, William and Warren, Paul},
  year = {1994},
  journal = {Psychological Review},
  volume = {101},
  number = {4},
  pages = {653--675},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/0033-295X.101.4.653},
  abstract = {Three experiments and a simulation study investigate competing featural and phonemic views of the representation of the speech input in access to the mental lexicon. Auditory lexical decision and gating tasks show that the processing consequences of subcategorical mismatches (conflicts between phonetic cues to speech segment identity) depend on the lexical status of the conflicting cues, such that conflicts that only involve nonwords do not disrupt performance. A further study, using a phonetic-decision task with the same stimuli, found the same pattern. A simulation study shows that the interactive activation model TRACE, with top-down feedback to a prelexical phonemic level, does not model these effects successfully. The authors argue instead for a direct access featural model, based on a distributed computational substrate, where featural information is mapped directly onto lexical representations. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Processes,Lexical Access,Phonemes,Phonetics,Word Recognition,Words (Phonetic Units)},
  file = {/Users/xzfang/Zotero/storage/QWV3R4HV/1995-08265-001.html}
}

@article{martin_independent_2019,
  title = {Independent Contributions of Semantic and Phonological Working Memory to Spontaneous Speech in Acute Stroke},
  author = {Martin, Randi C. and Schnur, Tatiana T.},
  year = {2019},
  month = mar,
  journal = {Cortex},
  volume = {112},
  pages = {58--68},
  issn = {00109452},
  doi = {10.1016/j.cortex.2018.11.017},
  abstract = {Patients with left hemisphere stroke often have language deficits which impair their ability to produce phrases and sentences. One possible source of these speech impairments is the disruption of verbal working memory (WM). Single-case studies of chronic stroke have suggested the existence of a WM capacity specific to maintaining semantic information that is critical for preparing multiple words in phrases prior to speech onset (Martin \& Freedman, 2001; Martin \& He, 2004; Martin, Miller, \& Vu, 2004; Freedman, Martin, \& Biegler, 2004). The current study tested this hypothesis by examining spontaneous narrative language production and working memory capacities in a large sample of individuals at the acute stage of stroke (N=36), prior to the reorganization of function or strategy development. Here we show using a multiple regression approach that patients' semantic but not phonological WM capacity had an independent contribution in predicting phrasal elaboration and increasing utterance length whereas patients' phonological but not semantic WM capacity had an independent contribution in predicting a more rapid speech rate. Importantly, neither WM capacity independently predicted grammatical abilities in speech, implying that the other relations did not result from overall severity. These results indicate that separable semantic and phonological WM components exist that support different aspects of narrative speech. To our knowledge, this is the first study to examine spontaneous speech in a large group of acute stroke patients demonstrating a critical relationship between working memory and the ability to produce more words in phrases and longer utterances.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IKCEAXI4/Martin and Schnur - 2019 - Independent contributions of semantic and phonolog.pdf}
}

@article{martin_independent_2019a,
  title = {Independent Contributions of Semantic and Phonological Working Memory to Spontaneous Speech in Acute Stroke.},
  author = {Martin, R. C. and Schnur, T. T.},
  year = {2019},
  month = mar,
  journal = {Cortex; a journal devoted to the study of the nervous system and behavior},
  volume = {112},
  pages = {58--68},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.11.017},
  abstract = {Patients with left hemisphere stroke often have language deficits which impair their ability to produce phrases and sentences. One possible source of these speech impairments is the disruption of verbal working memory (WM). Single-case studies of chronic ...},
  langid = {english},
  pmid = {30577977},
  file = {/Users/xzfang/Zotero/storage/M458C3T8/Martin and Schnur - 2019 - Independent contributions of semantic and phonolog.pdf}
}

@article{martin_mechanism_2017,
  title = {A Mechanism for the Cortical Computation of Hierarchical Linguistic Structure},
  author = {Martin, Andrea E. and Doumas, Leonidas A. A.},
  year = {2017},
  month = mar,
  journal = {PLOS Biology},
  volume = {15},
  number = {3},
  pages = {e2000663},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000663},
  abstract = {Biological systems often detect species-specific signals in the environment. In humans, speech and language are species-specific signals of fundamental biological importance. To detect the linguistic signal, human brains must form hierarchical representations from a sequence of perceptual inputs distributed in time. What mechanism underlies this ability? One hypothesis is that the brain repurposed an available neurobiological mechanism when hierarchical linguistic representation became an efficient solution to a computational problem posed to the organism. Under such an account, a single mechanism must have the capacity to perform multiple, functionally related computations, e.g., detect the linguistic signal and perform other cognitive functions, while, ideally, oscillating like the human brain. We show that a computational model of analogy, built for an entirely different purpose\textemdash learning relational reasoning\textemdash processes sentences, represents their meaning, and, crucially, exhibits oscillatory activation patterns resembling cortical signals elicited by the same stimuli. Such redundancy in the cortical and machine signals is indicative of formal and mechanistic alignment between representational structure building and ``cortical'' oscillations. By inductive inference, this synergy suggests that the cortical signal reflects structure generation, just as the machine signal does. A single mechanism\textemdash using time to encode information across a layered network\textemdash generates the kind of (de)compositional representational hierarchy that is crucial for human language and offers a mechanistic linking hypothesis between linguistic representation and cortical computation.},
  langid = {english},
  keywords = {Language,Learning,Neurolinguistics,Semantics,Sentence processing,Speech,Speech signal processing,Syntax},
  file = {/Users/xzfang/Zotero/storage/YXWEPZTW/Martin and Doumas - 2017 - A mechanism for the cortical computation of hierar.pdf;/Users/xzfang/Zotero/storage/A7X8B48P/article.html}
}

@article{martinez_poor_2022,
  title = {Poor Writing, Not Specialized Concepts, Drives Processing Difficulty in Legal Language},
  author = {Mart{\'i}nez, Eric and Mollica, Francis and Gibson, Edward},
  year = {2022},
  month = jul,
  journal = {Cognition},
  volume = {224},
  pages = {105070},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105070},
  abstract = {Despite their ever-increasing presence in everyday life, contracts remain notoriously inaccessible to laypeople. Why? Here, a corpus analysis (n {$\approx$}10 million words) revealed that contracts contain startlingly high proportions of certain difficult-to-process features\textendash including low-frequency jargon, center-embedded clauses (leading to long-distance syntactic dependencies), passive voice structures, and non-standard capitalization\textendash relative to nine other baseline genres of written and spoken English. Two experiments (N=184) further revealed that excerpts containing these features were recalled and comprehended at lower rates than excerpts without these features, even for experienced readers, and that center-embedded clauses inhibited recall more-so than other features. These findings (a) undermine the specialized concepts account of legal theory, according to which law is a system built upon expert knowledge of technical concepts; (b) suggest such processing difficulties result largely from working-memory limitations imposed by long-distance syntactic dependencies (i.e., poor writing) as opposed to a mere lack of specialized legal knowledge; and (c) suggest editing out problematic features of legal texts would be tractable and beneficial for society at-large.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SYCZGTUL/S0010027722000580.html}
}

@article{martinho_ducklings_2016,
  title = {Ducklings Imprint on the Relational Concept of ``Same or Different''},
  author = {Martinho, Antone and Kacelnik, Alex},
  year = {2016},
  month = jul,
  journal = {Science},
  volume = {353},
  number = {6296},
  pages = {286--288},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf4247},
  abstract = {The ability to identify and retain logical relations between stimuli and apply them to novel stimuli is known as relational concept learning. This has been demonstrated in a few animal species after extensive reinforcement training, and it reveals the brain's ability to deal with abstract properties. Here we describe relational concept learning in newborn ducklings without reinforced training. Newly hatched domesticated mallards that were briefly exposed to a pair of objects that were either the same or different in shape or color later preferred to follow pairs of new objects exhibiting the imprinted relation. Thus, even in a seemingly rigid and very rapid form of learning such as filial imprinting, the brain operates with abstract conceptual reasoning, a faculty often assumed to be reserved to highly intelligent organisms.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  langid = {english},
  pmid = {27418508},
  file = {/Users/xzfang/Zotero/storage/Z88GHPY4/Martinho and Kacelnik - 2016 - Ducklings imprint on the relational concept of â€œsa.pdf;/Users/xzfang/Zotero/storage/IM9TQHEF/286.html}
}

@article{masarwa_larger_2022,
  title = {Larger Images Are Better Remembered during Naturalistic Encoding},
  author = {Masarwa, Shaimaa and Kreichman, Olga and {Gilaie-Dotan}, Sharon},
  year = {2022},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {4},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2119614119},
  abstract = {We are constantly exposed to multiple visual scenes, and while freely viewing them without an intentional effort to memorize or encode them, only some are remembered. It has been suggested that image memory is influenced by multiple factors, such as depth of processing, familiarity, and visual category. However, this is typically investigated when people are instructed to perform a task (e.g., remember or make some judgment about the images), which may modulate processing at multiple levels and thus, may not generalize to naturalistic visual behavior. Visual memory is assumed to rely on high-level visual perception that shows a level of size invariance and therefore is not assumed to be highly dependent on image size. Here, we reasoned that during naturalistic vision, free of task-related modulations, bigger images stimulate more visual system processing resources (from retina to cortex) and would, therefore, be better remembered. In an extensive set of seven experiments, na\"ive participants (n = 182) were asked to freely view presented images (sized 3\textdegree{} to 24\textdegree ) without any instructed encoding task. Afterward, they were given a surprise recognition test (midsized images, 50\% already seen). Larger images were remembered better than smaller ones across all experiments ({$\sim$}20\% higher accuracy or {$\sim$}1.5 times better). Memory was proportional to image size, faces were better remembered, and outdoors the least. Results were robust even when controlling for image set, presentation order, screen resolution, image scaling at test, or the amount of information. While multiple factors affect image memory, our results suggest that low- to high-level processes may all contribute to image memory.},
  chapter = {Biological Sciences},
  copyright = {Copyright \textcopyright{} 2022 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {35046050},
  keywords = {images,memory,naturalistic,size,vision},
  file = {/Users/xzfang/Zotero/storage/2JWINN9L/Masarwa et al. - 2022 - Larger images are better remembered during natural.pdf}
}

@article{masutomi_sound_2016,
  title = {Sound Segregation via Embedded Repetition Is Robust to Inattention.},
  author = {Masutomi, Keiko and Barascud, Nicolas and Kashino, Makio and McDermott, Josh H. and Chait, Maria},
  year = {2016},
  month = mar,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {42},
  number = {3},
  pages = {386--400},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/xhp0000147},
  abstract = {The segregation of sound sources from the mixture of sounds that enters the ear is a core capacity of human hearing, but the extent to which this process is dependent on attention remains unclear. This study investigated the effect of attention on the ability to segregate sounds via repetition. We utilized a dual task design in which stimuli to be segregated were presented along with stimuli for a ``decoy'' task that required continuous monitoring. The task to assess segregation presented a target sound 10 times in a row, each time concurrent with a different distractor sound. McDermott, Wrobleski, and Oxenham (2011) demonstrated that repetition causes the target sound to be segregated from the distractors. Segregation was queried by asking listeners whether a subsequent probe sound was identical to the target. A control task presented similar stimuli but probed discrimination without engaging segregation processes. We present results from 3 different decoy tasks: a visual multiple object tracking task, a rapid serial visual presentation (RSVP) digit encoding task, and a demanding auditory monitoring task. Load was manipulated by using high- and low-demand versions of each decoy task. The data provide converging evidence of a small effect of attention that is nonspecific, in that it affected the segregation and control tasks to a similar extent. In all cases, segregation performance remained high despite the presence of a concurrent, objectively demanding decoy task. The results suggest that repetition-based segregation is robust to inattention.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SHJT7DWA/Masutomi et al. - 2016 - Sound segregation via embedded repetition is robus.pdf}
}

@article{mattys_integration_2005,
  title = {Integration of Multiple Speech Segmentation Cues: A Hierarchical Framework},
  shorttitle = {Integration of Multiple Speech Segmentation Cues},
  author = {Mattys, Sven L. and White, Laurence and Melhorn, James F.},
  year = {2005},
  month = nov,
  journal = {Journal of Experimental Psychology. General},
  volume = {134},
  number = {4},
  pages = {477--500},
  issn = {0096-3445},
  doi = {10.1037/0096-3445.134.4.477},
  abstract = {A central question in psycholinguistic research is how listeners isolate words from connected speech despite the paucity of clear word-boundary cues in the signal. A large body of empirical evidence indicates that word segmentation is promoted by both lexical (knowledge-derived) and sublexical (signal-derived) cues. However, an account of how these cues operate in combination or in conflict is lacking. The present study fills this gap by assessing speech segmentation when cues are systematically pitted against each other. The results demonstrate that listeners do not assign the same power to all segmentation cues; rather, cues are hierarchically integrated, with descending weights allocated to lexical, segmental, and prosodic cues. Lower level cues drive segmentation when the interpretive conditions are altered by a lack of contextual and lexical information or by white noise. Taken together, the results call for an integrated, hierarchical, and signal-contingent approach to speech segmentation.},
  langid = {english},
  pmid = {16316287},
  keywords = {Cues,Humans,Phonetics,Speech,Speech Perception},
  file = {/Users/xzfang/Zotero/storage/CHZD2A3A/Mattys et al. - 2005 - Integration of multiple speech segmentation cues .pdf}
}

@article{mattys_speech_2012,
  title = {Speech Recognition in Adverse Conditions: {{A}} Review},
  shorttitle = {Speech Recognition in Adverse Conditions},
  author = {Mattys, Sven L. and Davis, Matthew H. and Bradlow, Ann R. and Scott, Sophie K.},
  year = {2012},
  month = sep,
  journal = {Language and Cognitive Processes},
  volume = {27},
  number = {7-8},
  pages = {953--978},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690965.2012.705006},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5HDR8B65/Mattys et al. - 2012 - Speech recognition in adverse conditions A review.pdf}
}

@article{maye_weckud_2008,
  title = {The {{Weckud Wetch}} of the {{Wast}}: {{Lexical Adaptation}} to a {{Novel Accent}}},
  shorttitle = {The {{Weckud Wetch}} of the {{Wast}}},
  author = {Maye, Jessica and Aslin, Richard N. and Tanenhaus, Michael K.},
  year = {2008},
  month = apr,
  journal = {Cognitive Science},
  volume = {32},
  number = {3},
  pages = {543--562},
  issn = {03640213},
  doi = {10.1080/03640210802035357},
  abstract = {Two experiments investigated the mechanism by which listeners adjust their interpretation of accented speech that is similar to a regional dialect of American English. Only a subset of the vowels of English (the front vowels) were shifted during adaptation, which consisted of listening to a 20-min segment of the ``Wizard of Oz.'' Compared to a baseline (unadapted) condition, listeners showed significant adaptation to the accented speech, as indexed by increased word judgments on a lexical decision task. Adaptation also generalized to test words that had not been presented in the accented passage but that contained the shifted vowels. A control experiment showed that the adaptation effect was specific to the direction of the shift in the vowel space and not to a general relaxation of the criterion for what constitutes a good exemplar of the accented vowel category. Taken together, these results provide evidence for a context-specific vowel adaptation mechanism that enables a listener to adjust to the dialect of a particular talker.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SHJ2MCKL/Maye et al. - 2008 - The Weckud Wetch of the Wast Lexical Adaptation t.pdf}
}

@article{mcauley_altering_2020,
  title = {Altering the Rhythm of Target and Background Talkers Differentially Affects Speech Understanding},
  author = {McAuley, J. Devin and Shen, Yi and Dec, Sarah and Kidd, Gary R.},
  year = {2020},
  month = aug,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {6},
  pages = {3222--3233},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-020-02064-5},
  abstract = {Three experiments investigated listeners' ability to use speech rhythm to attend selectively to a single target talker presented in multi-talker babble (Experiments 1 and 2) and in speech-shaped noise (Experiment 3). Participants listened to spoken sentences of the form ``Ready [Call sign] go to [Color] [Number] now'' and reported the Color and Number spoken by a target talker (cued by the Call sign ``Baron''). Experiment 1 altered the natural rhythm of the target talker and background talkers for two-talker and six-talker backgrounds. Experiment 2 considered parametric rhythm alterations over a wider range, altering the rhythm of either the target or the background talkers. Experiments 1 and 2 revealed that altering the rhythm of the target talker, while keeping the rhythm of the background intact, reduced listeners' ability to report the Color and Number spoken by the target talker. Conversely, altering the rhythm of the background talkers, while keeping the target rhythm intact, improved listeners ability to report the Color and Number spoken by the target talker. Experiment 3, which embedded the target talker in speech-shaped noise rather than multi-talker babble, similarly reduced recognition of the target sentence with increased alteration of the target rhythm. This pattern of results favors a dynamic-attending theory-based selective-entrainment hypothesis over a disparity-based segregation hypothesis and an increased salience hypothesis.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DU2D6HD6/McAuley et al. - 2020 - Altering the rhythm of target and background talke.pdf}
}

@article{mcauley_effects_2021,
  title = {Effects of Speech-Rhythm Disruption on Selective Listening with a Single Background Talker},
  author = {McAuley, J. Devin and Shen, Yi and Smith, Toni and Kidd, Gary R.},
  year = {2021},
  month = jul,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {83},
  number = {5},
  pages = {2229--2240},
  issn = {1943-393X},
  doi = {10.3758/s13414-021-02298-x},
  abstract = {Recent work by McAuley et al. (Attention, Perception, \& Psychophysics, 82, 3222\textendash 3233, 2020) using the Coordinate Response Measure (CRM) paradigm with a multitalker background revealed that altering the natural rhythm of target speech amidst background speech worsens target recognition (a target-rhythm effect), while altering background speech rhythm improves target recognition (a background-rhythm effect). Here, we used a single-talker background to examine the role of specific properties of target and background sound patterns on selective listening without the complexity of multiple background stimuli. Experiment 1 manipulated the sex of the background talker, presented with a male target talker, to assess target and background-rhythm effects with and without a strong pitch cue to aid perceptual segregation. Experiment 2 used a vocoded single-talker background to examine target and background-rhythm effects with envelope-based speech rhythms preserved, but without semantic content or temporal fine structure. While a target-rhythm effect was present with all backgrounds, the background-rhythm effect was only observed for the same-sex background condition. Results provide additional support for a selective entrainment hypothesis, while also showing that the background-rhythm effect is not driven by envelope-based speech rhythm alone, and may be reduced or eliminated when pitch or other acoustic differences provide a strong basis for selective listening.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UB782DTM/McAuley et al. - 2021 - Effects of speech-rhythm disruption on selective l.pdf}
}

@article{mcauliffe_stimulusdirected_2016,
  title = {Stimulus-Directed Attention Attenuates Lexically-Guided Perceptual Learning},
  author = {McAuliffe, Michael and Babel, Molly},
  year = {2016},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {140},
  number = {3},
  pages = {1727--1738},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4962529},
  abstract = {Studies on perceptual learning are motivated by phonetic variation that listeners encounter across speakers, items, and context. In this study, the authors investigate what control the listener has over the perceptual learning of ambiguous /s/ pronunciations through inducing changes in their attentional set. Listeners' attention is manipulated during a lexical decision exposure task such that their attention is directed at the word-level for comprehension-oriented listening or toward the signal for perception-oriented listening. In a categorization task with novel words, listeners in the condition that maximally biased listeners toward comprehension-oriented attentional sets showed the most perceptual learning. Focus on higher levels of linguistic meaning facilitated generalization to new words. These results suggest that the way in which listeners attend to the speech stream affects how linguistic categories are updated, providing insight into the qualitative differences in perceptual learning between the psychophysics and language-focused literatures.},
  file = {/Users/xzfang/Zotero/storage/JRF2VUPH/1.html}
}

@article{mcclelland_are_2006,
  title = {Are There Interactive Processes in Speech Perception?},
  author = {McClelland, James L. and Mirman, Daniel and Holt, Lori L.},
  year = {2006},
  month = aug,
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {8},
  pages = {363--369},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.06.007},
  abstract = {Lexical information facilitates speech perception, especially when sounds are ambiguous or degraded. The interactive approach to understanding this effect posits that this facilitation is accomplished through bi-directional flow of information, allowing lexical knowledge to influence pre-lexical processes. Alternative autonomous theories posit feed-forward processing with lexical influence restricted to post-perceptual decision processes. We review evidence supporting the prediction of interactive models that lexical influences can affect pre-lexical mechanisms, triggering compensation, adaptation and retuning of phonological processes generally taken to be pre-lexical. We argue that these and other findings point to interactive processing as a fundamental principle for perception of speech and other modalities.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KLFHGEXQ/McClelland et al. - 2006 - Are there interactive processes in speech percepti.pdf;/Users/xzfang/Zotero/storage/QWXVTL7X/S1364661306001604.html}
}

@article{mcclelland_parallel_2003,
  title = {The Parallel Distributed Processing Approach to Semantic Cognition},
  author = {McClelland, James L. and Rogers, Timothy T.},
  year = {2003},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  volume = {4},
  number = {4},
  pages = {310--322},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn1076},
  abstract = {Semantic cognition encompasses human performance based on knowledge about the properties of objects, relations among objects and word meanings. One approach to semantic cognition has arisen within the parallel distributed processing (PDP) framework, in which cognitive processes arise from interactions of neurons through synaptic connections. The knowledge that governs processing is stored in the strengths of the connections and is acquired gradually through experience, simulating conceptual development in childhood. These ideas have been explored in a simulated neural network model that learns propositions about objects and their properties. The model is trained with propositions about several different plant and animal concepts, including trees, flowers, fish, birds and land animals. The model contains 'hidden' units between its inputs and outputs, over which it learns internal representations that capture semantic relationships between concepts. Learning is influenced by coherent covariation of properties \textemdash{} that is, by co-occurrence of the same ensemble of properties (has wings, has feathers, can fly) in a number of different items (in this case, all the birds). The model explains the tendency towards progressive differentiation of concepts observed in development and the reverse fine-to-coarse deterioration observed in a progressive neuropathological condition called semantic dementia. With appropriate assumptions about covariation of properties, and about the relative frequencies of concepts and of the words used to name them, the model also addresses many further findings in development, dementia and normal adult cognition. Like other, similarity-based theories, the model accounts for the influence of graded category membership on semantic task performance, and for frequency and typicality effects. It also provides a means of addressing some of the criticisms of these other theories. Specifically, it indicates how some properties of objects, including causal properties, come to be more important than other properties; why some groups of items seem to form natural or coherent categories; how domain-specific patterns of generalization and differentiation might arise; and how conceptual knowledge structures might reorganize over the course of development. The PDP approach might provide a mechanistic framework that can address many of the phenomena emphasized in an alternative approach based on naive domain theories specifying causal relations between objects and their properties. Some of the relevant phenomena have yet to be addressed by PDP models, leaving this as a task for the future.},
  copyright = {2003 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews},
  file = {/Users/xzfang/Zotero/storage/5DJIFP9Y/McClelland and Rogers - 2003 - The parallel distributed processing approach to se.pdf;/Users/xzfang/Zotero/storage/4DQG3JN3/nrn1076.html}
}

@article{mcclelland_trace_1986,
  title = {The {{TRACE}} Model of Speech Perception},
  author = {McClelland, James L and Elman, Jeffrey L},
  year = {1986},
  month = jan,
  journal = {Cognitive Psychology},
  volume = {18},
  number = {1},
  pages = {1--86},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(86)90015-0},
  abstract = {We describe a model called the TRACE model of speech perception. The model is based on the principles of interactive activation. Information processing takes place through the excitatory and inhibitory interactions of a large number of simple processing units, each working continuously to update its own activation on the basis of the activations of other units to which it is connected. The model is called the TRACE model because the network of units forms a dynamic processing structure called ``the Trace,'' which serves at once as the perceptual processing mechanism and as the system's working memory. The model is instantiated in two simulation programs. TRACE I, described in detail elsewhere, deals with short segments of real speech, and suggests a mechanism for coping with the fact that the cues to the identity of phonemes vary as a function of context. TRACE II, the focus of this article, simulates a large number of empirical findings on the perception of phonemes and words and on the interactions of phoneme and word perception. At the phoneme level, TRACE II simulates the influence of lexical information on the identification of phonemes and accounts for the fact that lexical effects are found under certain conditions but not others. The model also shows how knowledge of phonological constraints can be embodied in particular lexical items but can still be used to influence processing of novel, nonword utterances. The model also exhibits categorical perception and the ability to trade cues off against each other in phoneme identification. At the word level, the model captures the major positive feature of Marslen-Wilson's COHORT model of speech perception, in that it shows immediate sensitivity to information favoring one word or set of words over others. At the same time, it overcomes a difficulty with the COHORT model: it can recover from underspecification or mispronunciation of a word's beginning. TRACE II also uses lexical information to segment a stream of speech into a sequence of words and to find word beginnings and endings, and it simulates a number of recent findings related to these points. The TRACE model has some limitations, but we believe it is a step toward a psychologically and computationally adequate model of the process of speech perception.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9UXENP6Z/McClelland and Elman - 1986 - The TRACE model of speech perception.pdf;/Users/xzfang/Zotero/storage/D92N37L5/0010028586900150.html}
}

@article{mccormick_mindwandering_2018,
  title = {Mind-{{Wandering}} in {{People}} with {{Hippocampal Damage}}},
  author = {McCormick, Cornelia and Rosenthal, Clive R. and Miller, Thomas D. and Maguire, Eleanor A.},
  year = {2018},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {11},
  pages = {2745--2754},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1812-17.2018},
  abstract = {Subjective inner experiences, such as mind-wandering, represent the fundaments of human cognition. Although the precise function of mind-wandering is still debated, it is increasingly acknowledged to have influence across cognition on processes such as future planning, creative thinking, and problem-solving and even on depressive rumination and other mental health disorders. Recently, there has been important progress in characterizing mind-wandering and identifying the associated neural networks. Two prominent features of mind-wandering are mental time travel and visuospatial imagery, which are often linked with the hippocampus. People with selective bilateral hippocampal damage cannot vividly recall events from their past, envision their future, or imagine fictitious scenes. This raises the question of whether the hippocampus plays a causal role in mind-wandering and, if so, in what way. Leveraging a unique opportunity to shadow people (all males) with bilateral hippocampal damage for several days, we examined, for the first time, what they thought about spontaneously, without direct task demands. We found that they engaged in as much mind-wandering as control participants. However, whereas controls thought about the past, present, and future, imagining vivid visual scenes, hippocampal damage resulted in thoughts primarily about the present comprising verbally mediated semantic knowledge. These findings expose the hippocampus as a key pillar in the neural architecture of mind-wandering and also reveal its impact beyond episodic memory, placing it at the heart of our mental life. SIGNIFICANCE STATEMENT Humans tend to mind-wander {$\sim$}30\textendash 50\% of their waking time. Two prominent features of this pervasive form of thought are mental time travel and visuospatial imagery, which are often associated with the hippocampus. To examine whether the hippocampus plays a causal role in mind-wandering, we examined the frequency and phenomenology of mind-wandering in patients with selective bilateral hippocampal damage. We found that they engaged in as much mind-wandering as controls. However, hippocampal damage changed the form and content of mind-wandering from flexible, episodic, and scene based to abstract, semanticized, and verbal. These findings expose the hippocampus as a key pillar in the neural architecture of mind-wandering and reveal its impact beyond episodic memory, placing it at the heart of our mental life.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2018 McCormick et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {29440532},
  keywords = {amnesia,episodic,hippocampus,mental time travel,mind-wandering,scenes},
  file = {/Users/xzfang/Zotero/storage/R86BMDFF/McCormick et al. - 2018 - Mind-Wandering in People with Hippocampal Damage.pdf;/Users/xzfang/Zotero/storage/AIZTRHG3/2745.html}
}

@article{mccormick_short_2009,
  title = {Short Article: {{Is}} Morphological Decomposition Limited to Low-Frequency Words?},
  shorttitle = {Short Article},
  author = {McCormick, Samantha F. and Brysbaert, Marc and Rastle, Kathleen},
  year = {2009},
  month = sep,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {62},
  number = {9},
  pages = {1706--1715},
  issn = {1747-0218, 1747-0226},
  doi = {10.1080/17470210902849991},
  abstract = {On the basis of data from masked priming experiments, it has been argued that an automatic process of decomposition is applied to all morphologically structured stimuli, irrespective of their lexical characteristics (Rastle, Davis, \& New, 2004). So far, this claim has been tested only with respect to low-frequency primes and nonword primes. This is a limitation because some models of morphological processing postulate that only high-frequency complex words are recognized as whole forms. Thus, a more stringent test would be to determine whether high-frequency complex words also show evidence of masked priming. We report an experiment that compares masked-priming effects observed when the primes constitute morphologically structured nonwords (e.g., alarmer\textendash ALARM), low-frequency words with a mean frequency of 2 per million (e.g., notional\textendash NOTION), and high-frequency words with a mean frequency of 60 per million (e.g., national\textendash NATION). These three conditions yielded significant and equivalent effects, lending strong support to the notion of a routine form of decomposition that is applied to all morphologically structured stimuli.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/AA99LYGW/McCormick et al. - 2009 - Short article Is morphological decomposition limi.pdf}
}

@article{mcdermott_indifference_2016,
  title = {Indifference to Dissonance in Native {{Amazonians}} Reveals Cultural Variation in Music Perception},
  author = {McDermott, Josh H. and Schultz, Alan F. and Undurraga, Eduardo A. and Godoy, Ricardo A.},
  year = {2016},
  month = jul,
  journal = {Nature},
  volume = {535},
  number = {7613},
  pages = {547--550},
  issn = {1476-4687},
  doi = {10.1038/nature18635},
  abstract = {A native Amazonian society rated consonant and dissonant chords and vocal harmonies as equally pleasant, whereas Bolivian city- and town-dwellers preferred consonance, indicating that preference for consonance over dissonance is not universal and probably develops from exposure to particular types of polyphonic music.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SDT4TPL3/McDermott et al. - 2016 - Indifference to dissonance in native Amazonians re.pdf;/Users/xzfang/Zotero/storage/CRLJPFRU/nature18635.html}
}

@article{mcdermott_musical_2010,
  title = {Musical Intervals and Relative Pitch: {{Frequency}} Resolution, Not Interval Resolution, Is Special},
  shorttitle = {Musical Intervals and Relative Pitch},
  author = {McDermott, Josh H. and Keebler, Michael V. and Micheyl, Christophe and Oxenham, Andrew J.},
  year = {2010},
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {128},
  number = {4},
  pages = {1943--1951},
  issn = {0001-4966},
  doi = {10.1121/1.3478785},
  abstract = {Pitch intervals are central to most musical systems, which utilize pitch at the expense of other acoustic dimensions. It seemed plausible that pitch might uniquely permit precise perception of the interval separating two sounds, as this could help explain its importance in music. To explore this notion, a simple discrimination task was used to measure the precision of interval perception for the auditory dimensions of pitch, brightness, and loudness. Interval thresholds were then expressed in units of just-noticeable differences for each dimension, to enable comparison across dimensions. Contrary to expectation, when expressed in these common units, interval acuity was actually worse for pitch than for loudness or brightness. This likely indicates that the perceptual dimension of pitch is unusual not for interval perception per se, but rather for the basic frequency resolution it supports. The ubiquity of pitch in music may be due in part to this fine-grained basic resolution.},
  file = {/Users/xzfang/Zotero/storage/77MBB8WQ/McDermott et al. - 2010 - Musical intervals and relative pitch Frequency re.pdf;/Users/xzfang/Zotero/storage/H4PHNUT3/1.html}
}

@article{mcdermott_summary_2013,
  title = {Summary Statistics in Auditory Perception},
  author = {McDermott, Josh H. and Schemitsch, Michael and Simoncelli, Eero P.},
  year = {2013},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {16},
  number = {4},
  pages = {493--498},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3347},
  abstract = {Sensory signals are transduced at high resolution, but their structure must be stored in a more compact format. Here the authors show that the auditory system summarizes the temporal details of sounds using time-averaged statistics. Such statistical representations produce good categorical discrimination, but limit the ability to discern temporal detail.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Auditory system,Cortex,Neuronal physiology,Psychology},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Auditory system;Cortex;Neuronal physiology;Psychology Subject\_term\_id: auditory-system;cortex;neuronal-physiology;psychology},
  file = {/Users/xzfang/Zotero/storage/W5ML76AM/McDermott et al. - 2013 - Summary statistics in auditory perception.pdf;/Users/xzfang/Zotero/storage/D4LSDPT8/nn.html}
}

@article{mcdonagh_taxonomic_,
  title = {Do {{Taxonomic}} and {{Associative Relations Affect Word Production}} in the {{Same Way}}?},
  author = {McDonagh, Delaney C and Fisher, Anna V and Nozari, Nazbanou},
  pages = {7},
  abstract = {Naming a picture is more difficult in the context of a taxonomically-related picture. Disagreement exists on whether non-taxonomic relations, e.g., associations, have similar or different effects on picture naming. Past work has reported facilitation, interference and null results but with inconsistent methodologies. We paired the same target word (e.g., cow) with unrelated (pen), taxonomically-related (bear), and associatively-related (milk) items in different blocks, as participants repeatedly named one of the two pictures in randomized order. Significant interference was uncovered for the same target item in the taxonomic vs. unrelated and associative blocks. There was no robust evidence of interference in the associative blocks. If anything, evidence suggested that associatively-related items marginally facilitated production. This finding suggests that taxonomic and associative relations have different effects on picture naming and has implications for theoretical models of lexical selection and, more generally, for the computations involved in mapping semantic features to lexical items.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/24GHFF9F/McDonagh et al. - Do Taxonomic and Associative Relations Affect Word.pdf}
}

@article{mcdonnell_what_2009,
  title = {What {{Is Stochastic Resonance}}? {{Definitions}}, {{Misconceptions}}, {{Debates}}, and {{Its Relevance}} to {{Biology}}},
  shorttitle = {What {{Is Stochastic Resonance}}?},
  author = {McDonnell, Mark D. and Abbott, Derek},
  year = {2009},
  month = may,
  journal = {PLoS Computational Biology},
  volume = {5},
  number = {5},
  pages = {e1000348},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1000348},
  abstract = {Stochastic resonance is said to be observed when increases in levels of unpredictable fluctuations\textemdash e.g., random noise\textemdash cause an increase in a metric of the quality of signal transmission or detection performance, rather than a decrease. This counterintuitive effect relies on system nonlinearities and on some parameter ranges being ``suboptimal''. Stochastic resonance has been observed, quantified, and described in a plethora of physical and biological systems, including neurons. Being a topic of widespread multidisciplinary interest, the definition of stochastic resonance has evolved significantly over the last decade or so, leading to a number of debates, misunderstandings, and controversies. Perhaps the most important debate is whether the brain has evolved to utilize random noise in vivo, as part of the ``neural code''. Surprisingly, this debate has been for the most part ignored by neuroscientists, despite much indirect evidence of a positive role for noise in the brain. We explore some of the reasons for this and argue why it would be more surprising if the brain did not exploit randomness provided by noise\textemdash via stochastic resonance or otherwise\textemdash than if it did. We also challenge neuroscientists and biologists, both computational and experimental, to embrace a very broad definition of stochastic resonance in terms of signal-processing ``noise benefits'', and to devise experiments aimed at verifying that random variability can play a functional role in the brain, nervous system, or other areas of biology.},
  pmcid = {PMC2660436},
  pmid = {19562010},
  file = {/Users/xzfang/Zotero/storage/GNF9XBBM/McDonnell and Abbott - 2009 - What Is Stochastic Resonance Definitions, Misconc.pdf}
}

@article{mcgowan_social_2015,
  title = {Social {{Expectation Improves Speech Perception}} in {{Noise}}},
  author = {McGowan, Kevin B.},
  year = {2015},
  month = dec,
  journal = {Language and Speech},
  volume = {58},
  number = {4},
  pages = {502--521},
  issn = {0023-8309, 1756-6053},
  doi = {10.1177/0023830914565191},
  abstract = {Listeners' use of social information during speech perception was investigated by measuring transcription accuracy of Chinese-accented speech in noise while listeners were presented with a congruent Chinese face, an incongruent Caucasian face, or an uninformative silhouette. When listeners were presented with a Chinese face they transcribed more accurately than when presented with the Caucasian face. This difference existed both for listeners with a relatively high level of experience and for listeners with a relatively low level of experience with Chineseaccented English. Overall, these results are inconsistent with a model of social speech perception in which listener bias reduces attendance to the acoustic signal. These results are generally consistent with exemplar models of socially indexed speech perception predicting that activation of a social category will raise base activation levels of socially appropriate episodic traces, but the similar performance of more and less experienced listeners suggests the need for a more nuanced view with a role for both detailed experience and listener stereotypes.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/S4NEVJA4/McGowan - 2015 - Social Expectation Improves Speech Perception in N.pdf}
}

@article{mcgurk_hearing_1976,
  title = {Hearing Lips and Seeing Voices},
  author = {Mcgurk, Harry and Macdonald, John},
  year = {1976},
  month = dec,
  journal = {Nature},
  volume = {264},
  number = {5588},
  pages = {746--748},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/264746a0},
  abstract = {MOST verbal communication occurs in contexts where the listener can see the speaker as well as hear him. However, speech perception is normally regarded as a purely auditory process. The study reported here demonstrates a previously unrecognised influence of vision upon speech perception. It stems from an observation that, on being shown a film of a young woman's talking head, in which repeated utterances of the syllable [ba] had been dubbed on to lip movements for [ga], normal adults reported hearing [da]. With the reverse dubbing process, a majority reported hearing [bagba] or [gaba]. When these subjects listened to the soundtrack from the film, without visual input, or when they watched untreated film, they reported the syllables accurately as repetitions of [ba] or [ga]. Subsequent replications confirm the reliability of these findings; they have important implications for the understanding of speech perception.},
  copyright = {1976 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MVZU3INU/Mcgurk and Macdonald - 1976 - Hearing lips and seeing voices.pdf;/Users/xzfang/Zotero/storage/BE7895K5/264746a0.html}
}

@article{mckinney_features_a,
  ids = {mckinney_features_},
  title = {Features for {{Audio}} and {{Music Classification}}},
  author = {McKinney, Martin F and Breebaart, Jeroen},
  pages = {8},
  abstract = {Four audio feature sets are evaluated in their ability to classify five general audio classes and seven popular music genres. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for both music and audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FFJ7C6U9/McKinney and Breebaart - Features for Audio and Music Classiï¬cation.pdf}
}

@article{mclaughlin_hierarchical_2019,
  title = {Hierarchical Contributions of Linguistic Knowledge to Talker Identification: {{Phonological}} versus Lexical Familiarity},
  shorttitle = {Hierarchical Contributions of Linguistic Knowledge to Talker Identification},
  author = {McLaughlin, Deirdre E. and Carter, Yaminah D. and Cheng, Cecilia C. and Perrachione, Tyler K.},
  year = {2019},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {4},
  pages = {1088--1107},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-019-01778-5},
  abstract = {Listeners identify talkers more accurately when listening to their native language compared to an unfamiliar, foreign language. This language-familiarity effect in talker identification has been shown to arise from familiarity with both the sound patterns (phonetics and phonology) and the linguistic content (words) of one's native language. However, it has been unknown whether these two sources of information contribute independently to talker identification abilities, particularly whether hearing familiar words can facilitate talker identification in the absence of familiar phonetics. To isolate the contribution of lexical familiarity, we conducted three experiments that tested listeners' ability to identify talkers saying familiar words, but with unfamiliar phonetics. In two experiments, listeners identified talkers from recordings of their native language (English), an unfamiliar foreign language (Mandarin Chinese), or ``hybrid'' speech stimuli (sentences spoken in Mandarin, but which can be convincingly coerced to sound like English when presented with subtitles that prime plausible English-language lexical interpretations based on the Mandarin phonetics). In a third experiment, we explored natural variation in lexical-phonetic congruence as listeners identified talkers with varying degrees of a Mandarin accent. Priming listeners to hear English speech did not improve their ability to identify talkers speaking Mandarin, even after additional training, and talker identification accuracy decreased as talkers' phonetics became increasingly dissimilar to American English. Together, these experiments indicate that unfamiliar sound patterns preclude talker identification benefits otherwise afforded by familiar words. These results suggest that linguistic representations contribute hierarchically to talker identification; the facilitatory effect of familiar words requires the availability of familiar phonological forms.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/79EYVDZA/McLaughlin et al. - 2019 - Hierarchical contributions of linguistic knowledge.pdf}
}

@article{mclaughlindrewj._measuring_2021,
  title = {Measuring the {{Subjective Cost}} of {{Listening Effort Using}} a {{Discounting Task}}},
  author = {{McLaughlin Drew J.} and {Braver Todd S.} and {Peelle Jonathan E.}},
  year = {2021},
  month = feb,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {64},
  number = {2},
  pages = {337--347},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/2020_JSLHR-20-00086},
  abstract = {Purpose       Objective measures of listening effort have been gaining prominence, as they provide          metrics to quantify the difficulty of understanding speech under a variety of circumstances.          A key challenge has been to develop paradigms that enable the complementary measurement          of subjective listening effort in a quantitatively precise manner. In this study,          we introduce a novel decision-making paradigm to examine age-related and individual          differences in subjective effort during listening.              Method       Older and younger adults were presented with spoken sentences mixed with speech-shaped          noise at multiple signal-to-noise ratios (SNRs). On each trial, subjects were offered          the choice between completing an easier listening trial (presented at +20 dB SNR)          for a smaller monetary reward and completing a harder listening trial (presented at          either +4, 0, -4, -8, or -12 dB SNR) for a greater monetary reward. By varying the          amount of the reward offered for the easier option, the subjective value of performing          effortful listening trials at each SNR could be assessed.              Results       Older adults discounted the value of effortful listening to a greater degree than          young adults, opting to accept less money in order to avoid more difficult SNRs. Additionally,          older adults with poorer hearing and smaller working memory capacities were more likely          to choose easier trials; however, in younger adults, no relationship with hearing          or working memory was found. Self-reported measures of economic status did not affect          these relationships.              Conclusions       These findings suggest that subjective listening effort depends on factors including,          but not necessarily limited to, hearing and working memory. Additionally, this study          demonstrates that economic decision-making paradigms can be a useful approach for          assessing subjective listening effort and may prove beneficial in future research.},
  file = {/Users/xzfang/Zotero/storage/Y27MKJE8/McLaughlin Drew J. et al. - 2021 - Measuring the Subjective Cost of Listening Effort .pdf;/Users/xzfang/Zotero/storage/L5TVN3UR/2020_JSLHR-20-00086.html}
}

@article{mclennan_examining_2005,
  title = {Examining the {{Time Course}} of {{Indexical Specificity Effects}} in {{Spoken Word Recognition}}},
  author = {McLennan, Conor T. and Luce, Paul A.},
  year = {2005},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {31},
  number = {2},
  pages = {306--321},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/0278-7393.31.2.306},
  abstract = {Variability in talker identity and speaking rate, commonly referred to as indexical variation, has demonstrable effects on the speed and accuracy of spoken word recognition. The present study examines the time course of indexical specificity effects to evaluate the hypothesis that such effects occur relatively late in the perceptual processing of spoken words. In 3 long-term repetition priming experiments, the authors examined reaction times to targets that were primed by stimuli that matched or mismatched on the indexical variable of interest (either talker identity or speaking rate). Each experiment was designed to manipulate the speed with which participants processed the stimuli. The results demonstrate that indexical variability affects participants' perception of spoken words only when processing is relatively slow and effortful. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Priming,Reaction Time,Speech Perception,Word Recognition,Words (Phonetic Units)},
  file = {/Users/xzfang/Zotero/storage/JJXQ73P9/McLennan and Luce - 2005 - Examining the Time Course of Indexical Specificity.pdf;/Users/xzfang/Zotero/storage/7C78R9CF/2005-02160-010.html}
}

@article{mcmurray_gradient_2002,
  title = {Gradient Effects of Within-Category Phonetic Variation on Lexical Access},
  author = {McMurray, Bob and Tanenhaus, Michael K. and Aslin, Richard N.},
  year = {2002},
  month = dec,
  journal = {Cognition},
  volume = {86},
  number = {2},
  pages = {B33-B42},
  issn = {00100277},
  doi = {10.1016/S0010-0277(02)00157-9},
  abstract = {In order to determine whether small within-category differences in voice onset time (VOT) affect lexical access, eye movements were monitored as participants indicated which of four pictures was named by spoken stimuli that varied along a 0\textendash 40 ms VOT continuum. Within-category differences in VOT resulted in gradient increases in fixations to cross-boundary lexical competitors as VOT approached the category boundary. Thus, fine-grained acoustic/phonetic differences are preserved in patterns of lexical activation for competing lexical candidates and could be used to maximize the efficiency of on-line word recognition. q 2002 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5FVSRL8Y/McMurray et al. - 2002 - Gradient effects of within-category phonetic varia.pdf}
}

@article{mcmurray_gradient_2008,
  title = {Gradient Sensitivity to Within-Category Variation in Words and Syllables},
  author = {McMurray, Bob and Aslin, Richard N. and Tanenhaus, Michael K. and Spivey, Michael J. and Subik, Dana},
  year = {2008},
  month = dec,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {34},
  number = {6},
  pages = {1609--1631},
  issn = {0096-1523},
  doi = {10.1037/a0011747},
  abstract = {Five experiments monitored eye movements in phoneme and lexical identification tasks to examine the effect of within-category sub-phonetic variation on the perception of stop consonants. Experiment 1 demonstrated gradient effects along VOT continua made from natural speech, replicating results with synthetic speech (McMurray, Tanenhaus \& Aslin, Cognition, 2002). Experiments 2\textendash 5 used synthetic VOT continua to examine effects of response alternatives (2 vs. 4), task (lexical vs. phoneme decision), and type of token (word vs. CV). A gradient effect of VOT in at least one half of the continuum was observed in all conditions. These results suggest that during on-line spoken word recognition lexical competitors are activated in proportion to their continuous distance from a category boundary. This gradient processing may allow listeners to anticipate upcoming acoustic/phonetic information in the speech signal and dynamically compensate for acoustic variability.},
  pmcid = {PMC3011988},
  pmid = {19045996},
  file = {/Users/xzfang/Zotero/storage/KWPCSDHJ/McMurray et al. - 2008 - Gradient sensitivity to within-category variation .pdf}
}

@article{mcmurray_speech_2018,
  title = {Speech Categorization Develops Slowly through Adolescence},
  author = {McMurray, Bob and Danelz, Ani and Rigler, Hannah and Seedorff, Michael},
  year = {2018},
  month = aug,
  journal = {Developmental psychology},
  volume = {54},
  number = {8},
  pages = {1472--1491},
  issn = {0012-1649},
  doi = {10.1037/dev0000542},
  abstract = {The development of the ability to categorize speech sounds is often viewed as occurring primarily during infancy via perceptual learning mechanisms. However, a number of studies suggest that even after infancy, children's categories become more categorical and well-defined through about age 12. We investigated the cognitive changes that may be responsible for such development using a visual world paradigm experiment based on (). Children from three age groups (7\textendash 8, 12\textendash 13, and 17\textendash 18 years) heard a token from either a b/p or s/{$\Elzesh$} continua spanning two words (beach/peach, ship/sip), and selected its referent from a screen containing four pictures of potential lexical candidates. Eye-movements to each object were monitored as a measure of how strongly children were committing to each candidate as perception unfolds in real-time. Results showed an ongoing sharpening of speech categories through 18, which was particularly apparent during the early stages of real-time perception. When analysis targeted to specifically within-category sensitivity to continuous detail, children exhibited increasingly gradient categories over development, suggesting that increasing sensitivity to fine-grained detail in the signal enables these more discrete categorization. Together these suggest that speech development is a protracted process in which children's increasing sensitivity to within-category detail in the signal enables increasingly sharp phonetic categories.},
  pmcid = {PMC6062449},
  pmid = {29952600},
  file = {/Users/xzfang/Zotero/storage/R3QUUD2U/McMurray et al. - 2018 - Speech categorization develops slowly through adol.pdf}
}

@article{mcmurray_tracking_2008,
  title = {Tracking the Time Course of Phonetic Cue Integration during Spoken Word Recognition},
  author = {McMurray, Bob and Clayards, Meghan A. and Tanenhaus, Michael K. and Aslin, Richard N.},
  year = {2008},
  month = dec,
  journal = {Psychonomic bulletin \& review},
  volume = {15},
  number = {6},
  pages = {1064--1071},
  issn = {1069-9384},
  doi = {10.3758/PBR.15.6.1064},
  abstract = {Speech perception requires listeners to integrate multiple cues that each contribute to judgments about a phonetic category. Classic studies of trading relations assessed the weights attached to each cue, but did not explore the time-course of cue-integration. Here we provide the first direct evidence that asynchronous cues to both voicing (b/p) and manner (b/w) contrasts become available to the listener at different times during spoken word recognition. Using the Visual World paradigm, we show that the probability of eye movements to pictures of target and competitor objects diverge at different points in time after the onset of the target word. These points of divergence correspond to the availability of early (voice-onset-time or formant transition slope) and late (vowel length) cues to voicing and manner contrasts. These results support a model of cue-integration in which phonetic cues are used for lexical access as soon as they are available.},
  pmcid = {PMC2621311},
  pmid = {19001568},
  file = {/Users/xzfang/Zotero/storage/49U7X9R7/McMurray et al. - 2008 - Tracking the time course of phonetic cue integrati.pdf}
}

@article{mcmurray_withincategory_2009,
  title = {Within-Category {{VOT}} Affects Recovery from ``Lexical'' Garden Paths: {{Evidence}} against Phoneme-Level Inhibition},
  shorttitle = {Within-Category {{VOT}} Affects Recovery from ``Lexical'' Garden Paths},
  author = {McMurray, Bob and Tanenhaus, Michael K. and Aslin, Richard N.},
  year = {2009},
  month = jan,
  journal = {Journal of memory and language},
  volume = {60},
  number = {1},
  pages = {65--91},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2008.07.002},
  abstract = {Spoken word recognition shows gradient sensitivity to within-category voice onset time (VOT), as predicted by several current models of spoken word recognition, including TRACE (McClelland \& Elman, Cognitive Psychology, 1986). It remains unclear, however, whether this sensitivity is short-lived or whether it persists over multiple syllables. VOT continua were synthesized for pairs of words like barricade and parakeet, which differ in the voicing of their initial phoneme, but otherwise overlap for at least four phonemes, creating an opportunity for ``lexical garden-paths'' when listeners encounter the phonemic information consistent with only one member of the pair. Simulations established that phoneme-level inhibition in TRACE eliminates sensitivity to VOT too rapidly to influence recovery. However, in two Visual World experiments, look-contingent and response-contingent analyses demonstrated effects of word initial VOT on lexical garden-path recovery. These results are inconsistent with inhibition at the phoneme level and support models of spoken word recognition in which sub-phonetic detail is preserved throughout the processing system.},
  pmcid = {PMC2630474},
  pmid = {20046217},
  file = {/Users/xzfang/Zotero/storage/T9UEK43V/McMurray et al. - 2009 - Within-category VOT affects recovery from â€œlexical.pdf}
}

@misc{mcnabb_unnecessary_2021,
  title = {Unnecessary Reliance on Multilevel Modelling to Analyse Nested Data},
  author = {McNabb, Carolyn and Murayama, Kou},
  year = {2021},
  month = oct,
  institution = {{OSF Preprints}},
  doi = {10.31219/osf.io/h4s9f},
  abstract = {A simulation experiment to confirm the validity of summary-statistics approaches for analysing nested data},
  langid = {american},
  keywords = {Physical Sciences and Mathematics,Statistical Methodology,Statistics and Probability},
  file = {/Users/xzfang/Zotero/storage/WZAJYZCD/McNabb and Murayama - 2021 - Unnecessary reliance on multilevel modelling to an.pdf}
}

@article{mcpherson_diversity_2018,
  title = {Diversity in Pitch Perception Revealed by Task Dependence},
  author = {McPherson, Malinda J. and McDermott, Josh H.},
  year = {2018},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {1},
  pages = {52--66},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0261-8},
  abstract = {In a series of 11 experiments, the authors show that what has traditionally been considered 'pitch perception' is mediated by several different mechanisms.},
  copyright = {2017 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RDY8G7MJ/McPherson and McDermott - 2018 - Diversity in pitch perception revealed by task dep.pdf;/Users/xzfang/Zotero/storage/7ACVNQPQ/s41562-017-0261-8.html}
}

@techreport{mcpherson_harmonicity_2020,
  type = {Preprint},
  title = {Harmonicity Aids Hearing in Noise},
  author = {McPherson, Malinda J. and Grace, River C. and McDermott, Josh H.},
  year = {2020},
  month = sep,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.30.321000},
  abstract = {Hearing in noise is a core problem in audition, and a challenge for hearing-impaired listeners, yet the underlying mechanisms are poorly understood. We explored whether harmonic frequency relations, a signature property of many communication sounds, aid hearing in noise. We measured detection thresholds in noise for tones and speech synthesized to have harmonic or inharmonic spectra. Harmonic signals were consistently easier to detect than otherwise identical inharmonic signals. Harmonicity also improved discrimination of sounds in noise. In contrast to other documented effects of harmonicity, harmonic detection advantages were comparable in musicians and non-musicians. The results show that harmonicity is critical for hearing in noise, demonstrating a previously unappreciated aspect of auditory scene analysis. The consistency of the effect across synthetic and natural stimuli, as well as across musical expertise, suggests its importance in everyday hearing.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/X2ETH4HT/McPherson et al. - 2020 - Harmonicity aids hearing in noise.pdf}
}

@article{mcqueen_changing_2012,
  title = {Changing Only the Probability That Spoken Words Will Be Distorted Changes How They Are Recognized},
  author = {McQueen, James M. and Huettig, Falk},
  year = {2012},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {131},
  number = {1},
  pages = {509--517},
  issn = {1520-8524},
  doi = {10.1121/1.3664087},
  abstract = {An eye-tracking experiment examined contextual flexibility in speech processing in response to distortions in spoken input. Dutch participants heard Dutch sentences containing critical words and saw four-picture displays. The name of one picture either had the same onset phonemes as the critical word or had a different first phoneme and rhymed. Participants fixated on onset-overlap more than rhyme-overlap pictures, but this tendency varied with speech quality. Relative to a baseline with noise-free sentences, participants looked less at onset-overlap and more at rhyme-overlap pictures when phonemes in the sentences (but not in the critical words) were replaced by noises like those heard on a badly tuned AM radio. The position of the noises (word-initial or word-medial) had no effect. Noises elsewhere in the sentences apparently made evidence about the critical word less reliable: Listeners became less confident of having heard the onset-overlap name but also less sure of having not heard the rhyme-overlap name. The same acoustic information has different effects on spoken-word recognition as the probability of distortion changes.},
  langid = {english},
  pmid = {22280612},
  keywords = {Fixation; Ocular,Humans,Noise,Perceptual Distortion,Perceptual Masking,Phonetics,Photic Stimulation,Probability,Recognition; Psychology,Speech,Speech Perception},
  file = {/Users/xzfang/Zotero/storage/QQ7YZHRH/McQueen and Huettig - 2012 - Changing only the probability that spoken words wi.pdf}
}

@article{mcqueen_specialized_2020,
  title = {Specialized Memory Systems for Learning Spoken Words.},
  author = {McQueen, James M. and Eisner, Frank and Burgering, Merel A. and Vroomen, Jean},
  year = {2020},
  month = jan,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {46},
  number = {1},
  pages = {189--199},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000704},
  abstract = {Learning new words entails, inter alia, encoding of novel sound patterns and transferring those patterns from short-term to long-term memory. We report a series of 5 experiments that investigated whether the memory systems engaged in word learning are specialized for speech and whether utilization of these systems results in a benefit for word learning. Sine-wave synthesis (SWS) was applied to spoken nonwords, and listeners were or were not informed (through instruction and familiarization) that the SWS stimuli were derived from actual utterances. This allowed us to manipulate whether listeners would process sound sequences as speech or as nonspeech. In a sound\textendash picture association learning task, listeners who processed the SWS stimuli as speech consistently learned faster and remembered more associations than listeners who processed the same stimuli as nonspeech. The advantage of listening in ``speech mode'' was stable over the course of 7 days. These results provide causal evidence that access to a specialized, phonological short-term memory system is important for word learning. More generally, this study supports the notion that subsystems of auditory short-term memory are specialized for processing different types of acoustic information.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Q4TEPEJL/McQueen et al. - 2020 - Specialized memory systems for learning spoken wor.pdf}
}

@article{mcqueen_tracking_2007,
  title = {Tracking Recognition of Spoken Words by Tracking Looks to Printed Words},
  author = {Mcqueen, James M. and Viebahn, Malte C.},
  year = {2007},
  month = may,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {60},
  number = {5},
  pages = {661--671},
  publisher = {{SAGE Publications}},
  issn = {1747-0218},
  doi = {10.1080/17470210601183890},
  abstract = {Eye movements of Dutch participants were tracked as they looked at arrays of four words on a computer screen and followed spoken instructions (e.g., ``Klik op het woord buffel'': Click on the word buffalo). The arrays included the target (e.g., buffel), a phonological competitor (e.g., buffer, buffer), and two unrelated distractors. Targets were monosyllabic or bisyllabic, and competitors mismatched targets only on either their onset or offset phoneme and only by one distinctive feature. Participants looked at competitors more than at distractors, but this effect was much stronger for offset-mismatch than onset-mismatch competitors. Fixations to competitors started to decrease as soon as phonetic evidence disfavouring those competitors could influence behaviour. These results confirm that listeners continuously update their interpretation of words as the evidence in the speech signal unfolds and hence establish the viability of the methodology of using eye movements to arrays of printed words to track spoken-word recognition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EX6KIJGF/Mcqueen and Viebahn - 2007 - Tracking recognition of spoken words by tracking l.pdf}
}

@article{megevand_crossmodal_2020,
  title = {Crossmodal {{Phase Reset}} and {{Evoked Responses Provide Complementary Mechanisms}} for the {{Influence}} of {{Visual Speech}} in {{Auditory Cortex}}},
  author = {M{\'e}gevand, Pierre and Mercier, Manuel R. and Groppe, David M. and Golumbic, Elana Zion and Mesgarani, Nima and Beauchamp, Michael S. and Schroeder, Charles E. and Mehta, Ashesh D.},
  year = {2020},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {40},
  number = {44},
  pages = {8530--8542},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0555-20.2020},
  abstract = {Natural conversation is multisensory: when we can see the speaker's face, visual speech cues improve our comprehension. The neuronal mechanisms underlying this phenomenon remain unclear. The two main alternatives are visually mediated phase modulation of neuronal oscillations (excitability fluctuations) in auditory neurons and visual input-evoked responses in auditory neurons. Investigating this question using naturalistic audiovisual speech with intracranial recordings in humans of both sexes, we find evidence for both mechanisms. Remarkably, auditory cortical neurons track the temporal dynamics of purely visual speech using the phase of their slow oscillations and phase-related modulations in broadband high-frequency activity. Consistent with known perceptual enhancement effects, the visual phase reset amplifies the cortical representation of concomitant auditory speech. In contrast to this, and in line with earlier reports, visual input reduces the amplitude of evoked responses to concomitant auditory input. We interpret the combination of improved phase tracking and reduced response amplitude as evidence for more efficient and reliable stimulus processing in the presence of congruent auditory and visual speech inputs. SIGNIFICANCE STATEMENT Watching the speaker can facilitate our understanding of what is being said. The mechanisms responsible for this influence of visual cues on the processing of speech remain incompletely understood. We studied these mechanisms by recording the electrical activity of the human brain through electrodes implanted surgically inside the brain. We found that visual inputs can operate by directly activating auditory cortical areas, and also indirectly by modulating the strength of cortical responses to auditory input. Our results help to understand the mechanisms by which the brain merges auditory and visual speech into a unitary perception.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2020 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {33023923},
  keywords = {audiovisual speech,broadband high-frequency activity,crossmodal stimuli,intracranial electroencephalography,neuronal oscillations,phaseâ€“amplitude coupling},
  file = {/Users/xzfang/Zotero/storage/RD6CI98E/MÃ©gevand et al. - 2020 - Crossmodal Phase Reset and Evoked Responses Provid.pdf;/Users/xzfang/Zotero/storage/I97IRSZ4/8530.html}
}

@article{mehraei_influence_2018,
  title = {Influence of Talker Discontinuity on Cortical Dynamics of Auditory Spatial Attention},
  author = {Mehraei, Golbarg and {Shinn-Cunningham}, Barbara and Dau, Torsten},
  year = {2018},
  month = oct,
  journal = {NeuroImage},
  volume = {179},
  pages = {548--556},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.06.067},
  abstract = {In everyday acoustic scenes, listeners face the challenge of selectively attending to a sound source and maintaining attention on that source long enough to extract meaning. This task is made more daunting by frequent perceptual discontinuities in the acoustic scene: talkers move in space and conversations switch from one speaker to another in a background of many other sources. The inherent dynamics of such switches directly impact our ability to sustain attention. Here we asked how discontinuity in talker voice affects the ability to focus auditory attention to sounds from a particular location as well as neural correlates of underlying processes. During electroencephalography recordings, listeners attended to a stream of spoken syllables from one direction while ignoring distracting syllables from a different talker from the opposite hemifield. On some trials, the talker switched locations in the middle of the streams, creating a discontinuity. This switch disrupted attentional modulation of cortical responses; specifically, event-related potentials evoked by syllables in the to-be-attended direction were suppressed and power in alpha oscillations (8\textendash 12\,Hz) were reduced following the discontinuity. Importantly, at an individual level, the ability to maintain attention to a target stream and report its content, despite the discontinuity, correlates with the magnitude of the disruption of these cortical responses. These results have implications for understanding cortical mechanisms supporting attention. The changes in the cortical responses may serve as a predictor of how well individuals can communicate in complex acoustic scenes and may help in the development of assistive devices and interventions to aid clinical populations.},
  langid = {english},
  keywords = {Alpha lateralization,Auditory attention,Event-related potentials,Neural oscillations},
  file = {/Users/xzfang/Zotero/storage/7HICNX7L/Mehraei et al. - 2018 - Influence of talker discontinuity on cortical dyna.pdf;/Users/xzfang/Zotero/storage/ZFIAN37J/S1053811918305743.html}
}

@article{mehrer_individual_2020,
  title = {Individual Differences among Deep Neural Network Models},
  author = {Mehrer, Johannes and Spoerer, Courtney J. and Kriegeskorte, Nikolaus and Kietzmann, Tim C.},
  year = {2020},
  month = nov,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {1--12},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19632-w},
  abstract = {Deep neural networks (DNNs) excel at visual recognition tasks and are increasingly used as a modeling framework for neural computations in the primate brain. Just like individual brains, each DNN has a unique connectivity and representational profile. Here, we investigate individual differences among DNN instances that arise from varying only the random initialization of the network weights. Using tools typically employed in systems neuroscience, we show that this minimal change in initial conditions prior to training leads to substantial differences in intermediate and higher-level network representations despite similar network-level classification performance. We locate the origins of the effects in an under-constrained alignment of category exemplars, rather than misaligned category centroids. These results call into question the common practice of using single networks to derive insights into neural information processing and rather suggest that computational neuroscientists working with DNNs may need to base their inferences on groups of multiple network instances. Do artificial neural networks, like brains, exhibit individual differences? Using tools from systems neuroscience, this study reveals substantial variability in network-internal representations, calling into question the neuroscientific practice of using single networks as models of brain function.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Cognitive neuroscience,Computational neuroscience,Network models,Neuroscience},
  file = {/Users/xzfang/Zotero/storage/QNEUUW3G/Mehrer et al. - 2020 - Individual differences among deep neural network m.pdf;/Users/xzfang/Zotero/storage/TY52Q9ZR/s41467-020-19632-w.html}
}

@article{mehrpour_pinpointing_2021,
  title = {Pinpointing the Neural Signatures of Single-Exposure Visual Recognition Memory},
  author = {Mehrpour, Vahid and Meyer, Travis and Simoncelli, Eero P. and Rust, Nicole C.},
  year = {2021},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {18},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2021660118},
  abstract = {Memories of the images that we have seen are thought to be reflected in the reduction of neural responses in high-level visual areas such as inferotemporal (IT) cortex, a phenomenon known as repetition suppression (RS). We challenged this hypothesis with a task that required rhesus monkeys to report whether images were novel or repeated while ignoring variations in contrast, a stimulus attribute that is also known to modulate the overall IT response. The monkeys' behavior was largely contrast invariant, contrary to the predictions of an RS-inspired decoder, which could not distinguish responses to images that are repeated from those that are of lower contrast. However, the monkeys' behavioral patterns were well predicted by a linearly decodable variant in which the total spike count was corrected for contrast modulation. These results suggest that the IT neural activity pattern that best aligns with single-exposure visual recognition memory behavior is not RS but rather sensory referenced suppression: reductions in IT population response magnitude, corrected for sensory modulation.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2021 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {33903238},
  keywords = {contrast,familiarity,population decoding,recognition memory,repetition suppression},
  file = {/Users/xzfang/Zotero/storage/DVXUK7DT/e2021660118.html}
}

@article{meinecke_detection_2002,
  title = {Detection {{Performance}} in {{Pop-Out Tasks}}: {{Nonmonotonic Changes}} with {{Display Size}} and {{Eccentricity}}},
  shorttitle = {Detection {{Performance}} in {{Pop-Out Tasks}}},
  author = {Meinecke, Cristina and Donk, Mieke},
  year = {2002},
  month = may,
  journal = {Perception},
  volume = {31},
  number = {5},
  pages = {591--602},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0301-0066},
  doi = {10.1068/p3201},
  abstract = {We carried out three experiments to investigate detection performance in pop-out tasks and analysed how performance varied as a function of display size (number of elements) and retinal eccentricity of the target. Results showed that when display size was increased from 2 to 81 elements performance first decreased and then increased (replicating Sagi and Julesz, 1987 Spatial Vision2 39?49). Performance variations differed as a function of eccentricity and often were more pronounced in the periphery than in the foveal area. This retinal-eccentricity influence suggests that processes underlying detection performance in small display sizes are different from those in large display sizes. One should be careful when using the variation of display size as an instrument to analyse visual-search processes because this analysis could be based on a comparison between non-equivalent conditions.},
  file = {/Users/xzfang/Zotero/storage/GDFYGGLF/Meinecke and Donk - 2002 - Detection Performance in Pop-Out Tasks Nonmonoton.pdf}
}

@article{meirhaeghe_precise_2021,
  title = {A Precise and Adaptive Neural Mechanism for Predictive Temporal Processing in the Frontal Cortex},
  author = {Meirhaeghe, Nicolas and Sohn, Hansem and Jazayeri, Mehrdad},
  year = {2021},
  month = sep,
  journal = {Neuron},
  volume = {109},
  number = {18},
  pages = {2995-3011.e5},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.08.025},
  langid = {english},
  pmid = {34534456},
  keywords = {dorsomedial frontal cortex,neural population dynamics,predictive coding,sensorimotor adaptation,temporal scaling,time interval reproduction},
  file = {/Users/xzfang/Zotero/storage/BH58QXR9/Meirhaeghe et al. - 2021 - A precise and adaptive neural mechanism for predic.pdf;/Users/xzfang/Zotero/storage/INZH7R59/S0896-6273(21)00622-X.html}
}

@article{merchant_are_2014,
  title = {Are Non-Human Primates Capable of Rhythmic Entrainment? {{Evidence}} for the Gradual Audiomotor Evolution Hypothesis},
  shorttitle = {Are Non-Human Primates Capable of Rhythmic Entrainment?},
  author = {Merchant, Hugo and Honing, Henkjan},
  year = {2014},
  journal = {Frontiers in Neuroscience},
  volume = {7},
  pages = {274},
  issn = {1662-453X},
  doi = {10.3389/fnins.2013.00274},
  abstract = {We propose a decomposition of the neurocognitive mechanisms that might underlie interval-based timing and rhythmic entrainment. Next to reviewing the concepts central to the definition of rhythmic entrainment, we discuss recent studies that suggest rhythmic entrainment to be specific to humans and a selected group of bird species, but, surprisingly, is not obvious in non-human primates. On the basis of these studies we propose the gradual audiomotor evolution hypothesis that suggests that humans fully share interval-based timing with other primates, but only partially share the ability of rhythmic entrainment (or beat-based timing). This hypothesis accommodates the fact that non-human primates (i.e., macaques) performance is comparable to humans in single interval tasks (such as interval reproduction, categorization, and interception), but show differences in multiple interval tasks (such as rhythmic entrainment, synchronization, and continuation). Furthermore, it is in line with the observation that macaques can, apparently, synchronize in the visual domain, but show less sensitivity in the auditory domain. And finally, while macaques are sensitive to interval-based timing and rhythmic grouping, the absence of a strong coupling between the auditory and motor system of non-human primates might be the reason why macaques cannot rhythmically entrain in the way humans do.},
  file = {/Users/xzfang/Zotero/storage/QVFRDNPZ/Merchant and Honing - 2014 - Are non-human primates capable of rhythmic entrain.pdf}
}

@article{mesgarani_mechanisms_2014,
  title = {Mechanisms of Noise Robust Representation of Speech in Primary Auditory Cortex},
  author = {Mesgarani, N. and David, S. V. and Fritz, J. B. and Shamma, S. A.},
  year = {2014},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {18},
  pages = {6792--6797},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1318017111},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QAQP9XDE/Mesgarani et al. - 2014 - Mechanisms of noise robust representation of speec.pdf}
}

@article{mesgarani_phoneme_2008,
  title = {Phoneme Representation and Classification in Primary Auditory Cortex},
  author = {Mesgarani, Nima and David, Stephen V. and Fritz, Jonathan B. and Shamma, Shihab A.},
  year = {2008},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {123},
  number = {2},
  pages = {899--909},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2816572},
  abstract = {A controversial issue in neurolinguistics is whether basic neural auditory representations found in many animals can account for human perception of speech. This question was addressed by examining how a population of neurons in the primary auditory cortex (A1) of the na\"ive awake ferret encodes phonemes and whether this representation could account for the human ability to discriminate them. When neural responses were characterized and ordered by spectral tuning and dynamics, perceptually significant features including formant patterns in vowels and place and manner of articulation in consonants, were readily visualized by activity in distinct neural subpopulations. Furthermore, these responses faithfully encoded the similarity between the acoustic features of these phonemes. A simple classifier trained on the neural representation was able to simulate human phoneme confusion when tested with novel exemplars. These results suggest that A1 responses are sufficiently rich to encode and discriminate phoneme classes and that humans and animals may build upon the same general acoustic representations to learn boundaries for categorical and robust sound classification.},
  file = {/Users/xzfang/Zotero/storage/536KNGTT/Mesgarani et al. - 2008 - Phoneme representation and classification in prima.pdf}
}

@article{mesgarani_phonetic_2014,
  title = {Phonetic {{Feature Encoding}} in {{Human Superior Temporal Gyrus}}},
  author = {Mesgarani, N. and Cheung, C. and Johnson, K. and Chang, E. F.},
  year = {2014},
  month = feb,
  journal = {Science},
  volume = {343},
  number = {6174},
  pages = {1006--1010},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245994},
  abstract = {During speech perception, linguistic elements such as consonants and vowels are extracted from a complex acoustic speech signal. The superior temporal gyrus (STG) participates in high-order auditory processing of speech, but how it encodes phonetic information is poorly understood. We used high-density direct cortical surface recordings in humans while they listened to natural, continuous speech to reveal the STG representation of the entire English phonetic inventory. At single electrodes, we found response selectivity to distinct phonetic features. Encoding of acoustic properties was mediated by a distributed population response. Phonetic features could be directly related to tuning for spectrotemporal acoustic cues, some of which were encoded in a nonlinear fashion or by integration of multiple cues. These findings demonstrate the acoustic-phonetic representation of speech in human STG.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FA3QAK99/Mesgarani et al. - 2014 - Phonetic Feature Encoding in Human Superior Tempor.pdf}
}

@article{mesgarani_selective_2012,
  title = {Selective Cortical Representation of Attended Speaker in Multi-Talker Speech Perception},
  author = {Mesgarani, Nima and Chang, Edward F.},
  year = {2012},
  month = may,
  journal = {Nature},
  volume = {485},
  number = {7397},
  issn = {0028-0836},
  doi = {10.1038/nature11020},
  abstract = {Humans possess a remarkable ability to attend to a single speaker's voice in a multi-talker background\textendash. How the auditory system manages to extract intelligible speech under such acoustically complex and adverse listening conditions is not known, and, indeed, it is not clear how attended speech is internally represented,. Here, using multi-electrode surface recordings from the cortex of subjects engaged in a listening task with two simultaneous speakers, we demonstrate that population responses in non-primary human auditory cortex encode critical features of attended speech: speech spectrograms reconstructed based on cortical responses to the mixture of speakers reveal the salient spectral and temporal features of the attended speaker, as if subjects were listening to that speaker alone. A simple classifier trained solely on examples of single speakers can decode both attended words and speaker identity. We find that task performance is well predicted by a rapid increase in attention-modulated neural selectivity across both single-electrode and population-level cortical responses. These findings demonstrate that the cortical representation of speech does not merely reflect the external acoustic environment, but instead gives rise to the perceptual aspects relevant for the listener's intended goal.},
  pmcid = {PMC3870007},
  pmid = {22522927},
  file = {/Users/xzfang/Zotero/storage/CKG872DH/Mesgarani and Chang - 2012 - Selective cortical representation of attended spea.pdf}
}

@misc{metzing_journal_2002,
  title = {Journal of {{Memory}} and {{Language}} 49 (2003) 201\textendash 213},
  author = {Metzing, Charles and Brennan, Susan E.},
  year = {2002},
  abstract = {When conceptual pacts are broken: Partner-specific effects on the comprehension of referring expressions},
  file = {/Users/xzfang/Zotero/storage/6NLPTBGF/Metzing and Brennan - 2002 - Journal of Memory and Language 49 (2003) 201â€“213.pdf;/Users/xzfang/Zotero/storage/SSK83YES/summary.html}
}

@article{metzing_when_2003,
  title = {When Conceptual Pacts Are Broken: {{Partner-specific}} Effects on the Comprehension of Referring Expressions},
  shorttitle = {When Conceptual Pacts Are Broken},
  author = {Metzing, Charles and Brennan, Susan E.},
  year = {2003},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {49},
  number = {2},
  pages = {201--213},
  issn = {0749-596X},
  doi = {10.1016/S0749-596X(03)00028-7},
  abstract = {When two people in conversation refer repeatedly to objects, they typically converge on the same (or similar) referring expressions. The repeated use of expressions by people in the same conversation has been called lexical entrainment. Lexical entrainment may emerge from the precedent of associating objects with expressions (and the perspectives they encode), or else from achieving conceptual pacts, or temporary, flexible agreements to view an object in a particular way (in which case the precedent is encoded as specific to a particular partner). We had people interact with a confederate speaker, entraining on shared perspectives (e.g., ``the shiny cylinder'') during repeated references to objects. Then either the original speaker or a new speaker used either the original expression or a new one (``the silver pipe'') to refer to the previously discussed object. Upon hearing the original expressions, addressees looked at and then touched the target objects equally quickly regardless of speaker. However, with new expressions, there was partner-specific interference: addressees were slower to look at the object when the new expression was uttered by the original speaker than when the new expression was uttered by the new speaker. This suggests that the representations in memory from which entrainment emerges do encode a partner-specific cue, leading addressees to expect that a speaker should continue to use an entrained-upon expression unless a contrast in meaning is implicated. There appears to be no such interference when a new partner uses a new expression.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/V4JRYKRV/Metzing and Brennan - 2003 - When conceptual pacts are broken Partner-specific.pdf;/Users/xzfang/Zotero/storage/JDZXWX95/S0749596X03000287.html}
}

@article{meyer_frontal_2015,
  title = {Frontal\textendash Posterior Theta Oscillations Reflect Memory Retrieval during Sentence Comprehension},
  author = {Meyer, Lars and Grigutsch, Maren and Schmuck, Noura and Gaston, Phoebe and Friederici, Angela D.},
  year = {2015},
  month = oct,
  journal = {Cortex},
  volume = {71},
  pages = {205--218},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2015.06.027},
  abstract = {Successful working-memory retrieval requires that items be retained as distinct units. At the neural level, it has been shown that theta-band oscillatory power increases with the number of to-be-distinguished items during working-memory retrieval. Here we hypothesized that during sentence comprehension, verbal-working-memory retrieval demands lead to increased theta power over frontal cortex, supposedly supporting the distinction amongst stored items during verbal-working-memory retrieval. Also, synchronicity may increase between the frontal cortex and the posterior cortex, with the latter supposedly supporting item retention. We operationalized retrieval by using pronouns, which refer to and trigger the retrieval of antecedent nouns from a preceding sentence part. Retrieval demand was systematically varied by changing the pronoun antecedent: Either, it was non-embedded in the preceding main clause, and thus easy-to-retrieve across a single clause boundary, or embedded in the preceding subordinate clause, and thus hard-to-retrieve across a double clause boundary. We combined electroencephalography (EEG), scalp-level time\textendash frequency analysis, source localization, and source-level coherence analysis, observing a frontal-midline and broad left-hemispheric theta-power increase for embedded-antecedent compared to non-embedded-antecedent retrieval. Sources were localized to left-frontal, left-parietal, and bilateral-inferior-temporal cortices. Coherence analyses suggested synchronicity between left-frontal and left-parietal and between left-frontal and right-inferior-temporal cortices. Activity of an array of left-frontal, left-parietal, and bilateral-inferior-temporal cortices may thus assist retrieval during sentence comprehension, potentially indexing the orchestration of item distinction, verbal working memory, and long-term memory. Our results extend prior findings by mapping prior knowledge on the functional role of theta oscillations onto processes genuine to human sentence comprehension.},
  langid = {english},
  keywords = {Sentence comprehension,Source localization,Theta oscillations,Timeâ€“frequency analysis,Verbal working memory}
}

@article{meyer_neural_2018,
  title = {The Neural Oscillations of Speech Processing and Language Comprehension: State of the Art and Emerging Mechanisms},
  shorttitle = {The Neural Oscillations of Speech Processing and Language Comprehension},
  author = {Meyer, Lars},
  year = {2018},
  journal = {European Journal of Neuroscience},
  volume = {48},
  number = {7},
  pages = {2609--2621},
  issn = {1460-9568},
  doi = {10.1111/ejn.13748},
  abstract = {Neural oscillations subserve a broad range of functions in speech processing and language comprehension. On the one hand, speech contains\textemdash somewhat\textemdash repetitive trains of air pressure bursts that occur at three dominant amplitude modulation frequencies, physically marking the linguistically meaningful progressions of phonemes, syllables and intonational phrase boundaries. To these acoustic events, neural oscillations of isomorphous operating frequencies are thought to synchronise, presumably resulting in an implicit temporal alignment of periods of neural excitability to linguistically meaningful spectral information on the three low-level linguistic description levels. On the other hand, speech is a carrier signal that codes for high-level linguistic meaning, such as syntactic structure and semantic information\textemdash which cannot be read from stimulus acoustics, but must be acquired during language acquisition and decoded for language comprehension. Neural oscillations subserve the processing of both syntactic structure and semantic information. Here, I synthesise a mapping from each linguistic processing domain to a unique set of subserving oscillatory mechanisms\textemdash the mapping is plausible given the role ascribed to different oscillatory mechanisms in different subfunctions of cortical information processing and faithful to the underlying electrophysiology. In sum, the present article provides an accessible and extensive review of the functional mechanisms that neural oscillations subserve in speech processing and language comprehension.},
  langid = {english},
  keywords = {chunking,entrainment,memory,predictive coding},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.13748},
  file = {/Users/xzfang/Zotero/storage/RI5VND7F/Meyer - 2018 - The neural oscillations of speech processing and l.pdf;/Users/xzfang/Zotero/storage/M4BM95J7/ejn.html}
}

@article{meyer_synchronous_2020,
  title = {Synchronous, but Not Entrained: Exogenous and Endogenous Cortical Rhythms of Speech and Language Processing},
  shorttitle = {Synchronous, but Not Entrained},
  author = {Meyer, Lars and Sun, Yue and Martin, Andrea E.},
  year = {2020},
  month = nov,
  journal = {Language, Cognition and Neuroscience},
  volume = {35},
  number = {9},
  pages = {1089--1099},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2019.1693050},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XMH8L5AQ/Meyer et al. - 2020 - Synchronous, but not entrained exogenous and endo.pdf}
}

@article{meyers_differential_2018,
  title = {Differential {{Processing}} of {{Isolated Object}} and {{Multi-item Pop-Out Displays}} in {{LIP}} and {{PFC}}},
  author = {Meyers, Ethan M and Liang, Andy and Katsuki, Fumi and Constantinidis, Christos},
  year = {2018},
  month = nov,
  journal = {Cerebral Cortex},
  volume = {28},
  number = {11},
  pages = {3816--3828},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhx243},
  abstract = {Objects that are highly distinct from their surroundings appear to visually ``pop-out.'' This effect is present for displays in which: (1) a single cue object is shown on a blank background, and (2) a single cue object is highly distinct from surrounding objects; it is generally assumed that these 2 display types are processed in the same way. To directly examine this, we applied a decoding analysis to neural activity recorded from the lateral intraparietal (LIP) area and the dorsolateral prefrontal cortex (dlPFC). Our analyses showed that for the single-object displays, cue location information appeared earlier in LIP than in dlPFC. However, for the display with distractors, location information was substantially delayed in both brain regions, and information first appeared in dlPFC. Additionally, we see that pattern of neural activity is similar for both types of displays and across different color transformations of the stimuli, indicating that location information is being coded in the same way regardless of display type. These results lead us to hypothesize that 2 different pathways are involved processing these 2 types of pop-out displays.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5TGV5K3M/bhx243_supplemental_material_revised.docx;/Users/xzfang/Zotero/storage/QUD7FWEM/Meyers et al. - 2018 - Differential Processing of Isolated Object and Mul.pdf}
}

@article{meyers_dynamic_2018,
  title = {Dynamic Population Coding and Its Relationship to Working Memory},
  author = {Meyers, Ethan M.},
  year = {2018},
  month = nov,
  journal = {Journal of Neurophysiology},
  volume = {120},
  number = {5},
  pages = {2260--2268},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00225.2018},
  abstract = {For over 45 years, neuroscientists have conducted experiments aimed at understanding the neural basis of working memory. Early results examining individual neurons highlighted that information is stored in working memory in persistent sustained activity where neurons maintained elevated firing rates over extended periods of time. However, more recent work has emphasized that information is often stored in working memory in dynamic population codes, where different neurons contain information at different periods in time. In this paper, I review findings that show that both sustained activity as well as dynamic codes are present in the prefrontal cortex and other regions during memory delay periods. I also review work showing that dynamic codes are capable of supporting working memory and that such dynamic codes could easily be ``readout'' by downstream regions. Finally, I discuss why dynamic codes could be useful for enabling animals to solve tasks that involve working memory. Although additional work is still needed to know definitively whether dynamic coding is critical for working memory, the findings reviewed here give insight into how different codes could contribute to working memory, which should be useful for guiding future research.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3XN4WDLH/Meyers - 2018 - Dynamic population coding and its relationship to .pdf}
}

@article{meyers_neural_2013,
  title = {The Neural Decoding Toolbox},
  author = {Meyers, Ethan},
  year = {2013},
  journal = {Frontiers in Neuroinformatics},
  volume = {7},
  pages = {8},
  issn = {1662-5196},
  doi = {10.3389/fninf.2013.00008},
  abstract = {Population decoding is a powerful way to analyze neural data, however, currently only a small percentage of systems neuroscience researchers use this method. In order to increase the use of population decoding, we have created the Neural Decoding Toolbox (NDT) which is a Matlab package that makes it easy to apply population decoding analyses to neural activity. The design of the toolbox revolves around four abstract object classes which enables users to interchange particular modules in order to try different analyses while keeping the rest of the processing stream intact. The toolbox is capable of analyzing data from many different types of recording modalities, and we give examples of how it can be used to decode basic visual information from neural spiking activity and how it can be used to examine how invariant the activity of a neural population is to stimulus transformations. Overall this toolbox will make it much easier for neuroscientists to apply population decoding analyses to their data, which should help increase the pace of discovery in neuroscience.},
  file = {/Users/xzfang/Zotero/storage/LSDUKMT7/Meyers - 2013 - The neural decoding toolbox.pdf}
}

@article{meylan_evaluating_2021,
  title = {Evaluating Models of Robust Word Recognition with Serial Reproduction},
  author = {Meylan, Stephan C. and Nair, Sathvik and Griffiths, Thomas L.},
  year = {2021},
  month = may,
  journal = {Cognition},
  volume = {210},
  pages = {104553},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104553},
  abstract = {Spoken communication occurs in a ``noisy channel'' characterized by high levels of environmental noise, variability within and between speakers, and lexical and syntactic ambiguity. Given these properties of the received linguistic input, robust spoken word recognition\textemdash and language processing more generally\textemdash relies heavily on listeners' prior knowledge to evaluate whether candidate interpretations of that input are more or less likely. Here we compare several broad-coverage probabilistic generative language models in their ability to capture human linguistic expectations. Serial reproduction, an experimental paradigm where spoken utterances are reproduced by successive participants similar to the children's game of ``Telephone,'' is used to elicit a sample that reflects the linguistic expectations of English-speaking adults. When we evaluate a suite of probabilistic generative language models against the yielded chains of utterances, we find that those models that make use of abstract representations of preceding linguistic context (i.e., phrase structure) best predict the changes made by people in the course of serial reproduction. A logistic regression model predicting which words in an utterance are most likely to be lost or changed in the course of spoken transmission corroborates this result. We interpret these findings in light of research highlighting the interaction of memory-based constraints and representations in language processing.},
  langid = {english},
  keywords = {Generative language models,Iterated learning,Noisy-channel communication,Sentence processing,Serial reproduction,Spoken word recognition},
  file = {/Users/xzfang/Zotero/storage/D58RLET8/S0010027720303723.html}
}

@article{middlebrooks_search_2021,
  title = {A {{Search}} for a {{Cortical Map}} of {{Auditory Space}}},
  author = {Middlebrooks, John C.},
  year = {2021},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {41},
  number = {27},
  pages = {5772--5778},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0501-21.2021},
  abstract = {This is the story of a search for a cortical map of auditory space. The search began with a study that was reported in the first issue of The Journal of Neuroscience (Middlebrooks and Pettigrew, 1981). That paper described some unexpected features of spatial sensitivity in the auditory cortex while failing to demonstrate the expected map. In the ensuing 40 years, we have encountered the following: panoramic spatial coding by single neurons; a rich variety of response patterns that are unmasked in the absence of general anesthesia; sharpening of spatial sensitivity when an animal is engaged in a listening task; and reorganization of spatial sensitivity in the presence of competing sounds. We have not encountered a map, but not through lack of trying. On the basis of years of negative results by our group and others, and positive results that are inconsistent with static point-to-point topography, we are confident in concluding that there just ain't no map. Instead, we have come to appreciate the highly dynamic spatial properties of cortical neurons, which serve the needs of listeners in a changing sonic environment.},
  chapter = {Progressions},
  copyright = {Copyright \textcopyright{} 2021 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {34011526},
  file = {/Users/xzfang/Zotero/storage/QMPTSQNX/5772.html}
}

@article{millet_inductive_2021,
  title = {Inductive Biases, Pretraining and Fine-Tuning Jointly Account for Brain Responses to Speech},
  author = {Millet, Juliette and King, Jean-Remi},
  year = {2021},
  month = feb,
  journal = {arXiv:2103.01032 [cs, eess, q-bio]},
  eprint = {2103.01032},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, q-bio},
  abstract = {Our ability to comprehend speech remains, to date, unrivaled by deep learning models. This feat could result from the brain's ability to fine-tune generic sound representations for speech-specific processes. To test this hypothesis, we compare i) five types of deep neural networks to ii) human brain responses elicited by spoken sentences and recorded in 102 Dutch subjects using functional Magnetic Resonance Imaging (fMRI). Each network was either trained on an acoustics scene classification, a speech-to-text task (based on Bengali, English, or Dutch), or not trained. The similarity between each model and the brain is assessed by correlating their respective activations after an optimal linear projection. The differences in brain-similarity across networks revealed three main results. First, speech representations in the brain can be accounted for by random deep networks. Second, learning to classify acoustic scenes leads deep nets to increase their brain similarity. Third, learning to process phonetically-related speech inputs (i.e., Dutch vs English) leads deep nets to reach higher levels of brain-similarity than learning to process phonetically-distant speech inputs (i.e. Dutch vs Bengali). Together, these results suggest that the human brain fine-tunes its heavily-trained auditory hierarchy to learn to process speech.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/xzfang/Zotero/storage/NGEPXCJ6/Millet and King - 2021 - Inductive biases, pretraining and fine-tuning join.pdf;/Users/xzfang/Zotero/storage/5RL4H2RP/2103.html}
}

@article{milne_online_2020,
  title = {An Online Headphone Screening Test Based on Dichotic Pitch},
  author = {Milne, Alice E. and Bianco, Roberta and Poole, Katarina C. and Zhao, Sijia and Oxenham, Andrew J. and Billig, Alexander J. and Chait, Maria},
  year = {2020},
  month = oct,
  journal = {bioRxiv},
  pages = {2020.07.21.214395},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.07.21.214395},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Online experimental platforms can be used as an alternative, or complement, to lab-based research. However, when conducting auditory experiments via online methods, the researcher has limited control over the participants' listening environment. We offer a new method to probe one aspect of that environment, headphone use. Headphones not only provide better control of sound presentation but can also ``shield'' the listener from background noise. Here we present a rapid (\&lt; 3 minute) headphone screening test based on Huggins Pitch (HP), a perceptual phenomenon that can only be detected when stimuli are presented dichotically. We validate this test using a cohort of ``Trusted'' online participants who completed the test using both headphones and loudspeakers. The same participants were also used to test an existing headphone test (AP test; Woods et al., 2017, \emph{Attention Perception Psychophysics}). We demonstrate that compared to the AP test, the HP test has a higher selectivity for headphone users, rendering it as a compelling alternative to existing methods. Overall, the new HP test correctly detects 80\% of headphone users and has a false positive rate of 20\%. Moreover, we demonstrate that combining the HP test with an additional test - either the AP test or an alternative based on a beat test (BT) - can lower the false positive rate to {$\sim$}7\%. This should be useful in situations where headphone use is particularly critical (e.g. dichotic or spatial manipulations). Code for implementing the new tests is publicly available in JavaScript and through Gorilla (gorilla.sc).{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PSRYLRF2/Milne et al. - 2020 - An online headphone screening test based on dichot.pdf;/Users/xzfang/Zotero/storage/J4QFMXDD/2020.07.21.214395v4.html}
}

@article{miranda_double_2007,
  title = {Double Dissociation between Rules and Memory in Music: {{An}} Event-Related Potential Study},
  shorttitle = {Double Dissociation between Rules and Memory in Music},
  author = {Miranda, Robbin A. and Ullman, Michael T.},
  year = {2007},
  month = nov,
  journal = {NeuroImage},
  volume = {38},
  number = {2},
  pages = {331--345},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2007.07.034},
  abstract = {Language and music share a number of characteristics. Crucially, both domains depend on both rules and memorized representations. Double dissociations between the neurocognition of rule-governed and memory-based knowledge have been found in language but not music. Here, the neural bases of both of these aspects of music were examined with an event-related potential (ERP) study of note violations in melodies. Rule-only violations consisted of out-of-key deviant notes that violated tonal harmony rules in novel (unfamiliar) melodies. Memory-only violations consisted of in-key deviant notes in familiar well-known melodies; these notes followed musical rules but deviated from the actual melodies. Finally, out-of-key notes in familiar well-known melodies constituted violations of both rules and memory. All three conditions were presented, within-subjects, to healthy young adults, half musicians and half non-musicians. The results revealed a double dissociation, independent of musical training, between rules and memory: both rule violation conditions, but not the memory-only violations, elicited an early, somewhat right-lateralized anterior-central negativity (ERAN), consistent with previous studies of rule violations in music, and analogous to the early left-lateralized anterior negativities elicited by rule violations in language. In contrast, both memory violation conditions, but not the rule-only violation, elicited a posterior negativity that might be characterized as an N400, an ERP component that depends, at least in part, on the processing of representations stored in long-term memory, both in language and in other domains. The results suggest that the neurocognitive rule/memory dissociation extends from language to music, further strengthening the similarities between the two domains.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6VC8Z624/Miranda and Ullman - 2007 - Double dissociation between rules and memory in mu.pdf;/Users/xzfang/Zotero/storage/6VEFHGGG/S1053811907006635.html}
}

@article{mirault_you_2018,
  title = {You {{That Read Wrong Again}}! {{A Transposed-Word Effect}} in {{Grammaticality Judgments}}},
  author = {Mirault, Jonathan and Snell, Joshua and Grainger, Jonathan},
  year = {2018},
  month = dec,
  journal = {Psychological Science},
  volume = {29},
  number = {12},
  pages = {1922--1929},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797618806296},
  abstract = {We report a novel transposed-word effect in speeded grammaticality judgments made about five-word sequences. The critical ungrammatical test sequences were formed by transposing two adjacent words from either a grammatical base sequence (e.g., ``The white cat was big'' became ``The white was cat big'') or an ungrammatical base sequence (e.g., ``The white cat was slowly'' became ``The white was cat slowly''). These were intermixed with an equal number of correct sentences for the purpose of the grammaticality judgment task. In a laboratory experiment (N = 57) and an online experiment (N = 94), we found that ungrammatical decisions were harder to make when the ungrammatical sequence originated from a grammatically correct base sequence. This provides the first demonstration that the encoding of word order retains a certain amount of uncertainty. We further argue that the novel transposed-word effect reflects parallel processing of words during written sentence comprehension combined with top-down constraints from sentence-level structures.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/U9IRZKNA/Mirault et al. - 2018 - You That Read Wrong Again! A Transposed-Word Effec.pdf}
}

@article{mirman_interactive_2006,
  title = {An Interactive {{Hebbian}} Account of Lexically Guided Tuning of Speech Perception},
  author = {Mirman, Daniel and McClelland, James L. and Holt, Lori L.},
  year = {2006},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {13},
  number = {6},
  pages = {958--965},
  issn = {1531-5320},
  doi = {10.3758/BF03213909},
  abstract = {We describe an account of lexically guided tuning of speech perception based on interactive processing and Hebbian learning. Interactive feedback provides lexical information to prelexical levels, and Hebbian learning uses that information to retune the mapping from auditory input to prelexical representations of speech. Simulations of an extension of the TRACE model of speech perception are presented that demonstrate the efficacy of this mechanism. Further simulations show that acoustic similarity can account for the patterns of speaker generalization. This account addresses the role of lexical information in guiding both perception and learning with a single set of principles of information propagation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LDQP5RTB/Mirman et al. - 2006 - An interactive Hebbian account of lexically guided.pdf}
}

@article{mirman_theories_2011,
  title = {Theories of Spoken Word Recognition Deficits in {{Aphasia}}: {{Evidence}} from Eye-Tracking and Computational Modeling},
  shorttitle = {Theories of Spoken Word Recognition Deficits in {{Aphasia}}},
  author = {Mirman, Daniel and Yee, Eiling and Blumstein, Sheila E. and Magnuson, James S.},
  year = {2011},
  month = may,
  journal = {Brain and Language},
  volume = {117},
  number = {2},
  pages = {53--68},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2011.01.004},
  abstract = {We used eye-tracking to investigate lexical processing in aphasic participants by examining the fixation time course for rhyme (e.g., carrot\textendash parrot) and cohort (e.g., beaker\textendash beetle) competitors. Broca's aphasic participants exhibited larger rhyme competition effects than age-matched controls. A re-analysis of previously reported data (Yee, Blumstein, \& Sedivy, 2008) confirmed that Wernicke's aphasic participants exhibited larger cohort competition effects. Individual-level analyses revealed a negative correlation between rhyme and cohort competition effect size across both groups of aphasic participants. Computational model simulations were performed to examine which of several accounts of lexical processing deficits in aphasia might account for the observed effects. Simulation results revealed that slower deactivation of lexical competitors could account for increased cohort competition in Wernicke's aphasic participants; auditory perceptual impairment could account for increased rhyme competition in Broca's aphasic participants; and a perturbation of a parameter controlling selection among competing alternatives could account for both patterns, as well as the correlation between the effects. In light of these simulation results, we discuss theoretical accounts that have the potential to explain the dynamics of spoken word recognition in aphasia and the possible roles of anterior and posterior brain regions in lexical processing and cognitive control.},
  langid = {english},
  keywords = {Aphasia,Computational models,Eye-tracking,Growth curve analysis,Spoken word recognition},
  file = {/Users/xzfang/Zotero/storage/6SWL76ED/Mirman et al. - 2011 - Theories of spoken word recognition deficits in Ap.pdf}
}

@article{misra_language_2021,
  title = {Do Language Models Learn Typicality Judgments from Text?},
  author = {Misra, Kanishka and Ettinger, Allyson and Rayz, Julia Taylor},
  year = {2021},
  month = may,
  journal = {arXiv:2105.02987 [cs]},
  eprint = {2105.02987},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Building on research arguing for the possibility of conceptual and categorical knowledge acquisition through statistics contained in language, we evaluate predictive language models (LMs) -- informed solely by textual input -- on a prevalent phenomenon in cognitive science: typicality. Inspired by experiments that involve language processing and show robust typicality effects in humans, we propose two tests for LMs. Our first test targets whether typicality modulates LM probabilities in assigning taxonomic category memberships to items. The second test investigates sensitivities to typicality in LMs' probabilities when extending new information about items to their categories. Both tests show modest -- but not completely absent -- correspondence between LMs and humans, suggesting that text-based exposure alone is insufficient to acquire typicality knowledge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/42XELGA3/Misra et al. - 2021 - Do language models learn typicality judgments from.pdf;/Users/xzfang/Zotero/storage/GIH74G8B/2105.html}
}

@article{miton_graphic_2021,
  title = {Graphic Complexity in Writing Systems},
  author = {Miton, Helena and Morin, Olivier},
  year = {2021},
  month = sep,
  journal = {Cognition},
  volume = {214},
  pages = {104771},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104771},
  abstract = {A writing system is a graphic code, i.e., a system of standardized pairings between symbols and meanings in which symbols take the form of images that can endure. The visual character of writing implies that written characters have to fit constraints of the human visual system. One aspect of this optimization lays in the graphic complexity of the characters used by scripts. Scripts are sets of graphic characters used for the written form of one language or more. Using computational methods over a large and diverse dataset (over 47,000 characters, from over 133 scripts), we answer three central questions about the visual complexity of written characters and the evolution of writing: (1) What determines character complexity? (2) Can we find traces of evolutionary change in character complexity? (3) Is complexity distributed in a way that makes character recognition easier? Our study suggests that (1) character complexity depends primarily on which linguistic unit the characters encode, and that (2) there is little evidence of evolutionary change in character complexity. Additionally (3) for an individual character, the half which is encountered first while reading tends to be more complex than that which is encountered last.},
  langid = {english},
  keywords = {Cultural evolution,Graphic complexity,Laterality,Letters,Visual complexity,Writing systems},
  file = {/Users/xzfang/Zotero/storage/N288TEA8/Miton and Morin - 2021 - Graphic complexity in writing systems.pdf;/Users/xzfang/Zotero/storage/F92R27LG/S0010027721001906.html}
}

@article{moghaddam_variation_2015,
  title = {Variation in Essential Oil Composition and Antioxidant Activity of Cumin ({{Cuminum}} Cyminum {{L}}.) Fruits during Stages of Maturity},
  author = {Moghaddam, Mohammad and Miran, Seyed Naser Khaleghi and Pirbalouti, Abdollah Ghasemi and Mehdizadeh, Leila and Ghaderi, Yadollah},
  year = {2015},
  month = aug,
  journal = {Industrial Crops and Products},
  volume = {70},
  pages = {163--169},
  issn = {09266690},
  doi = {10.1016/j.indcrop.2015.03.031},
  langid = {english}
}

@article{molinaro_speechbrain_2021,
  ids = {molinaro_speechbrain_2021a},
  title = {Speech-Brain Phase Coupling Is Enhanced in Low Contextual Semantic Predictability Conditions},
  author = {Molinaro, Nicola and Lizarazu, Mikel and Baldin, Veronica and {P{\'e}rez-Navarro}, Jose and Lallier, Marie and {R{\'i}os-L{\'o}pez}, Paula},
  year = {2021},
  month = jun,
  journal = {Neuropsychologia},
  volume = {156},
  pages = {107830},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2021.107830},
  abstract = {Semantic prediction and cortical entrainment to the acoustic landmarks of the speech envelope are two fundamental yet qualitatively different mechanisms that facilitate speech comprehension. However, it is not clear how and to what extent those mechanisms interact with each other. On the one hand, richer semantic context could enhance the perceptual representation of a predictable stimulus, thus improving speech entrainment. On the other hand, pre-activating an upcoming item could inhibit further bottom-up analyses to minimize processing costs, thus weakening speech entrainment. To test these competing hypotheses, we recorded EEG activity from 27 participants while they listened to a 14-min recording of text reading. The passage contained target words presented twice: once in a highly constraining and once in a minimally constraining context. First, we measured event related potentials on target words in the two conditions. In line with previous research, we showed that semantic predictability modulated the N400 amplitude: words in minimally constraining contexts elicited larger negative amplitudes than words in highly constraining contexts between 250 and 450~ms. Second, we evaluated speech entrainment effects by analyzing phase alignment between neural activity and the envelope of target words. Importantly, we found increased speech entrainment for words in minimally constraining compared to highly constraining contexts between 400 and 450~ms. Both effects were located in central electrodes and were significantly correlated. Our results indicate a trade-off between semantic pre-activation and cortical entrainment to speech and support the cost minimization hypothesis.},
  langid = {english},
  keywords = {N400,Phase locking value,Predictive processing,Speech comprehension,Speech entrainment},
  file = {/Users/xzfang/Zotero/storage/4NUG94ME/Molinaro et al. - 2021 - Speech-brain phase coupling is enhanced in low con.pdf}
}

@article{mollica_composition_2020,
  title = {Composition Is the Core Driver of the Language-Selective Network},
  author = {Mollica, Francis and Siegelman, Matthew and Diachek, Evgeniia and Piantadosi, Steven T. and Mineroff, Zachary and Futrell, Richard and Kean, Hope and Qian, Peng and Fedorenko, Evelina},
  year = {2020},
  month = jan,
  journal = {Neurobiology of Language},
  pages = {1--67},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00005},
  abstract = {The fronto-temporal language network responds robustly and selectively to sentences. But the features of linguistic input that drive this response and the computations these language areas support remain debated. Two key features of sentences are typically confounded in natural linguistic input: words in sentences a) are semantically and syntactically combinable into phrase- and clause-level meanings, and b) occur in an order licensed by the language's grammar. Inspired by recent psycholinguistic work establishing that language processing is robust to word order violations, we hypothesized that the core linguistic computation is composition, and, thus, can take place even when the word order violates the grammatical constraints of the language. This hypothesis predicts that a linguistic string should elicit a sentence-level response in the language network as long as the words in that string can enter into dependency relationships as in typical sentences. We tested this prediction across two fMRI experiments (total N=47) by introducing a varying number of local word swaps into naturalistic sentences, leading to progressively less syntactically well-formed strings. Critically, local dependency relationships were preserved because combinable words remained close to each other. As predicted, word order degradation did not decrease the magnitude of the BOLD response in the language network, except when combinable words were so far apart that composition among nearby words was highly unlikely. This finding demonstrates that composition is robust to word order violations, and that the language regions respond as strongly as they do to naturalistic linguistic input as long as composition can take place.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CJGWYTPG/Mollica et al. - 2020 - Composition is the core driver of the language-sel.pdf}
}

@article{molloy_auditory_2019,
  title = {Auditory {{Figure-Ground Segregation Is Impaired}} by {{High Visual Load}}},
  author = {Molloy, Katharine and Lavie, Nilli and Chait, Maria},
  year = {2019},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {9},
  pages = {1699--1708},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2518-18.2018},
  abstract = {Figure-ground segregation is fundamental to listening in complex acoustic environments. An ongoing debate pertains to whether segregation requires attention or is ``automatic'' and preattentive. In this magnetoencephalography study, we tested a prediction derived from load theory of attention (e.g., Lavie, 1995) that segregation requires attention but can benefit from the automatic allocation of any ``leftover'' capacity under low load. Complex auditory scenes were modeled with stochastic figure-ground stimuli (Teki et al., 2013), which occasionally contained repeated frequency component ``figures.'' Naive human participants (both sexes) passively listened to these signals while performing a visual attention task of either low or high load. While clear figure-related neural responses were observed under conditions of low load, high visual load substantially reduced the neural response to the figure in auditory cortex (planum temporale, Heschl's gyrus). We conclude that fundamental figure-ground segregation in hearing is not automatic but draws on resources that are shared across vision and audition. SIGNIFICANCE STATEMENT This work resolves a long-standing question of whether figure-ground segregation, a fundamental process of auditory scene analysis, requires attention or is underpinned by automatic, encapsulated computations. Task-irrelevant sounds were presented during performance of a visual search task. We revealed a clear magnetoencephalography neural signature of figure-ground segregation in conditions of low visual load, which was substantially reduced in conditions of high visual load. This demonstrates that, although attention does not need to be actively allocated to sound for auditory segregation to occur, segregation depends on shared computational resources across vision and hearing. The findings further highlight that visual load can impair the computational capacity of the auditory system, even when it does not simply dampen auditory responses as a whole.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2019 Molloy et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {30541915},
  keywords = {attention,auditory scene analysis,load theory,magnetoencephalography,MEG,multisensory},
  file = {/Users/xzfang/Zotero/storage/QWC36E2W/Molloy et al. - 2019 - Auditory Figure-Ground Segregation Is Impaired by .pdf;/Users/xzfang/Zotero/storage/5PCVW7NW/1699.html}
}

@article{monahan_phonological_2018,
  title = {Phonological {{Knowledge}} and {{Speech Comprehension}}},
  author = {Monahan, Philip J.},
  year = {2018},
  journal = {Annual Review of Linguistics},
  volume = {4},
  number = {1},
  pages = {21--47},
  doi = {10.1146/annurev-linguistics-011817-045537},
  abstract = {Comprehending speech in our native language is an impressionistically effortless and routine task. We often give little consideration to its complexity. Only in particularly challenging situations (e.g., in noisy environments, when hearing significantly accented speech) do some of these intricacies become apparent. Higher-order knowledge constrains sensory perception and has been demonstrated to play a crucial role in other domains of human language processing. Moreover, incorporating measures of brain activity during online speech comprehension has just begun to highlight the extent to which top-down information flow and predictive processes are integral to sensory perception. This review argues that our phonological system, at a relatively abstract level, is one such source of higher-order knowledge. In particular, I discuss the extent to which phonological distinctive features play a role in perception and predictive processing during speech comprehension with reference to behavioral and neurophysiological data. This line of research represents a tractable linking of linguistic theory with models of perception and speech comprehension in the brain.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-linguistics-011817-045537},
  file = {/Users/xzfang/Zotero/storage/DSZCJMZW/Monahan - 2018 - Phonological Knowledge and Speech Comprehension.pdf}
}

@article{monti_boundaries_2009,
  title = {The Boundaries of Language and Thought in Deductive Inference},
  author = {Monti, Martin M. and Parsons, Lawrence M. and Osherson, Daniel N.},
  year = {2009},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {30},
  pages = {12554--12559},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0902422106},
  abstract = {Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \ldots{} then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited ``core'' regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general ``support'' areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.},
  chapter = {Social Sciences},
  copyright = {\textcopyright{} 2009},
  langid = {english},
  pmid = {19617569},
  keywords = {fMRI,logic,reasoning,semantics,syntax},
  file = {/Users/xzfang/Zotero/storage/Q362W9AM/Monti et al. - 2009 - The boundaries of language and thought in deductiv.pdf}
}

@article{moore_linking_2021,
  title = {Linking Hippocampal Multiplexed Tuning, {{Hebbian}} Plasticity and Navigation},
  author = {Moore, Jason J. and Cushman, Jesse D. and Acharya, Lavanya and Popeney, Briana and Mehta, Mayank R.},
  year = {2021},
  month = oct,
  journal = {Nature},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03989-z},
  abstract = {Three major pillars of hippocampal function are spatial navigation1, Hebbian synaptic plasticity2 and spatial selectivity3. The hippocampus is also implicated in episodic memory4, but the precise link between these four functions is missing. Here we report the multiplexed selectivity of dorsal CA1 neurons while rats performed a virtual navigation task using only distal visual cues5, similar to the standard water maze test of spatial memory1. Neural responses primarily encoded path distance from the start point and the head angle of rats, with a weak allocentric spatial component similar to that in primates but substantially weaker than in rodents in the real world. Often, the same cells multiplexed and encoded path distance, angle and allocentric position in a sequence, thus encoding a journey-specific episode. The strength of neural activity and tuning strongly correlated with performance, with a temporal relationship indicating neural responses influencing behaviour and vice versa. Consistent with computational models of associative and causal Hebbian learning6,7, neural responses showed increasing clustering8 and became better predictors of behaviourally relevant variables, with the average neurometric curves exceeding and converging to psychometric curves. Thus, hippocampal neurons multiplex and exhibit highly plastic, task- and experience-dependent tuning to path-centric and allocentric variables to form episodic sequences supporting navigation.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Hippocampus,Spatial memory},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Hippocampus;Spatial memory Subject\_term\_id: hippocampus;spatial-memory},
  file = {/Users/xzfang/Zotero/storage/LLZBARKP/Moore et al. - 2021 - Linking hippocampal multiplexed tuning, Hebbian pl.pdf;/Users/xzfang/Zotero/storage/EVTGHXQJ/s41586-021-03989-z.html}
}

@article{morey_preregistered_2021,
  title = {A Pre-Registered, Multi-Lab Non-Replication of the Action-Sentence Compatibility Effect ({{ACE}})},
  author = {Morey, Richard D. and Kaschak, Michael P. and {D{\'i}ez-{\'A}lamo}, Antonio M. and Glenberg, Arthur M. and Zwaan, Rolf A. and Lakens, Dani{\"e}l and Ib{\'a}{\~n}ez, Agust{\'i}n and Garc{\'i}a, Adolfo and Gianelli, Claudia and Jones, John L. and Madden, Julie and Alifano, Florencia and Bergen, Benjamin and Bloxsom, Nicholas G. and Bub, Daniel N. and Cai, Zhenguang G. and Chartier, Christopher R. and Chatterjee, Anjan and Conwell, Erin and Cook, Susan Wagner and Davis, Joshua D. and Evers, Ellen R. K. and Girard, Sandrine and Harter, Derek and Hartung, Franziska and Herrera, Eduar and Huettig, Falk and Humphries, Stacey and Juanchich, Marie and K{\"u}hne, Katharina and Lu, Shulan and Lynes, Tom and Masson, Michael E. J. and Ostarek, Markus and Pessers, Sebastiaan and Reglin, Rebecca and Steegen, Sara and Thiessen, Erik D. and Thomas, Laura E. and Trott, Sean and Vandekerckhove, Joachim and Vanpaemel, Wolf and Vlachou, Maria and Williams, Kristina and {Ziv-Crispel}, Noam},
  year = {2021},
  month = nov,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-01927-8},
  abstract = {The Action-sentence Compatibility Effect (ACE) is a well-known demonstration of the role of motor activity in the comprehension of language. Participants are asked to make sensibility judgments on sentences by producing movements toward the body or away from the body. The ACE is the finding that movements are faster when the direction of the movement (e.g., toward) matches the direction of the action in the to-be-judged sentence (e.g., Art gave you the pen describes action toward you). We report on a pre-registered, multi-lab replication of one version of the ACE. The results show that none of the 18 labs involved in the study observed a reliable ACE, and that the meta-analytic estimate of the size of the ACE was essentially zero.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/T94F4T4I/Morey et al. - 2021 - A pre-registered, multi-lab non-replication of the.pdf}
}

@article{morgan_input_2021,
  title = {Beyond Input: {{Language}} Learners Produce Novel Relative Clause Types without Exposure.},
  shorttitle = {Beyond Input},
  author = {Morgan, Adam M. and Ferreira, Victor S.},
  year = {2021},
  month = jul,
  journal = {Journal of Cognitive Psychology},
  pages = {1--35},
  issn = {2044-5911, 2044-592X},
  doi = {10.1080/20445911.2021.1928678},
  abstract = {Syntax famously consists of abstract hierarchical representations. Less famously, most theories of syntax also assume a higher level of abstract representation: one that abstracts over the hierarchical representations. The existence of such representations would imply that, under certain circumstances, speakers should be able to produce syntactic structures they have never been exposed to. We test this prediction directly. In particular, different types of relative clauses have different surface word orders. These may be represented in two ways: with many individual representations, or with one general representation. If the latter, then learning one type of relative clause amounts to learning them all. We teach participants a novel grammar for only some relative clause types (e.g. just subject relative clauses) and test their knowledge of other types (e.g. object relative clauses). Across experiments, participants consistently produced untrained types, providing the first experimental evidence for this higher level of abstract syntactic knowledge.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QBRXZ2LQ/Morgan and Ferreira - 2021 - Beyond input Language learners produce novel rela.pdf}
}

@article{morgan_statistical_2019,
  title = {Statistical Learning and {{Gestalt-like}} Principles Predict Melodic Expectations},
  author = {Morgan, Emily and Fogel, Allison and Nair, Anjali and Patel, Aniruddh D.},
  year = {2019},
  month = aug,
  journal = {Cognition},
  volume = {189},
  pages = {23--34},
  issn = {00100277},
  doi = {10.1016/j.cognition.2018.12.015},
  abstract = {Expectation, or prediction, has become a major theme in cognitive science. Music offers a powerful system for studying how expectations are formed and deployed in the processing of richly structured sequences that unfold rapidly in time. We ask to what extent expectations about an upcoming note in a melody are driven by two distinct factors: Gestalt-like principles grounded in the auditory system (e.g. a preference for subsequent notes to move in small intervals), and statistical learning of melodic structure. We use multinomial regression modeling to evaluate the predictions of computationally implemented models of melodic expectation against behavioral data from a musical cloze task, in which participants hear a novel melodic opening and are asked to sing the note they expect to come next. We demonstrate that both Gestalt-like principles and statistical learning contribute to listeners' online expectations. In conjunction with results in the domain of language, our results point to a largerthan-previously-assumed role for statistical learning in predictive processing across cognitive domains, even in cases that seem potentially governed by a smaller set of theoretically motivated rules. However, we also find that both of the models tested here leave much variance in the human data unexplained, pointing to a need for models of melodic expectation that incorporate underlying hierarchical and/or harmonic structure. We propose that our combined behavioral (melodic cloze) and modeling (multinomial regression) approach provides a powerful method for further testing and development of models of melodic expectation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2H2888BF/Morgan et al. - 2019 - Statistical learning and Gestalt-like principles p.pdf}
}

@article{morgan_thattrace_2022,
  title = {The That-Trace Effect and Island Boundary-Gap Effect Are the Same: {{Demonstrating}} Equivalence with Null Hypothesis Significance Testing and Psychometrics},
  shorttitle = {The That-Trace Effect and Island Boundary-Gap Effect Are the Same},
  author = {Morgan, Adam M.},
  year = {2022},
  journal = {Glossa Psycholinguistics},
  volume = {1},
  number = {1},
  issn = {2767-0279},
  doi = {10.5070/G601140},
  abstract = {This paper demonstrates a novel approach in experimental syntax, leveraging psychometric methods to resolve a decades-old puzzle.\&nbsp; Specifically, gaps in subject position are more acceptable than gaps in object position in non-islands, while the reverse is true in islands (the Island Boundary-Gap Effect).\&nbsp; Attempts at explaining this asymmetry generally attribute it to a violation of the same constraint that renders gaps unacceptable after the overt complementizer `that' (the That-Trace Effect).\&nbsp; However, the two effects involve distinct syntactic structures, and there is no a priori reason to believe they are the same beyond the elegance of a unified account.\&nbsp; One limitation has been the difficulty of testing for equivalence in the Null Hypothesis Significance Testing framework: when two constructs behave similarly, it generally constitutes an uninterpretable null result. Experiments 1 and 2 use standard experimental methods to circumvent this problem, but ultimately provide evidence that is at best just consistent with equivalence.\&nbsp; Experiment 3 demonstrates a novel approach which shows that individual differences in the That-Trace Effect correlate with individual differences in the Island Boundary-Gap Effect, after removing correlated variance from carefully-chosen controls.\&nbsp; This psychometric approach provides positive evidence that the two effects do indeed derive from the same underlying phenomenon.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EXCYKPUA/Morgan - 2022 - The that-trace effect and island boundary-gap effe.pdf;/Users/xzfang/Zotero/storage/2MKBYXZK/1gp237sm.html}
}

@article{morillon_motor_2017,
  title = {Motor Origin of Temporal Predictions in Auditory Attention},
  author = {Morillon, Benjamin and Baillet, Sylvain},
  year = {2017},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {42},
  pages = {E8913-E8921},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1705373114},
  abstract = {In behavior, action and perception are inherently interdependent. However, the actual mechanistic contributions of the motor system to sensory processing are unknown. We present neurophysiological evidence that the motor system is involved in predictive timing, a brain function that aligns temporal fluctuations of attention with the timing of events in a task-relevant stream, thus facilitating sensory selection and optimizing behavior. In a magnetoencephalography experiment involving auditory temporal attention, participants had to disentangle two streams of sound on the unique basis of endogenous temporal cues. We show that temporal predictions are encoded by interdependent delta and beta neural oscillations originating from the left sensorimotor cortex, and directed toward auditory regions. We also found that overt rhythmic movements improved the quality of temporal predictions and sharpened the temporal selection of relevant auditory information. This latter behavioral and functional benefit was associated with increased signaling of temporal predictions in right-lateralized frontoparietal associative regions. In sum, this study points at a covert form of auditory active sensing. Our results emphasize the key role of motor brain areas in providing contextual temporal information to sensory regions, driving perceptual and behavioral selection.},
  chapter = {PNAS Plus},
  copyright = {\textcopyright{}  . http://www.pnas.org/site/misc/userlicense.xhtml},
  langid = {english},
  pmid = {28973923},
  keywords = {auditory perception,magnetoencephalography,psychophysics,rhythm,sensorimotor},
  file = {/Users/xzfang/Zotero/storage/2BZ95G9R/Morillon and Baillet - 2017 - Motor origin of temporal predictions in auditory a.pdf;/Users/xzfang/Zotero/storage/JBX2NE5U/E8913.html}
}

@article{morillon_prominence_2019,
  title = {Prominence of Delta Oscillatory Rhythms in the Motor Cortex and Their Relevance for Auditory and Speech Perception},
  author = {Morillon, Benjamin and Arnal, Luc H. and Schroeder, Charles E. and Keitel, Anne},
  year = {2019},
  month = dec,
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {107},
  pages = {136--142},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2019.09.012},
  abstract = {In the motor cortex, beta oscillations ({$\sim$}12\textendash 30\,Hz) are generally considered a principal rhythm contributing to movement planning and execution. Beta oscillations cohabit and dynamically interact with slow delta oscillations (0.5\textendash 4\,Hz), but the role of delta oscillations and the subordinate relationship between these rhythms in the perception-action loop remains unclear. Here, we review evidence that motor delta oscillations shape the dynamics of motor behaviors and sensorimotor processes, in particular during auditory perception. We describe the functional coupling between delta and beta oscillations in the motor cortex during spontaneous and planned motor acts. In an active sensing framework, perception is strongly shaped by motor activity, in particular in the delta band, which imposes temporal constraints on the sampling of sensory information. By encoding temporal contextual information, delta oscillations modulate auditory processing and impact behavioral outcomes. Finally, we consider the contribution of motor delta oscillations in the perceptual analysis of speech signals, providing a contextual temporal frame to optimize the parsing and processing of slow linguistic information.},
  langid = {english},
  keywords = {Active inference,Active sensing,Audio-motor coupling,Delta,Neural oscillations,Rhythm,Speech perception},
  file = {/Users/xzfang/Zotero/storage/BK6B8ICF/Morillon et al. - 2019 - Prominence of delta oscillatory rhythms in the mot.pdf}
}

@article{morton_predictive_2016,
  title = {A Predictive Framework for Evaluating Models of Semantic Organization in Free Recall},
  author = {Morton, Neal W and Polyn, Sean M.},
  year = {2016},
  month = jan,
  journal = {Journal of Memory and Language},
  volume = {86},
  pages = {119--140},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.10.002},
  abstract = {Research in free recall has demonstrated that semantic associations reliably influence the organization of search through episodic memory. However, the specific structure of these associations and the mechanisms by which they influence memory search remain unclear. We introduce a likelihood-based model-comparison technique, which embeds a model of semantic structure within the context maintenance and retrieval (CMR) model of human memory search. Within this framework, model variants are evaluated in terms of their ability to predict the specific sequence in which items are recalled. We compare three models of semantic structure, latent semantic analysis (LSA), global vectors (GloVe), and word association spaces (WAS), and find that models using WAS have the greatest predictive power. Furthermore, we find evidence that semantic and temporal organization is driven by distinct item and context cues, rather than a single context cue. This finding provides important constraint for theories of memory search.},
  langid = {english},
  keywords = {Clustering,Computational model,Episodic memory,Memory search},
  file = {/Users/xzfang/Zotero/storage/XSEBMUIU/Morton and Polyn - 2016 - A predictive framework for evaluating models of se.pdf;/Users/xzfang/Zotero/storage/5Q7WG2QU/S0749596X15001199.html}
}

@article{moses_realtime_2019,
  title = {Real-Time Decoding of Question-and-Answer Speech Dialogue Using Human Cortical Activity},
  author = {Moses, David A. and Leonard, Matthew K. and Makin, Joseph G. and Chang, Edward F.},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {3096},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10994-4},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/29MXSCZ3/Moses et al. - 2019 - Real-time decoding of question-and-answer speech d.pdf}
}

@article{murai_serial_2021,
  title = {Serial Dependence Revealed in History-Dependent Perceptual Templates},
  author = {Murai, Yuki and Whitney, David},
  year = {2021},
  month = jul,
  journal = {Current Biology},
  volume = {31},
  number = {14},
  pages = {3185-3191.e3},
  issn = {09609822},
  doi = {10.1016/j.cub.2021.05.006},
  abstract = {In any given perceptual task, the visual system selectively weighs or filters incoming information. The particular set of weights or filters form a kind of template, which reveals the regions or types of information that are particularly useful for a given perceptual decision.1,2 Unfortunately, sensory input is noisy and ever changing. To compensate for these fluctuations, the visual system could adopt a strategy of biasing the templates such that they reflect a temporal smoothing of input, which would be a form of serial dependence.3\textendash 5 Here, we demonstrate that perceptual templates are, in fact, altered by serial dependence. Using a simple orientation detection task and classification-image technique, we found that perceptual templates are systematically biased toward previously seen, task-irrelevant orientations. The results of an orientation discrimination task suggest that this shift in perceptual template derives from a change in the perceptual appearance of orientation. Our study reveals how serial dependence biases internal templates of orientation and suggests that the sensitivity of classification-image techniques in general could be improved by taking into account history-dependent fluctuations in templates.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XKJDUVVE/Murai and Whitney - 2021 - Serial dependence revealed in history-dependent pe.pdf}
}

@article{murphy_eeg_2011,
  title = {{{EEG}} Decoding of Semantic Category Reveals Distributed Representations for Single Concepts},
  author = {Murphy, Brian and Poesio, Massimo and Bovolo, Francesca and Bruzzone, Lorenzo and Dalponte, Michele and Lakany, Heba},
  year = {2011},
  month = apr,
  journal = {Brain and Language},
  volume = {117},
  number = {1},
  pages = {12--22},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2010.09.013},
  abstract = {Achieving a clearer picture of categorial distinctions in the brain is essential for our understanding of the conceptual lexicon, but much more fine-grained investigations are required in order for this evidence to contribute to lexical research. Here we present a collection of advanced data-mining techniques that allows the category of individual concepts to be decoded from single trials of EEG data. Neural activity was recorded while participants silently named images of mammals and tools, and category could be detected in single trials with an accuracy well above chance, both when considering data from single participants, and when group-training across participants. By aggregating across all trials, single concepts could be correctly assigned to their category with an accuracy of 98\%. The pattern of classifications made by the algorithm confirmed that the neural patterns identified are due to conceptual category, and not any of a series of processing-related confounds. The time intervals, frequency bands and scalp locations that proved most informative for prediction permit physiological interpretation: the widespread activation shortly after appearance of the stimulus (from 100ms) is consistent both with accounts of multi-pass processing, and distributed representations of categories. These methods provide an alternative to fMRI for fine-grained, large-scale investigations of the conceptual lexicon.},
  langid = {english},
  keywords = {Categorisation,Concepts,Data mining,Distributed representations,EEG,Exclusion of confounds,Machine learning,Semantics},
  file = {/Users/xzfang/Zotero/storage/H9N25AHG/Murphy et al. - 2011 - EEG decoding of semantic category reveals distribu.pdf;/Users/xzfang/Zotero/storage/ICV76Y89/S0093934X10001811.html}
}

@article{murray_stable_2017,
  title = {Stable Population Coding for Working Memory Coexists with Heterogeneous Neural Dynamics in Prefrontal Cortex},
  author = {Murray, John D. and Bernacchia, Alberto and Roy, Nicholas A. and Constantinidis, Christos and Romo, Ranulfo and Wang, Xiao-Jing},
  year = {2017},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {2},
  pages = {394--399},
  file = {/Users/xzfang/Zotero/storage/R92U9I9I/394.html}
}

@techreport{musk_integrated_2019,
  type = {Preprint},
  title = {An Integrated Brain-Machine Interface Platform with Thousands of Channels},
  author = {Musk, Elon and {Neuralink}},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/703801},
  abstract = {Brain-machine interfaces (BMIs) hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical BMIs have not yet been widely adopted, in part because modest channel counts have limited their potential. In this white paper, we describe Neuralink's first steps toward a scalable high-bandwidth BMI system. We have built arrays of small and flexible electrode ``threads'', with as many as 3,072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: the package for 3,072 channels occupies less than (23 \texttimes{} 18.5 \texttimes{} 2) mm3. A single USB-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 85.5 \% in chronically implanted electrodes. Neuralink's approach to BMI has unprecedented packaging density and scalability in a clinically relevant package.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZLDJZM8Q/Musk and Neuralink - 2019 - An integrated brain-machine interface platform wit.pdf}
}

@misc{musz_neural_2021,
  title = {Neural Signatures of Compression in the Retelling of Past Events},
  author = {Musz, Elizabeth and Chen, Janice},
  year = {2021},
  month = sep,
  pages = {2021.09.21.461232},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.09.21.461232},
  abstract = {When we retell our past experiences, we aim to reproduce some version of the original events; this reproduced version is often temporally compressed relative to the original. How does such compression of memories manifest in brain activity? One possibility is that a compressed retrieved memory manifests as a neural pattern which is more dissimilar to the original, relative to a more detailed or vivid memory. However, we argue that measuring raw dissimilarity alone is insufficient, as it confuses a variety of interesting and uninteresting changes. To address this problem, we examine brain pattern changes that are consistent across people. We show that temporal compression in individuals' retelling of past events predicts systematic encoding-to-recall transformations in a number of higher associative regions. These findings elucidate how neural representations are not simply reactivated, but can also be transformed due to temporal compression during a universal form of human memory expression: verbal retelling.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/88U573ZT/Musz and Chen - 2021 - Neural signatures of compression in the retelling .pdf;/Users/xzfang/Zotero/storage/YN2LTVMJ/2021.09.21.461232v1.html}
}

@article{myers_inferior_2009,
  title = {Inferior {{Frontal Regions Underlie}} the {{Perception}} of {{Phonetic Category Invariance}}},
  author = {Myers, Emily B. and Blumstein, Sheila E. and Walsh, Edward and Eliassen, James},
  year = {2009},
  month = jul,
  journal = {Psychological Science},
  volume = {20},
  number = {7},
  pages = {895--903},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2009.02380.x},
  abstract = {The problem of mapping differing sensory stimuli onto a common category is fundamental to human cognition. Listeners perceive stable phonetic categories despite many sources of acoustic variability. What are the neural mechanisms that underlie this perceptual stability? In this functional magnetic resonance imaging study, a short-interval habituation paradigm was used to investigate neural sensitivity to acoustic changes within and between phonetic categories. A region in the left inferior frontal sulcus showed a pattern of activation consistent with phonetic invariance: insensitivity to acoustic changes within a phonetic category and sensitivity to changes between phonetic categories. Left superior temporal regions, in contrast, showed graded sensitivity to both within- and between-category changes. These results suggest that perceptual insensitivity to changes within a phonetic category may arise from decision-related mechanisms in the left prefrontal cortex and add to a growing body of literature suggesting that the inferior prefrontal cortex plays a domain-general role in computing category representations.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/WLKGU3TX/Myers et al. - 2009 - Inferior Frontal Regions Underlie the Perception o.pdf}
}

@article{naatanen_early_1978,
  title = {Early Selective-Attention Effect on Evoked Potential Reinterpreted},
  author = {N{\"a}{\"a}t{\"a}nen, R. and Gaillard, A. W. K. and M{\"a}ntysalo, S.},
  year = {1978},
  month = jul,
  journal = {Acta Psychologica},
  volume = {42},
  number = {4},
  pages = {313--329},
  issn = {0001-6918},
  doi = {10.1016/0001-6918(78)90006-9},
  abstract = {In a dichotic listening situation stimuli were presented one at a time and at random to either ear of the subject at constant inter-stimulus intervals of 800 msec. The subject's task was to detect and count occasional slightly different stimuli in one ear. In Experiment 1, these `signal' stimuli were slightly louder, and in Experiment 2 they had a slightly higher pitch, than the much more frequent, `standard', stimuli. In both experiments signals occured randomly at either ear. Separate evoked potentials from three different locations were recorded for each of the four kinds of stimuli (attended signals, unattended signals, attended standards, unattended standards). Contrary to Hillyard et al. (1973), no early (N1 component) evoked-potential enhancement was observed to stimuli to the attended ear as compared with those to the unattended ear, but there was a later negative shift superimposed on potentials elicited by the former stimuli. This negative shift was considered identical to the N1 enhancement of Hillyard and his colleagues which in the present study was forced, by the longer inter-stimulus interval used, to demonstrate temporal dissociation with the N1 component. The `Hillyard effect' was, consequently, explained as being caused by a superimposition of a CNV kind of negative shift on the evoked potential to the attended stimuli rather than by a growth of the `real' N1 component of the evoked potential.},
  langid = {english}
}

@article{naatanen_mismatch_2007,
  title = {The Mismatch Negativity ({{MMN}}) in Basic Research of Central Auditory Processing: {{A}} Review},
  shorttitle = {The Mismatch Negativity ({{MMN}}) in Basic Research of Central Auditory Processing},
  author = {N{\"a}{\"a}t{\"a}nen, R. and Paavilainen, P. and Rinne, T. and Alho, K.},
  year = {2007},
  month = dec,
  journal = {Clinical Neurophysiology},
  volume = {118},
  number = {12},
  pages = {2544--2590},
  issn = {13882457},
  doi = {10.1016/j.clinph.2007.04.026},
  abstract = {In the present article, the basic research using the mismatch negativity (MMN) and analogous results obtained by using the magnetoencephalography (MEG) and other brain-imaging technologies is reviewed. This response is elicited by any discriminable change in auditory stimulation but recent studies extended the notion of the MMN even to higher-order cognitive processes such as those involving grammar and semantic meaning. Moreover, MMN data also show the presence of automatic intelligent processes such as stimulus anticipation at the level of auditory cortex. In addition, the MMN enables one to establish the brain processes underlying the initiation of attention switch to, conscious perception of, sound change in an unattended stimulus stream.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SL9UNXCN/NÃ¤Ã¤tÃ¤nen et al. - 2007 - The mismatch negativity (MMN) in basic research of.pdf}
}

@article{naatanen_n1_1987,
  title = {The {{N1 Wave}} of the {{Human Electric}} and {{Magnetic Response}} to {{Sound}}: {{A Review}} and an {{Analysis}} of the {{Component Structure}}},
  shorttitle = {The {{N1 Wave}} of the {{Human Electric}} and {{Magnetic Response}} to {{Sound}}},
  author = {N{\"a}{\"a}t{\"a}nen, Risto and Picton, Terence},
  year = {1987},
  journal = {Psychophysiology},
  volume = {24},
  number = {4},
  pages = {375--425},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.1987.tb00311.x},
  abstract = {This paper reviews the literature on the Nl wave of the human auditory evoked potential. It concludes that at least six different cerebral processes can contribute to (he negative wave recorded from the scalp with a peak latency between 50 and 150 ms: a component generated in the auditory-cortex on the supratemporal plane, a component generated in the association cortex on the lateral aspect of the temporal and parietal cortex, a component generated in the motor and premotor cortices, the mismatch negativity, a temporal component of the processing negativity, and a frontal component of the processing negativity, The first three, which can be considered `true' N1 components, are controlled by the physical and temporal aspects of the stimulus and by the general state of the subject. The other three components are not necessarily elicited by a stimulus but depend on the conditions in which the stimulus occurs. They often last much longer than the true N1 components that they overlap.},
  langid = {english},
  keywords = {Auditory evoked potentials,Components,Event-related potentials,Magnetic responses,Mismatch negativity,N1 wave,Processing negativity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.1987.tb00311.x},
  file = {/Users/xzfang/Zotero/storage/Z6M6SVSK/j.1469-8986.1987.tb00311.html}
}

@article{naatanen_primitive_2001,
  title = {"{{Primitive}} Intelligence" in the Auditory Cortex},
  author = {N{\"a}{\"a}t{\"a}nen, R. and Tervaniemi, M. and Sussman, E. and Paavilainen, P. and Winkler, I.},
  year = {2001},
  month = may,
  journal = {Trends in Neurosciences},
  volume = {24},
  number = {5},
  pages = {283--288},
  issn = {0166-2236},
  doi = {10.1016/s0166-2236(00)01790-2},
  abstract = {The everyday auditory environment consists of multiple simultaneously active sources with overlapping temporal and spectral acoustic properties. Despite the seemingly chaotic composite signal impinging on our ears, the resulting perception is of an orderly "auditory scene" that is organized according to sources and auditory events, allowing us to select messages easily, recognize familiar sound patterns, and distinguish deviant or novel ones. Recent data suggest that these perceptual achievements are mainly based on processes of a cognitive nature ("sensory intelligence") in the auditory cortex. Even higher cognitive processes than previously thought, such as those that organize the auditory input, extract the common invariant patterns shared by a number of acoustically varying sounds, or anticipate the auditory events of the immediate future, occur at the level of sensory cortex (even when attention is not directed towards the sensory input).},
  langid = {english},
  pmid = {11311381},
  keywords = {Auditory Cortex,Auditory Perception,Humans,Intelligence}
}

@article{nadig_evidence_2002,
  title = {Evidence of {{Perspective-Taking Constraints}} in {{Children}}'s {{On-Line Reference Resolution}}},
  author = {Nadig, Aparna S. and Sedivy, Julie C.},
  year = {2002},
  month = jul,
  journal = {Psychological Science},
  volume = {13},
  number = {4},
  pages = {329--336},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.0956-7976.2002.00460.x},
  abstract = {Young children's communication has often been characterized as egocentric. Some researchers claim that the processing of language involves an initial stage that relies on egocentric heuristics, even in adults. Such an account, combined with general developmental difficulties with late-stage processes, could provide an explanation for much of children's egocentric communication. However, the experimental data reported in this article do not support such an account: In an elicited-production task, 5- to 6-year-old children were found to be sensitive to their partner's perspective. Moreover, in an on-line comprehension task, they showed sensitivity to common-ground information from the initial stages of language processing. We propose that mutual knowledge is not distinct from other knowledge relevant for language processing, and exerts early effects on processing in proportion to its salience and reliability.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GR5TAS5B/Nadig and Sedivy - 2002 - Evidence of Perspective-Taking Constraints in Chil.pdf}
}

@article{nagy_critical_1990,
  title = {Critical Color Differences Determined with a Visual Search Task},
  author = {Nagy, Allen L. and Sanchez, Robert R.},
  year = {1990},
  month = jul,
  journal = {JOSA A},
  volume = {7},
  number = {7},
  pages = {1209--1217},
  publisher = {{Optical Society of America}},
  issn = {1520-8532},
  doi = {10.1364/JOSAA.7.001209},
  abstract = {Response times were measured for a visual search task in which the observer was required to find a target that differed from distracting stimuli only in color. In the first experiment the search time was measured as a function of display density for both small and large color differences. With small color differences response time increased with display density, indicating a serial search, but with large color differences response time was constant, indicating a parallel search. In the second experiment the color difference required for parallel search was measured in eight different directions from the distracter chromaticity. These color differences were much larger than threshold color differences and were not well represented by the ellipse used to describe the threshold contour around a point in color space.},
  copyright = {\&\#169; 1990 Optical Society of America},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/385PG7J8/fulltext.html}
}

@article{naim_fundamental_2020,
  title = {Fundamental {{Law}} of {{Memory Recall}}},
  author = {Naim, Michelangelo and Katkov, Mikhail and Romani, Sandro and Tsodyks, Misha},
  year = {2020},
  month = jan,
  journal = {Physical Review Letters},
  volume = {124},
  number = {1},
  pages = {018101},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.124.018101},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/K6DD7J9R/Naim et al. - 2020 - Fundamental Law of Memory Recall.pdf}
}

@article{najemnik_optimal_2005,
  title = {Optimal Eye Movement Strategies in Visual Search},
  author = {Najemnik, Jiri and Geisler, Wilson S.},
  year = {2005},
  month = mar,
  journal = {Nature},
  volume = {434},
  number = {7031},
  pages = {387--391},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature03390},
  abstract = {Few activities are more important for survival than searching the local area with the eyes to find relevant objects: food, predators, potential mates, oncoming cars. Nonetheless, eye movements recorded during visual search often appear haphazard; it has even been suggested that gaze directions are selected randomly. A study in human subjects tasked to spot a target hidden in a cluttered background now shows that the process is far from random: human eye movements are very near to the mathematically determined optimal strategy. The model developed for this work can also be used to analyse search strategies in other species, and in the refinement of robotic vision systems.},
  copyright = {2005 Macmillan Magazines Ltd.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UD5IGAU6/Najemnik and Geisler - 2005 - Optimal eye movement strategies in visual search.pdf;/Users/xzfang/Zotero/storage/EUKDMT4J/nature03390.html}
}

@article{namboodiri_learning_2021,
  title = {The Learning of Prospective and Retrospective Cognitive Maps within Neural Circuits},
  author = {Namboodiri, Vijay Mohan K. and Stuber, Garret D.},
  year = {2021},
  month = oct,
  journal = {Neuron},
  volume = {0},
  number = {0},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.09.034},
  langid = {english},
  pmid = {34678148},
  file = {/Users/xzfang/Zotero/storage/NUHXLIKR/Namboodiri and Stuber - 2021 - The learning of prospective and retrospective cogn.pdf;/Users/xzfang/Zotero/storage/CL8XBVG3/S0896-6273(21)00707-8.html}
}

@article{nasr_cardinal_2012,
  title = {A {{Cardinal Orientation Bias}} in {{Scene-Selective Visual Cortex}}},
  author = {Nasr, Shahin and Tootell, Roger B. H.},
  year = {2012},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {32},
  number = {43},
  pages = {14921--14926},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2036-12.2012},
  abstract = {It has long been known that human vision is more sensitive to contours at cardinal (horizontal and vertical) orientations, compared with oblique orientations; this is the ``oblique effect.'' However, the real-world relevance of the oblique effect is not well understood. Experiments here suggest that this effect is linked to scene perception, via a common bias in the image statistics of scenes. This statistical bias for cardinal orientations is found in many ``carpentered environments'' such as buildings and indoor scenes, and some natural scenes. In Experiment 1, we confirmed the presence of a perceptual oblique effect in a specific set of scene stimuli. Using those scenes, we found that a well known ``scene-selective'' visual cortical area (the parahippocampal place area; PPA) showed distinctively higher functional magnetic resonance imaging (fMRI) activity to cardinal versus oblique orientations. This fMRI-based oblique effect was not observed in other cortical areas (including scene-selective areas transverse occipital sulcus and retrosplenial cortex), although all three scene-selective areas showed the expected inversion effect to scenes. Experiments 2 and 3 tested for an analogous selectivity for cardinal orientations using computer-generated arrays of simple squares and line segments, respectively. The results confirmed the preference for cardinal orientations in PPA, thus demonstrating that the oblique effect can also be produced in PPA by simple geometrical images, with statistics similar to those in scenes. Thus, PPA shows distinctive fMRI selectivity for cardinal orientations across a broad range of stimuli, which may reflect a perceptual oblique effect.},
  chapter = {Brief Communications},
  copyright = {Copyright \textcopyright{} 2012 the authors 0270-6474/12/3214921-06\$15.00/0. This article is freely available online through the J Neurosci Open Choice option.},
  langid = {english},
  pmid = {23100415},
  file = {/Users/xzfang/Zotero/storage/6SITCHKR/Nasr and Tootell - 2012 - A Cardinal Orientation Bias in Scene-Selective Vis.pdf}
}

@article{nasr_number_2019,
  title = {Number Detectors Spontaneously Emerge in a Deep Neural Network Designed for Visual Object Recognition},
  author = {Nasr, Khaled and Viswanathan, Pooja and Nieder, Andreas},
  year = {2019},
  month = may,
  journal = {Science Advances},
  volume = {5},
  number = {5},
  pages = {eaav7903},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aav7903},
  abstract = {Humans and animals have a ``number sense,'' an innate capability to intuitively assess the number of visual items in a set, its numerosity. This capability implies that mechanisms to extract numerosity indwell the brain's visual system, which is primarily concerned with visual object recognition. Here, we show that network units tuned to abstract numerosity, and therefore reminiscent of real number neurons, spontaneously emerge in a biologically inspired deep neural network that was merely trained on visual object recognition. These numerosity-tuned units underlay the network's number discrimination performance that showed all the characteristics of human and animal number discriminations as predicted by the Weber-Fechner law. These findings explain the spontaneous emergence of the number sense based on mechanisms inherent to the visual system. A deep neural network trained only on visual object recognition develops tuned number detectors reminiscent of real neurons. A deep neural network trained only on visual object recognition develops tuned number detectors reminiscent of real neurons.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6ZMJ7LFJ/Nasr et al. - 2019 - Number detectors spontaneously emerge in a deep ne.pdf;/Users/xzfang/Zotero/storage/VKKUIM39/eaav7903.html}
}

@article{nastase_narratives_2021,
  title = {Narratives: {{fMRI}} Data for Evaluating Models of Naturalistic Language Comprehension},
  shorttitle = {Narratives},
  author = {Nastase, Samuel A. and Liu, Yun-Fei and Hillman, Hanna and Zadbood, Asieh and Hasenfratz, Liat and Keshavarzian, Neggin and Chen, Janice and Honey, Christopher J. and Yeshurun, Yaara and Regev, Mor and Nguyen, Mai and Chang, Claire H. C. and Baldassano, Christopher and Lositsky, Olga and Simony, Erez and Chow, Michael A. and Leong, Yuan Chang and Brooks, Paula P. and Micciche, Emily and Choe, Gina and Goldstein, Ariel and Vanderwal, Tamara and Halchenko, Yaroslav O. and Norman, Kenneth A. and Hasson, Uri},
  year = {2021},
  month = mar,
  journal = {bioRxiv},
  pages = {2020.12.23.424091},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.23.424091},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The ``Narratives'' collection aggregates a variety of functional MRI datasets collected while human subjects listened to naturalistic spoken stories. The current release includes 345 subjects, 891 functional scans, and 27 diverse stories of varying duration totaling {$\sim$}4.6 hours of unique stimuli ({$\sim$}43,000 words). This data collection is well-suited for naturalistic neuroimaging analysis, and is intended to serve as a benchmark for models of language and narrative comprehension. We provide standardized MRI data accompanied by rich metadata, preprocessed versions of the data ready for immediate use, and the spoken story stimuli with time-stamped phoneme- and word-level transcripts. All code and data are publicly available with full provenance in keeping with current best practices in transparent and reproducible neuroimaging.{$<$}/p{$>$}},
  chapter = {Confirmatory Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BI5Z3D98/Nastase et al. - 2021 - Narratives fMRI data for evaluating models of nat.pdf;/Users/xzfang/Zotero/storage/NRX8YWAN/2020.12.23.html}
}

@article{newman_perceptual_2001,
  title = {The Perceptual Consequences of Within-Talker Variability in Fricative Production},
  author = {Newman, Rochelle S. and Clouse, Sheryl A. and Burnham, Jessica L.},
  year = {2001},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {109},
  number = {3},
  pages = {1181--1196},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.1348009},
  file = {/Users/xzfang/Zotero/storage/4T7AI8MC/Newman et al. - 2001 - The perceptual consequences of within-talker varia.pdf;/Users/xzfang/Zotero/storage/Z4Y5PET9/1.html}
}

@article{nicenboim_are_2020,
  title = {Are Words Pre-Activated Probabilistically during Sentence Comprehension? {{Evidence}} from New Data and a {{Bayesian}} Random-Effects Meta-Analysis Using Publicly Available Data},
  shorttitle = {Are Words Pre-Activated Probabilistically during Sentence Comprehension?},
  author = {Nicenboim, Bruno and Vasishth, Shravan and R{\"o}sler, Frank},
  year = {2020},
  month = may,
  journal = {Neuropsychologia},
  volume = {142},
  pages = {107427},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2020.107427},
  abstract = {Several studies (e.g., Wicha et al., 2003b; DeLong et al., 2005) have shown that readers use information from the sentential context to predict nouns (or some of their features), and that predictability effects can be inferred from the EEG signal in determiners or adjectives appearing before the predicted noun. While these findings provide evidence for the pre-activation proposal, recent replication attempts together with inconsistencies in the results from the literature cast doubt on the robustness of this phenomenon. Our study presents the first attempt to use the effect of gender on predictability in German to study the pre-activation hypothesis, capitalizing on the fact that all German nouns have a gender and that their preceding determiners can show an unambiguous gender marking when the noun phrase has accusative case. Despite having a relatively large sample size (of 120 subjects), both our preregistered and exploratory analyses failed to yield conclusive evidence for or against an effect of pre-activation. The sign of the effect is, however, in the expected direction: the more unexpected the gender of the determiner, the larger the negativity. The recent, inconclusive replication attempts by Nieuwland et al. (2018) and others also show effects with signs in the expected direction. We conducted a Bayesian random-effects meta-analysis using our data and the publicly available data from these recent replication attempts. Our meta-analysis shows a relatively clear but very small effect that is consistent with the pre-activation account and demonstrates a very important advantage of the Bayesian data analysis methodology: we can incrementally accumulate evidence to obtain increasingly precise estimates of the effect of interest.},
  langid = {english},
  keywords = {Bayesian meta-analysis,ERP,Grammatical gender,Pre-activation,Predictions},
  file = {/Users/xzfang/Zotero/storage/XIHJQJ8K/Nicenboim et al. - 2020 - Are words pre-activated probabilistically during s.pdf;/Users/xzfang/Zotero/storage/L2XW68ZS/S0028393220300981.html}
}

@article{niebergall_expansion_2011,
  title = {Expansion of {{MT Neurons Excitatory Receptive Fields}} during {{Covert Attentive Tracking}}},
  author = {Niebergall, Robert and Khayat, Paul S. and Treue, Stefan and {Martinez-Trujillo}, Julio C.},
  year = {2011},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {31},
  number = {43},
  pages = {15499--15510},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2822-11.2011},
  abstract = {Primates can attentively track moving objects while keeping gaze stationary. The neural mechanisms underlying this ability are poorly understood. We investigated this issue by recording responses of neurons in area MT of two rhesus monkeys while they performed two different tasks. During the Attend-Fixation task, two moving random dot patterns (RDPs) translated across a screen at the same speed and in the same direction while the animals directed gaze to a fixation spot and detected a change in its luminance. During the Tracking task, the animals kept gaze on the fixation spot and attentively tracked the two RDPs to report a change in the local speed of one of the patterns' dots. In both conditions, neuronal responses progressively increased as the RDPs entered the neurons' receptive field (RF), peaked when they reached its center, and decreased as they translated away. This response profile was well described by a Gaussian function with its center of gravity indicating the RF center and its flanks the RF excitatory borders. During Tracking, responses were increased relative to Attend-Fixation, causing the Gaussian profiles to expand. Such increases were proportionally larger in the RF periphery than at its center, and were accompanied by a decrease in the trial-to-trial response variability (Fano factor) relative to Attend-Fixation. These changes resulted in an increase in the neurons' performance at detecting targets at longer distances from the RF center. Our results show that attentive tracking dynamically changes MT neurons' RF profiles, ultimately improving the neurons' ability to encode the tracked stimulus features.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2011 the authors 0270-6474/11/3115499-12\$15.00/0.  This article is freely available online through the J Neurosci Open Choice option.},
  langid = {english},
  pmid = {22031896},
  file = {/Users/xzfang/Zotero/storage/KNZS2B24/Niebergall et al. - 2011 - Expansion of MT Neurons Excitatory Receptive Field.pdf;/Users/xzfang/Zotero/storage/QKGTDX3V/15499.html}
}

@article{nieuwland_commentary_2021,
  title = {Commentary: {{Rational Adaptation}} in {{Lexical Prediction}}: {{The Influence}} of {{Prediction Strength}}},
  shorttitle = {Commentary},
  author = {Nieuwland, Mante},
  year = {2021},
  journal = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  file = {/Users/xzfang/Zotero/storage/3G4TK6VU/Nieuwland - 2021 - Commentary Rational Adaptation in Lexical Predict.pdf}
}

@article{nieuwland_dissociable_2020,
  title = {Dissociable Effects of Prediction and Integration during Language Comprehension: Evidence from a Large-Scale Study Using Brain Potentials},
  shorttitle = {Dissociable Effects of Prediction and Integration during Language Comprehension},
  author = {Nieuwland, Mante S. and Barr, Dale J. and Bartolozzi, Federica and {Busch-Moreno}, Simon and Darley, Emily and Donaldson, David I. and Ferguson, Heather J. and Fu, Xiao and Heyselaar, Evelien and Huettig, Falk and Matthew Husband, E. and Ito, Aine and Kazanina, Nina and Kogan, Vita and Koh{\'u}t, Zdenko and Kulakova, Eugenia and M{\'e}zi{\`e}re, Diane and {Politzer-Ahles}, Stephen and Rousselet, Guillaume and Rueschemeyer, Shirley-Ann and Segaert, Katrien and Tuomainen, Jyrki and Von Grebmer Zu Wolfsthurn, Sarah},
  year = {2020},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1791},
  pages = {20180522},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2018.0522},
  abstract = {Composing sentence meaning is easier for predictable words than for unpredictable words. Are predictable words genuinely predicted, or simply more plausible and therefore easier to integrate with sentence context? We addressed this persistent and fundamental question using data from a recent, large-scale (n = 334) replication study, by investigating the effects of word predictability and sentence plausibility on the N400, the brain's electrophysiological index of semantic processing. A spatio-temporally fine-grained mixed-effect multiple regression analysis revealed overlapping effects of predictability and plausibility on the N400, albeit with distinct spatio-temporal profiles. Our results challenge the view that the predictability-dependent N400 reflects the effects of either prediction or integration, and suggest that semantic facilitation of predictable words arises from a cascade of processes that activate and integrate word meaning with context into a sentence-level meaning.This article is part of the theme issue `Towards mechanistic models of meaning composition'.},
  file = {/Users/xzfang/Zotero/storage/5YYX5E2M/Nieuwland et al. - 2020 - Dissociable effects of prediction and integration .pdf;/Users/xzfang/Zotero/storage/9H5AIECF/rstb.2018.html}
}

@article{nieuwland_early_2019,
  title = {Do `Early' Brain Responses Reveal Word Form Prediction during Language Comprehension? {{A}} Critical Review},
  shorttitle = {Do `Early' Brain Responses Reveal Word Form Prediction during Language Comprehension?},
  author = {Nieuwland, Mante S.},
  year = {2019},
  month = jan,
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {96},
  pages = {367--400},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2018.11.019},
  abstract = {Current theories of language comprehension posit that readers and listeners routinely try to predict the meaning but also the visual or sound form of upcoming words. Whereas most neuroimaging studies on word prediction focus on the N400 ERP or its magnetic equivalent, various studies claim that word form prediction manifests itself in `early', pre-N400 brain responses (e.g., ELAN, M100, P130, N1, P2, N200/PMN, N250). Modulations of these components are often taken as evidence that word form prediction impacts early sensory processes (the sensory hypothesis) or, alternatively, the initial stages of word recognition before word meaning is integrated with sentence context (the recognition hypothesis). Here, I comprehensively review studies on sentence- or discourse-level language comprehension that report such effects of prediction on early brain responses. I conclude that the reported evidence for the sensory hypothesis or word recognition hypothesis is weak and inconsistent, and highlight the urgent need for replication of previous findings. I discuss the implications and challenges to current theories of linguistic prediction and suggest avenues for future research.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZPE27SZ8/Nieuwland - 2019 - Do â€˜earlyâ€™ brain responses reveal word form predic.pdf}
}

@article{nieuwland_how_2021,
  title = {How `Rational' Is Semantic Prediction? {{A}} Critique and Re-Analysis of {{Delaney-Busch}}, {{Morgan}}, {{Lau}}, and {{Kuperberg}} (2019)},
  shorttitle = {How `Rational' Is Semantic Prediction?},
  author = {Nieuwland, Mante S.},
  year = {2021},
  month = oct,
  journal = {Cognition},
  volume = {215},
  pages = {104848},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104848},
  abstract = {In a recent article in Cognition, Delaney-Busch et al. (2019) claim evidence for `rational', Bayesian adaptation of semantic predictions, using ERP data from Lau, Holcomb, and Kuperberg (2013). Participants read associatively related and unrelated prime-target word pairs in a first block with only 10\% related trials and a second block with 50\%. Related words elicited smaller N400s than unrelated words, and this difference was strongest in the second block, suggesting greater engagement in predictive processing. Using a rational adaptor model, Delaney-Busch et al. argue that the stronger N400 reduction for related words in the second block developed as a function of the number of related trials, and concluded therefore that participants predicted related words more strongly when their predictions were fulfilled more often. In this critique, I discuss two critical flaws in their analyses, namely the confounding of prediction effects with those of lexical frequency and the neglect of data from the first block. Re-analyses suggest a different picture: related words by themselves did not yield support for their conclusion, and the effect of relatedness gradually strengthened in othe two blocks in a similar way. Therefore, the N400 did not yield evidence that participants rationally adapted their semantic predictions. Within the framework proposed by Delaney-Busch et al., presumed semantic predictions may even be thought of as `irrational'. While these results yielded no evidence for rational or probabilistic prediction, they do suggest that participants became increasingly better at predicting target words from prime words.},
  langid = {english},
  keywords = {Bayesian adaptation,Expectation adaptation,Forward association,N400,Probabilistic prediction,Rational adaptation,Semantic priming},
  file = {/Users/xzfang/Zotero/storage/MXUA8QPK/Nieuwland - 2021 - How â€˜rationalâ€™ is semantic prediction A critique .pdf;/Users/xzfang/Zotero/storage/XDTSVKNN/S0010027721002675.html}
}

@article{nieuwland_largescale_2018,
  title = {Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension},
  author = {Nieuwland, Mante S and {Politzer-Ahles}, Stephen and Heyselaar, Evelien and Segaert, Katrien and Darley, Emily and Kazanina, Nina and Von Grebmer Zu Wolfsthurn, Sarah and Bartolozzi, Federica and Kogan, Vita and Ito, Aine and M{\'e}zi{\`e}re, Diane and Barr, Dale J and Rousselet, Guillaume A and Ferguson, Heather J and {Busch-Moreno}, Simon and Fu, Xiao and Tuomainen, Jyrki and Kulakova, Eugenia and Husband, E Matthew and Donaldson, David I and Koh{\'u}t, Zdenko and Rueschemeyer, Shirley-Ann and Huettig, Falk},
  editor = {{Shinn-Cunningham}, Barbara G},
  year = {2018},
  month = apr,
  journal = {eLife},
  volume = {7},
  pages = {e33468},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.33468},
  abstract = {Do people routinely pre-activate the meaning and even the phonological form of upcoming words? The most acclaimed evidence for phonological prediction comes from a 2005 Nature Neuroscience publication by DeLong, Urbach and Kutas, who observed a graded modulation of electrical brain potentials (N400) to nouns and preceding articles by the probability that people use a word to continue the sentence fragment (`cloze'). In our direct replication study spanning 9 laboratories (N=334), pre-registered replication-analyses and exploratory Bayes factor analyses successfully replicated the noun-results but, crucially, not the article-results. Pre-registered single-trial analyses also yielded a statistically significant effect for the nouns but not the articles. Exploratory Bayesian single-trial analyses showed that the article-effect may be non-zero but is likely far smaller than originally reported and too small to observe without very large sample sizes. Our results do not support the view that readers routinely pre-activate the phonological form of predictable words.},
  keywords = {language comprehension,N400,prediction},
  file = {/Users/xzfang/Zotero/storage/9IZGCD5L/Nieuwland et al. - 2018 - Large-scale replication study reveals a limit on p.pdf}
}

@article{nieuwland_testing_2005,
  title = {Testing the Limits of the Semantic Illusion Phenomenon: {{ERPs}} Reveal Temporary Semantic Change Deafness in Discourse Comprehension},
  shorttitle = {Testing the Limits of the Semantic Illusion Phenomenon},
  author = {Nieuwland, Mante S. and Van Berkum, Jos J. A.},
  year = {2005},
  month = aug,
  journal = {Cognitive Brain Research},
  volume = {24},
  number = {3},
  pages = {691--701},
  issn = {0926-6410},
  doi = {10.1016/j.cogbrainres.2005.04.003},
  abstract = {In general, language comprehension is surprisingly reliable. Listeners very rapidly extract meaning from the unfolding speech signal, on a word-by-word basis, and usually successfully. Research on `semantic illusions' however suggests that under certain conditions, people fail to notice that the linguistic input simply doesn't make sense. In the current event-related brain potentials (ERP) study, we examined whether listeners would, under such conditions, spontaneously detect an anomaly in which a human character central to the story at hand (e.g., ``a tourist'') was suddenly replaced by an inanimate object (e.g., ``a suitcase''). Because this replacement introduced a very powerful coherence break, we expected listeners to immediately notice the anomaly and generate the standard ERP effect associated with incoherent language, the N400 effect. However, instead of the standard N400 effect, anomalous words elicited a positive ERP effect from about 500\textendash 600 ms onwards. The absence of an N400 effect suggests that subjects did not immediately notice the anomaly, and that for a few hundred milliseconds the comprehension system has converged on an apparently coherent but factually incorrect interpretation. The presence of the later ERP effect indicates that subjects were processing for comprehension and did ultimately detect the anomaly. Therefore, we take the absence of a regular N400 effect as the online manifestation of a temporary semantic illusion. Our results also show that even attentive listeners sometimes fail to notice a radical change in the nature of a story character, and therefore suggest a case of short-lived `semantic change deafness' in language comprehension.},
  langid = {english},
  keywords = {Animacy,Change deafness,Discourse comprehension,EEG,N400,Semantic illusion},
  file = {/Users/xzfang/Zotero/storage/4BG4ASBM/Nieuwland and Van Berkum - 2005 - Testing the limits of the semantic illusion phenom.pdf;/Users/xzfang/Zotero/storage/BPBMBDJX/S0926641005001102.html}
}

@article{nieuwland_when_2006,
  title = {When {{Peanuts Fall}} in {{Love}}: {{N400 Evidence}} for the {{Power}} of {{Discourse}}},
  shorttitle = {When {{Peanuts Fall}} in {{Love}}},
  author = {Nieuwland, Mante S. and Van Berkum, Jos J. A.},
  year = {2006},
  month = jul,
  journal = {Journal of Cognitive Neuroscience},
  volume = {18},
  number = {7},
  pages = {1098--1111},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn.2006.18.7.1098},
  abstract = {Abstract             In linguistic theories of how sentences encode meaning, a distinction is often made between the context-free rule-based combination of lexical-semantic features of the words within a sentence (``semantics''), and the contributions made by wider context (``pragmatics''). In psycholinguistics, this distinction has led to the view that listeners initially compute a local, context-independent meaning of a phrase or sentence before relating it to the wider context. An important aspect of such a two-step perspective on interpretation is that local semantics cannot initially be overruled by global contextual factors. In two spoken-language event-related potential experiments, we tested the viability of this claim by examining whether discourse context can overrule the impact of the core lexical-semantic feature animacy, considered to be an innate organizing principle of cognition. Two-step models of interpretation predict that verb-object animacy violations, as in ``The girl comforted the clock,'' will always perturb the unfolding interpretation process, regardless of wider context. When presented in isolation, such anomalies indeed elicit a clear N400 effect, a sign of interpretive problems. However, when the anomalies were embedded in a supportive context (e.g., a girl talking to a clock about his depression), this N400 effect disappeared completely. Moreover, given a suitable discourse context (e.g., a story about an amorous peanut), animacy-violating predicates (``the peanut was in love'') were actually processed more easily than canonical predicates (``the peanut was salted''). Our findings reveal that discourse context can immediately overrule local lexical-semantic violations, and therefore suggest that language comprehension does not involve an initially context-free semantic analysis.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/985UQU69/Nieuwland and Van Berkum - 2006 - When Peanuts Fall in Love N400 Evidence for the P.pdf}
}

@article{nieuwland_who_2014,
  title = {``{{Who}}'s He?'' {{Event-related}} Brain Potentials and Unbound Pronouns},
  shorttitle = {``{{Who}}'s He?},
  author = {Nieuwland, Mante S.},
  year = {2014},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {76},
  pages = {1--28},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2014.06.002},
  abstract = {Three experiments used event-related potentials to examine the processing consequences of gender-mismatching pronouns (e.g., ``The aunt found out that he had won the lottery''), which have been shown to elicit P600 effects when judged as syntactically anomalous (Osterhout \& Mobley, 1995). In each experiment, mismatching pronouns elicited a sustained, frontal negative shift (Nref) compared to matching pronouns: when participants were instructed to posit a new referent for mismatching pronouns (Experiment 1), and without this instruction (Experiments 2 and 3). In Experiments 1 and 2, the observed Nref was robust only in individuals with higher reading span scores. In Experiment 1, participants with lower reading span showed P600 effects instead, consistent with an attempt at coreferential interpretation despite gender mismatch. The results from the experiments combined suggest that, in absence of an acceptability judgment task, people are more likely to interpret mismatching pronouns as referring to an unknown, unheralded antecedent than as a grammatically anomalous anaphor for a given antecedent.},
  langid = {english},
  keywords = {Ambiguity,Gender,Nref,P600,Pronouns,Sentence comprehension},
  file = {/Users/xzfang/Zotero/storage/SBAU682V/Nieuwland - 2014 - â€œWhoâ€™s heâ€ Event-related brain potentials and unb.pdf;/Users/xzfang/Zotero/storage/DQABI4L6/S0749596X14000606.html}
}

@article{nigam_n400_1992,
  title = {N400 to {{Semantically Anomalous Pictures}} and {{Words}}},
  author = {Nigam, Arti and Hoffman, James and Simons, Robert},
  year = {1992},
  month = jan,
  journal = {Journal of cognitive neuroscience},
  volume = {4},
  pages = {15--22},
  doi = {10.1162/jocn.1992.4.1.15},
  abstract = {The N400 component of the human event-related brain potential appears to be related to violations of semantic expectancy during language comprehension. The present experiment investigated whether the N400 is related specifically to activity in a language system or is an index of a conceptual system that is accessed by both pictures and words. Sentences were visually presented one word at a time with the last word being replaced in one condition by a line drawing representing the same concept (eg, the word "socks" was replaced by a picture of socks). The N400 recorded in the Pictures Condition was found to be identical to the N400 generated by words in terms of amplitude, scalp distribution, and latency. These results suggest that the N400 is an index of activity in a conceptual memory that is accessed by both pictures and words.}
}

@article{njie_talker_2022,
  title = {Talker and Accent Familiarity Yield Advantages for Voice Identity Perception: {{A}} Voice Sorting Study},
  shorttitle = {Talker and Accent Familiarity Yield Advantages for Voice Identity Perception},
  author = {Njie, Sheriff and Lavan, Nadine and McGettigan, Carolyn},
  year = {2022},
  month = mar,
  journal = {Memory \& Cognition},
  issn = {1532-5946},
  doi = {10.3758/s13421-022-01296-0},
  abstract = {In the current study, we examine and compare the effects of talker and accent familiarity in the context of a voice identity sorting task, using naturally varying voice recording samples from the TV show Derry Girls. Voice samples were thus all spoken with a regional accent of UK/Irish English (from [London]derry). We tested four listener groups: Listeners were either familiar or unfamiliar with the TV show (and therefore the talker identities) and were either highly familiar or relatively less familiar with Northern Irish accents. Both talker and accent familiarity significantly improved accuracy of voice identity sorting performance. However, the talker familiarity benefits were overall larger, and more consistent. We discuss the results in light of a possible hierarchy of familiarity effects and argue that our findings may provide additional evidence for interactions of speech and identity processing pathways in voice identity perception. We also identify some key limitations in the current work and provide suggestions for future studies to address these.},
  langid = {english},
  keywords = {Accent,Familiarity,Voice identity,Within-person variability},
  file = {/Users/xzfang/Zotero/storage/WC3CNN3P/Njie et al. - 2022 - Talker and accent familiarity yield advantages for.pdf}
}

@article{nobre_anticipated_2018,
  title = {Anticipated Moments: Temporal Structure in Attention},
  shorttitle = {Anticipated Moments},
  author = {Nobre, Anna C. and {van Ede}, Freek},
  year = {2018},
  month = jan,
  journal = {Nature Reviews Neuroscience},
  volume = {19},
  number = {1},
  pages = {34--48},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn.2017.141},
  abstract = {Attention enables the prioritization and selection of relevant sensory inputs and appropriate responses. Understanding the cognitive and neural mechanisms by which attention is allocated to relevant moments in time provides a necessary complement to the study of spatial, feature-based and object-based attention.At least four types of informative temporal structures enable temporal expectations to guide attention in time: cued associations, hazard rates, rhythms and sequences. Their impacts on perception and action need not always run through common mechanisms and may often interact.Investigations of how temporal expectations are controlled and utilized by the brain are only beginning to gain ground but already suggest that there are multiple mechanisms at play, involving, among others, changes in the strength, timing and synchrony of neuronal activity.Temporal expectations often co-occur with spatial and feature-based expectations, amplifying their impact on neural responses and performance. Accordingly, temporal expectations may often run through other, receptive-field-based, attentional biases.Although the study of temporal attention takes its roots in the domains of perception and action, it is likely to be important across many cognitive domains (working memory, reinforcement learning and so on) and may contribute to a better understanding of many cognitive disorders.},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Attention,Cognitive neuroscience,Learning and memory,Perception,Sensory processing},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Attention;Cognitive neuroscience;Learning and memory;Perception;Sensory processing Subject\_term\_id: attention;cognitive-neuroscience;learning-and-memory;perception;sensory-processing},
  file = {/Users/xzfang/Zotero/storage/RHAY5YED/Nobre and van Ede - 2018 - Anticipated moments temporal structure in attenti.pdf;/Users/xzfang/Zotero/storage/4CWBPQ99/nrn.2017.html}
}

@article{nobre_premembering_2019,
  title = {Premembering {{Experience}}: {{A Hierarchy}} of {{Time-Scales}} for {{Proactive Attention}}},
  shorttitle = {Premembering {{Experience}}},
  author = {Nobre, Anna C. and Stokes, Mark G.},
  year = {2019},
  month = oct,
  journal = {Neuron},
  volume = {104},
  number = {1},
  pages = {132--146},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.08.030},
  abstract = {Memories are about the past, but they serve the future. Memory research often emphasizes the former aspect: focusing on the functions that re-constitute (re-member) experience and elucidating the various types of memories and their interrelations, timescales, and neural bases. Here we highlight the prospective nature of memory in guiding selective attention, focusing on functions that use previous experience to anticipate the relevant events about to unfold\textemdash to ``premember'' experience. Memories of various types and timescales play a fundamental role in guiding perception and performance adaptively, proactively, and dynamically. Consonant with this perspective, memories are often recorded according to expected future demands. Using working memory as an example, we consider how mnemonic content is selected and represented for future use. This perspective moves away from the traditional representational account of memory toward a functional account in which forward-looking memory traces are informationally and computationally tuned for interacting with incoming sensory signals to guide adaptive behavior.},
  langid = {english},
  keywords = {attention,decision-making,episodic memory,hippocampus,implicit memory,memory,prefrontal cortex,priming,working memory},
  file = {/Users/xzfang/Zotero/storage/DY2YTKUT/Nobre and Stokes - 2019 - Premembering Experience A Hierarchy of Time-Scale.pdf}
}

@article{norman-haignere_distinct_2015,
  title = {Distinct {{Cortical Pathways}} for {{Music}} and {{Speech Revealed}} by {{Hypothesis-Free Voxel Decomposition}}},
  author = {{Norman-Haignere}, Sam and Kanwisher, Nancy G. and McDermott, Josh H.},
  year = {2015},
  month = dec,
  journal = {Neuron},
  volume = {88},
  number = {6},
  pages = {1281--1296},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2015.11.035},
  abstract = {The organization of human auditory cortex remains unresolved, due in part to the small stimulus sets common to fMRI studies and the overlap of neural populations within voxels. To address these challenges, we measured fMRI responses to 165 natural sounds and inferred canonical response profiles (``components'') whose weighted combinations explained voxel responses throughout auditory cortex. This analysis revealed six components, each with interpretable response characteristics despite being unconstrained by prior functional hypotheses. Four components embodied selectivity for particular acoustic features (frequency, spectrotemporal modulation, pitch). Two others exhibited pronounced selectivity for music and speech, respectively, and were not explainable by standard acoustic features. Anatomically, music and speech selectivity concentrated in distinct regions of non-primary auditory cortex. However, music selectivity was weak in raw voxel responses, and its detection required a decomposition method. Voxel decomposition identifies primary dimensions of response variation across natural sounds, revealing distinct cortical pathways for music and speech.},
  pmcid = {PMC4740977},
  pmid = {26687225},
  file = {/Users/xzfang/Zotero/storage/2C6V4DEB/Norman-Haignere et al. - 2015 - Distinct Cortical Pathways for Music and Speech Re.pdf}
}

@article{norman-haignere_divergence_2019,
  title = {Divergence in the Functional Organization of Human and Macaque Auditory Cortex Revealed by {{fMRI}} Responses to Harmonic Tones},
  author = {{Norman-Haignere}, Sam V. and Kanwisher, Nancy and McDermott, Josh H. and Conway, Bevil R.},
  year = {2019},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {22},
  number = {7},
  pages = {1057--1060},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0410-7},
  abstract = {Norman-Haignere et al. report that humans but not macaque monkeys possess cortical regions with a strong preference for harmonic tones compared to noise. This species difference may be driven by the demands of speech and music perception in humans.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RGPMJG9Q/Norman-Haignere et al. - 2019 - Divergence in the functional organization of human.pdf;/Users/xzfang/Zotero/storage/6MCNQEPF/s41593-019-0410-7.html}
}

@techreport{norman-haignere_intracranial_2019,
  type = {Preprint},
  title = {Intracranial Recordings from Human Auditory Cortex Reveal a Neural Population Selective for Song},
  author = {{Norman-Haignere}, Sam V and Feather, Jenelle and Boebinger, Dana and Brunner, Peter and Ritaccio, Anthony and McDermott, Josh H and Schalk, Gerwin and Kanwisher, Nancy},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/696161},
  abstract = {Abstract           How are neural representations of music organized in the human brain? While neuroimaging has suggested some segregation between responses to music and other sounds, it remains unclear whether finer-grained organization exists within the domain of music. To address this question, we measured cortical responses to natural sounds using intracranial recordings from human patients and inferred canonical response components using a data-driven decomposition algorithm. The inferred components replicated many prior findings including distinct neural selectivity for speech and music. Our key novel finding is that one component responded nearly exclusively to music with singing. Song selectivity was not explainable by standard acoustic features and was co-located with speech- and music-selective responses in the middle and anterior superior temporal gyrus. These results suggest that neural representations of music are fractionated into subpopulations selective for different types of music, at least one of which is specialized for the analysis of song.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/X858FHFZ/Norman-Haignere et al. - 2019 - Intracranial recordings from human auditory cortex.pdf}
}

@techreport{norman-haignere_multiscale_2020,
  type = {Preprint},
  title = {Multiscale Integration Organizes Hierarchical Computation in Human Auditory Cortex},
  author = {{Norman-Haignere}, Sam V and Long, Laura K. and Devinsky, Orrin and Doyle, Werner and Irobunda, Ifeoma and Merricks, Edward M. and Feldstein, Neil A. and McKhann, Guy M. and Schevon, Catherine A. and Flinker, Adeen and Mesgarani, Nima},
  year = {2020},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.30.321687},
  abstract = {Abstract           To derive meaning from sound, the brain must integrate information across tens (e.g. phonemes) to hundreds (e.g. words) of milliseconds, but the neural computations that enable multiscale integration remain unclear. Prior evidence suggests that human auditory cortex analyzes sound using both generic acoustic features (e.g. spectrotemporal modulation) and category-specific computations, but how these putatively distinct computations integrate temporal information is unknown. To answer this question, we developed a novel method to estimate neural integration periods and applied the method to intracranial recordings from human epilepsy patients. We show that integration periods increase three-fold as one ascends the auditory cortical hierarchy. Moreover, we find that electrodes with short integration periods (\textasciitilde 50-150 ms) respond selectively to spectrotemporal modulations, while electrodes with long integration periods (\textasciitilde 200-300 ms) show prominent selectivity for sound categories such as speech and music. These findings reveal how multiscale temporal analysis organizes hierarchical computation in human auditory cortex.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UBRTG83W/Norman-Haignere et al. - 2020 - Multiscale integration organizes hierarchical comp.pdf}
}

@article{norman-haignere_neural_2018,
  title = {Neural Responses to Natural and Model-Matched Stimuli Reveal Distinct Computations in Primary and Nonprimary Auditory Cortex},
  author = {{Norman-Haignere}, Sam V. and McDermott, Josh H.},
  year = {2018},
  month = dec,
  journal = {PLOS Biology},
  volume = {16},
  number = {12},
  pages = {e2005127},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2005127},
  abstract = {A central goal of sensory neuroscience is to construct models that can explain neural responses to natural stimuli. As a consequence, sensory models are often tested by comparing neural responses to natural stimuli with model responses to those stimuli. One challenge is that distinct model features are often correlated across natural stimuli, and thus model features can predict neural responses even if they do not in fact drive them. Here, we propose a simple alternative for testing a sensory model: we synthesize a stimulus that yields the same model response as each of a set of natural stimuli, and test whether the natural and ``model-matched'' stimuli elicit the same neural responses. We used this approach to test whether a common model of auditory cortex\textemdash in which spectrogram-like peripheral input is processed by linear spectrotemporal filters\textemdash can explain fMRI responses in humans to natural sounds. Prior studies have that shown that this model has good predictive power throughout auditory cortex, but this finding could reflect feature correlations in natural stimuli. We observed that fMRI responses to natural and model-matched stimuli were nearly equivalent in primary auditory cortex (PAC) but that nonprimary regions, including those selective for music or speech, showed highly divergent responses to the two sound sets. This dissociation between primary and nonprimary regions was less clear from model predictions due to the influence of feature correlations across natural stimuli. Our results provide a signature of hierarchical organization in human auditory cortex, and suggest that nonprimary regions compute higher-order stimulus properties that are not well captured by traditional models. Our methodology enables stronger tests of sensory models and could be broadly applied in other domains.},
  langid = {english},
  keywords = {Auditory cortex,Bioacoustics,Forecasting,Functional magnetic resonance imaging,Matched filters,Neurons,Research validity,Statistical noise},
  file = {/Users/xzfang/Zotero/storage/LCIMVQ7H/Norman-Haignere and McDermott - 2018 - Neural responses to natural and model-matched stim.pdf;/Users/xzfang/Zotero/storage/X7DC7Z3S/article.html}
}

@article{norman-haignere_pitchresponsive_2016,
  title = {Pitch-{{Responsive Cortical Regions}} in {{Congenital Amusia}}},
  author = {{Norman-Haignere}, Sam V. and Albouy, Philippe and Caclin, Anne and McDermott, Josh H. and Kanwisher, Nancy G. and Tillmann, Barbara},
  year = {2016},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {36},
  number = {10},
  pages = {2986--2994},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2705-15.2016},
  abstract = {Congenital amusia is a lifelong deficit in music perception thought to reflect an underlying impairment in the perception and memory of pitch. The neural basis of amusic impairments is actively debated. Some prior studies have suggested that amusia stems from impaired connectivity between auditory and frontal cortex. However, it remains possible that impairments in pitch coding within auditory cortex also contribute to the disorder, in part because prior studies have not measured responses from the cortical regions most implicated in pitch perception in normal individuals. We addressed this question by measuring fMRI responses in 11 subjects with amusia and 11 age- and education-matched controls to a stimulus contrast that reliably identifies pitch-responsive regions in normal individuals: harmonic tones versus frequency-matched noise. Our findings demonstrate that amusic individuals with a substantial pitch perception deficit exhibit clusters of pitch-responsive voxels that are comparable in extent, selectivity, and anatomical location to those of control participants. We discuss possible explanations for why amusics might be impaired at perceiving pitch relations despite exhibiting normal fMRI responses to pitch in their auditory cortex: (1) individual neurons within the pitch-responsive region might exhibit abnormal tuning or temporal coding not detectable with fMRI, (2) anatomical tracts that link pitch-responsive regions to other brain areas (e.g., frontal cortex) might be altered, and (3) cortical regions outside of pitch-responsive cortex might be abnormal. The ability to identify pitch-responsive regions in individual amusic subjects will make it possible to ask more precise questions about their role in amusia in future work. SIGNIFICANCE STATEMENT The neural causes of congenital amusia, a lifelong deficit in pitch and music perception, are not fully understood. We tested the hypothesis that amusia is due to abnormalities in brain regions that respond selectively to sounds with a pitch in normal listeners. Surprisingly, amusic individuals exhibited pitch-responsive regions that were similar to normal-hearing controls in extent, selectivity, and anatomical location. We discuss how our results inform current debates on the neural basis of amusia and how the ability to identify pitch-responsive regions in amusic subjects will make it possible to ask more precise questions about their role in amusic deficits.},
  copyright = {Copyright \textcopyright{} 2016 the authors 0270-6474/16/362986-09\$15.00/0},
  langid = {english},
  pmid = {26961952},
  keywords = {amusia,auditory cortex,fMRI,music,pitch},
  file = {/Users/xzfang/Zotero/storage/DFIX2JEF/Norman-Haignere et al. - 2016 - Pitch-Responsive Cortical Regions in Congenital Am.pdf;/Users/xzfang/Zotero/storage/DXTFNJN7/2986.html}
}

@article{norris_bayesian_2006,
  title = {The {{Bayesian}} Reader: {{Explaining}} Word Recognition as an Optimal {{Bayesian}} Decision Process.},
  shorttitle = {The {{Bayesian}} Reader},
  author = {Norris, Dennis},
  year = {2006},
  journal = {Psychological Review},
  volume = {113},
  number = {2},
  pages = {327--357},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.113.2.327},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/V7U37ZZ4/Norris - 2006 - The Bayesian reader Explaining word recognition a.pdf}
}

@article{norris_perceptual_2003,
  title = {Perceptual Learning in Speech},
  author = {Norris, D and McQueen, James M. and Cutler, Anne},
  year = {2003},
  month = sep,
  journal = {Cognitive Psychology},
  volume = {47},
  number = {2},
  pages = {204--238},
  issn = {00100285},
  doi = {10.1016/S0010-0285(03)00006-9},
  abstract = {This study demonstrates that listeners use lexical knowledge in perceptual learning of speech sounds. Dutch listeners first made lexical decisions on Dutch words and nonwords. The final fricative of 20 critical words had been replaced by an ambiguous sound, between [f] and [s]. One group of listeners heard ambiguous [f]-final words (e.g., [WItlo?], from witlof, chicory) and unambiguous [s]-final words (e.g., naaldbos, pine forest). Another group heard the reverse (e.g., ambiguous [na:ldbo?], unambiguous witlof). Listeners who had heard [?] in [f]-final words were subsequently more likely to categorize ambiguous sounds on an [f]\textendash [s] continuum as [f] than those who heard [?] in [s]-final words. Control conditions ruled out alternative explanations based on selective adaptation and contrast. Lexical information can thus be used to train categorization of speech. This use of lexical information differs from the on-line lexical feedback embodied in interactive models of speech perception. In contrast to online feedback, lexical feedback for learning is of benefit to spoken word recognition (e.g., in adapting to a newly encountered dialect).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7D2EVDE6/Norris - 2003 - Perceptual learning in speech.pdf}
}

@article{norris_reading_2012,
  title = {Reading through a Noisy Channel: {{Why}} There's Nothing Special about the Perception of Orthography},
  shorttitle = {Reading through a Noisy Channel},
  author = {Norris, Dennis and Kinoshita, Sachiko},
  year = {2012},
  journal = {Psychological Review},
  volume = {119},
  number = {3},
  pages = {517--545},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/a0028450},
  abstract = {The goal of research on how letter identity and order are perceived during reading is often characterized as one of ``cracking the orthographic code.'' Here, we suggest that there is no orthographic code to crack: Words are perceived and represented as sequences of letters, just as in a dictionary. Indeed, words are perceived and represented in exactly the same way as other visual objects. The phenomena that have been taken as evidence for specialized orthographic representations can be explained by assuming that perception involves recovering information that has passed through a noisy channel: the early stages of visual perception. The noisy channel introduces uncertainty into letter identity, letter order, and even whether letters are present or absent. We develop a computational model based on this simple principle and show that it can accurately simulate lexical decision data from the lexicon projects in English, French, and Dutch, along with masked priming data that have been taken as evidence for specialized orthographic representations. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Computational Modeling,Orthography,Perception,Reading,Statistical Probability},
  file = {/Users/xzfang/Zotero/storage/WZZAYK5V/Norris and Kinoshita - 2012 - Reading through a noisy channel Why there's nothi.pdf;/Users/xzfang/Zotero/storage/A79XGXPQ/2012-14482-001.html}
}

@article{norris_shortlist_2008,
  title = {Shortlist {{B}}: {{A Bayesian}} Model of Continuous Speech Recognition.},
  shorttitle = {Shortlist {{B}}},
  author = {Norris, Dennis and McQueen, James M.},
  year = {2008},
  month = apr,
  journal = {Psychological Review},
  volume = {115},
  number = {2},
  pages = {357--395},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.115.2.357},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7K9KUECT/Norris and McQueen - 2008 - Shortlist B A Bayesian model of continuous speech.pdf}
}

@article{nosofsky_tests_2018,
  title = {Tests of an Exemplar-Memory Model of Classification Learning in a High-Dimensional Natural-Science Category Domain.},
  author = {Nosofsky, Robert M. and Sanders, Craig A. and McDaniel, Mark A.},
  year = {2018},
  month = mar,
  journal = {Journal of Experimental Psychology: General},
  volume = {147},
  number = {3},
  pages = {328--353},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000369},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GENAZFEK/Nosofsky et al. - 2018 - Tests of an exemplar-memory model of classificatio.pdf}
}

@article{nozari_comprehension_2011,
  title = {Is Comprehension Necessary for Error Detection? {{A}} Conflict-Based Account of Monitoring in Speech Production},
  shorttitle = {Is Comprehension Necessary for Error Detection?},
  author = {Nozari, Nazbanou and Dell, Gary S. and Schwartz, Myrna F.},
  year = {2011},
  month = aug,
  journal = {Cognitive psychology},
  volume = {63},
  number = {1},
  pages = {1--33},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2011.05.001},
  abstract = {Despite the existence of speech errors, verbal communication is successful because speakers can detect (and correct) their errors. The standard theory of speech-error detection, the perceptual-loop account, posits that the comprehension system monitors production output for errors. Such a comprehension-based monitor, however, cannot explain the double dissociation between comprehension and error-detection ability observed in the aphasic patients. We propose a new theory of speech-error detection which is instead based on the production process itself. The theory borrows from studies of forced-choice-response tasks the notion that error detection is accomplished by monitoring response conflict via a frontal brain structure, such as the anterior cingulate cortex. We adapt this idea to the two-step model of word production, and test the model-derived predictions on a sample of aphasic patients. Our results show a strong correlation between patients' error-detection ability and the model's characterization of their production skills, and no significant correlation between error detection and comprehension measures, thus supporting a production-based monitor, generally, and the implemented conflict-based monitor in particular. The successful application of the conflict-based theory to error-detection in linguistic, as well as non-linguistic domains points to a domain-general monitoring system.},
  pmcid = {PMC3135428},
  pmid = {21652015},
  file = {/Users/xzfang/Zotero/storage/LFEC2YBH/Nozari et al. - 2011 - Is comprehension necessary for error detection A .pdf}
}

@article{nozari_select_2019,
  title = {To Select or to Wait? {{Response}} to the Commentaries},
  shorttitle = {To Select or to Wait?},
  author = {Nozari, Nazbanou and Hepner, Christopher R.},
  year = {2019},
  month = aug,
  journal = {Cognitive Neuropsychology},
  volume = {36},
  number = {5-6},
  pages = {226--233},
  publisher = {{Routledge}},
  issn = {0264-3294},
  doi = {10.1080/02643294.2019.1632280},
  abstract = {In [Nozari, N., \& Hepner, C. R. (2018). To select or to wait? The importance of criterion setting in debates of competitive lexical selection. Cognitive Neuropsychology. Advance online publication. doi:10.1080/02643294.2018.1476335], we proposed a theoretical framework for reconciling two seemingly irreconcilable theories of lexical selection: competitive vs. non-competitive selection. The key point in this framework is the division of language production into two separate\textemdash albeit interacting\textemdash systems: a decision-making framework and a multi-layered system which maps meaning to sound. Technically, this can be accomplished by superimposing a signal detection model onto the distributions of conflict derived from the core dynamics of mapping semantic features to lexical representations. Based on this framework, we argued that a flexible selection criterion could accommodate patterns predicted by both competitive and non-competitive models of lexical selection. Five excellent commentaries posed various questions regarding the necessity, applicability, and scope of the proposed framework. This paper addresses those questions.},
  pmid = {31238793},
  keywords = {competition,criterion setting,Lexical selection,signal detection theory,word production},
  annotation = {\_eprint: https://doi.org/10.1080/02643294.2019.1632280},
  file = {/Users/xzfang/Zotero/storage/K7RCE877/02643294.2019.html}
}

@article{nunez_contours_2012,
  title = {Contours of Time: {{Topographic}} Construals of Past, Present, and Future in the {{Yupno}} Valley of {{Papua New Guinea}}},
  shorttitle = {Contours of Time},
  author = {N{\'u}{\~n}ez, Rafael and Cooperrider, Kensy and Doan, D and Wassmann, J{\"u}rg},
  year = {2012},
  month = jul,
  journal = {Cognition},
  volume = {124},
  number = {1},
  pages = {25--35},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2012.03.007},
  abstract = {Time, an everyday yet fundamentally abstract domain, is conceptualized in terms of space throughout the world's cultures. Linguists and psychologists have presented evidence of a widespread pattern in which deictic time\textemdash past, present, and future\textemdash is construed along the front/back axis, a construal that is linear and ego-based. To investigate the universality of this pattern, we studied the construal of deictic time among the Yupno, an indigenous group from the mountains of Papua New Guinea, whose language makes extensive use of allocentric topographic (uphill/downhill) terms for describing spatial relations. We measured the pointing direction of Yupno speakers' gestures\textemdash produced naturally and without prompting\textemdash as they explained common expressions related to the past, present, and future. Results show that the Yupno spontaneously construe deictic time spatially in terms of allocentric topography: the past is construed as downhill, the present as co-located with the speaker, and the future as uphill. Moreover, the Yupno construal is not linear, but exhibits a particular geometry that appears to reflect the local terrain. The findings shed light on how, our universal human embodiment notwithstanding, linguistic, cultural, and environmental pressures come to shape abstract concepts.},
  langid = {english},
  keywords = {Abstraction,Cognitive diversity,Deictic time,Frames of reference,Gesture,Papua New Guinea,Time concepts,Topography},
  file = {/Users/xzfang/Zotero/storage/CQSNMBPR/NÃºÃ±ez et al. - 2012 - Contours of time Topographic construals of past, .pdf;/Users/xzfang/Zotero/storage/BUWVHXPJ/S0010027712000571.html}
}

@article{nunez_future_2006,
  title = {With the {{Future Behind Them}}: {{Convergent Evidence From Aymara Language}} and {{Gesture}} in the {{Crosslinguistic Comparison}} of {{Spatial Construals}} of {{Time}}},
  shorttitle = {With the {{Future Behind Them}}},
  author = {N{\'u}{\~n}ez, Rafael E. and Sweetser, Eve},
  year = {2006},
  journal = {Cognitive Science},
  volume = {30},
  number = {3},
  pages = {401--450},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog0000_62},
  abstract = {Cognitive research on metaphoric concepts of time has focused on differences between moving Ego and moving time models, but even more basic is the contrast between Ego- and temporal-reference-point models. Dynamic models appear to be quasi-universal cross-culturally, as does the generalization that in Ego-reference-point models, FUTURE IS IN FRONT OF EGO and PAST IS IN BACK OF EGO. The Aymara language instead has a major static model of time wherein FUTURE IS BEHIND EGO and PAST IS IN FRONT OF EGO; linguistic and gestural data give strong confirmation of this unusual culture-specific cognitive pattern. Gestural data provide crucial information unavailable to purely linguistic analysis, suggesting that when investigating conceptual systems both forms of expression should be analyzed complementarily. Important issues in embodied cognition are raised: how fully shared are bodily grounded motivations for universal cognitive patterns, what makes a rare pattern emerge, and what are the cultural entailments of such patterns?},
  langid = {english},
  keywords = {Aymara,Conceptual metaphor,Conceptual systems,Embodied cognition,Gestures,Inferential organization,Spatial construals of time},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog0000\_62},
  file = {/Users/xzfang/Zotero/storage/7DN26QGW/NÃºÃ±ez and Sweetser - 2006 - With the Future Behind Them Convergent Evidence F.pdf;/Users/xzfang/Zotero/storage/JSHABDEX/s15516709cog0000_62.html}
}

@article{nusbaum_paying_1992,
  title = {Paying {{Attention}} to {{Differences Among Talkers}}},
  author = {Nusbaum, Howard and Morin, Todd},
  year = {1992},
  journal = {Speech perception, production and \ldots},
  abstract = {Paying Attention to Differences Among Talkers},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MDK4HWMJ/Nusbaum - 1992 - Paying Attention to Differences Among Talkers.pdf;/Users/xzfang/Zotero/storage/JEUI729G/Paying_Attention_to_Differences_Among_Talkers.html}
}

@article{nusbaum_talker_1997,
  title = {Talker {{Normalization}}: {{Phonetic Constancy}} as a {{Cognitive Process}}},
  author = {Nusbaum, Howard and Magnuson, James},
  year = {1997},
  pages = {27},
  abstract = {Differences between talkers result in increased variability in the mapping between acoustic patterns and linguistic categories. Typically theories of talker normalization have been specific to the problem of talker variability rather than proposing broader solutions to the overall problem of lack of invariance. Our view is that listeners achieve phonetic constancy by processes that can be described better in terms of general cognitive principles rather than a collection of specific mechanisms each addressing a different form of variability. We will discuss our cognitive framework for speech perception in relation to specific data on normalization processing, and outline the role of attention, learning, and theory-based categorization in phonetic constancy.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8DC5GP7I/Nusbaum - Talker Normalization Phonetic Constancy as a Cogn.pdf}
}

@misc{nussbaum_vocal_2021,
  title = {Vocal Emotion Adaptation Aftereffects within and across Speaker Genders: {{Roles}} of Timbre and Fundamental Frequency},
  shorttitle = {Vocal Emotion Adaptation Aftereffects within and across Speaker Genders},
  author = {Nussbaum, Christine and von Eiff, Celina Isabelle and Skuk, Verena G. and Schweinberger, Stefan R.},
  year = {2021},
  month = mar,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/zcypa},
  abstract = {Although previous research demonstrated perceptual aftereffects in emotional voice adaptation, the contribution of different vocal cues to these effects is unclear. In two experiments, we used parameter-specific morphing of adaptor voices to investigate the relative roles of fundamental frequency (F0) and timbre in vocal emotion adaptation, using angry and fearful utterances. Participants adapted to voices containing emotion-specific information in either F0 or timbre, with all other parameters kept constant at an intermediate 50\% morph level. Full emotional adaptors and ambiguous adaptors were used as reference conditions. Adaptors were either of the same (Experiment 1) or opposite speaker gender (Experiment 2) of target voices. In Experiment 1, we found consistent aftereffects in all adaptation conditions. Crucially, aftereffects following timbre adaptors were much larger than following F0 adaptors and were only marginally smaller than those following full adaptors. In Experiment 2, adaptation aftereffects appeared massively and proportionally reduced, with differences between morph types being no longer significant. These results suggest that timbre plays a larger role than F0 in vocal emotion adaptation, and that vocal emotion adaptation is compromised by eliminating gender-congruency between adaptors and targets. Our findings also add to mounting evidence suggesting a major role of timbre in auditory adaptation.},
  keywords = {Audition,Emotion,fundamental frequency (F0),gender-congruency,parameter-specific voice morphing,Perception,Social and Behavioral Sciences,timbre,vocal emotion adaptation},
  file = {/Users/xzfang/Zotero/storage/3LRF3ILC/Nussbaum et al. - 2021 - Vocal emotion adaptation aftereffects within and a.pdf}
}

@article{nye_improving_2021,
  title = {Improving {{Coherence}} and {{Consistency}} in {{Neural Sequence Models}} with {{Dual-System}}, {{Neuro-Symbolic Reasoning}}},
  author = {Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B. and Lake, Brenden M.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02794 [cs]},
  eprint = {2107.02794},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Human reasoning can often be understood as an interplay between two systems: the intuitive and associative ("System 1") and the deliberative and logical ("System 2"). Neural sequence models -- which have been increasingly successful at performing complex, structured tasks -- exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/44E5449X/Nye et al. - 2021 - Improving Coherence and Consistency in Neural Sequ.pdf;/Users/xzfang/Zotero/storage/9KF2H4DM/2107.html}
}

@incollection{nygaard_perceptual_2021,
  title = {Perceptual {{Integration}} of {{Linguistic}} and {{Non-Linguistic Properties}} of {{Speech}}},
  booktitle = {The {{Handbook}} of {{Speech Perception}}},
  author = {Nygaard, Lynne C. and Tzeng, Christina Y.},
  year = {2021},
  pages = {398--427},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119184096.ch15},
  abstract = {Speech is a complex auditory signal that contains multiple layers of linguistic and non-linguistic structures. This chapter discusses empirical and theoretical work examining the extent to which linguistic and non-linguistic properties are independently processed and represented. It considers research examining the impact of socially conditioned and linguistically relevant variation on the perception of speech and reviews how familiarity with this lawful variation impacts listeners' perception of both linguistic and non-linguistic forms. The chapter argues that variation due to talker and other factors is highly informative and has perceptual consequences for linguistic processing, this variation is integral to the representation and processing of spoken language, and that models of speech perception must necessarily include mechanisms for tracking and representing informative variation in linguistic form. A seminal demonstration of the dependence of talker recognition on phonetic instantiation comes from the investigation of talker identification from sinewave replicas of speech.},
  chapter = {15},
  copyright = {\textcopyright{} 2021 John Wiley \& Sons, Inc.},
  isbn = {978-1-119-18409-6},
  langid = {english},
  keywords = {linguistic processing,non-linguistic properties,speech perception,spoken language,systematic variation,talker recognition},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119184096.ch15},
  file = {/Users/xzfang/Zotero/storage/XGN83S4B/Nygaard and Tzeng - 2021 - Perceptual Integration of Linguistic and Non-Lingu.pdf;/Users/xzfang/Zotero/storage/KNDDSDBC/9781119184096.html}
}

@article{nygaard_speech_1994,
  title = {{{SPEECH PERCEPTION AS A TALKER-CONTINGENT PROCESS}}},
  author = {Nygaard, Lynne C. and Sommers, Mitchell S. and Pisoni, David B.},
  year = {1994},
  month = jan,
  journal = {Psychological science},
  volume = {5},
  number = {1},
  pages = {42--46},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1994.tb00612.x},
  abstract = {To determine how familiarity with a talker's voice affects perception of spoken words, we trained two groups of subjects to recognize a set of voices over a 9-day period. One group then identified novel words produced by the same set of talkers at four signal-to-noise ratios. Control subjects identified the same words produced by a different set of talkers. The results showed that the ability to identify a talker's voice improved intelligibility of novel words produced by that talker. The results suggest that speech perception may involve talker-contingent processes whereby perceptual learning of aspects of the vocal source facilitates the subsequent phonetic analysis of the acoustic signal.},
  pmcid = {PMC3081685},
  pmid = {21526138},
  file = {/Users/xzfang/Zotero/storage/EH7UIQLG/Nygaard et al. - 1994 - SPEECH PERCEPTION AS A TALKER-CONTINGENT PROCESS.pdf}
}

@article{nygaard_talkerspecific_1998,
  title = {Talker-Specific Learning in Speech Perception},
  author = {Nygaard, Lynne C. and Pisoni, David B.},
  year = {1998},
  month = jan,
  journal = {Perception \& Psychophysics},
  volume = {60},
  number = {3},
  pages = {355--376},
  issn = {1532-5962},
  doi = {10.3758/BF03206860},
  abstract = {The effects of perceptual learning of talker identity on the recognition of spoken words and sentences were investigated in three experiments. In each experiment, listeners were trained to learn a set of 10 talkers' voices and were then given an intelligibility test to assess the influence of learning the voices on the processing of the linguistic content of speech. In the first experiment, listeners learned voices from isolated words and were then tested with novel isolated words mixed in noise. The results showed that listeners who were given words produced by familiar talkers at test showed better identification performance than did listeners who were given words produced by unfamiliar talkers. In the second experiment, listeners learned novel voices from sentence-length utterances and were then presented with isolated words. The results showed that learning a talker's voice from sentences did not generalize well to identification of novel isolated words. In the third experiment, listeners learned voices from sentence-length utterances and were then given sentence-length utterances produced by familiar and unfamiliar talkers at test. We found that perceptual learning of novel voices from sentence-length utterances improved speech intelligibility for words in sentences. Generalization and transfer from voice learning to linguistic processing was found to be sensitive to the talker-specific information available during learning and test. These findings demonstrate that increased sensitivity to talker-specific information affects the perception of the linguistic properties of speech in isolated words and sentences.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QKL7QS3X/Nygaard and Pisoni - 1998 - Talker-specific learning in speech perception.pdf}
}

@article{obleser_functional_2007,
  title = {Functional {{Integration}} across {{Brain Regions Improves Speech Perception}} under {{Adverse Listening Conditions}}},
  author = {Obleser, J. and Wise, R. J. S. and Alex Dresner, M. and Scott, S. K.},
  year = {2007},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {27},
  number = {9},
  pages = {2283--2289},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4663-06.2007},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/47RLEYSX/Obleser et al. - 2007 - Functional Integration across Brain Regions Improv.pdf}
}

@article{obleser_magnetic_2004,
  title = {Magnetic {{Brain Response Mirrors Extraction}} of {{Phonological Features}} from {{Spoken Vowels}}},
  author = {Obleser, Jonas and Lahiri, Aditi and Eulitz, Carsten},
  year = {2004},
  month = jan,
  journal = {Journal of Cognitive Neuroscience},
  volume = {16},
  number = {1},
  pages = {31--39},
  issn = {0898-929X},
  doi = {10.1162/089892904322755539},
  abstract = {This study further elucidates determinants of vowel perception in the human auditory cortex. The vowel inventory of a given language can be classified on the basis of phonological features which are closely linked to acoustic properties. A cortical representation of speech sounds based on these phonological features might explain the surprisingly inverse correlation between immense variance in the acoustic signal and high accuracy of speech recognition. We investigated timing and mapping of the N100m elicited by 42 tokens of seven natural German vowels varying along the phonological features tongue height (corresponding to the frequency of the first formant) and place of articulation (corresponding to the frequency of the second and third formants). Auditoryevoked fields were recorded using a 148-channel whole-head magnetometer while subjects performed target vowel detection tasks. Source location differences appeared to be driven by place of articulation: Vowels with mutually exclusive place of articulation features, namely, coronal and dorsal elicited separate centers of activation along the posterior-anterior axis. Additionally, the time course of activation as reflected in the N100m peak latency distinguished between vowel categories especially when the spatial distinctiveness of cortical activation was low. In sum, results suggest that both N100m latency and source location as well as their interaction reflect properties of speech stimuli that correspond to abstract phonological features.},
  file = {/Users/xzfang/Zotero/storage/DLFXDLLX/Obleser et al. - 2004 - Magnetic Brain Response Mirrors Extraction of Phon.pdf;/Users/xzfang/Zotero/storage/ZSH43GQM/Magnetic-Brain-Response-Mirrors-Extraction-of.html}
}

@article{obleser_neural_2019,
  title = {Neural {{Entrainment}} and {{Attentional Selection}} in the {{Listening Brain}}},
  author = {Obleser, Jonas and Kayser, Christoph},
  year = {2019},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {11},
  pages = {913--926},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2019.08.004},
  langid = {english},
  pmid = {31606386},
  keywords = {attention,auditory cortex,hearing,low-frequency oscillations,phase coding,speech tracking},
  file = {/Users/xzfang/Zotero/storage/YKAVTK78/Obleser and Kayser - 2019 - Neural Entrainment and Attentional Selection in th.pdf;/Users/xzfang/Zotero/storage/4QR6IYQN/S1364-6613(19)30205-0.html}
}

@article{obrien_accessibility_2010,
  title = {Accessibility of {{Outdated Information}}},
  author = {O'Brien, Edward and Cook, Anne and Gu{\'e}raud, Sabine},
  year = {2010},
  month = jul,
  journal = {Journal of experimental psychology. Learning, memory, and cognition},
  volume = {36},
  pages = {979--91},
  doi = {10.1037/a0019763},
  abstract = {In 2 previous studies (O'Brien, Rizzella, Albrecht, \& Halleran, 1998; Zwaan \& Madden, 2004), researchers have provided conflicting accounts about whether outdated information continues to influence the comprehension of subsequent text. The current set of experiments was designed to explore further the impact of outdated information on comprehension. First, we examined factors that may have contributed to Zwaan and Madden's (2004) finding that outdated information did not influence comprehension. Experiments 1a and 1b demonstrated that when Zwaan and Madden's target sentences were rewritten to move the targeted anaphor away from the end of the sentence, the impact of outdated information emerged with their materials. With a new set of materials, Experiment 2 demonstrated that outdated information continued to disrupt comprehension, even when the updating information created an irreversible change-in-state of a primary object in the story. The results of all 3 experiments are consistent with a passive reactivation process in which outdated information can influence comprehension processes.}
}

@article{obrien_context_2020,
  title = {Context Effects on Phoneme Categorization in Children with Dyslexia},
  author = {O'Brien, Gabrielle E. and Gijbels, Liesbeth and Yeatman, Jason D.},
  year = {2020},
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {148},
  number = {4},
  pages = {2209--2222},
  issn = {0001-4966},
  doi = {10.1121/10.0002181},
  abstract = {Research shows that, on average, children with dyslexia behave less categorically in phoneme categorization tasks. This study investigates three subtle ways that struggling readers may perform differently than their typically developing peers in this experimental context: sensitivity to the frequency distribution from which speech tokens are drawn, bias induced by previous stimulus presentations, and fatigue during the course of the task. We replicate findings that reading skill is related to categorical labeling, but we do not find evidence that sensitivity to the stimulus frequency distribution, the influence of previous stimulus presentations, and a measure of task engagement differs in children with dyslexia. It is, therefore, unlikely that the reliable relationship between reading skill and categorical labeling is attributable to artifacts of the task design, abnormal neural encoding, or executive function. Rather, categorical labeling may index a general feature of linguistic development whose causal relationship to literacy remains to be ascertained.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VZVMCR82/O'Brien et al. - 2020 - Context effects on phoneme categorization in child.pdf}
}

@article{obrien_reading_2018,
  title = {Reading Ability and Phoneme Categorization},
  author = {O'Brien, Gabrielle E. and McCloy, Daniel R. and Kubota, Emily C. and Yeatman, Jason D.},
  year = {2018},
  month = nov,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {16842},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-34823-8},
  abstract = {Dyslexia is associated with abnormal performance on many auditory psychophysics tasks, particularly those involving the categorization of speech sounds. However, it is debated whether those apparent auditory deficits arise from (a) reduced sensitivity to particular acoustic cues, (b) the difficulty of experimental tasks, or (c) unmodeled lapses of attention. Here we investigate the relationship between phoneme categorization and reading ability, with special attention to the nature of the cue encoding the phoneme contrast (static versus dynamic), differences in task paradigm difficulty, and methodological details of psychometric model fitting. We find a robust relationship between reading ability and categorization performance, show that task difficulty cannot fully explain that relationship, and provide evidence that the deficit is not restricted to dynamic cue contrasts, contrary to prior reports. Finally, we demonstrate that improved modeling of behavioral responses suggests that performance does differ between children with dyslexia and typical readers, but that the difference may be smaller than previously reported.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Z6MYWGSN/Oâ€™Brien et al. - 2018 - Reading ability and phoneme categorization.pdf;/Users/xzfang/Zotero/storage/5UY23HYI/s41598-018-34823-8.html}
}

@article{oconnell_neurophysiology_2021,
  title = {Neurophysiology of {{Human Perceptual Decision-Making}}},
  author = {O'Connell, Redmond G. and Kelly, Simon P.},
  year = {2021},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {44},
  number = {1},
  pages = {annurev-neuro-092019-100200},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-092019-100200},
  abstract = {The discovery of neural signals that reflect the dynamics of perceptual decision formation has had a considerable impact. Not only do such signals enable detailed investigations of the neural implementation of the decisionmaking process but they also can expose key elements of the brain's decision algorithms. For a long time, such signals were only accessible through direct animal brain recordings, and progress in human neuroscience was hampered by the limitations of noninvasive recording techniques. However, recent methodological advances are increasingly enabling the study of human brain signals that finely trace the dynamics of the unfolding decision process. In this review, we highlight how human neurophysiological data are now being leveraged to furnish new insights into the multiple processing levels involved in forming decisions, to inform the construction and evaluation of mathematical models that can explain intra- and interindividual differences, and to examine how key ancillary processes interact with core decision circuits.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LJSZLMIY/O'Connell and Kelly - 2021 - Neurophysiology of Human Perceptual Decision-Makin.pdf}
}

@article{ocraven_fmri_1999,
  title = {{{fMRI}} Evidence for Objects as the Units of Attentional Selection},
  author = {O'Craven, Kathleen M. and Downing, Paul E. and Kanwisher, Nancy},
  year = {1999},
  month = oct,
  journal = {Nature},
  volume = {401},
  number = {6753},
  pages = {584--587},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/44134},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/97P7Y89U/O'Craven et al. - 1999 - fMRI evidence for objects as the units of attentio.pdf}
}

@article{oja_simplified_1982,
  title = {Simplified Neuron Model as a Principal Component Analyzer},
  author = {Oja, Erkki},
  year = {1982},
  pages = {7},
  abstract = {A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TPYBS5MH/Oja - Simplified neuron model as a principal component a.pdf}
}

@article{okazawa_representational_2021,
  title = {Representational Geometry of Perceptual Decisions in the Monkey Parietal Cortex},
  author = {Okazawa, Gouki and Hatch, Christina E. and Mancoo, Allan and Machens, Christian K. and Kiani, Roozbeh},
  year = {2021},
  month = jul,
  journal = {Cell},
  volume = {184},
  number = {14},
  pages = {3748-3761.e18},
  publisher = {{Elsevier}},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2021.05.022},
  langid = {english},
  pmid = {34171308},
  keywords = {circuit model,decision making,face perception,frontal cortex,macaque monkey,motion perception,neural response manifold,parietal cortex,representational geometry,task difficulty},
  file = {/Users/xzfang/Zotero/storage/J7KUL3YZ/Okazawa et al. - 2021 - Representational geometry of perceptual decisions .pdf;/Users/xzfang/Zotero/storage/4VHWINN6/S0092-8674(21)00652-8.html}
}

@article{olshausen_what_2004,
  title = {What Is the Other 85\% of {{V1}} Doing?},
  author = {Olshausen, Bruno A},
  year = {2004},
  pages = {29},
  abstract = {This article will pose the following challenge: that despite four decades of research characterizing the response properties of V1 neurons, we still do not have a decent picture of how V1 really operates\textemdash i.e., how a population of its neurons represents natural scenes under realistic viewing conditions. We identify five problems with the current view that stem largely from biases in the design and execution of experiments, in addition to the contributions of non-linearities in the cortex that are not well understood. Our purpose is to open the window to new theories, a number of which we describe along with some proposals for testing them.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/AMC3YQNS/Olshausen - What is the other 85% of V1 doing.pdf}
}

@incollection{onton_informationbased_2006,
  title = {Information-Based Modeling of Event-Related Brain Dynamics},
  booktitle = {Progress in {{Brain Research}}},
  author = {Onton, Julie and Makeig, Scott},
  editor = {Neuper, Christa and Klimesch, Wolfgang},
  year = {2006},
  month = jan,
  series = {Event-{{Related Dynamics}} of {{Brain Oscillations}}},
  volume = {159},
  pages = {99--120},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(06)59007-7},
  abstract = {We discuss the theory and practice of applying independent component analysis (ICA) to electroencephalographic (EEG) data. ICA blindly decomposes multi-channel EEG data into maximally independent component processes (ICs) that typically express either particularly brain generated EEG activities or some type of non-brain artifacts (line or other environmental noise, eye blinks and other eye movements, or scalp or heart muscle activity). Each brain and non-brain IC is identified with an activity time course (its `activation') and a set of relative strengths of its projections (by volume conduction) to the recording electrodes (its `scalp map'). Many non-articraft IC scalp maps strongly resemble the projection of a single dipole, allowing the location and orientation of the best-fitting equivalent dipole (or other source model) to be easily determined. In favorable circumstances, ICA decomposition of high-density scalp EEG data appears to allow concurrent monitoring, with high time resolution, of separate EEG activities in twenty or more separate cortical EEG source areas. We illustrate the differences between ICA and traditional approaches to EEG analysis by comparing time courses and mean event related spectral perturbations (ERSPs) of scalp channel and IC data. Comparing IC activities across subjects necessitates clustering of similar Ics based on common dynamic and/or spatial features. We discuss and illustrate such a component clustering strategy. In sum, continued application of ICA methods in EEG research should continue to yield new insights into the nature and role of the complex macroscopic cortical dynamics captured by scalp electrode recordings.},
  langid = {english},
  keywords = {EEG source localization,event-related potentials (ERPs),event-related spectral perturbation (ERSP),independent component (IC) clustering,independent component analysis (ICA)}
}

@misc{oppenheim_behavioral_2021,
  title = {Behavioral Interference or Facilitation Does Not Distinguish between Competitive and Noncompetitive Accounts of Lexical Selection in Word Production.},
  author = {Oppenheim, Gary and Nozari, Nazbanou},
  year = {2021},
  month = apr,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/rjezp},
  abstract = {One of the major debates in the field of word production is whether lexical selection is competitive or not. For nearly half a century, semantic interference effects in picture naming latencies have been claimed as evidence for competitive (relative threshold) models of lexical selection, while semantic facilitation effects have been claimed as evidence for non-competitive (simple threshold) models instead. In this paper, we use a computational modeling approach to compare the consequences of competitive and noncompetitive selection algorithms for blocked cyclic picture naming latencies, combined with two approaches to representing taxonomic and thematic semantic features. We show that although our simple model can capture both semantic interference and facilitation, the presence or absence of competition in the selection mechanism is unrelated to the polarity of these semantic effects. These results question the validity of prior assumptions and offer new perspectives on the origins of interference and facilitation in language production.},
  keywords = {Cognitive Neuroscience,Cognitive Psychology,Computational Linguistics,computational modeling,Concepts and Categories,cumulative semantic interference,Language,Learning,Linguistics,Neuroscience,other,Psycholinguistics and Neurolinguistics,Psychology,semantic interference,Social and Behavioral Sciences,taxonomic representation,thematic representation,word production},
  file = {/Users/xzfang/Zotero/storage/HXGQLRIX/Oppenheim and Nozari - 2021 - Behavioral interference or facilitation does not d.pdf}
}

@article{osterhout_brain_2002,
  title = {Brain Potentials Elicited by Prose-Embedded Linguistic Anomalies},
  author = {Osterhout, Lee and Allen, Mark D. and Mclaughlin, Judith and Inoue, Kayo},
  year = {2002},
  month = dec,
  journal = {Memory \& Cognition},
  volume = {30},
  number = {8},
  pages = {1304--1312},
  issn = {0090-502X, 1532-5946},
  doi = {10.3758/BF03213412},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NGCZAWF3/Osterhout et al. - 2002 - Brain potentials elicited by prose-embedded lingui.pdf}
}

@article{osterhout_eventrelated_1992,
  title = {Event-Related Brain Potentials Elicited by Syntactic Anomaly},
  author = {Osterhout, Lee and Holcomb, Phillip J},
  year = {1992},
  month = dec,
  journal = {Journal of Memory and Language},
  volume = {31},
  number = {6},
  pages = {785--806},
  issn = {0749-596X},
  doi = {10.1016/0749-596X(92)90039-Z},
  abstract = {Event-related brain potentials (ERPs) were recorded from 13 scalp electrodes while subjects read sentences containing syntactic ambiguities. Words which were inconsitent with the ``preferred'' sentence structure elicited a brain potential (P600) quite distinct from the potential previously observed following contextually inappropriate words (N400). Furthermore, final words in sentences typically judged to be unacceptable elicited an N400-like effect, relative to final words in sentences typically judged to be acceptable. These findings suggest that ERPs are sensitive to syntactic anomaly, including anomaly engendered by disambiguating material following erroneous analysis of a syntactically ambiguous string (the ``garden path'' effect). We evaluate the speculation that the P600 and N400 effects are elicited as a function of anomaly type (syntactic and semantic, respectively).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EDUE9WCM/Osterhout and Holcomb - 1992 - Event-related brain potentials elicited by syntact.pdf;/Users/xzfang/Zotero/storage/VSI6W4ZV/0749596X9290039Z.html}
}

@article{ostrand_repeat_2019,
  title = {Repeat after Us: {{Syntactic}} Alignment Is Not Partner-Specific},
  shorttitle = {Repeat after Us},
  author = {Ostrand, Rachel and Ferreira, Victor S.},
  year = {2019},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {108},
  pages = {104037},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2019.104037},
  abstract = {Conversational partners match each other's speech, a process known as alignment. Such alignment can be partner-specific, when speakers match particular partners' production distributions, or partner-independent, when speakers match aggregated linguistic statistics across their input. However, partner-specificity has only been assessed in situations where it had clear communicative utility, and non-alignment might cause communicative difficulty. Here, we investigate whether speakers align partner-specifically even without a communicative need, and thus whether the mechanism driving alignment is sensitive to communicative and social factors of the linguistic context. In five experiments, participants interacted with two experimenters, each with unique and systematic syntactic preferences (e.g., Experimenter A only produced double object datives and Experimenter B only produced prepositional datives). Across multiple exposure conditions, participants engaged in partner-independent but not partner-specific alignment. Thus, when partner-specificity does not add communicative utility, speakers align to aggregate, partner-independent statistical distributions, supporting a communicatively-modulated mechanism underlying alignment.},
  langid = {english},
  keywords = {Adaptation,Alignment,Communicative utility,Partner-independent,Partner-specific,Syntax},
  file = {/Users/xzfang/Zotero/storage/D4U66VJZ/Ostrand and Ferreira - 2019 - Repeat after us Syntactic alignment is not partne.pdf;/Users/xzfang/Zotero/storage/Q5JAKNV5/S0749596X19300683.html}
}

@article{osullivan_attentional_2015,
  title = {Attentional {{Selection}} in a {{Cocktail Party Environment Can Be Decoded}} from {{Single-Trial EEG}}},
  author = {O'Sullivan, James A. and Power, Alan J. and Mesgarani, Nima and Rajaram, Siddharth and Foxe, John J. and {Shinn-Cunningham}, Barbara G. and Slaney, Malcolm and Shamma, Shihab A. and Lalor, Edmund C.},
  year = {2015},
  month = jul,
  journal = {Cerebral Cortex},
  volume = {25},
  number = {7},
  pages = {1697--1706},
  issn = {1047-3211},
  doi = {10.1093/cercor/bht355},
  abstract = {How humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial ({$\approx$}60 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a significant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at {$\sim$}200 ms as being critical for solving the cocktail party problem. These findings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural brain\textendash computer interfaces.},
  file = {/Users/xzfang/Zotero/storage/LIPKU7C9/O'Sullivan et al. - 2015 - Attentional Selection in a Cocktail Party Environm.pdf;/Users/xzfang/Zotero/storage/YCVVBADE/457492.html}
}

@article{osullivan_hierarchical_2019,
  title = {Hierarchical {{Encoding}} of {{Attended Auditory Objects}} in {{Multi-talker Speech Perception}}},
  author = {O'Sullivan, James and Herrero, Jose and Smith, Elliot and Schevon, Catherine and McKhann, Guy M. and Sheth, Sameer A. and Mehta, Ashesh D. and Mesgarani, Nima},
  year = {2019},
  month = dec,
  journal = {Neuron},
  volume = {104},
  number = {6},
  pages = {1195-1209.e3},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.09.007},
  abstract = {Humans can easily focus on one speaker in a multi-talker acoustic environment, but how different areas of the human auditory cortex (AC) represent the acoustic components of mixed speech is unknown. We obtained invasive recordings from the primary and nonprimary AC in neurosurgical patients as they listened to multi-talker speech. We found that neural sites in the primary AC responded to individual speakers in the mixture and were relatively unchanged by attention. In contrast, neural sites in the nonprimary AC were less discerning of individual speakers but selectively represented the attended speaker. Moreover, the encoding of the attended speaker in the nonprimary AC was invariant to the degree of acoustic overlap with the unattended speaker. Finally, this emergent representation of attended speech in the nonprimary AC was linearly predictable from the primary AC responses. Our results reveal the neural computations underlying the hierarchical formation of auditory objects in human AC during multi-talker speech perception.},
  langid = {english},
  keywords = {auditory object,cocktail party,encoding,Heschlâ€™s gyrus,hierarchical,human auditory cortex,multi-talker,speech perception,superior temporal gyrus},
  file = {/Users/xzfang/Zotero/storage/UXW4RW7L/Oâ€™Sullivan et al. - 2019 - Hierarchical Encoding of Attended Auditory Objects.pdf;/Users/xzfang/Zotero/storage/5X9SK3IR/S0896627319307809.html}
}

@article{overath_cortical_2015,
  title = {The Cortical Analysis of Speech-Specific Temporal Structure Revealed by Responses to Sound Quilts},
  author = {Overath, Tobias and McDermott, Josh H and Zarate, Jean Mary and Poeppel, David},
  year = {2015},
  month = jun,
  journal = {Nature neuroscience},
  volume = {18},
  number = {6},
  pages = {903--911},
  issn = {1097-6256},
  doi = {10.1038/nn.4021},
  abstract = {Speech contains temporal structure that the brain must analyze to enable linguistic processing. To investigate the neural basis of this analysis, we used sound quilts, stimuli constructed by shuffling segments of a natural sound, approximately preserving its properties on short timescales while disrupting them on longer scales. We generated quilts from foreign speech to eliminate language cues and manipulated the extent of natural acoustic structure by varying the segment length. Using functional magnetic resonance imaging, we identified bilateral regions of the superior temporal sulcus (STS) whose responses varied with segment length. This effect was absent in primary auditory cortex and did not occur for quilts made from other natural sounds or acoustically matched synthetic sounds, suggesting tuning to speech-specific spectrotemporal structure. When examined parametrically, the STS response increased with segment length up to \textasciitilde 500 ms. Our results identify a locus of speech analysis in human auditory cortex that is distinct from lexical, semantic or syntactic processes.},
  pmcid = {PMC4769593},
  pmid = {25984889},
  file = {/Users/xzfang/Zotero/storage/768P2IBJ/Overath et al. - 2015 - The cortical analysis of speech-specific temporal .pdf}
}

@article{owen_highlevel_2021,
  title = {High-Level Cognition during Story Listening Is Reflected in High-Order Dynamic Correlations in Neural Activity Patterns},
  author = {Owen, Lucy L. W. and Chang, Thomas H. and Manning, Jeremy R.},
  year = {2021},
  month = sep,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {1--14},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25876-x},
  abstract = {Our thoughts arise from coordinated patterns of interactions between brain structures that change with our ongoing experiences. High-order dynamic correlations in neural activity patterns reflect different subgraphs of the brain's functional connectome that display homologous lower-level dynamic correlations. Here we test the hypothesis that high-level cognition is reflected in high-order dynamic correlations in brain activity patterns. We develop an approach to estimating high-order dynamic correlations in timeseries data, and we apply the approach to neuroimaging data collected as human participants either listen to a ten-minute story or listen to a temporally scrambled version of the story. We train across-participant pattern classifiers to decode (in held-out data) when in the session each neural activity snapshot was collected. We find that classifiers trained to decode from high-order dynamic correlations yield the best performance on data collected as participants listened to the (unscrambled) story. By contrast, classifiers trained to decode data from scrambled versions of the story yielded the best performance when they were trained using first-order dynamic correlations or non-correlational activity patterns. We suggest that as our thoughts become more complex, they are reflected in higher-order patterns of dynamic network interactions throughout the brain. Coordinated patterns of brain activity reflect cognitive processes. Here the authors use a mathematical framework for describing dynamic patterns in brain networks to show they organize in a fractal-like hierarchy during story listening.},
  copyright = {2021 The Author(s)},
  langid = {english},
  annotation = {Cc\_license\_type: cc\_by Primary\_atype: Research Subject\_term: Cognitive neuroscience;Language;Network models Subject\_term\_id: cognitive-neuroscience;language;network-models},
  file = {/Users/xzfang/Zotero/storage/XS5TU885/Owen et al. - 2021 - High-level cognition during story listening is ref.pdf;/Users/xzfang/Zotero/storage/IKMDTYWQ/s41467-021-25876-x.html}
}

@article{ozker_double_2017,
  title = {A {{Double Dissociation}} between {{Anterior}} and {{Posterior Superior Temporal Gyrus}} for {{Processing Audiovisual Speech Demonstrated}} by {{Electrocorticography}}},
  author = {Ozker, Muge and Schepers, Inga M. and Magnotti, John F. and Yoshor, Daniel and Beauchamp, Michael S.},
  year = {2017},
  month = jun,
  journal = {Journal of cognitive neuroscience},
  volume = {29},
  number = {6},
  pages = {1044--1060},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01110},
  abstract = {Human speech can be comprehended using only auditory information from the talker's voice. However, comprehension is improved if the talker's face is visible, especially if the auditory information is degraded as occurs in noisy environments or with hearing loss. We explored the neural substrates of audiovisual speech perception using electrocorticography, direct recording of neural activity using electrodes implanted on the cortical surface. We observed a double dissociation in the responses to audiovisual speech with clear and noisy auditory component within the superior temporal gyrus (STG), a region long known to be important for speech perception. Anterior STG showed greater neural activity to audiovisual speech with clear auditory component, whereas posterior STG showed similar or greater neural activity to audiovisual speech in which the speech was replaced with speech-like noise. A distinct border between the two response patterns was observed, demarcated by a landmark corresponding to the posterior margin of Heschl's gyrus. To further investigate the computational roles of both regions, we considered Bayesian models of multisensory integration, which predict that combining the independent sources of information available from different modalities should reduce variability in the neural responses. We tested this prediction by measuring the variability of the neural responses to single audiovisual words. Posterior STG showed smaller variability than anterior STG during presentation of audiovisual speech with noisy auditory component. Taken together, these results suggest that posterior STG but not anterior STG is important for multisensory integration of noisy auditory and visual speech.},
  pmcid = {PMC5604231},
  pmid = {28253074},
  file = {/Users/xzfang/Zotero/storage/KMCLC3QA/Ozker et al. - 2017 - A Double Dissociation between Anterior and Posteri.pdf}
}

@article{paczynski_multiple_2012,
  title = {Multiple {{Influences}} of {{Semantic Memory}} on {{Sentence Processing}}: {{Distinct Effects}} of {{Semantic Relatedness}} on {{Violations}} of {{Real-World Event}}/{{State Knowledge}} and {{Animacy Selection Restrictions}}},
  shorttitle = {Multiple {{Influences}} of {{Semantic Memory}} on {{Sentence Processing}}},
  author = {Paczynski, Martin and Kuperberg, Gina R.},
  year = {2012},
  month = nov,
  journal = {Journal of memory and language},
  volume = {67},
  number = {4},
  pages = {426--448},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.07.003},
  abstract = {We aimed to determine whether semantic relatedness between an incoming word and its preceding context can override expectations based on two types of stored knowledge: real-world knowledge about the specific events and states conveyed by a verb, and the verb's broader selection restrictions on the animacy of its argument. We recorded event-related potentials on post-verbal Agent arguments as participants read and made plausibility judgments about passive English sentences. The N400 evoked by incoming animate Agent arguments that violated expectations based on real-world event/state knowledge, was strongly attenuated when they were semantically related to the context. In contrast, semantic relatedness did not modulate the N400 evoked by inanimate Agent arguments that violated the preceding verb's animacy selection restrictions. These findings suggest that, under these task and experimental conditions, semantic relatedness can facilitate processing of post-verbal animate arguments that violate specific expectations based on real-world event/state knowledge, but only when the semantic features of these arguments match the coarser-grained animacy restrictions of the verb. Animacy selection restriction violations also evoked a P600 effect, which was not modulated by semantic relatedness, suggesting that it was triggered by propositional impossibility. Together, these data indicate that the brain distinguishes between real-world event/state knowledge and animacy-based selection restrictions during online processing.},
  pmcid = {PMC3532895},
  pmid = {23284226},
  file = {/Users/xzfang/Zotero/storage/QGP7RZ98/Paczynski and Kuperberg - 2012 - Multiple Influences of Semantic Memory on Sentence.pdf}
}

@article{pagan_inhibitory_2016,
  title = {An Inhibitory Influence of Transposed-Letter Neighbors on Eye Movements during Reading},
  author = {Pag{\'a}n, Ascensi{\'o}n and Paterson, Kevin B. and Blythe, Hazel I. and Liversedge, Simon P.},
  year = {2016},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {1},
  pages = {278--284},
  issn = {1531-5320},
  doi = {10.3758/s13423-015-0869-5},
  abstract = {Previous research has shown that prior exposure to a word's substitution neighbor earlier in the same sentence can disrupt processing of that word, indicating that interword lexical priming occurs naturally during reading, due to the competition between lexical candidates during word identification. Through the present research, we extended these findings by investigating the effects of prior exposure to a word's transposed-letter neighbor (TLN) earlier in a sentence. TLNs are constituted from the same letters, but in different orders. The findings revealed an inhibitory TLN effect, with longer total reading times for target words, and increased regressions to prime and target words, when the target followed a TLN rather than a control word. These findings indicate that prior exposure to a TLN can disrupt word identification during reading. We suggest that this is caused by a failure of word identification, due to the initial misidentification of the target word (potentially as its TLN) triggering postlexical checking.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4WXDP2AF/PagÃ¡n et al. - 2016 - An inhibitory influence of transposed-letter neigh.pdf}
}

@article{paller_eventrelated_1992,
  title = {Event-{{Related Potentials Elicited}} by {{Deviant Endings}} to {{Melodies}}},
  author = {Paller, Ken A. and McCarthy, Gregory and Wood, Charles C.},
  year = {1992},
  journal = {Psychophysiology},
  volume = {29},
  number = {2},
  pages = {202--206},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.1992.tb01686.x},
  abstract = {Event-related potentials were recorded from scalp electrodes while subjects listened to well-known melodies. The melodies ended either with the expected note or with a different note. This design was a nonlinguistic analogue of the design used by Kutas and Hillyard (1980b), who first reported that anomalous terminal words in sentences elicited N400 potentials. However, Besson and Macar (1987) reported that deviant terminal notes in melodies did not elicit N400 potentials. In the present study, additional time was allowed for expectations to develop for the terminal note. Deviant terminal notes did not elicit N400s. In both studies, however, the deviant notes elicited P300-like waves. This outcome raised the possibility that N400 might have been masked by the positive potential. In a second condition in which P300 amplitude was minimized, N400s were again not evident. These results thus illustrate two additional situations in which nonlinguistic stimuli that deviated from a sequential pattern failed to elicit N400 potentials.},
  langid = {english},
  keywords = {ERPs,N400,P300},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.1992.tb01686.x},
  file = {/Users/xzfang/Zotero/storage/CED2TZVA/Paller et al. - 1992 - Event-Related Potentials Elicited by Deviant Endin.pdf;/Users/xzfang/Zotero/storage/RBSQP5U4/j.1469-8986.1992.tb01686.html}
}

@article{pan_conceptual_2021,
  title = {Conceptual Representations in Bicultural Bilinguals: {{An ERP}} Approach},
  shorttitle = {Conceptual Representations in Bicultural Bilinguals},
  author = {Pan, Xuan and Xiong, Andy and Jouravlev, Olessia and Jared, Debra},
  year = {2021},
  journal = {Bilingualism: Language and Cognition},
  pages = {1--13},
  publisher = {{Cambridge University Press}},
  issn = {1366-7289, 1469-1841},
  doi = {10.1017/S1366728921000262},
  abstract = {We investigated conceptual representations for translation word pairs in bilinguals who learned their languages in different cultural contexts. Mandarin\textendash English bilinguals were presented with a word, and then a picture, and decided if they matched. Both behavioural and ERP data were collected. In one session, words were in English and in another they were the Mandarin translations. Critical pictures matched the prior word and were either biased to Chinese or Canadian culture. There was an interaction of test language and picture type in RT and errors in the behavioural data, and in five components in the ERP data, indicating that the task was easier when the culture depicted in the picture was congruent with the language of the preceding word. These findings provide evidence that the specific perceptual experiences that bilinguals encounter when learning words in each language have an impact on the semantic features that are activated by those words.},
  langid = {english},
  keywords = {bicultural,bilinguals,conceptual representations,translation words,wordâ€“picture matching task},
  file = {/Users/xzfang/Zotero/storage/QP3599J4/Pan et al. - Conceptual representations in bicultural bilingual.pdf;/Users/xzfang/Zotero/storage/LY4Q6GYJ/8EC1403C1A85B0348E045A35A5BB08F0.html}
}

@article{papadimitriou_when_2022,
  title = {When Classifying Grammatical Role, {{BERT}} Doesn't Care about Word Order... except When It Matters},
  author = {Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.06204 [cs]},
  eprint = {2203.06204},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey "The chef chopped the onion," not "The onion chopped the chef." Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like "The onion chopped the chef". We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/S5B3NF42/Papadimitriou et al. - 2022 - When classifying grammatical role, BERT doesn't ca.pdf;/Users/xzfang/Zotero/storage/68EEXJBV/2203.html}
}

@article{papafragou_does_2008,
  title = {Does Language Guide Event Perception? {{Evidence}} from Eye Movements},
  shorttitle = {Does Language Guide Event Perception?},
  author = {Papafragou, Anna and Hulbert, Justin and Trueswell, John},
  year = {2008},
  month = jul,
  journal = {Cognition},
  volume = {108},
  number = {1},
  pages = {155--184},
  issn = {00100277},
  doi = {10.1016/j.cognition.2008.02.007},
  abstract = {Languages differ in how they encode motion. When describing bounded motion, English speakers typically use verbs that convey information about manner (e.g., slide, skip, walk) rather than path (e.g., approach, ascend), whereas Greek speakers do the opposite. We investigated whether this strong cross-language difference influences how people allocate attention during motion perception. We compared eye movements from Greek and English speakers as they viewed motion events while (a) preparing verbal descriptions or (b) memorizing the events. During the verbal description task, speakers' eyes rapidly focused on the event components typically encoded in their native language, generating significant cross-language differences even during the first second of motion onset. However, when freely inspecting ongoing events, as in the memorization task, people allocated attention similarly regardless of the language they speak. Differences between language groups arose only after the motion stopped, such that participants spontaneously studied those aspects of the scene that their language does not routinely encode in verbs. These findings offer a novel perspective on the relation between language and perceptual/cognitive processes. They indicate that attention allocation during event perception is not affected by the perceiver's native language; effects of language arise only when linguistic forms are recruited to achieve the task, such as when committing facts to memory.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LNR7N6CB/Papafragou et al. - 2008 - Does language guide event perception Evidence fro.pdf}
}

@article{papesh_eye_2016,
  title = {Eye Movements Reveal Fast, Voice-Specific Priming.},
  author = {Papesh, Megan H. and Goldinger, Stephen D. and Hout, Michael C.},
  year = {2016},
  month = mar,
  journal = {Journal of Experimental Psychology: General},
  volume = {145},
  number = {3},
  pages = {314--337},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000135},
  abstract = {In spoken word perception, voice specificity effects are well-documented: When people hear repeated words in some task, performance is generally better when repeated items are presented in their originally heard voices, relative to changed voices. A key theoretical question about voice specificity effects concerns their time-course: Some studies suggest that episodic traces exert their influence late in lexical processing (the time-course hypothesis; McLennan \& Luce, 2005), whereas others suggest that episodic traces influence immediate, online processing. We report two eye-tracking studies investigating the time-course of voice-specific priming within and across cognitive tasks. In Experiment 1, participants performed modified lexical decision or semantic classification to words spoken by four speakers. The tasks required participants to click a red ``x'' or a blue ``+'' located randomly within separate visual half-fields, necessitating trial-by-trial visual search with consistent half-field response mapping. After a break, participants completed a second block with new and repeated items, half spoken in changed voices. Voice effects were robust very early, appearing in saccade initiation times. Experiment 2 replicated this pattern while changing tasks across blocks, ruling out a response priming account. In the General Discussion, we address the time-course hypothesis, focusing on the challenge it presents for empirical disconfirmation, and highlighting the broad importance of indexical effects, beyond studies of priming.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/V57YSTZL/Papesh et al. - 2016 - Eye movements reveal fast, voice-specific priming..pdf}
}

@article{parker_negative_2016,
  title = {Negative Polarity Illusions and the Format of Hierarchical Encodings in Memory},
  author = {Parker, Dan and Phillips, Colin},
  year = {2016},
  month = dec,
  journal = {Cognition},
  volume = {157},
  pages = {321--339},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2016.08.016},
  abstract = {Linguistic illusions have provided valuable insights into how we mentally navigate complex representations in memory during language comprehension. Two notable cases involve illusory licensing of agreement and negative polarity items (NPIs), where comprehenders fleetingly accept sentences with unlicensed agreement or an unlicensed NPI, but judge those same sentences as unacceptable after more reflection. Existing accounts have argued that illusions are a consequence of faulty memory access processes, and make the additional assumption that the encoding of the sentence remains fixed over time. This paper challenges the predictions made by these accounts, which assume that illusions should generalize to a broader set of structural environments and a wider range of syntactic and semantic phenomena. We show across seven reading-time and acceptability judgment experiments that NPI illusions can be reliably switched ``on'' and ``off'', depending on the amount of time from when the potential licensor is processed until the NPI is encountered. But we also find that the same profile does not extend to agreement illusions. This contrast suggests that the mechanisms responsible for switching the NPI illusion on and off are not shared across all illusions. We argue that the contrast reflects changes over time in the encoding of the semantic/pragmatic representations that can license NPIs. Just as optical illusions have been informative about the visual system, selective linguistic illusions are informative not only about the nature of the access mechanisms, but also about the nature of the encoding mechanisms.},
  langid = {english},
  keywords = {Agreement,Binding,Linguistic illusions,Memory,Negative polarity,Representation,Sentence processing}
}

@article{parras_neurons_2017,
  title = {Neurons along the Auditory Pathway Exhibit a Hierarchical Organization of Prediction Error},
  author = {Parras, Gloria G. and {Nieto-Diego}, Javier and Carbajal, Guillermo V. and {Vald{\'e}s-Baizabal}, Catalina and Escera, Carles and Malmierca, Manuel S.},
  year = {2017},
  month = dec,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {2148},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-02038-6},
  abstract = {Perception is characterized by a reciprocal exchange of predictions and prediction error signals between neural regions. However, the relationship between such sensory mismatch responses and hierarchical predictive processing has not yet been demonstrated at the neuronal level in the auditory pathway. We recorded single-neuron activity from different auditory centers in anaesthetized rats and awake mice while animals were played a sequence of sounds, designed to separate the responses due to prediction error from those due to adaptation effects. Here we report that prediction error is organized hierarchically along the central auditory pathway. These prediction error signals are detectable in subcortical regions and increase as the signals move towards auditory cortex, which in turn demonstrates a large-scale mismatch potential. Finally, the predictive activity of single auditory neurons underlies automatic deviance detection at subcortical levels of processing. These results demonstrate that prediction error is a fundamental component of singly auditory neuron responses.},
  copyright = {2017 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cognitive neuroscience;Cortex;Midbrain;Sensory processing;Thalamus Subject\_term\_id: cognitive-neuroscience;cortex;midbrain;sensory-processing;thalamus},
  file = {/Users/xzfang/Zotero/storage/34TV6PE8/Parras et al. - 2017 - Neurons along the auditory pathway exhibit a hiera.pdf;/Users/xzfang/Zotero/storage/Z43PD5N2/s41467-017-02038-6.html}
}

@article{patel_empirical_2003,
  title = {An Empirical Comparison of Rhythm in Language and Music},
  author = {Patel, Aniruddh D and Daniele, Joseph R},
  year = {2003},
  month = feb,
  journal = {Cognition},
  volume = {87},
  number = {1},
  pages = {B35-B45},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(02)00187-7},
  abstract = {Musicologists and linguists have often suggested that the prosody of a culture's spoken language can influence the structure of its instrumental music. However, empirical data supporting this idea have been lacking. This has been partly due to the difficulty of developing and applying comparable quantitative measures to melody and rhythm in speech and music. This study uses a recently-developed measure for the study of speech rhythm to compare rhythmic patterns in English and French language and classical music. We find that English and French musical themes are significantly different in this measure of rhythm, which also differentiates the rhythm of spoken English and French. Thus, there is an empirical basis for the claim that spoken prosody leaves an imprint on the music of a culture.},
  langid = {english},
  keywords = {Musical rhythm,Speech rhythm,Stress-timing,Syllable-timing},
  file = {/Users/xzfang/Zotero/storage/WRIGQYLY/S0010027702001877.html}
}

@article{patel_evolutionary_2014,
  title = {The Evolutionary Neuroscience of Musical Beat Perception: The {{Action Simulation}} for {{Auditory Prediction}} ({{ASAP}}) Hypothesis},
  shorttitle = {The Evolutionary Neuroscience of Musical Beat Perception},
  author = {Patel, Aniruddh D. and Iversen, John R.},
  year = {2014},
  journal = {Frontiers in Systems Neuroscience},
  volume = {8},
  pages = {57},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2014.00057},
  abstract = {Every human culture has some form of music with a beat: a perceived periodic pulse that structures the perception of musical rhythm and which serves as a framework for synchronized movement to music. What are the neural mechanisms of musical beat perception, and how did they evolve? One view, which dates back to Darwin and implicitly informs some current models of beat perception, is that the relevant neural mechanisms are relatively general and are widespread among animal species. On the basis of recent neural and cross-species data on musical beat processing, this paper argues for a different view. Here we argue that beat perception is a complex brain function involving temporally-precise communication between auditory regions and motor planning regions of the cortex (even in the absence of overt movement). More specifically, we propose that simulation of periodic movement in motor planning regions provides a neural signal that helps the auditory system predict the timing of upcoming beats. This ``action simulation for auditory prediction'' (ASAP) hypothesis leads to testable predictions. We further suggest that ASAP relies on dorsal auditory pathway connections between auditory regions and motor planning regions via the parietal cortex, and suggest that these connections may be stronger in humans than in non-human primates due to the evolution of vocal learning in our lineage. This suggestion motivates cross-species research to determine which species are capable of human-like beat perception, i.e., beat perception that involves accurate temporal prediction of beat times across a fairly broad range of tempi.},
  file = {/Users/xzfang/Zotero/storage/WCNL9CGY/Patel and Iversen - 2014 - The evolutionary neuroscience of musical beat perc.pdf}
}

@article{patel_musically_2005,
  title = {Musically Tone-Deaf Individuals Have Difficulty Discriminating Intonation Contours Extracted from Speech},
  author = {Patel, Aniruddh D. and Foxton, Jessica M. and Griffiths, Timothy D.},
  year = {2005},
  month = dec,
  journal = {Brain and Cognition},
  volume = {59},
  number = {3},
  pages = {310--313},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2004.10.003},
  abstract = {Musically tone-deaf individuals have psychophysical deficits in detecting pitch changes, yet their discrimination of intonation contours in speech appears to be normal. One hypothesis for this dissociation is that intonation contours use coarse pitch contrasts which exceed the pitch-change detection thresholds of tone-deaf individuals (Peretz \& Hyde, 2003). We test this idea by presenting intonation contours for discrimination, both in the context of the original sentences in which they occur and in a ``pure'' form dissociated from any phonetic context. The pure form consists of gliding-pitch analogs of the original intonation contours which exactly follow their pattern of pitch and timing. If the spared intonation perception of tone-deaf individuals is due to the coarse pitch contrasts of intonation, then such individuals should discriminate the original sentences and the gliding-pitch analogs equally well. In contrast, we find that discrimination of the gliding-pitch analogs is severely degraded. Thus it appears that the dissociation between spoken and musical pitch perception in tone-deaf individuals is due to a deficit at a higher level than simple pitch-change detection.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SHH73ARF/S0278262604003069.html}
}

@article{patel_processing_1998,
  title = {Processing {{Syntactic Relations}} in {{Language}} and {{Music}}: {{An Event-Related Potential Study}}},
  shorttitle = {Processing {{Syntactic Relations}} in {{Language}} and {{Music}}},
  author = {Patel, Aniruddh D. and Gibson, Edward and Ratner, Jennifer and Besson, Mireille and Holcomb, Phillip J.},
  year = {1998},
  month = nov,
  journal = {Journal of Cognitive Neuroscience},
  volume = {10},
  number = {6},
  pages = {717--733},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/089892998563121},
  abstract = {Abstract             In order to test the language-specificity of a known neural correlate of syntactic processing [the P600 event-related brain potential (ERP) component], this study directly compared ERPs elicited by syntactic incongruities in language and music. Using principles of phrase structure for language and principles of harmony and key-relatedness for music, sequences were constructed in which an element was either congruous, moderately incongruous, or highly incongruous with the preceding structural context. A within-subjects design using 15 musically educated adults revealed that linguistic and musical structural incongruities elicited positivities that were statistically indistinguishable in a specified latency range. In contrast, a music-specific ERP component was observed that showed antero-temporal right-hemisphere lateralization. The results argue against the language-specificity of the P600 and suggest that language and music can be studied in parallel to address questions of neural specificity in cognitive processing.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/T8YD8DTV/Patel et al. - 1998 - Processing Syntactic Relations in Language and Mus.pdf}
}

@article{patterson_where_2007,
  title = {Where Do You Know What You Know? {{The}} Representation of Semantic Knowledge in the Human Brain},
  shorttitle = {Where Do You Know What You Know?},
  author = {Patterson, Karalyn and Nestor, Peter J. and Rogers, Timothy T.},
  year = {2007},
  month = dec,
  journal = {Nature Reviews Neuroscience},
  volume = {8},
  number = {12},
  pages = {976--987},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn2277},
  abstract = {Semantic memory corresponds to people's general conceptual knowledge about objects and events, including knowledge about their characteristic properties and behaviours, as well as knowledge about the words we use to name and describe objects and events in speech.Whereas episodic memory encompasses memory for specific episodes or situations in one's life, semantic memory encompasses factual knowledge divorced from any specific situational context: ``a scallop is an edible sea creature'' (semantic) as opposed to ``I ate scallops for supper last night'' (episodic).Essentially all theories agree that a widely distributed brain network is responsible for our semantic knowledge of modality-specific features (for example, what a scallop looks or tastes like); but the theories differ on whether this network is sufficient for all of the functions of semantic memory.The theory highlighted in this review proposes that conceptual knowledge requires an amodal hub, which itself contains no semantic features but rather represents the semantic similarity among concepts \textemdash{} for example, the semantic similarity between scallops and prawns, despite their differences in virtually every modality-specific attribute.This theory predicts that a lesion of the specific brain region supporting the amodal hub should disrupt all abilities requiring central conceptual knowledge, independent of the modality of input (such as objects, words or sounds) or output (such as speaking, drawing or using objects) and independent of the type of concept (living things, man-made objects and abstract ideas, for example).Patients with semantic dementia, a neurodegenerative syndrome resulting from focal atrophy of the anterior temporal lobes (ATL) bilaterally, show precisely this pattern of semantic degradation across all modalities and all types of conceptual knowledge; therefore, semantic dementia suggests that the ATL supports an amodal hub.Functional neuroimaging studies of semantic processing only sometimes reveal activation in the ATL. The likelihood of activation in this region, however, can be predicted by a combination of the specific imaging techniques employed and the specificity of semantic processing required by the imaging task.Simulations of semantic memory in connectionist models suggest one reason why the semantic network might require a hub: without such an architecture, it is not clear how the system can learn representations that capture semantic similarity relations.},
  copyright = {2007 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews},
  file = {/Users/xzfang/Zotero/storage/5X34L2YL/Patterson et al. - 2007 - Where do you know what you know The representatio.pdf;/Users/xzfang/Zotero/storage/BM3QV4ZG/nrn2277.html}
}

@article{pauley_agerelated_2021,
  title = {Age-Related Declines in Neural Selectivity Manifest Differentially during Encoding and Recognition},
  author = {Pauley, Claire and Sommer, Verena R. and Kobelt, Malte and Keresztes, Attila and {Werkle-Bergner}, Markus and Sander, Myriam C.},
  year = {2021},
  month = apr,
  journal = {bioRxiv},
  pages = {2021.04.29.441936},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.04.29.441936},
  abstract = {{$<$}p{$>$}One important factor contributing to age-related memory decline is the loss of distinctiveness with which information is represented in brain activity. This loss in neural selectivity may be driven by neural attenuation (i.e., reduced activation to target stimuli) or neural broadening (i.e., increased activation to non-target stimuli). In this fMRI study, we assessed age differences in neural selectivity during first encoding, repeated encoding, and recognition, as well as the underlying pattern (broadening versus attenuation). We found lower neural selectivity in older compared to younger adults during all memory stages. Crucially, while reduced selectivity in older adults was due to neural broadening during first encoding, it was driven by neural attenuation during recognition, but revealed no clear pattern during repeated encoding. Our findings suggest that intrinsic differences between memory stages may interact with neural activity to manifest as either neural broadening or attenuation. Moreover, despite these differential patterns, neural selectivity was highly correlated across memory stages, indicating that one common mechanism may underly distinct expressions of age-related neural dedifferentiation.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NYKZG2HT/Pauley et al. - 2021 - Age-related declines in neural selectivity manifes.pdf;/Users/xzfang/Zotero/storage/8EEV9K4G/2021.04.29.html}
}

@article{paxton_argument_2013,
  title = {Argument Disrupts Interpersonal Synchrony},
  author = {Paxton, Alexandra and Dale, Rick},
  year = {2013},
  month = nov,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  number = {11},
  pages = {2092--2102},
  issn = {1747-0218, 1747-0226},
  doi = {10.1080/17470218.2013.853089},
  abstract = {Research on interpersonal convergence and synchrony characterizes the way in which interacting individuals come to have more similar affect, behaviour, and cognition over time. Although its dynamics have been explored in many settings, convergence during conflict has been almost entirely overlooked. We present a simple but ecologically valid study comparing how different situational contexts that highlight affiliation and argument impact interpersonal convergence of body movement and to what degree emotional states affect convergence in both conversational settings. Using linear mixed-effect models, we found that in-phase bodily synchrony decreases significantly during argument. However, affective changes did not significantly predict changes in levels of interpersonal synchrony, suggesting that differences in affect valences between affiliation and argument cannot solely explain our results.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KQMVJSD6/Paxton and Dale - 2013 - Argument disrupts interpersonal synchrony.pdf}
}

@article{payeur_burstdependent_2021,
  title = {Burst-Dependent Synaptic Plasticity Can Coordinate Learning in Hierarchical Circuits},
  author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A. and Naud, Richard},
  year = {2021},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {24},
  number = {7},
  pages = {1010--1019},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00857-x},
  abstract = {Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well established that it depends on pre- and postsynaptic activity. However, models that rely solely on pre- and postsynaptic activity for synaptic changes have, so far, not been able to account for learning complex tasks that demand credit assignment in hierarchical networks. Here we show that if synaptic plasticity is regulated by high-frequency bursts of spikes, then pyramidal neurons higher in a hierarchical circuit can coordinate the plasticity of lower-level connections. Using simulations and mathematical analyses, we demonstrate that, when paired with short-term synaptic dynamics, regenerative activity in the apical dendrites and synaptic plasticity in feedback pathways, a burst-dependent learning rule can solve challenging tasks that require deep network architectures. Our results demonstrate that well-known properties of dendrites, synapses and synaptic plasticity are sufficient to enable sophisticated learning in hierarchical circuits.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Learning algorithms,Sensory processing,Spike-timing-dependent plasticity},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Learning algorithms;Sensory processing;Spike-timing-dependent plasticity Subject\_term\_id: learning-algorithms;sensory-processing;spike-timing-dependent-plasticity},
  file = {/Users/xzfang/Zotero/storage/UP63MZVT/Payeur et al. - 2021 - Burst-dependent synaptic plasticity can coordinate.pdf;/Users/xzfang/Zotero/storage/KJ2AQN4N/s41593-021-00857-x.html}
}

@article{payne_pace_2017,
  title = {Pace {{Yourself}}: {{Intraindividual Variability}} in {{Context Use Revealed}} by {{Self-paced Event-related Brain Potentials}}},
  shorttitle = {Pace {{Yourself}}},
  author = {Payne, Brennan R. and Federmeier, Kara D.},
  year = {2017},
  month = may,
  journal = {Journal of Cognitive Neuroscience},
  volume = {29},
  number = {5},
  pages = {837--854},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01090},
  abstract = {Event-related brain potentials (ERPs) have revealed multiple mechanisms by which contextual constraints impact language processing. At the same time, little work has examined the trial-to-trial dynamics of context use in the brain. In the current study, we probed intraindividual variability in behavioral and neural indices of context processing during reading. In a concurrent self-paced reading and ERP paradigm, participants read sentences that were either strongly or weakly constraining completed with an expected or unexpected target word. Our findings revealed substantial within-subject variability in behavioral and neural responses to contextual constraints. First, context-based amplitude reductions of the N400, a component linked to semantic memory access, were largest among trials eliciting the slowest RTs. Second, the RT distribution of unexpected words in strongly constraining contexts was positively skewed, reflecting an increased proportion of very slow RTs to trials that violated semantic predictions. Among those prediction-violating trials eliciting faster RTs, a late sustained anterior positivity was observed. However, among trials producing the differentially slowed RTs to prediction violations, we observed a markedly earlier effect of constraint in the form of an anterior N2, a component linked to conflict resolution and the cognitive control of behavior. The current study provides the first neurophysiological evidence for the direct role of cognitive control functions in the volitional control of reading. Collectively, our findings suggest that context use varies substantially within individual participants and that coregistering behavioral and neural indices of online sentence processing offers a window into these single-item dynamics.},
  file = {/Users/xzfang/Zotero/storage/6ZNI25ER/Payne and Federmeier - 2017 - Pace Yourself Intraindividual Variability in Cont.pdf;/Users/xzfang/Zotero/storage/35YP9RNY/Pace-Yourself-Intraindividual-Variability-in.html}
}

@article{pearce_expectation_2006,
  title = {Expectation in {{Melody}}: {{The Influence}} of {{Context}} and {{Learning}}},
  shorttitle = {Expectation in {{Melody}}},
  author = {Pearce, Marcus T. and Wiggins, Geraint A.},
  year = {2006},
  month = jul,
  journal = {Music Perception},
  volume = {23},
  number = {5},
  pages = {377--405},
  issn = {0730-7829, 1533-8312},
  doi = {10.1525/mp.2006.23.5.377},
  abstract = {The Implication-Realization (IR) theory (Narmour, 1990) posits two cognitive systems involved in the generation of melodic expectations: The first consists of a limited number of symbolic rules that are held to be innate and universal; the second reflects the top-down influences of acquired stylistic knowledge. Aspects of both systems have been implemented as quantitative models in research which has yielded empirical support for both components of the theory (Cuddy \& Lunny, 1995; Krumhansl, 1995a, 1995b; Schellenberg, 1996, 1997). However, there is also evidence that the implemented bottom-up rules constitute too inflexible a model to account for the influence of the musical experience of the listener and the melodic context in which expectations are elicited. A theory is presented, according to which both bottom-up and top-down descriptions of observed patterns of melodic expectation may be accounted for in terms of the induction of statistical regularities in existing musical repertoires. A computational model that embodies this theory is developed and used to reanalyze existing experimental data on melodic expectancy. The results of three experiments with increasingly complex melodic stimuli demonstrate that this model is capable of accounting for listeners' expectations as well as or better than the two-factor model of Schellenberg (1997).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EFUYK7ZV/Pearce and Wiggins - 2006 - Expectation in Melody The Influence of Context an.pdf}
}

@article{pearce_statistical_2018,
  title = {Statistical Learning and Probabilistic Prediction in Music Cognition: Mechanisms of Stylistic Enculturation},
  shorttitle = {Statistical Learning and Probabilistic Prediction in Music Cognition},
  author = {Pearce, Marcus T.},
  year = {2018},
  journal = {Annals of the New York Academy of Sciences},
  volume = {1423},
  number = {1},
  pages = {378--395},
  issn = {1749-6632},
  doi = {10.1111/nyas.13654},
  abstract = {Music perception depends on internal psychological models derived through exposure to a musical culture. It is hypothesized that this musical enculturation depends on two cognitive processes: (1) statistical learning, in which listeners acquire internal cognitive models of statistical regularities present in the music to which they are exposed; and (2) probabilistic prediction based on these learned models that enables listeners to organize and process their mental representations of music. To corroborate these hypotheses, I review research that uses a computational model of probabilistic prediction based on statistical learning (the information dynamics of music (IDyOM) model) to simulate data from empirical studies of human listeners. The results show that a broad range of psychological processes involved in music perception\textemdash expectation, emotion, memory, similarity, segmentation, and meter\textemdash can be understood in terms of a single, underlying process of probabilistic prediction using learned statistical models. Furthermore, IDyOM simulations of listeners from different musical cultures demonstrate that statistical learning can plausibly predict causal effects of differential cultural exposure to musical styles, providing a quantitative model of cultural distance. Understanding the neural basis of musical enculturation will benefit from close coordination between empirical neuroimaging and computational modeling of underlying mechanisms, as outlined here.},
  copyright = {\textcopyright{} 2018 The Authors. Annals of the New York Academy of Sciences published by Wiley Periodicals, Inc. on behalf of New York Academy of Sciences.},
  langid = {english},
  keywords = {enculturation,IDyOM,music perception,probabilistic prediction,statistical learning},
  annotation = {\_eprint: https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/nyas.13654},
  file = {/Users/xzfang/Zotero/storage/X5DN7YBN/Pearce - 2018 - Statistical learning and probabilistic prediction .pdf;/Users/xzfang/Zotero/storage/K8Y3YKKH/nyas.html}
}

@article{pearce_unsupervised_2010,
  title = {Unsupervised Statistical Learning Underpins Computational, Behavioural, and Neural Manifestations of Musical Expectation},
  author = {Pearce, Marcus T. and Ruiz, Mar{\'i}a Herrojo and Kapasi, Selina and Wiggins, Geraint A. and Bhattacharya, Joydeep},
  year = {2010},
  month = mar,
  journal = {NeuroImage},
  volume = {50},
  number = {1},
  pages = {302--313},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2009.12.019},
  abstract = {The ability to anticipate forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations are critical to the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the conditional probability (and information content) of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation was found between the probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400\textendash 450~ms), (ii) beta band (14\textendash 30~Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity.},
  langid = {english}
}

@article{peirce_psychopy2_2019,
  title = {{{PsychoPy2}}: {{Experiments}} in Behavior Made Easy},
  shorttitle = {{{PsychoPy2}}},
  author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
  year = {2019},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {1},
  pages = {195--203},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01193-y},
  abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PCUJD9GR/Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf}
}

@article{peng_influence_2010,
  title = {The Influence of Language Experience on Categorical Perception of Pitch Contours},
  author = {Peng, Gang and Zheng, Hong-Ying and Gong, Tao and Yang, Ruo-Xiao and Kong, Jiang-Ping and Wang, William S. -Y.},
  year = {2010},
  month = oct,
  journal = {Journal of Phonetics},
  volume = {38},
  number = {4},
  pages = {616--624},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2010.09.003},
  abstract = {Previous research on categorical perception of pitch contours has mainly considered the contrast between tone language and non-tone language listeners. This study investigates not only the influence of tone language vs. non-tone language experience (German vs. Chinese), but also the influence of different tone inventories (Mandarin tones vs. Cantonese tones), on the categorical perception of pitch contours. The results show that the positions of the identification boundaries do not differ significantly across the 3 groups of listeners, i.e., Mandarin, Cantonese, and German, but that the boundary widths do differ significantly between tone language (Mandarin and Cantonese) listeners and non-tone language (German) listeners, with broader boundary widths for non-tone language listeners. In the discrimination tasks, the German listeners exhibit only psychophysical boundaries, whereas Chinese listeners exhibit linguistic boundaries, and these linguistic boundaries are further shaped by the different tone inventories.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SXB6AS69/Peng et al. - 2010 - The influence of language experience on categorica.pdf}
}

@article{perea_does_2003,
  title = {Does Jugde Activate {{COURT}}? {{Transposed-letter}} Similarity Effects in Masked Associative Priming},
  shorttitle = {Does Jugde Activate {{COURT}}?},
  author = {Perea, Manuel and Lupker, Stephen J.},
  year = {2003},
  month = sep,
  journal = {Memory \& Cognition},
  volume = {31},
  number = {6},
  pages = {829--841},
  issn = {0090-502X, 1532-5946},
  doi = {10.3758/BF03196438},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5PFPDDU5/Perea and Lupker - 2003 - Does jugde activate COURT Transposed-letter simil.pdf}
}

@article{perea_does_2018,
  title = {Does Visual Letter Similarity Modulate Masked Form Priming in Young Readers of {{Arabic}}?},
  author = {Perea, Manuel and Abu Mallouh, Reem and Mohammed, Ahmed and Khalifa, Batoul and Carreiras, Manuel},
  year = {2018},
  month = may,
  journal = {Journal of Experimental Child Psychology},
  volume = {169},
  pages = {110--117},
  issn = {00220965},
  doi = {10.1016/j.jecp.2017.12.004},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J5DNEMY7/Perea et al. - 2018 - Does visual letter similarity modulate masked form.pdf}
}

@article{perea_frequency_2005,
  title = {The Frequency Effect for Pseudowords in the Lexical Decision Task},
  author = {Perea, Manuel and Rosa, Eva and G{\'o}mez, Consolaci{\'o}n},
  year = {2005},
  month = feb,
  journal = {Perception \& Psychophysics},
  volume = {67},
  number = {2},
  pages = {301--314},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03206493},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9N6YRYG8/Perea et al. - 2005 - The frequency effect for pseudowords in the lexica.pdf}
}

@article{perea_lettercase_2015,
  title = {Letter-Case Information and the Identification of Brand Names},
  author = {Perea, Manuel and Jim{\'e}nez, Mar{\'i}a and Talero, Fernanda and L{\'o}pez-Ca{\~n}ada, Soraya},
  year = {2015},
  journal = {British Journal of Psychology},
  volume = {106},
  number = {1},
  pages = {162--173},
  issn = {2044-8295},
  doi = {10.1111/bjop.12071},
  abstract = {A central tenet of most current models of visual-word recognition is that lexical units are activated on the basis of case-invariant abstract letter representations. Here, we examined this assumption by using a unique type of words: brand names. The rationale of the experiments is that brand names are archetypically printed either in lowercase (e.g., adidas) or uppercase (e.g., IKEA). This allows us to present the brand names in their standard or non-standard case configuration (e.g., adidas, IKEA vs. ADIDAS, ikea, respectively). We conducted two experiments with a brand-decision task (`is it a brand name?'): a single-presentation experiment and a masked priming experiment. Results in the single-presentation experiment revealed faster identification times of brand names in their standard case configuration than in their non-standard case configuration (i.e., adidas faster than ADIDAS; IKEA faster than ikea). In the masked priming experiment, we found faster identification times of brand names when they were preceded by an identity prime that matched its standard case configuration than when it did not (i.e., faster response times to adidas-adidas than to ADIDAS-adidas). Taken together, the present findings strongly suggest that letter-case information forms part of a brand name's graphemic information, thus posing some limits to current models of visual-word recognition.},
  copyright = {\textcopyright{} 2014 The British Psychological Society},
  langid = {english},
  keywords = {familiarity,lexical access,word recognition},
  annotation = {\_eprint: https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/bjop.12071},
  file = {/Users/xzfang/Zotero/storage/DUCRGH99/Perea et al. - 2015 - Letter-case information and the identification of .pdf;/Users/xzfang/Zotero/storage/FZ6DUJ75/bjop.html}
}

@incollection{perea_neighborhood_2015,
  title = {Neighborhood {{Effects}} in {{Visual Word Recognition}} and {{Reading}}},
  booktitle = {The {{Oxford Handbook}} of {{Reading}}},
  author = {Perea, Manuel},
  editor = {Pollatsek, Alexander and Treiman, Rebecca},
  year = {2015},
  month = sep,
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199324576.013.7},
  abstract = {This chapter discusses research on how words that are orthographically (or phonologically) similar to a printed word influence the speed and accuracy of its encoding.The relevant set of words (the word's neighbors) was previously defined to be those lexical units differing from the target stimulus by a single letter in a given position. Recent evidence has revealed that a better definition of a word's neighborhood includes lexical units of different length and lexical units created by transpositions.The study of a word's neighborhood has revealed that that the activation of neighbors may interfere with the processing of the target words in word-identification tasks and during sentence reading, supporting the basic claims of interactive activation models. Some challenges to the current definitions of the sets of word neighborhoods are also examined, in particular the need to include differences between how consonants and vowels are encoded during word processing.},
  isbn = {978-0-19-932457-6},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8NJAMZZD/Pollatsek et al. - 2015 - Neighborhood Effects in Visual Word Recognition an.pdf}
}

@article{perea_r34d1ng_2008,
  title = {{{R34D1NG W0RD5 W1TH NUMB3R5}}.},
  author = {Perea, Manuel and Du{\~n}abeitia, Jon Andoni and Carreiras, Manuel},
  year = {2008},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {34},
  number = {1},
  pages = {237--241},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.34.1.237},
  abstract = {Letter identities and number identities are usually thought to imply different cortical mechanisms. Specifically, the left fusiform gyrus responds more to letters than to digits (T. A. Polk et al., 2002). However, a widely circulated statement on the internet illustrates that it is possible to use numbers (leet digits) as parts of words, 4ND TH3 R35ULT1NG S3NT3NC3 C4N B3 R34D W1TH0UT GR34T 3FF0RT. Two masked priming lexical decision experiments were conducted to determine whether leet digits produce (automatic) lexical activation. Results showed that words are identified substantially faster when they are preceded by a masked leet word (M4T3R14L\textendash MATERIAL) than when they are preceded by a control condition with other letters or digits. In addition, there was only a negligible advantage of the identity condition over the related leet condition. This leet-priming effect is not specific to numbers: A prime in which leet digits are replaced by letter-like symbols (M{$\varhexagonlrbonds$}T\texteuro R!{$\varhexagonlrbonds$}L\textendash MATERIAL) facilitates word processing to the same degree as an identity prime. Therefore, the cognitive system regularizes the shape of the leet digits and letter-like symbols embedded in words with very little cost.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PI2FLIRH/Perea et al. - 2008 - R34D1NG W0RD5 W1TH NUMB3R5..pdf}
}

@article{perea_resolving_2015,
  title = {Resolving the Locus of {{cAsE aLtErNaTiOn}} Effects in Visual Word Recognition: {{Evidence}} from Masked Priming},
  shorttitle = {Resolving the Locus of {{cAsE aLtErNaTiOn}} Effects in Visual Word Recognition},
  author = {Perea, Manuel and {Vergara-Mart{\'i}nez}, Marta and Gomez, Pablo},
  year = {2015},
  month = sep,
  journal = {Cognition},
  volume = {142},
  pages = {39--43},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.05.007},
  abstract = {Determining the factors that modulate the early access of abstract lexical representations is imperative for the formulation of a comprehensive neural account of visual-word identification. There is a current debate on whether the effects of case alternation (e.g., tRaIn vs. train) have an early or late locus in the word-processing stream. Here we report a lexical decision experiment using a technique that taps the early stages of visual-word recognition (i.e., masked priming). In the design, uppercase targets could be preceded by an identity/unrelated prime that could be in lowercase or alternating case (e.g., table-TABLE vs. crash-TABLE; tAbLe-TABLE vs. cRaSh-TABLE). Results revealed that the lowercase and alternating case primes were equally effective at producing an identity priming effect. This finding demonstrates that case alternation does not hinder the initial access to the abstract lexical representations during visual-word recognition.},
  langid = {english},
  keywords = {Case alternation,Lexical access,Masked priming},
  file = {/Users/xzfang/Zotero/storage/ILPPCSF7/Perea et al. - 2015 - Resolving the locus of cAsE aLtErNaTiOn effects in.pdf}
}

@article{perea_suppression_2011,
  title = {Suppression of Mirror Generalization for Reversible Letters: {{Evidence}} from Masked Priming},
  shorttitle = {Suppression of Mirror Generalization for Reversible Letters},
  author = {Perea, Manuel and {Moret-Tatay}, Carmen and Panadero, Victoria},
  year = {2011},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {65},
  number = {3},
  pages = {237--246},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2011.04.005},
  abstract = {Readers of the Roman script must ``unlearn'' some forms of mirror generalization when processing printed stimuli (i.e., herb and herd are different words). Here we examine whether the suppression of mirror generalization is a process that affects all letters or whether it mostly affects reversible letters (i.e., b/d). Three masked priming lexical decision experiments were conducted to examine how the cognitive system processes mirror images of reversible vs. non-reversible letters embedded in Spanish words. Repetition priming effects relative to the mirror-letter condition were substantially greater when the critical letter was reversible (e.g., idea-IDEA vs. ibea-IDEA) than when the critical letter was not reversible (e.g., arena-ARENA vs. aena-ARENA), Furthermore, responses to target words were substantially slower when they were preceded by prime containing a reversible mirror-letter (e.g., ibea-IDEA) than when preceded by a control prime (ilea-IDEA). This inhibitory effect did not occur when the mirror image of the critical letter does not form a grapheme (i.e., aena-ARENA vs. aena-ARENA). Thus, the cognitive system suppresses mirror images of reversible letters \textendash{} but not of non-reversible letters. We examine the implications of these findings for models of visual-word recognition.},
  langid = {english},
  keywords = {Masked priming,Mirror letters,Orthographic processing},
  file = {/Users/xzfang/Zotero/storage/IWATFDWI/Perea et al. - 2011 - Suppression of mirror generalization for reversibl.pdf;/Users/xzfang/Zotero/storage/HKU28U5G/S0749596X11000441.html}
}

@article{perea_transposedletter_2008,
  title = {Transposed-Letter Priming Effects for Close versus Distant Transpositions},
  author = {Perea, Manuel and Du{\~n}abeitia, Jon Andoni and Carreiras, Manuel},
  year = {2008},
  journal = {Experimental Psychology},
  volume = {55},
  number = {6},
  pages = {384--393},
  issn = {1618-3169},
  doi = {10.1027/1618-3169.55.6.384},
  abstract = {Transposing two internal letters of a word produces a perceptually similar item (e.g., CHOLOCATE being processed as CHOCOLATE). To determine the precise nature of the encoding of letter position within a word, we examined the effect of the number of intervening letters in transposed-letter effects with a masked priming procedure. In Experiment 1, letter transposition could involve adjacent letters (chocloate-CHOCOLATE) and nonadjacent letters with two intervening letters (choaolcte-CHOCOLATE). Results showed that the magnitude of the transposed-letter priming effect--relative to the appropriate control condition--was greater when the transposition involved adjacent letters than when it involved nonadjacent letters. In Experiment 2, we included a letter transposition condition using nonadjacent letters with one intervening letter (cholocate-CHOCOLATE). Results showed that the transposed-letter priming effect was of the same size for nonadjacent transpositions that involved one or two intervening letters. In addition, transposed-letter priming effects were smaller in the two nonadjacent conditions than in the adjacent condition. We examine the implications of these findings for models of visual-word recognition.},
  langid = {english},
  pmid = {19130764},
  keywords = {Distance Perception,Humans,Linguistics,Perceptual Masking,Verbal Behavior,Visual Perception,Vocabulary},
  file = {/Users/xzfang/Zotero/storage/TG83JP4Z/Perea et al. - 2008 - Transposed-letter priming effects for close versus.pdf}
}

@article{perea_where_2017,
  title = {Where Is the Locus of the Lowercase Advantage during Sentence Reading?},
  author = {Perea, Manuel and Rosa, Eva and Marcet, Ana},
  year = {2017},
  month = jun,
  journal = {Acta Psychologica},
  volume = {177},
  pages = {30--35},
  issn = {0001-6918},
  doi = {10.1016/j.actpsy.2017.04.007},
  abstract = {While most models of visual word identification and reading posit that a word's visual codes are rapidly transformed onto case-invariant representations (i.e., table and TABLE would equally activate the word unit corresponding to ``table''), a number of experiments have shown a lowercase advantage in various word identification and reading tasks. In the present experiment, we examined the locus of this lowercase advantage by comparing the pattern of eye movements when reading sentences in lowercase vs. uppercase. Each sentence contained a target word that was high or low in word-frequency. Overall, results showed faster reading times for lowercase than for uppercase sentences. More important, while the word-frequency effect was sizeable in the first-fixation durations on the target word, the lowercase advantage only arose in the gaze durations (i.e., the sum of durations of first-pass fixations on the target word, including refixations). Furthermore, we found an effect of word-frequency, but not of letter case, in the first-fixation duration on target words with multiple first-pass fixations. Taken together, these findings suggest that the lowercase advantage reflects operations that do not occur in the initial contact with the lexical entries.},
  langid = {english},
  keywords = {Eye movements,Letter case,Reading,Word-frequency},
  file = {/Users/xzfang/Zotero/storage/UQMZW773/Perea et al. - 2017 - Where is the locus of the lowercase advantage duri.pdf;/Users/xzfang/Zotero/storage/MW3SY92U/S0001691816302967.html}
}

@article{pereira_universal_2018,
  title = {Toward a Universal Decoder of Linguistic Meaning from Brain Activation},
  author = {Pereira, Francisco and Lou, Bin and Pritchett, Brianna and Ritter, Samuel and Gershman, Samuel J. and Kanwisher, Nancy and Botvinick, Matthew and Fedorenko, Evelina},
  year = {2018},
  month = dec,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {963},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-03068-4},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DMVRXF4K/Pereira et al. - 2018 - Toward a universal decoder of linguistic meaning f.pdf}
}

@article{peretz_congenital_2002,
  title = {Congenital {{Amusia}}: {{A Disorder}} of {{Fine-Grained Pitch Discrimination}}},
  shorttitle = {Congenital {{Amusia}}},
  author = {Peretz, Isabelle and Ayotte, Julie and Zatorre, Robert J. and Mehler, Jacques and Ahad, Pierre and Penhune, Virginia B. and Jutras, Beno{\^{\i}}t},
  year = {2002},
  month = jan,
  journal = {Neuron},
  volume = {33},
  number = {2},
  pages = {185--191},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(01)00580-3},
  abstract = {We report the first documented case of congenital amusia. This disorder refers to a musical disability that cannot be explained by prior brain lesion, hearing loss, cognitive deficits, socioaffective disturbance, or lack of environmental stimulation. This musical impairment is diagnosed in a middle-aged woman, hereafter referred to as Monica, who lacks most basic musical abilities, including melodic discrimination and recognition, despite normal audiometry and above-average intellectual, memory, and language skills. The results of psychophysical tests show that Monica has severe difficulties with detecting pitch changes. The data suggest that music-processing difficulties may result from problems in fine-grained discrimination of pitch, much in the same way as many language-processing difficulties arise from deficiencies in auditory temporal resolution.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/U5NU74CT/S0896627301005803.html}
}

@article{peretz_prevalence_2017,
  title = {Prevalence of Congenital Amusia},
  author = {Peretz, Isabelle and Vuvan, Dominique T.},
  year = {2017},
  month = may,
  journal = {European Journal of Human Genetics},
  volume = {25},
  number = {5},
  pages = {625--630},
  publisher = {{Nature Publishing Group}},
  issn = {1476-5438},
  doi = {10.1038/ejhg.2017.15},
  abstract = {Congenital amusia (commonly known as tone deafness) is a lifelong musical disorder that affects 4\% of the population according to a single estimate based on a single test from 1980. Here we present the first large-based measure of prevalence with a sample of 20\,000 participants, which does not rely on self-referral. On the basis of three objective tests and a questionnaire, we show that (a) the prevalence of congenital amusia is only 1.5\%, with slightly more females than males, unlike other developmental disorders where males often predominate; (b) self-disclosure is a reliable index of congenital amusia, which suggests that congenital amusia is hereditary, with 46\% first-degree relatives similarly affected; (c) the deficit is not attenuated by musical training and (d) it emerges in relative isolation from other cognitive disorder, except for spatial orientation problems. Hence, we suggest that congenital amusia is likely to result from genetic variations that affect musical abilities specifically.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature.},
  langid = {english},
  keywords = {Genetics of the nervous system,Human behaviour},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Genetics of the nervous system;Human behaviour Subject\_term\_id: genetics-of-the-nervous-system;human-behaviour},
  file = {/Users/xzfang/Zotero/storage/JXTE9IH2/Peretz and Vuvan - 2017 - Prevalence of congenital amusia.pdf;/Users/xzfang/Zotero/storage/5MSQFKQC/ejhg201715.html}
}

@article{peretz_varieties_2003,
  title = {Varieties of {{Musical Disorders}}},
  author = {Peretz, Isabelle and Champod, Anne Sophie and Hyde, Krista},
  year = {2003},
  month = nov,
  journal = {Annals of the New York Academy of Sciences},
  volume = {999},
  number = {1},
  pages = {58--75},
  issn = {00778923, 17496632},
  doi = {10.1196/annals.1284.006},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3VBH2B3T/Peretz et al. - 2003 - Varieties of Musical Disorders.pdf}
}

@article{perfors_learnability_2011,
  title = {The Learnability of Abstract Syntactic Principles},
  author = {Perfors, Amy and Tenenbaum, Joshua B. and Regier, Terry},
  year = {2011},
  month = mar,
  journal = {Cognition},
  volume = {118},
  number = {3},
  pages = {306--338},
  issn = {00100277},
  doi = {10.1016/j.cognition.2010.11.001},
  abstract = {Children acquiring language infer the correct form of syntactic constructions for which they appear to have little or no direct evidence, avoiding simple but incorrect generalizations that would be consistent with the data they receive. These generalizations must be guided by some inductive bias \textendash{} some abstract knowledge \textendash{} that leads them to prefer the correct hypotheses even in the absence of directly supporting evidence. What form do these inductive constraints take? It is often argued or assumed that they reflect innately specified knowledge of language. A classic example of such an argument moves from the phenomenon of auxiliary fronting in English interrogatives to the conclusion that children must innately know that syntactic rules are defined over hierarchical phrase structures rather than linear sequences of words (e.g., Chomsky, 1965, 1971, 1980; Crain \& Nakayama, 1987). Here we use a Bayesian framework for grammar induction to address a version of this argument and show that, given typical child-directed speech and certain innate domain-general capacities, an ideal learner could recognize the hierarchical phrase structure of language without having this knowledge innately specified as part of the language faculty. We discuss the implications of this analysis for accounts of human language acquisition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6D5ICWQG/Perfors et al. - 2011 - The learnability of abstract syntactic principles.pdf}
}

@article{perkins_eighteenmonthold_2021,
  title = {Eighteen-Month-Old Infants Represent Nonlocal Syntactic Dependencies},
  author = {Perkins, Laurel and Lidz, Jeffrey},
  year = {2021},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {41},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2026469118},
  abstract = {The human ability to produce and understand an indefinite number of sentences is driven by syntax, a cognitive system that can combine a finite number of primitive linguistic elements to build arbitrarily complex expressions. The expressive power of syntax comes in part from its ability to encode potentially unbounded dependencies over abstract structural configurations. How does such a system develop in human minds? We show that 18-mo-old infants are capable of representing abstract nonlocal dependencies, suggesting that a core property of syntax emerges early in development. Our test case is English wh-questions, in which a fronted wh-phrase can act as the argument of a verb at a distance (e.g., What did the chef burn?). Whereas prior work has focused on infants' interpretations of these questions, we introduce a test to probe their underlying syntactic representations, independent of meaning. We ask when infants know that an object wh-phrase and a local object of a verb cannot co-occur because they both express the same argument relation (e.g., *What did the chef burn the pizza). We find that 1) 18 mo olds demonstrate awareness of this complementary distribution pattern and thus represent the nonlocal grammatical dependency between the wh-phrase and the verb, but 2) younger infants do not. These results suggest that the second year of life is a period of active syntactic development, during which the computational capacities for representing nonlocal syntactic dependencies become evident.},
  chapter = {Social Sciences},
  copyright = {Copyright \textcopyright{} 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {34607945},
  keywords = {language acquisition,nonlocal dependencies,syntax,wh-questions},
  file = {/Users/xzfang/Zotero/storage/BPWDIW7J/Perkins and Lidz - 2021 - Eighteen-month-old infants represent nonlocal synt.pdf;/Users/xzfang/Zotero/storage/BAFDPX34/e2026469118.html}
}

@article{perrachione_dysfunction_2016,
  title = {Dysfunction of {{Rapid Neural Adaptation}} in {{Dyslexia}}},
  author = {Perrachione, Tyler K. and Del Tufo, Stephanie N. and Winter, Rebecca and Murtagh, Jack and Cyr, Abigail and Chang, Patricia and Halverson, Kelly and Ghosh, Satrajit S. and Christodoulou, Joanna A. and Gabrieli, John D.E.},
  year = {2016},
  month = dec,
  journal = {Neuron},
  volume = {92},
  number = {6},
  pages = {1383--1397},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.11.020},
  abstract = {Identification of specific neurophysiological dysfunctions resulting in selective reading difficulty (dyslexia) has remained elusive. In addition to impaired reading development, individuals with dyslexia frequently exhibit behavioral deficits in perceptual adaptation. Here, we assessed neurophysiological adaptation to stimulus repetition in adults and children with dyslexia for a wide variety of stimuli, spoken words, written words, visual objects, and faces. For every stimulus type, individuals with dyslexia exhibited significantly diminished neural adaptation compared to controls in stimulus-specific cortical areas. Better reading skills in adults and children with dyslexia were associated with greater repetition-induced neural adaptation. These results highlight a dysfunction of rapid neural adaptation as a core neurophysiological difference in dyslexia that may underlie impaired reading development. Reduced neurophysiological adaptation may relate to prior reports of reduced behavioral adaptation in dyslexia and may reveal a difference in brain functions that ultimately results in a specific reading impairment.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LCID2ZYB/Perrachione et al. - 2016 - Dysfunction of Rapid Neural Adaptation in Dyslexia.pdf}
}

@article{pesnotlerousseau_musical_2021,
  title = {Musical {{Expertise Is Associated}} with {{Improved Neural Statistical Learning}} in the {{Auditory Domain}}},
  author = {Pesnot Lerousseau, Jacques and Sch{\"o}n, Daniele},
  year = {2021},
  month = may,
  journal = {Cerebral Cortex},
  number = {bhab128},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab128},
  abstract = {It is poorly known whether musical training is associated with improvements in general cognitive abilities, such as statistical learning (SL). In standard SL paradigms, musicians have shown better performances than nonmusicians. However, this advantage could be due to differences in auditory discrimination, in memory or truly in the ability to learn sequence statistics. Unfortunately, these different hypotheses make similar predictions in terms of expected results. To dissociate them, we developed a Bayesian model and recorded electroencephalography (EEG). Our results confirm that musicians perform approximately\,15\% better than nonmusicians at predicting items in auditory sequences that embed either low or high-order statistics. These higher performances are explained in the model by parameters governing the learning of high-order statistics and the selection stage noise. EEG recordings reveal a neural underpinning of the musician's advantage: the P300 amplitude correlates with the surprise elicited by each item, and so, more strongly for musicians. Finally, early EEG components correlate with the surprise elicited by low-order statistics, as opposed to late EEG components that correlate with the surprise elicited by high-order statistics and this effect is stronger for musicians. Overall, our results demonstrate that musical expertise is associated with improved neural SL in the auditory domain.It is poorly known whether musical training leads to improvements in general cognitive skills. One fundamental cognitive ability, SL, is thought to be enhanced in musicians, but previous studies have reported mixed results. This is because such musician's advantage can embrace very different explanations, such as improvement in auditory discrimination or in memory. To solve this problem, we developed a Bayesian model and recorded EEG to dissociate these explanations. Our results reveal that musical expertise is truly associated with an improved ability to learn sequence statistics, especially high-order statistics. This advantage is reflected in the electroencephalographic recordings, where the P300 amplitude is more sensitive to surprising items in musicians than in nonmusicians.},
  file = {/Users/xzfang/Zotero/storage/2672EWSI/Pesnot Lerousseau and SchÃ¶n - 2021 - Musical Expertise Is Associated with Improved Neur.pdf;/Users/xzfang/Zotero/storage/TJJWH9QX/6278410.html}
}

@incollection{phillips_grammatical_2011,
  title = {5 {{Grammatical Illusions}} and {{Selective Fallibility}} in {{Real-Time Language Comprehension}}},
  booktitle = {Syntax and {{Semantics}}},
  author = {Phillips, Colin and Wagers, Matthew W. and Lau, Ellen F.},
  year = {2011},
  volume = {37},
  pages = {147--180},
  publisher = {{Emerald Group Publishing}},
  address = {{Bingley}},
  doi = {10.1108/S0092-4563(2011)0000037009},
  abstract = {Grammatical constraints impose diverse requirements on the relations between words and phrases in a sentence. Research on the online implementation of grammatical constraints reveals a strikingly uneven profile. The parser shows impressive accuracy in the application of some rather complex constraints, but makes many errors in the implementation of some relatively simple constraints. Just as the study of optical illusions has played an important role in the study of visual perception, the parser's highly selective vulnerability to interference and ``grammatical illusions'' provides a valuable tool for understanding how speakers encode and navigate complex linguistic representations in real time.},
  isbn = {978-1-78052-374-3},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4YQ34M6J/Phillips et al. - 2011 - 5 Grammatical Illusions and Selective Fallibility .pdf}
}

@article{piazza_humans_2013,
  title = {Humans {{Use Summary Statistics}} to {{Perceive Auditory Sequences}}},
  author = {Piazza, Elise A. and Sweeny, Timothy D. and Wessel, David and Silver, Michael A. and Whitney, David},
  year = {2013},
  month = aug,
  journal = {Psychological Science},
  volume = {24},
  number = {8},
  pages = {1389--1397},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797612473759},
  abstract = {In vision, humans use summary statistics (e.g., the average facial expression of a crowd) to efficiently perceive the gist of groups of features. Here, we present direct evidence that ensemble coding is also important for auditory processing. We found that listeners could accurately estimate the mean frequency of a set of logarithmically spaced pure tones presented in a temporal sequence (Experiment 1). Their performance was severely reduced when only a subset of tones from a given sequence was presented (Experiment 2), which demonstrates that ensemble coding is based on a substantial number of the tones in a sequence. This precise ensemble coding occurred despite very limited representation of individual tones from the sequence: Listeners were poor at identifying specific individual member tones (Experiment 3) and at determining their positions in the sequence (Experiment 4). Together, these results indicate that summary statistical coding is not limited to visual processing and is an important auditory mechanism for extracting ensemble frequency information from sequences of sounds.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FVFMA8GG/Piazza et al. - 2013 - Humans Use Summary Statistics to Perceive Auditory.pdf}
}

@article{piazza_infant_2020,
  title = {Infant and {{Adult Brains Are Coupled}} to the {{Dynamics}} of {{Natural Communication}}},
  author = {Piazza, Elise A. and Hasenfratz, Liat and Hasson, Uri and {Lew-Williams}, Casey},
  year = {2020},
  month = jan,
  journal = {Psychological Science},
  volume = {31},
  number = {1},
  pages = {6--17},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797619878698},
  abstract = {Infancy is the foundational period for learning from adults, and the dynamics of the social environment have long been considered central to children's development. Here, we reveal a novel, naturalistic approach for studying live interactions between infants and adults. Using functional near-infrared spectroscopy (fNIRS), we simultaneously and continuously measured the brains of infants (N = 18; 9\textendash 15 months of age) and an adult while they communicated and played with each other. We found that time-locked neural coupling within dyads was significantly greater when dyad members interacted with each other than with control individuals. In addition, we characterized the dynamic relationship between neural activation and the moment-to-moment fluctuations of mutual gaze, joint attention to objects, infant emotion, and adult speech prosody. This investigation advances what is currently known about how the brains and behaviors of infants both shape and reflect those of adults during real-life communication.},
  langid = {english},
  keywords = {communication,development,functional near-infrared spectroscopy,infancy,language,naturalistic,neural coupling,open data,open materials},
  file = {/Users/xzfang/Zotero/storage/LDK55ZKV/Piazza et al. - 2020 - Infant and Adult Brains Are Coupled to the Dynamic.pdf}
}

@article{piazza_rapid_2018,
  title = {Rapid {{Adaptation}} to the {{Timbre}} of {{Natural Sounds}}},
  author = {Piazza, Elise A. and Theunissen, Fr{\'e}d{\'e}ric E. and Wessel, David and Whitney, David},
  year = {2018},
  month = dec,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {13826},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-32018-9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4XLT8XRC/Piazza et al. - 2018 - Rapid Adaptation to the Timbre of Natural Sounds.pdf}
}

@article{pickering_people_2007,
  title = {Do People Use Language Production to Make Predictions during Comprehension?},
  author = {Pickering, Martin J. and Garrod, Simon},
  year = {2007},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {11},
  number = {3},
  pages = {105--110},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.12.002},
  abstract = {We present the case that language comprehension involves making simultaneous predictions at different linguistic levels and that these predictions are generated by the language production system. Recent research suggests that ease of comprehending predictable elements is due to prediction rather than facilitated integration, and that comprehension is accompanied by covert imitation. We argue that comprehenders use prediction and imitation to construct an `emulator', using the production system, and combine predictions with the input dynamically. Such a process helps to explain the rapidity of comprehension and the robust interpretation of ambiguous or noisy input. This framework is in line with a general trend in cognitive science to incorporate action systems into perceptual systems and has broad implications for understanding the links between language production and comprehension.},
  langid = {english}
}

@article{picton_human_1974,
  title = {Human Auditory Evoked Potentials. {{II}}: {{Effects}} of Attention},
  shorttitle = {Human Auditory Evoked Potentials. {{II}}},
  author = {Picton, T. W. and Hillyard, S. A.},
  year = {1974},
  month = jan,
  journal = {Electroencephalography and Clinical Neurophysiology},
  volume = {36},
  pages = {191--200},
  issn = {0013-4694},
  doi = {10.1016/0013-4694(74)90156-4},
  abstract = {Attention directed toward auditory stimuli, in order to detect an occasional fainter ``signal'' stimulus, caused a substantial increase in the N1 (83 msec) and P2 (161 msec) components of the auditory evoked potential without any change in preceding components. This evidence shows that human auditory attention is not mediated by a peripheral gating mechanism. The evoked response to the detected signal stimulus also contained a large P3 (450 msec) wave that was topographically distinct from the preceding components. This late positive wave could also be recorded in response to a detected omitted stimulus in a regular train and therefore seemed to index a stimulus-independent perceptual decision process. R\'esum\'e Le fait de diriger l'attention vers des stimuli auditifs, afin de d\'etecter un stimulus signal occasionnel plus faible, provoque une augmentation substantielle des composantes N1 (83 msec) et P2 (161 msec) du potentiel auditif \'evoqu\'e, sans aucune modification des composantes ant\'erieures. Cette donn\'ee montre que l'attention auditive chez l'homme n'est pas transmise par un m\'ecanisme d'ouverture p\'eriph\'erique. La r\'eponse \'evoqu\'ee au stimulus signal d\'etect\'e contient \'egalement une grande onde P3 (450 msec) qui est topographiquement distincte des composantes pr\'ec\'edentes. Cette onde positive tardive peut \'egalement \^etre enregistr\'ee en r\'eponse \`a l'omission d'un stimulus, d\'etect\'ee \`a l'int\'erieur d'un train r\'egulier, et semble ainsi constituer le t\'emoin d'un processus de d\'ecision perceptuelle ind\'ependant du stimulus.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RHMSGQ5P/Picton and Hillyard - 1974 - Human auditory evoked potentials. II Effects of a.pdf;/Users/xzfang/Zotero/storage/CEZY8BAV/0013469474901564.html}
}

@inproceedings{pierrehumbert_2002_,
  title = {2002. {{Word-specific}} Phonetics},
  booktitle = {Laboratory Phonology {{VII}}, 101\textendash 140. {{Berlin}}: {{Mouton}} de {{Gruyter}}},
  author = {Pierrehumbert, Janet B.},
  abstract = {A long-standing forte of the Laboratory Phonology series has been work on phonetic implementation of phonological representations. Numerous studies in this series have elucidated the patterns of variation in the realization of phonological categories in different segmental and prosodic contexts, and such studies now provide one of the main lines of evidence about the cognitive representation},
  file = {/Users/xzfang/Zotero/storage/VFBI4W8F/Pierrehumbert - 2002. Word-specific phonetics.pdf;/Users/xzfang/Zotero/storage/4VP2SY3N/summary.html}
}

@article{pierrehumbert_phonetic_,
  title = {The {{Phonetic Grounding}} of {{Phonology}}},
  author = {Pierrehumbert, Janet B},
  pages = {25},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/AFU38LQJ/Pierrehumbert - The Phonetic Grounding of Phonology.pdf}
}

@article{pierrehumbert_phonetic_2003,
  title = {Phonetic Diversity, Statistical Learning, and Acquisition of Phonology},
  author = {Pierrehumbert, Janet B.},
  year = {2003},
  journal = {Language and Speech},
  volume = {46},
  number = {Pt 2-3},
  pages = {115--154},
  issn = {0023-8309},
  doi = {10.1177/00238309030460020501},
  abstract = {In learning to perceive and produce speech, children master complex language-specific patterns. Daunting language-specific variation is found both in the segmental domain and in the domain of prosody and intonation. This article reviews the challenges posed by results in phonetic typology and sociolinguistics for the theory of language acquisition. It argues that categories are initiated bottom-up from statistical modes in use of the phonetic space, and sketches how exemplar theory can be used to model the updating of categories once they are initiated. It also argues that bottom-up initiation of categories is successful thanks to the perception-production loop operating in the speech community. The behavior of this loop means that the superficial statistical properties of speech available to the infant indirectly reflect the contrastiveness and discriminability of categories in the adult grammar. The article also argues that the developing system is refined using internal feedback from type statistics over the lexicon, once the lexicon is well-developed. The application of type statistics to a system initiated with surface statistics does not cause a fundamental reorganization of the system. Instead, it exploits confluences across levels of representation which characterize human language and make bootstrapping possible.},
  langid = {english},
  pmid = {14748442},
  keywords = {Child; Preschool,Cognition,Feedback; Psychological,Humans,Infant,Phonation,Psycholinguistics,Psychometrics,Speech,Speech Perception},
  file = {/Users/xzfang/Zotero/storage/NDVLKRRX/Pierrehumbert - 2003 - Phonetic diversity, statistical learning, and acqu.pdf}
}

@article{pierrehumbert_phonetic_2003a,
  title = {Phonetic {{Diversity}}, {{Statistical Learning}}, and {{Acquisition}} of {{Phonology}}},
  author = {Pierrehumbert, Janet B.},
  year = {2003},
  month = jun,
  journal = {Language and Speech},
  volume = {46},
  number = {2-3},
  pages = {115--154},
  publisher = {{SAGE Publications Ltd}},
  issn = {0023-8309},
  doi = {10.1177/00238309030460020501},
  abstract = {In learning to perceive and produce speech, children master complex language-specific patterns. Daunting language-specific variation is found both in the segmental domain and in the domain of prosody and intonation. This article reviews the challenges posed by results in phonetic typology and sociolinguistics for the theory of language acquisition. It argues that categories are initiated bottom-up from statistical modesin use of the phonetic space, and sketches how exemplar theory can be used to model the updating of categories once they are initiated. It also argues that bottom-upinitiation of categories is successful thanks to the perceptionproduction loop operating in the speech community. The behavior of this loop means that the superficial statistical properties of speech available to the infant indirectly reflect the contrastiveness and discriminability of categories in the adult grammar. The article also argues that the developing system is refined using internal feedback from type statistics over the lexicon, once the lexicon is well-developed. The application of type statistics to a system initiated with surface statistics does not cause a fundamental reorganization of the system. Instead, it exploits confluences across levels of representation which characterize human language and make bootstrapping possible.},
  langid = {english},
  keywords = {categorization,exemplars,phonotactics,prosody,statistical learning},
  file = {/Users/xzfang/Zotero/storage/HVE69IAH/Pierrehumbert - 2003 - Phonetic Diversity, Statistical Learning, and Acqu.pdf}
}

@article{pierrehumbert_phonological_2016,
  title = {Phonological {{Representation}}: {{Beyond Abstract Versus Episodic}}},
  shorttitle = {Phonological {{Representation}}},
  author = {Pierrehumbert, Janet B.},
  year = {2016},
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {2},
  number = {1},
  pages = {33--52},
  publisher = {{Annual Reviews}},
  issn = {2333-9683},
  doi = {10.1146/annurev-linguistics-030514-125050},
  abstract = {Phonological representations capture information about individual word forms and about the general characteristics of word forms in a language. To support the processing of novel word forms as well as familiar word forms in novel contexts, an abstract level of representation is needed in which many phonetic details and contextual features are disregarded. At the same time, evidence has accumulated that such details are retained in memory and used in processing individual words and indexical features of language. Taken together, these results mean that a hybrid model of phonological representation is needed. The abstract level supports generalizations based on lexical type statistics and fast adaptation to communicative requirements through the reuse of existing categories. A richly detailed level of representation is implicated in word-specific phonetic patterns, the detailed dynamics of regular sound changes, and active associations of phonetic patterns with gender, age, and dialect.},
  file = {/Users/xzfang/Zotero/storage/MQ9DEPLJ/Pierrehumbert - 2016 - Phonological Representation Beyond Abstract Versu.pdf;/Users/xzfang/Zotero/storage/T8NWWLY5/annurev-linguistics-030514-125050.html}
}

@article{pierrehumbert_phonological_2016a,
  title = {Phonological {{Representation}}: {{Beyond Abstract Versus Episodic}}},
  shorttitle = {Phonological {{Representation}}},
  author = {Pierrehumbert, Janet B.},
  year = {2016},
  journal = {Annual Review of Linguistics},
  volume = {2},
  number = {1},
  pages = {33--52},
  doi = {10.1146/annurev-linguistics-030514-125050},
  abstract = {Phonological representations capture information about individual word forms and about the general characteristics of word forms in a language. To support the processing of novel word forms as well as familiar word forms in novel contexts, an abstract level of representation is needed in which many phonetic details and contextual features are disregarded. At the same time, evidence has accumulated that such details are retained in memory and used in processing individual words and indexical features of language. Taken together, these results mean that a hybrid model of phonological representation is needed. The abstract level supports generalizations based on lexical type statistics and fast adaptation to communicative requirements through the reuse of existing categories. A richly detailed level of representation is implicated in word-specific phonetic patterns, the detailed dynamics of regular sound changes, and active associations of phonetic patterns with gender, age, and dialect.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-linguistics-030514-125050},
  file = {/Users/xzfang/Zotero/storage/H8MEC2ZM/Pierrehumbert - 2016 - Phonological Representation Beyond Abstract Versu.pdf}
}

@book{pierrehumbert_wordspecific_2008,
  title = {Word-Specific Phonetics},
  author = {Pierrehumbert, Janet B.},
  year = {2008},
  month = aug,
  journal = {Laboratory Phonology 7},
  pages = {101--140},
  publisher = {{De Gruyter Mouton}},
  abstract = {Word-specific phonetics was published in Laboratory Phonology 7 on page 101.},
  chapter = {Laboratory Phonology 7},
  isbn = {978-3-11-019710-5},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UI9TW77T/html.html}
}

@article{pimentel_how_2021,
  title = {How ({{Non-}}){{Optimal}} Is the {{Lexicon}}?},
  author = {Pimentel, Tiago and Nikkarinen, Irene and Mahowald, Kyle and Cotterell, Ryan and Blasi, Dami{\'a}n},
  year = {2021},
  month = apr,
  abstract = {The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf's law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world's languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon's optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes -- as measured by code length.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/23EQ4G39/Pimentel et al. - 2021 - How (Non-)Optimal is the Lexicon.pdf;/Users/xzfang/Zotero/storage/97NRRKFA/2104.html}
}

@article{pinet_electrophysiological_2020,
  title = {Electrophysiological {{Correlates}} of {{Monitoring}} in {{Typing}} with and without {{Visual Feedback}}},
  author = {Pinet, Svetlana and Nozari, Nazbanou},
  year = {2020},
  month = apr,
  journal = {Journal of Cognitive Neuroscience},
  volume = {32},
  number = {4},
  pages = {603--620},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01500},
  abstract = {New theories of monitoring in language production, regardless of their mechanistic differences, all posit monitoring mechanisms that share general computational principles with action monitoring. This perspective, if accurate, would predict that many electrophysiological signatures of performance monitoring should be recoverable from language production tasks. In this study, we examined both error-related and feedback-related EEG indices of performance monitoring in the context of a typing-to-dictation task. To disentangle the contribution of the external from internal monitoring processes, we created a condition where participants immediately saw the word they typed (the immediate-feedback condition) versus one in which displaying the word was delayed until the end of the trial (the delayed-feedback condition). The removal of immediate visual feedback prompted a stronger reliance on internal monitoring processes, which resulted in lower correction rates and a clear error-related negativity. Compatible with domain-general monitoring views, an error positivity was only recovered under conditions where errors were detected or had a high likelihood of being detected. Examination of the feedback-related indices (feedback-related negativity and frontocentral positivity) revealed a two-stage process of integration of internal and external information. The recovery of a full range of well-established EEG indices of action monitoring in a language production task strongly endorses domain-general views of monitoring. Such indices, in turn, are helpful in understanding how information from different monitoring channels are combined.},
  file = {/Users/xzfang/Zotero/storage/JDUFPLM2/Pinet and Nozari - 2020 - Electrophysiological Correlates of Monitoring in T.pdf;/Users/xzfang/Zotero/storage/DWM7IAGN/Electrophysiological-Correlates-of-Monitoring-in.html}
}

@article{pinet_twisting_2018,
  title = {``{{Twisting}} Fingers'': {{The}} Case for Interactivity in Typed Language Production},
  shorttitle = {``{{Twisting}} Fingers''},
  author = {Pinet, Svetlana and Nozari, Nazbanou},
  year = {2018},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {4},
  pages = {1449--1457},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-018-1452-7},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SKC6VAZ5/Pinet and Nozari - 2018 - â€œTwisting fingersâ€ The case for interactivity in .pdf}
}

@article{pinet_using_,
  title = {Using {{Signal Detection Theory}} to {{Investigate}} the {{Role}} of {{Visual Information}} in {{Performance Monitoring}} in {{Typing}}},
  author = {Pinet, Svetlana and Nozari, Nazbanou},
  pages = {6},
  abstract = {This paper uses the signal detection theory (SDT) to investigate the contribution of visual information to two monitoring-dependent functions, metacognitive awareness of errors and error corrections. Data from two experiments show that complete removal of visual outcome results in a mild decrease in error awareness and a much more significant decrease in correction rates. Partially restoring visual information by including positional information (as in masked password typing) causes a modest but statistically significant improvement in correction performance. Interestingly, participants treat the change to the quality of information differently across the tasks, with more conservative behavior (avoiding false alarms) in the correction task. These findings show the SDT's ability to quantify, in a graded manner, the contribution of specific types of information to monitoring in complex tasks, while also providing additional information about how participants handle the change to the quality of information in a task-dependent manner.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/G6RF9F8K/Pinet and Nozari - Using Signal Detection Theory to Investigate the R.pdf}
}

@article{pinheiro-chagas_decoding_2019,
  title = {Decoding the Processing Stages of Mental Arithmetic with Magnetoencephalography},
  author = {{Pinheiro-Chagas}, Pedro and Piazza, Manuela and Dehaene, Stanislas},
  year = {2019},
  month = may,
  journal = {Cortex},
  series = {Architecture of Mathematical Cognition},
  volume = {114},
  pages = {124--139},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.07.018},
  abstract = {Elementary arithmetic is highly prevalent in our daily lives. However, despite decades of research, we are only beginning to understand how the brain solves simple calculations. Here, we applied machine learning techniques to magnetoencephalography (MEG) signals in an effort to decompose the successive processing stages and mental transformations underlying elementary arithmetic. Adults subjects verified single-digit addition and subtraction problems such as 3~+~2~=~9 in which each successive symbol was presented sequentially. MEG signals revealed a cascade of partially overlapping brain states. While the first operand could be transiently decoded above chance level, primarily based on its visual properties, the decoding of the second operand was more accurate and lasted longer. Representational similarity analyses suggested that this decoding rested on both visual and magnitude codes. We were also able to decode the operation type (additions vs. subtraction) during practically the entire trial after the presentation of the operation sign. At the decision stage, MEG indicated a fast and highly overlapping temporal dynamics for (1) identifying the proposed result, (2) judging whether it was correct or incorrect, and (3) pressing the response button. Surprisingly, however, the internally computed result could not be decoded. Our results provide a first comprehensive picture of the unfolding processing stages underlying arithmetic calculations at a single-trial level, and suggest that externally and internally generated neural codes may have different neural substrates.},
  langid = {english},
  keywords = {Decoding,Magnetoencephalography,Mental arithmetic,Representational similarity analysis},
  file = {/Users/xzfang/Zotero/storage/M9JJTKN5/Pinheiro-Chagas et al. - 2019 - Decoding the processing stages of mental arithmeti.pdf}
}

@article{pion-tonachini_iclabel_2019,
  title = {{{ICLabel}}: {{An}} Automated Electroencephalographic Independent Component Classifier, Dataset, and Website},
  shorttitle = {{{ICLabel}}},
  author = {{Pion-Tonachini}, Luca and {Kreutz-Delgado}, Ken and Makeig, Scott},
  year = {2019},
  month = sep,
  journal = {NeuroImage},
  volume = {198},
  pages = {181--197},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.05.026},
  abstract = {The electroencephalogram (EEG) provides a non-invasive, minimally restrictive, and relatively low-cost measure of mesoscale brain dynamics with high temporal resolution. Although signals recorded in parallel by multiple, near-adjacent EEG scalp electrode channels are highly-correlated and combine signals from many different sources, biological and non-biological, independent component analysis (ICA) has been shown to isolate the various source generator processes underlying those recordings. Independent components (IC) found by ICA decomposition can be manually inspected, selected, and interpreted, but doing so requires both time and practice as ICs have no order or intrinsic interpretations and therefore require further study of their properties. Alternatively, sufficiently-accurate automated IC classifiers can be used to classify ICs into broad source categories, speeding the analysis of EEG studies with many subjects and enabling the use of ICA decomposition in near-real-time applications. While many such classifiers have been proposed recently, this work presents the ICLabel project comprised of (1) the ICLabel dataset containing spatiotemporal measures for over 200,000 ICs from more than 6,000 EEG recordings and matching component labels for over 6,000 of those ICs, all using common average reference, (2) the ICLabel website for collecting crowdsourced IC labels and educating EEG researchers and practitioners about IC interpretation, and (3) the automated ICLabel classifier, freely available for MATLAB. The ICLabel classifier improves upon existing methods in two ways: by improving the accuracy of the computed label estimates and by enhancing its computational efficiency. The classifier outperforms or performs comparably to the previous best publicly available automated IC component classification method for all measured IC categories while computing those labels ten times faster than that classifier as shown by a systematic comparison against other publicly available EEG IC classifiers.},
  pmcid = {PMC6592775},
  pmid = {31103785},
  file = {/Users/xzfang/Zotero/storage/PKQ8RKRC/Pion-Tonachini et al. - 2019 - ICLabel An automated electroencephalographic inde.pdf}
}

@article{pluhacek_number_2021,
  title = {Number of Flankers Influences Foveal Crowding and Contour Interaction Differently},
  author = {Pluh{\'a}{\v c}ek, Franti{\v s}ek and Musilov{\'a}, Lenka and Bedell, Harold E. and Siderov, John},
  year = {2021},
  month = feb,
  journal = {Vision Research},
  volume = {179},
  pages = {9--18},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2020.11.002},
  abstract = {Nearby flanking objects degrade visual resolution. If the flankers are similar to the acuity target, this influence is called crowding (CW), whereas if the flanking stimuli are simple bars then the phenomenon is known as contour interaction (CI). The aim of this study was to compare the influence of the number and position of flankers on foveal CW and CI to investigate possible differences in mechanism of these two effects. Five normal observers viewed single, foveally presented Sloan letters surrounded by 1, 2 or 4 flankers (either a Sloan letter or one-stroke-width bars), presented at several edge-to-edge separations. Single flankers were presented in the right, left, top or bottom position, 2 flankers were placed equally to the right and left or top and bottom of the central target, and 4 flankers were equally spaced in all four directions. Percent correct letter identification was determined for each type, number, position and separation of flankers and confusion matrices were constructed for separations equal to 20\% and 100\% letter width. Increasing the number of flankers caused an increase in the magnitude of both phenomena. CW showed a greater magnitude than CI for higher numbers of flankers. Analysis of confusion matrices suggests that in addition to the edge-to-edge interaction that appears to mediate CI, letter substitution and feature pooling contribute significantly to CW when higher numbers of flankers are presented. Foveal CW is more strongly influenced by an increase in the number of flankers than CI, which can be explained by the presence of additional interaction effects.},
  langid = {english},
  keywords = {Confusion matrices,Contour interaction,Crowding,Fovea,Number of flankers,Position of flankers},
  file = {/Users/xzfang/Zotero/storage/DTR2LCHR/PluhÃ¡Äek et al. - 2021 - Number of flankers influences foveal crowding and .pdf;/Users/xzfang/Zotero/storage/I9QXFCP4/S0042698920301784.html}
}

@article{poder_effect_2007,
  title = {Effect of Colour Pop-out on the Recognition of Letters in Crowding Conditions},
  author = {P{\~o}der, Endel},
  year = {2007},
  month = nov,
  journal = {Psychological Research},
  volume = {71},
  number = {6},
  pages = {641--645},
  issn = {1430-2772},
  doi = {10.1007/s00426-006-0053-7},
  abstract = {The crowding effect of adjacent objects on the recognition of a target can be reduced when target and flankers differ in some feature, that is irrelevant to the recognition task. In this study, the mechanisms of this effect were explored using targets and flankers of the same and different colours. It was found that facilitation nearly equal to that of differently coloured targets and flankers can be observed with a differently coloured background blob in the location of the target. The different-colour effect does not require advance knowledge of the target and flanker colours, but the effect increases in the course of three trials with constant mapping of colours. The results are consistent with the notion of exogenous attention that facilitates the processing at the most salient locations in the visual field.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/K3M9LZMJ/PÃµder - 2007 - Effect of colour pop-out on the recognition of let.pdf}
}

@article{pogue_talkerspecific_2016,
  title = {Talker-{{Specific Generalization}} of {{Pragmatic Inferences}} Based on {{Under-}} and {{Over-Informative Prenominal Adjective Use}}},
  author = {Pogue, Amanda and Kurumada, Chigusa and Tanenhaus, Michael K.},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.02035},
  abstract = {According to Grice's (1975) Maxim of Quantity, rational talkers formulate their utterances to be as economical as possible while conveying all necessary information. Naturally produced referential expressions, however, often contain more or less information than what is predicted to be optimal given a rational speaker model. How do listeners cope with these variations in the linguistic input? We argue that listeners navigate the variability in referential resolution by calibrating their expectations for the amount of linguistic signal to be expended for a certain meaning and by doing so in a context- or a talker-specific manner. Focusing on talker-specificity, we present four experiments. We first establish that speakers will generalize information from a single pair of adjectives to unseen adjectives in a speaker-specific manner (Experiment 1). Initially focusing on exposure to underspecified utterances, Experiment 2 examines: a) the dimension of generalization; b) effects of the strength of the evidence (implicit or explicit); and c) individual differences in dimensions of generalization. Experiments 3 and 4 ask parallel questions for exposure to over-specified utterances, where we predict more conservative generalization because, in spontaneous utterances, talkers are more likely to over-modify than under-modify.},
  langid = {english},
  keywords = {adaptation,generalization,Informativity,pragmatics,Referential expressions,sentence processing},
  file = {/Users/xzfang/Zotero/storage/DI4G2Z84/Pogue et al. - 2016 - Talker-Specific Generalization of Pragmatic Infere.pdf}
}

@article{poldrack_can_2006,
  title = {Can Cognitive Processes Be Inferred from Neuroimaging Data?},
  author = {Poldrack, Russell A.},
  year = {2006},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {2},
  pages = {59--63},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2005.12.004},
  abstract = {There is much interest currently in using functional neuroimaging techniques to understand better the nature of cognition. One particular practice that has become common is `reverse inference', by which the engagement of a particular cognitive process is inferred from the activation of a particular brain region. Such inferences are not deductively valid, but can still provide some information. Using a Bayesian analysis of the BrainMap neuroimaging database, I characterize the amount of additional evidence in favor of the engagement of a cognitive process that can be offered by a reverse inference. Its usefulness is particularly limited by the selectivity of activation in the region of interest. I argue that cognitive neuroscientists should be circumspect in the use of reverse inference, particularly when selectivity of the region in question cannot be established or is known to be weak.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/89IX446A/Poldrack - 2006 - Can cognitive processes be inferred from neuroimag.pdf;/Users/xzfang/Zotero/storage/8ZCH5ELK/S1364661305003360.html}
}

@article{politzer-ahles_ganong_2020,
  title = {Ganong Effects for Frequency May Not Be Robust},
  author = {{Politzer-Ahles}, Stephen and Lee, Ka Keung and Shen, Lue},
  year = {2020},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {147},
  number = {1},
  pages = {EL37-EL42},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/10.0000562},
  abstract = {The Ganong effect\textemdash more identifications of a certain phoneme in a context where that phoneme would yield a real word than a context where that phoneme would yield a pseudoword\textemdash has been widely replicated. Few studies, however, have tested whether this effect occurs for frequency contrasts. In the present study, participants' likelihood of identifying an ambiguous sound as aspirated was tested in acoustically identical continua in contexts where the identification of the sound as aspirated would either yield a lower- or higher-frequency word than the identification of the sound as unaspirated would. No frequency-based Ganong effect was found.},
  file = {/Users/xzfang/Zotero/storage/4U9N4X6Y/Politzer-Ahles et al. - 2020 - Ganong effects for frequency may not be robust.pdf;/Users/xzfang/Zotero/storage/7RCDYISC/10.html}
}

@article{ponce_evolving_2019,
  title = {Evolving {{Images}} for {{Visual Neurons Using}} a {{Deep Generative Network Reveals Coding Principles}} and {{Neuronal Preferences}}},
  author = {Ponce, Carlos R. and Xiao, Will and Schade, Peter F. and Hartmann, Till S. and Kreiman, Gabriel and Livingstone, Margaret S.},
  year = {2019},
  month = may,
  journal = {Cell},
  volume = {177},
  number = {4},
  pages = {999-1009.e10},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2019.04.005},
  abstract = {What specific features should visual neurons encode, given the infinity of real-world images and the limited number of neurons available to represent them? We investigated neuronal selectivity in monkey inferotemporal cortex via the vast hypothesis space of a generative deep neural network, avoiding assumptions about features or semantic categories. A genetic algorithm searched this space for stimuli that maximized neuronal firing. This led to the evolution of rich synthetic images of objects with complex combinations of shapes, colors, and textures, sometimes resembling animals or familiar people, other times revealing novel patterns that did not map to any clear~semantic category. These results expand our conception of the dictionary of features encoded in the cortex, and the approach can potentially reveal the internal representations of any system whose input can be captured by a generative model.},
  langid = {english},
  keywords = {generative adversarial network,inferotemporal cortex,neural networks},
  file = {/Users/xzfang/Zotero/storage/IX8MUVQG/Ponce et al. - 2019 - Evolving Images for Visual Neurons Using a Deep Ge.pdf}
}

@article{popham_visual_2021,
  title = {Visual and Linguistic Semantic Representations Are Aligned at the Border of Human Visual Cortex},
  author = {Popham, Sara F. and Huth, Alexander G. and Bilenko, Natalia Y. and Deniz, Fatma and Gao, James S. and {Nunez-Elizalde}, Anwar O. and Gallant, Jack L.},
  year = {2021},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {24},
  number = {11},
  pages = {1628--1636},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00921-6},
  abstract = {Semantic information in the human brain is organized into multiple networks, but the fine-grain relationships between them are poorly understood. In this study, we compared semantic maps obtained from two functional magnetic resonance imaging experiments in the same participants: one that used silent movies as stimuli and another that used narrative stories. Movies evoked activity from a network of modality-specific, semantically selective areas in visual cortex. Stories evoked activity from another network of semantically selective areas immediately anterior to visual cortex. Remarkably, the pattern of semantic selectivity in these two distinct networks corresponded along the boundary of visual cortex: for visual categories represented posterior to the boundary, the same categories were represented linguistically on the anterior side. These results suggest that these two networks are smoothly joined to form one contiguous map.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Language,Neural encoding,Perception,Visual system},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Language;Neural encoding;Perception;Visual system Subject\_term\_id: language;neural-encoding;perception;visual-system},
  file = {/Users/xzfang/Zotero/storage/RKECE9IU/Popham et al. - 2021 - Visual and linguistic semantic representations are.pdf;/Users/xzfang/Zotero/storage/8W6JNHW2/s41593-021-00921-6.html}
}

@article{poppels_structuresensitive_,
  title = {Structure-Sensitive {{Noise Inference}}: {{Comprehenders Expect Exchange Errors}}},
  author = {Poppels, Till and Levy, Roger P},
  pages = {6},
  abstract = {Previous research has found that comprehenders are willing to adopt non-literal interpretations of sentences whose literal reading is unlikely. Several studies found evidence that comprehenders decide whether or not a given utterance should be taken at face value in accordance with principles of Bayesian rationality, by weighing the prior probability of potential interpretations against the degree to which they are (in)consistent with the literal form of the utterance. While all of these results are consistent with string-edit noise models, many error processes are known to be sensitive to the underlying linguistic structure of the intended utterance. Here, we explore the case of exchange errors and provide experimental evidence that comprehenders' noise model is structure-sensitive. Our results add further support to the noisy-channel theory of language comprehension, extend the set of known noise operations to include positional exchanges, and show that comprehenders' noise models are well-adapted to structure-sensitive sources of signal corruption during communication.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/PRYV39VK/Poppels and Levy - Structure-sensitive Noise Inference Comprehenders.pdf}
}

@article{posner_decay_1967,
  title = {Decay of {{Visual Information}} from a {{Single Letter}}},
  author = {Posner, Michael I. and Keele, Steven W.},
  year = {1967},
  month = oct,
  journal = {Science},
  volume = {158},
  number = {3797},
  pages = {137--139},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.158.3797.137},
  abstract = {If the trace of a letter can be matched more rapidly with a physically identical letter (as in the pair AA) than it can be with a letter having only the same name (as in the pair Aa), then the trace must preserve the visual aspect of the letter. The visual information from a single letter decays in about 1.5 seconds if the task provides little incentive for preservation.},
  chapter = {Reports},
  copyright = {\textcopyright{} 1967},
  langid = {english},
  pmid = {6054814},
  file = {/Users/xzfang/Zotero/storage/BAS5FL7B/137.html}
}

@article{pouw_timing_2022,
  title = {Timing in Conversation Is Dynamically Adjusted Turn by Turn in Dyadic Telephone Conversations},
  author = {Pouw, Wim and Holler, Judith},
  year = {2022},
  month = may,
  journal = {Cognition},
  volume = {222},
  pages = {105015},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105015},
  abstract = {Conversational turn taking in humans involves incredibly rapid responding. The timing mechanisms underpinning such responses have been heavily debated, including questions such as who is doing the timing. Similar to findings on rhythmic tapping to a metronome, we show that floor transfer offsets (FTOs) in telephone conversations are serially dependent, such that FTOs are lag-1 negatively autocorrelated. Finding this serial dependence on a turn-by-turn basis (lag-1) rather than on the basis of two or more turns, suggests a counter-adjustment mechanism operating at the level of the dyad in FTOs during telephone conversations, rather than a more individualistic self-adjustment within speakers. This finding, if replicated, has major implications for models describing turn taking, and confirms the joint, dyadic nature of human conversational dynamics. Future research is needed to see how pervasive serial dependencies in FTOs are, such as for example in richer communicative face-to-face contexts where visual signals affect conversational timing.},
  langid = {english},
  keywords = {Conversation,Floor transfer,Negative Lag-1 autocorrelation,Rhythm,Timing,Turn taking},
  file = {/Users/xzfang/Zotero/storage/LRQVEEL9/Pouw and Holler - 2022 - Timing in conversation is dynamically adjusted tur.pdf;/Users/xzfang/Zotero/storage/94YIWW2H/S0010027722000038.html}
}

@article{powell_social_2018,
  title = {Social {{Origins}} of {{Cortical Face Areas}}},
  author = {Powell, Lindsey J. and Kosakowski, Heather L. and Saxe, Rebecca},
  year = {2018},
  month = sep,
  journal = {Trends in cognitive sciences},
  volume = {22},
  number = {9},
  pages = {752--763},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2018.06.009},
  abstract = {Recently acquired fMRI data from human and macaque infants provide novel insight into the origins of cortical networks specialized for perceiving faces. Data from both species converge: cortical regions responding preferentially to faces are present and spatially organized early in infancy, though fully selective face areas emerge much later. What explains the earliest cortical responses to faces? We review two proposed mechanisms: proto-organization for simple shapes in visual cortex, and an innate subcortical schematic face template. We propose, in addition, a third mechanism. Infants choose to look at faces in order to engage in positively-valenced, contingent social interactions. Activity in medial prefrontal cortex during social interactions may, directly or indirectly, guide the organization of cortical face areas.},
  pmcid = {PMC6098735},
  pmid = {30041864},
  file = {/Users/xzfang/Zotero/storage/36CA2RMD/Powell et al. - 2018 - Social Origins of Cortical Face Areas.pdf}
}

@article{powell_using_2018,
  title = {Using Individual Functional Channels of Interest to Study Cortical Development with {{fNIRS}}},
  author = {Powell, Lindsey J. and Deen, Ben and Saxe, Rebecca},
  year = {2018},
  journal = {Developmental Science},
  volume = {21},
  number = {4},
  pages = {e12595},
  issn = {1467-7687},
  doi = {10.1111/desc.12595},
  abstract = {Functional near-infrared spectroscopy (fNIRS) is a noninvasive neuroimaging technique that could be uniquely effective for investigating cortical function in human infants. However, prior efforts have been hampered by the difficulty of aligning arrays of fNIRS optodes placed on the scalp to anatomical or functional regions of underlying cortex. This challenge can be addressed by identifying channels of interest in individual participants, and then testing the reliability of those channels' response profiles in independent data. Using this approach, cortical regions with preferential responses to faces versus scenes, and to scenes versus faces, were observed reliably in both adults and infants. By contrast, standard analysis techniques did not reliably identify significant responses to both categories in either age group. These results reveal scene-responsive regions, and confirm face-responsive regions, in preverbal infants. More generally, the analysis approach will be a robust and sensitive tool for future characterization of the early functional development of the human brain.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/desc.12595},
  file = {/Users/xzfang/Zotero/storage/PZXYKB44/Powell et al. - 2018 - Using individual functional channels of interest t.pdf;/Users/xzfang/Zotero/storage/JLP5JI5S/desc.html}
}

@article{power_what_2012,
  title = {At What Time Is the Cocktail Party? {{A}} Late Locus of Selective Attention to Natural Speech},
  shorttitle = {At What Time Is the Cocktail Party?},
  author = {Power, Alan J. and Foxe, John J. and Forde, Emma-Jane and Reilly, Richard B. and Lalor, Edmund C.},
  year = {2012},
  journal = {European Journal of Neuroscience},
  volume = {35},
  number = {9},
  pages = {1497--1503},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.2012.08060.x},
  abstract = {Distinguishing between speakers and focusing attention on one speaker in multi-speaker environments is extremely important in everyday life. Exactly how the brain accomplishes this feat and, in particular, the precise temporal dynamics of this attentional deployment are as yet unknown. A long history of behavioral research using dichotic listening paradigms has debated whether selective attention to speech operates at an early stage of processing based on the physical characteristics of the stimulus or at a later stage during semantic processing. With its poor temporal resolution fMRI has contributed little to the debate, while EEG\textendash ERP paradigms have been hampered by the need to average the EEG in response to discrete stimuli which are superimposed onto ongoing speech. This presents a number of problems, foremost among which is that early attention effects in the form of endogenously generated potentials can be so temporally broad as to mask later attention effects based on the higher level processing of the speech stream. Here we overcome this issue by utilizing the AESPA (auditory evoked spread spectrum analysis) method which allows us to extract temporally detailed responses to two concurrently presented speech streams in natural cocktail-party-like attentional conditions without the need for superimposed probes. We show attentional effects on exogenous stimulus processing in the 200\textendash 220 ms range in the left hemisphere. We discuss these effects within the context of research on auditory scene analysis and in terms of a flexible locus of attention that can be deployed at a particular processing stage depending on the task.},
  copyright = {\textcopyright{} 2012 The Authors. European Journal of Neuroscience \textcopyright{} 2012 Federation of European Neuroscience Societies and Blackwell Publishing Ltd},
  langid = {english},
  keywords = {AESPA,EEG,exogenous processing,multi-speaker environments},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-9568.2012.08060.x},
  file = {/Users/xzfang/Zotero/storage/PVQKD8CX/Power et al. - 2012 - At what time is the cocktail party A late locus o.pdf;/Users/xzfang/Zotero/storage/H87Q6ZCN/j.1460-9568.2012.08060.html}
}

@article{pramod_improving_2020,
  title = {Improving {{Machine Vision}} Using {{Human Perceptual Representations}}: {{The Case}} of {{Planar Reflection Symmetry}} for {{Object Classification}}},
  shorttitle = {Improving {{Machine Vision}} Using {{Human Perceptual Representations}}},
  author = {Pramod, R. T. and Sp, A.},
  year = {2020},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3008107},
  abstract = {Achieving human-like visual abilities is a holy grail for machine vision, yet precisely how insights from human vision can improve machines has remained unclear. Here, we demonstrate two key conceptual advances: First, we show that most machine vision models are systematically different from human object perception. To do so, we collected a large dataset of perceptual distances between isolated objects in humans and asked whether these perceptual data can be predicted by many common machine vision algorithms. We found that while the best algorithms explain 70\% of the variance in the perceptual data, all the algorithms we tested make systematic errors on several types of objects. In particular, machine algorithms underestimated distances between symmetric objects compared to human perception. Second, we show that fixing these systematic biases can lead to substantial gains in classification performance. In particular, augmenting a state-of-the-art convolutional neural network with planar/reflection symmetry scores along multiple axes produced significant improvements in classification accuracy (1-10\%) across categories. These results show that machine vision can be improved by discovering and fixing systematic differences from human vision.},
  keywords = {Computational modeling,Computational models of Vision,Machine vision,Object Recognition,Perception and Psychophysics,Prediction algorithms,Search problems,Systematics,Task analysis,Visualization},
  file = {/Users/xzfang/Zotero/storage/TXMIMV5W/Pramod and Sp - 2020 - Improving Machine Vision using Human Perceptual Re.pdf}
}

@misc{prasad_p600_2020,
  title = {The {{P600}} for Singular ``They'': {{How}} the Brain Reacts When {{John}} Decides to Treat Themselves to Sushi.},
  shorttitle = {The {{P600}} for Singular ``They''},
  author = {Prasad, Grusha and Morris, Joanna},
  year = {2020},
  month = jan,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/hwzke},
  abstract = {There has been increased awareness that individuals need not have a binary gender identity (i.e., male or female), but rather, gender identities exist on a spectrum. With this increased awareness, there has also been an increase in the use of they as a singular pronoun when referring to individuals with a non-binary gender identity. Has the processing of singular they changed along with a change in its usage? Previous studies have demonstrated that sentences in which they is co-indexed with singular antecedents, are judged acceptable and are easy to process, but only if the antecedents are non-referential and/or have ambiguous gender; co-indexing they with referential antecedents with unambiguous gender (e.g., Mary) results in lower acceptability ratings and greater processing effort. We investigated whether participants who frequently interacted with individuals with a non-binary gender identity and/or identified as having a non-binary gender themselves would process sentences in which themselves was co-indexed with singular antecedents similarly. We found a significant P600 effect for sentences in which themselves was co-indexed with singular referential antecedents with unambiguous gender, but failed to find a P600 effect when the antecedents were non-referential and/or had an ambiguous gender. This pattern of results is consistent with behavioural results from previous studies, suggesting that the change in the usage of singular they has not resulted in a corresponding change in the way in which this pronoun is processed.},
  keywords = {Linguistics,Psycholinguistics and Neurolinguistics,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/N584UIJT/Prasad and Morris - 2020 - The P600 for singular â€œtheyâ€ How the brain reacts.pdf}
}

@article{price_causal_2016,
  title = {Causal {{Evidence}} for a {{Mechanism}} of {{Semantic Integration}} in the {{Angular Gyrus}} as {{Revealed}} by {{High-Definition Transcranial Direct Current Stimulation}}},
  author = {Price, Amy Rose and Peelle, Jonathan E. and Bonner, Michael F. and Grossman, Murray and Hamilton, Roy H.},
  year = {2016},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {36},
  number = {13},
  pages = {3829--3838},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3120-15.2016},
  abstract = {A defining aspect of human cognition is the ability to integrate conceptual information into complex semantic combinations. For example, we can comprehend ``plaid'' and ``jacket'' as individual concepts, but we can also effortlessly combine these concepts to form the semantic representation of ``plaid jacket.'' Many neuroanatomic models of semantic memory propose that heteromodal cortical hubs integrate distributed semantic features into coherent representations. However, little work has specifically examined these proposed integrative mechanisms and the causal role of these regions in semantic integration. Here, we test the hypothesis that the angular gyrus (AG) is critical for integrating semantic information by applying high-definition transcranial direct current stimulation (tDCS) to an fMRI-guided region-of-interest in the left AG. We found that anodal stimulation to the left AG modulated semantic integration but had no effect on a letter-string control task. Specifically, anodal stimulation to the left AG resulted in faster comprehension of semantically meaningful combinations like ``tiny radish'' relative to non-meaningful combinations, such as ``fast blueberry,'' when compared to the effects observed during sham stimulation and stimulation to a right-hemisphere control brain region. Moreover, the size of the effect from brain stimulation correlated with the degree of semantic coherence between the word pairs. These findings demonstrate that the left AG plays a causal role in the integration of lexical-semantic information, and that high-definition tDCS to an associative cortical hub can selectively modulate integrative processes in semantic memory. SIGNIFICANCE STATEMENT A major goal of neuroscience is to understand the neural basis of behaviors that are fundamental to human intelligence. One essential behavior is the ability to integrate conceptual knowledge from semantic memory, allowing us to construct an almost unlimited number of complex concepts from a limited set of basic constituents (e.g., ``leaf'' and ``wet'' can be combined into the more complex representation ``wet leaf''). Here, we present a novel approach to studying integrative processes in semantic memory by applying focal brain stimulation to a heteromodal cortical hub implicated in semantic processing. Our findings demonstrate a causal role of the left angular gyrus in lexical-semantic integration and provide motivation for novel therapeutic applications in patients with lexical-semantic deficits.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2016 the authors 0270-6474/16/363829-10\$15.00/0},
  langid = {english},
  pmid = {27030767},
  keywords = {brain stimulation,compositionality,inferior parietal cortex,semantic integration,semantic memory,tDCS},
  file = {/Users/xzfang/Zotero/storage/Y3CCJCCY/Price et al. - 2016 - Causal Evidence for a Mechanism of Semantic Integr.pdf;/Users/xzfang/Zotero/storage/9AJ2EJIK/3829.html}
}

@article{price_predicting_2010,
  title = {Predicting {{Language Outcome}} and {{Recovery After Stroke}} ({{PLORAS}})},
  author = {Price, CJ and Seghier, ML and Leff, AP},
  year = {2010},
  month = apr,
  journal = {Nature reviews. Neurology},
  volume = {6},
  number = {4},
  pages = {202--210},
  issn = {1759-4758},
  doi = {10.1038/nrneurol.2010.15},
  abstract = {The ability of comprehend and produce speech after stroke depends on whether the areas of the brain that support language have been damaged. Here we review two different ways to predict language outcome after stroke. The first depends on understanding the neural circuits that support language. This model-based approach is a challenging endeavor because language is a complex cognitive function that involves the interaction of many different brain areas. The second approach does not require an understanding of why a lesion impairs language, instead, predictions are made on the basis of how previous patients with the same lesion recovered. This requires a database storing the speech and language abilities of a large population of patients who have, between them, incurred a comprehensive range of focal brain damage. In addition it requires a system that converts an MRI scan from a new patient into a 3D description of the lesion and then compares this lesion to all others on the database. The outputs of this system are the longitudinal language outcomes of corresponding patients in the database. This will provide a new patient, their carers and the clinician team managing them the range of likely recovery patterns over a variety of language measures.},
  pmcid = {PMC3556582},
  pmid = {20212513},
  file = {/Users/xzfang/Zotero/storage/AL4VUD8D/Price et al. - 2010 - Predicting Language Outcome and Recovery After Str.pdf}
}

@article{price_predicting_2010a,
  title = {Predicting {{Language Outcome}} and {{Recovery After Stroke}} ({{PLORAS}})},
  author = {Price, CJ and Seghier, ML and Leff, AP},
  year = {2010},
  month = apr,
  journal = {Nature reviews. Neurology},
  volume = {6},
  number = {4},
  pages = {202--210},
  issn = {1759-4758},
  doi = {10.1038/nrneurol.2010.15},
  abstract = {The ability of comprehend and produce speech after stroke depends on whether the areas of the brain that support language have been damaged. Here we review two different ways to predict language outcome after stroke. The first depends on understanding the neural circuits that support language. This model-based approach is a challenging endeavor because language is a complex cognitive function that involves the interaction of many different brain areas. The second approach does not require an understanding of why a lesion impairs language, instead, predictions are made on the basis of how previous patients with the same lesion recovered. This requires a database storing the speech and language abilities of a large population of patients who have, between them, incurred a comprehensive range of focal brain damage. In addition it requires a system that converts an MRI scan from a new patient into a 3D description of the lesion and then compares this lesion to all others on the database. The outputs of this system are the longitudinal language outcomes of corresponding patients in the database. This will provide a new patient, their carers and the clinician team managing them the range of likely recovery patterns over a variety of language measures.},
  pmcid = {PMC3556582},
  pmid = {20212513},
  file = {/Users/xzfang/Zotero/storage/63W27JZQ/Price et al. - 2010 - Predicting Language Outcome and Recovery After Str.pdf}
}

@article{price_review_2012,
  title = {A Review and Synthesis of the First 20years of {{PET}} and {{fMRI}} Studies of Heard Speech, Spoken Language and Reading},
  author = {Price, Cathy J.},
  year = {2012},
  month = aug,
  journal = {NeuroImage},
  series = {20 {{YEARS OF fMRI}}},
  volume = {62},
  number = {2},
  pages = {816--847},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2012.04.062},
  abstract = {The anatomy of language has been investigated with PET or fMRI for more than 20years. Here I attempt to provide an overview of the brain areas associated with heard speech, speech production and reading. The conclusions of many hundreds of studies were considered, grouped according to the type of processing, and reported in the order that they were published. Many findings have been replicated time and time again leading to some consistent and undisputable conclusions. These are summarised in an anatomical model that indicates the location of the language areas and the most consistent functions that have been assigned to them. The implications for cognitive models of language processing are also considered. In particular, a distinction can be made between processes that are localized to specific structures (e.g. sensory and motor processing) and processes where specialisation arises in the distributed pattern of activation over many different areas that each participate in multiple functions. For example, phonological processing of heard speech is supported by the functional integration of auditory processing and articulation; and orthographic processing is supported by the functional integration of visual processing, articulation and semantics. Future studies will undoubtedly be able to improve the spatial precision with which functional regions can be dissociated but the greatest challenge will be to understand how different brain regions interact with one another in their attempts to comprehend and produce language.},
  langid = {english},
  keywords = {Auditory speech,Comprehension,fMRI,Language,PET,Reading,Speech production},
  file = {/Users/xzfang/Zotero/storage/JFVCXTJM/Price - 2012 - A review and synthesis of the first 20years of PET.pdf}
}

@article{proix_imagined_2022,
  title = {Imagined Speech Can Be Decoded from Low- and Cross-Frequency Intracranial {{EEG}} Features},
  author = {Proix, Timoth{\'e}e and Delgado Saa, Jaime and Christen, Andy and Martin, Stephanie and Pasley, Brian N. and Knight, Robert T. and Tian, Xing and Poeppel, David and Doyle, Werner K. and Devinsky, Orrin and Arnal, Luc H. and M{\'e}gevand, Pierre and Giraud, Anne-Lise},
  year = {2022},
  month = jan,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {48},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-27725-3},
  abstract = {Reconstructing intended speech from neural activity using brain-computer interfaces holds great promises for people with severe speech production deficits. While decoding overt speech has progressed, decoding imagined speech has met limited success, mainly because the associated neural signals are weak and variable compared to overt speech, hence difficult to decode by learning algorithms. We obtained three electrocorticography datasets from 13 patients, with electrodes implanted for epilepsy evaluation, who performed overt and imagined speech production tasks. Based on recent theories of speech neural processing, we extracted consistent and specific neural features usable for future brain computer interfaces, and assessed their performance to discriminate speech items in articulatory, phonetic, and vocalic representation spaces. While high-frequency activity provided the best signal for overt speech, both low- and higher-frequency power and local cross-frequency contributed to imagined speech decoding, in particular in phonetic and vocalic, i.e. perceptual, spaces. These findings show that low-frequency power and cross-frequency dynamics contain key information for imagined speech decoding.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Language,Neuroscience},
  file = {/Users/xzfang/Zotero/storage/PHFKVIGU/Proix et al. - 2022 - Imagined speech can be decoded from low- and cross.pdf;/Users/xzfang/Zotero/storage/D8LHHNJW/s41467-021-27725-3.html}
}

@article{proksch_motor_2020,
  title = {Motor and {{Predictive Processes}} in {{Auditory Beat}} and {{Rhythm Perception}}},
  author = {Proksch, Shannon and Comstock, Daniel C. and M{\'e}d{\'e}, Butovens and Pabst, Alexandria and Balasubramaniam, Ramesh},
  year = {2020},
  month = sep,
  journal = {Frontiers in Human Neuroscience},
  volume = {14},
  pages = {578546},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2020.578546},
  abstract = {In this article, we review recent advances in research on rhythm and musical beat perception, focusing on the role of predictive processes in auditory motor interactions. We suggest that experimental evidence of the motor system's role in beat perception, including in passive listening, may be explained by the generation and maintenance of internal predictive models, concordant with the Active Inference framework of sensory processing. We highlight two complementary hypotheses for the neural underpinnings of rhythm perception: The Action Simulation for Auditory Prediction hypothesis (Patel and Iversen, 2014) and the Gradual Audiomotor Evolution hypothesis (Merchant and Honing, 2014) and review recent experimental progress supporting each of these hypotheses. While initial formulations of ASAP and GAE explain different aspects of beat-based timing\textendash the involvement of motor structures in the absence of movement, and physical entrainment to an auditory beat respectively\textendash we suggest that work under both hypotheses provide converging evidence toward understanding the predictive role of the motor system in the perception of rhythm, and the specific neural mechanisms involved. We discuss future experimental work necessary to further evaluate the causal neural mechanisms underlying beat and rhythm perception.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/L4C2JIUX/Proksch et al. - 2020 - Motor and Predictive Processes in Auditory Beat an.pdf}
}

@misc{pudhiyidath_representations_2021,
  title = {Representations of Temporal Community Structure in Hippocampus and Precuneus Predict Inductive Reasoning Decisions},
  author = {Pudhiyidath, Athula and Morton, Neal W. and Duran, Rodrigo Viveros and Schapiro, Anna C. and Momennejad, Ida and {Hinojosa-Rowland}, Demitrius M. and Molitor, Robert J. and Preston, Alison R.},
  year = {2021},
  month = oct,
  pages = {2021.10.12.462707},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.10.12.462707},
  abstract = {Our understanding of the world is shaped by inferences about underlying structure. For example, at the gym, you might notice that the same people tend to arrive around the same time and infer that they are friends that work out together. Consistent with this idea, after participants are presented with a temporal sequence of objects that follows an underlying community structure, they are biased to infer that objects from the same community share the same properties. Here, we used fMRI to measure neural representations of objects after temporal community structure learning and examine how these representations support inference about object relationships. We found that community structure learning affected inferred object similarity: when asked to spatially group items based on their experience, participants tended to group together objects from the same community. Neural representations in perirhinal cortex predicted individual differences in object grouping, suggesting that high-level object representations are affected by temporal community learning. Furthermore, participants were biased to infer that objects from the same community would share the same properties. Using computational modeling of temporal learning and inference decisions, we found that inductive reasoning is influenced by both detailed knowledge of temporal statistics and abstract knowledge of the temporal communities. The fidelity of temporal community representations in hippocampus and precuneus predicted the degree to which temporal community membership biased reasoning decisions. Our results suggest that temporal knowledge is represented at multiple levels of abstraction, and that perirhinal cortex, hippocampus, and precuneus may support inference based on this knowledge.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CK7UAUMI/Pudhiyidath et al. - 2021 - Representations of temporal community structure in.pdf;/Users/xzfang/Zotero/storage/4H7VENPJ/2021.10.12.html}
}

@article{qasim_phase_2021,
  title = {Phase Precession in the Human Hippocampus and Entorhinal Cortex},
  author = {Qasim, Salman E. and Fried, Itzhak and Jacobs, Joshua},
  year = {2021},
  month = jun,
  journal = {Cell},
  volume = {184},
  number = {12},
  pages = {3242-3255.e10},
  publisher = {{Elsevier}},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2021.04.017},
  langid = {english},
  pmid = {33979655},
  keywords = {entorhinal cortex,frontal cortex,goal-directed navigation,hippocampus,phase precession,place cells,temporal coding,theta oscillations},
  file = {/Users/xzfang/Zotero/storage/TSFT8UAU/Qasim et al. - 2021 - Phase precession in the human hippocampus and ento.pdf;/Users/xzfang/Zotero/storage/NRZ92YWU/S0092-8674(21)00496-7.html}
}

@article{qian_comparison_2018,
  title = {A Comparison of Online and Offline Measures of Good-Enough Processing in Garden-Path Sentences},
  author = {Qian, Zhiying and Garnsey, Susan and Christianson, Kiel},
  year = {2018},
  month = feb,
  journal = {Language, Cognition and Neuroscience},
  volume = {33},
  number = {2},
  pages = {227--254},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2017.1379606},
  abstract = {In two self-paced reading and one ERP experiments, this study tested the good-enough processing account, which states that readers sometimes misinterpret sentences like While the man hunted the deer ran into the woods because they fail to fully revise the syntactic structure [Christianson, K., Hollingworth, A., Halliwell, J. F., \& Ferreira, F. (2001). Thematic roles assigned along the garden path linger. Cognitive Psychology, 42, 368\textendash 407. doi:10.1006/cogp.2001.0752]. Such an account predicts more evidence of reanalysis at the disambiguation on correctly- than incorrectly-answered trials. Experiment 1, which asked Did the man hunt the deer? and Experiment 2, which asked Did the sentence explicitly say that the man hunted the deer? showed no difference in reading time between trials with correct and incorrect responses. Experiment 3 found the amplitude of P600 was unrelated to comprehension accuracy. These results converged to suggest that failure to reanalyse ambiguous sentences is not the primary reason for misinterpretation. Three norming studies revealed instead response accuracy was influenced by likelihood of events described in the sentences and questions.},
  keywords = {Garden-path sentences,good-enough processing,P600,plausibility,reanalysis},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2017.1379606},
  file = {/Users/xzfang/Zotero/storage/ZMLEV68H/Qian et al. - 2018 - A comparison of online and offline measures of goo.pdf;/Users/xzfang/Zotero/storage/JUYKGYNU/23273798.2017.html}
}

@article{quianquiroga_extracting_2009,
  title = {Extracting Information from Neuronal Populations: Information Theory and Decoding Approaches},
  shorttitle = {Extracting Information from Neuronal Populations},
  author = {Quian Quiroga, Rodrigo and Panzeri, Stefano},
  year = {2009},
  month = mar,
  journal = {Nature Reviews Neuroscience},
  volume = {10},
  number = {3},
  pages = {173--185},
  issn = {1471-0048},
  doi = {10.1038/nrn2578},
  abstract = {Recording from neuronal populations is a promising and powerful neuroscience technique; however, interpreting the resulting spike trains presents several challenges. Quian Quiroga and Panzeri discuss how decoding algorithms and information theory can be used to extract information from population recordings.},
  copyright = {2009 Nature Publishing Group},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/B3E74KQS/nrn2578.html}
}

@article{quigley_forgotten_2021,
  title = {Forgotten Rhythms? {{Revisiting}} the First Evidence for Rhythms in Cognition},
  shorttitle = {Forgotten Rhythms?},
  author = {Quigley, Cliodhna},
  year = {2021},
  journal = {European Journal of Neuroscience},
  volume = {n/a},
  number = {n/a},
  issn = {1460-9568},
  doi = {10.1111/ejn.15450},
  abstract = {Practically every neuroscientist knows that human brain rhythms were first recorded in the 1920s by Hans Berger, who coined the term `alpha waves' for the regular activity of around 10 cycles per second that was clearly visible in many of his recordings. Almost 100 years later, alpha rhythms are still the subject of active investigation and continue to intrigue researchers. What we have perhaps forgotten though, is the clever experimentation that was carried out during the first decades of electroencephalogram (EEG) research, often using sophisticated, custom-made analysis and stimulation devices. Here, I review selected findings from the early EEG literature regarding the character, origin, and meaning of human brain rhythms, beginning with Berger's publications and then focusing on the use of regular visual stimulation as a tool to understand intrinsic brain rhythms. It is clear that many of these findings are still relevant to open questions about the role of rhythmic brain activity. In addition, they also contain some general lessons for contemporary neuroscientists, meaning that there is great value in looking back at these forgotten publications.},
  langid = {english},
  keywords = {alpha rhythms,good scientific practice,Hans Berger,history of neuroscience,photic driving},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.15450},
  file = {/Users/xzfang/Zotero/storage/2LJR2JAL/Quigley - Forgotten rhythms Revisiting the first evidence f.pdf;/Users/xzfang/Zotero/storage/ISUE32I9/ejn.html}
}

@article{quillen_distinct_2021,
  title = {Distinct {{Neural Correlates}} of {{Linguistic}} and {{Non-Linguistic Demand}}},
  author = {Quillen, Ian A. and Yen, Melodie and Wilson, Stephen M.},
  year = {2021},
  month = mar,
  journal = {Neurobiology of Language},
  volume = {2},
  number = {2},
  pages = {202--225},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00031},
  abstract = {In this study, we investigated how the brain responds to task difficulty in linguistic and non-linguistic contexts. This is important for the interpretation of functional imaging studies of neuroplasticity in post-stroke aphasia, because of the inherent difficulty of matching or controlling task difficulty in studies with neurological populations. Twenty neurologically normal individuals were scanned with fMRI as they performed a linguistic task and a non-linguistic task, each of which had two levels of difficulty. Critically, the tasks were matched across domains (linguistic, non-linguistic) for accuracy and reaction time, such that the differences between the easy and difficult conditions were equivalent across domains. We found that non-linguistic demand modulated the same set of multiple demand (MD) regions that have been identified in many prior studies. In contrast, linguistic demand modulated MD regions to a much lesser extent, especially nodes belonging to the dorsal attention network. Linguistic demand modulated a subset of language regions, with the left inferior frontal gyrus most strongly modulated. The right hemisphere region homotopic to Broca's area was also modulated by linguistic but not non-linguistic demand. When linguistic demand was mapped relative to non-linguistic demand, we also observed domain by difficulty interactions in temporal language regions as well as a widespread bilateral semantic network. In sum, linguistic and non-linguistic demand have strikingly different neural correlates. These findings can be used to better interpret studies of patients recovering from aphasia. Some reported activations in these studies may reflect task performance differences, while others can be more confidently attributed to neuroplasticity.},
  file = {/Users/xzfang/Zotero/storage/RYNX3WGG/Quillen et al. - 2021 - Distinct Neural Correlates of Linguistic and Non-L.pdf;/Users/xzfang/Zotero/storage/2EGYY5HY/Distinct-Neural-Correlates-of-Linguistic-and-Non.html}
}

@article{quiroga_concept_2012,
  title = {Concept Cells: The Building Blocks of Declarative Memory Functions},
  shorttitle = {Concept Cells},
  author = {Quiroga, Rodrigo Quian},
  year = {2012},
  month = aug,
  journal = {Nature Reviews Neuroscience},
  volume = {13},
  number = {8},
  pages = {587--597},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3251},
  abstract = {Neurons in the human medial temporal lobe respond in a selective and abstract manner to particular persons or objects. Rodrigo Quian Quiroga argues that these 'concept cells' are crucial for memory functions and the transition between related concepts that leads to the flow of consciousness.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RERT8S7N/Quiroga - 2012 - Concept cells the building blocks of declarative .pdf;/Users/xzfang/Zotero/storage/B38IT8G2/nrn3251.html}
}

@article{quiroga-martinez_decomposing_2020,
  title = {Decomposing Neural Responses to Melodic Surprise in Musicians and Non-Musicians: {{Evidence}} for a Hierarchy of Predictions in the Auditory System},
  shorttitle = {Decomposing Neural Responses to Melodic Surprise in Musicians and Non-Musicians},
  author = {{Quiroga-Martinez}, D. R. and Hansen, N. C. and H{\o}jlund, A. and Pearce, M. and Brattico, E. and Vuust, P.},
  year = {2020},
  month = jul,
  journal = {NeuroImage},
  volume = {215},
  pages = {116816},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.116816},
  abstract = {Neural responses to auditory surprise are typically studied with highly unexpected, disruptive sounds. Consequently, little is known about auditory prediction in everyday contexts that are characterized by fine-grained, non-disruptive fluctuations of auditory surprise. To address this issue, we used IDyOM, a computational model of auditory expectation, to obtain continuous surprise estimates for a set of newly composed melodies. Our main goal was to assess whether the neural correlates of non-disruptive surprising sounds in a musical context are affected by musical expertise. Using magnetoencephalography (MEG), auditory responses were recorded from musicians and non-musicians while they listened to the melodies. Consistent with a previous study, the amplitude of the N1m component increased with higher levels of computationally estimated surprise. This effect, however, was not different between the two groups. Further analyses offered an explanation for this finding: Pitch interval size itself, rather than probabilistic prediction, was responsible for the modulation of the N1m, thus pointing to low-level sensory adaptation as the underlying mechanism. In turn, the formation of auditory regularities and proper probabilistic prediction were reflected in later components: The mismatch negativity (MMNm) and the P3am, respectively. Overall, our findings reveal a hierarchy of expectations in the auditory system and highlight the need to properly account for sensory adaptation in research addressing statistical learning.},
  langid = {english},
  keywords = {Hierarchy,IDyOM,Music,Prediction,Surprise},
  file = {/Users/xzfang/Zotero/storage/EJ6AXT5C/Quiroga-Martinez et al. - 2020 - Decomposing neural responses to melodic surprise i.pdf}
}

@article{rabinowitz_constructing_2013,
  title = {Constructing {{Noise-Invariant Representations}} of {{Sound}} in the {{Auditory Pathway}}},
  author = {Rabinowitz, Neil C. and Willmore, Ben D. B. and King, Andrew J. and Schnupp, Jan W. H.},
  year = {2013},
  month = nov,
  journal = {PLOS Biology},
  volume = {11},
  number = {11},
  pages = {e1001710},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1001710},
  abstract = {Along the auditory pathway from auditory nerve to midbrain to cortex, individual neurons adapt progressively to sound statistics, enabling the discernment of foreground sounds, such as speech, over background noise.},
  langid = {english},
  keywords = {Ambient noise,Auditory pathway,Background signal noise,Coding mechanisms,Modulation,Neurons,Signal decoders,Speech signal processing},
  file = {/Users/xzfang/Zotero/storage/KXJD68QP/Rabinowitz et al. - 2013 - Constructing Noise-Invariant Representations of So.pdf;/Users/xzfang/Zotero/storage/GNK5AKAR/article.html}
}

@article{rabovsky_modelling_2018,
  title = {Modelling the {{N400}} Brain Potential as Change in a Probabilistic Representation of Meaning},
  author = {Rabovsky, Milena and Hansen, Steven S. and McClelland, James L.},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {693--705},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0406-4},
  abstract = {The N400 evoked potential is a window to meaning in the brain, but it remains incompletely understood. The authors provide a unified explanation of the N400 in a neural network model that avoids the commitments of traditional approaches to meaning in language.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/C5Y6BRMX/Rabovsky et al. - 2018 - Modelling the N400 brain potential as change in a .pdf;/Users/xzfang/Zotero/storage/2TUVEQ9H/s41562-018-0406-4.html}
}

@article{rafidi_role_,
  title = {The Role of Syntax in Semantic Processing: {{A}} Study of Active and Passive Sentences},
  author = {Rafidi, Nicole},
  pages = {16},
  abstract = {Background As you read this sentence, you are performing the complex task of composing the meaning of these words into a whole proposition. A multitude of work in psycholinguistics has shown that humans use both the semantics (the meaning of the words) and the syntax (their order) of a sentence to process its meaning. However, the rules by which the brain combines this information are largely unknown. High temporal resolution non-invasive neuroimaging techniques, such as magnetoencephalography (MEG) have great potential to answer this question, because they measure neural activity on the timescale of natural reading. Aim Using a machine learning approach, we contrast sentences with the same semantics and different syntax in order to shed light on the processing stages used by the brain when reading English sentences. We examine the robustness of these results to various analysis design choices to maximize sensitivity in future confirmatory analyses. Data We collected MEG data from 8 participants while they read active- and passive-voiced versions of the same propositions. Each participant read 16 propositions made up of 8 nouns and 4 verbs. The 32 sentences were repeated 15 times and we use an average of some or all of these repetitions for subsequent analysis. Methods We apply a Naive Bayes classifier at each point in the MEG activity timeseries to generate classification accuracy timeseries for each word in the sentence, cross-validating in a leave-one-proposition-out manner. We seek time windows of above chance accuracy in order to assert when certain words are accessed. To test the robustness of the observed effect, we vary different aspects of the data analysis and observe the effect on decoding accuracy. We also assessed which neural regions contribute most to decoding accuracy at different times. Results When reading passive-voiced sentences, participants access words relevant to the proposition multiple times throughout the sentence. This draws a sharp distinction with the processing of active-voiced sentences, in which participants seem to access relevant words only once, at word presentation. The result is highly robust to a variety of analysis decisions, and seems, to our surprise, to originate in occipital regions of the brain. Conclusions This result is consistent with prior work demonstrating reanalysis in syntactically complex sentences, in which participants correct an erroneous initial reading of the sentence. Our work elucidates the timing of reanalysis, which is late in the sentence and is consistent with theory on the mental storage of sentence components by role. Via reanalysis, participants can arrive at the same meaning through different syntactic paths. These results demonstrate the potential of machine learning approaches to reveal neural processing stages in an interpretable manner.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5HG283YX/Raï¬di - The role of syntax in semantic processing A study.pdf}
}

@article{rajalingham_inferior_2020,
  title = {The Inferior Temporal Cortex Is a Potential Cortical Precursor of Orthographic Processing in Untrained Monkeys},
  author = {Rajalingham, Rishi and Kar, Kohitij and Sanghavi, Sachi and Dehaene, Stanislas and DiCarlo, James J.},
  year = {2020},
  month = aug,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3886},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17714-3},
  abstract = {The ability to recognize written letter strings is foundational to human reading, but the underlying neuronal mechanisms remain largely unknown. Recent behavioral research in baboons suggests that non-human primates may provide an opportunity to investigate this question. We recorded the activity of hundreds of neurons in V4 and the inferior temporal cortex (IT) while na\"ive macaque monkeys passively viewed images of letters, English words and non-word strings, and tested the capacity of those neuronal representations to support a battery of orthographic processing tasks. We found that simple linear read-outs of IT (but not V4) population responses achieved high performance on all tested tasks, even matching the performance and error patterns of baboons on word classification. These results show that the IT cortex of untrained primates can serve as a precursor of orthographic processing, suggesting that the acquisition of reading in humans relies on the recycling of a brain network evolved for other visual functions.},
  copyright = {2020 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Object vision;Reading Subject\_term\_id: object-vision;reading},
  file = {/Users/xzfang/Zotero/storage/H6UKTQ4R/Rajalingham et al. - 2020 - The inferior temporal cortex is a potential cortic.pdf}
}

@article{rao_predictive_1999,
  title = {Predictive Coding in the Visual Cortex: A Functional Interpretation of Some Extra-Classical Receptive-Field Effects},
  shorttitle = {Predictive Coding in the Visual Cortex},
  author = {Rao, Rajesh P. N. and Ballard, Dana H.},
  year = {1999},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {2},
  number = {1},
  pages = {79--87},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/4580},
  abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
  copyright = {1999 Nature America Inc.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/Users/xzfang/Zotero/storage/QNVUI3YY/Rao and Ballard - 1999 - Predictive coding in the visual cortex a function.pdf;/Users/xzfang/Zotero/storage/4T4PIZJK/nn0199_79.html}
}

@article{rastle_broth_2004,
  title = {The Broth in My Brother's Brothel: {{Morpho-orthographic}} Segmentation in Visual Word Recognition},
  shorttitle = {The Broth in My Brother's Brothel},
  author = {Rastle, Kathleen and Davis, Matthew H. and New, Boris},
  year = {2004},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {11},
  number = {6},
  pages = {1090--1098},
  issn = {1531-5320},
  doi = {10.3758/BF03196742},
  abstract = {Much research suggests that words comprising more than one morpheme are represented in a ``decomposed'' manner in the visual word recognition system. In the research presented here, we investigate what information is used to segment a word into its morphemic constituents and, in particular, whether semantic information plays a role in that segmentation. Participants made visual lexical decisions to stem targets preceded by masked primes sharing (1) a semantically transparent morphological relationship with the target (e.g.,cleaner-CLEAN), (2) an apparent morphological relationship but no semantic relationship with the target (e.g.,corner-CORN), and (3) a nonmorphological form relationship with the target (e.g.,brothel-BROTH). Results showed significant and equivalent masked priming effects in cases in which primes and targets appeared to be morphologically related, and priming in these conditions could be distinguished from nonmorphological form priming. We argue that these findings suggest a level of representation at which apparently complex words are decomposed on the basis of their morpho-orthographic properties. Implications of these findings for computational models of reading are discussed.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/62S7S6W5/Rastle et al. - 2004 - The broth in my brotherâ€™s brothel Morpho-orthogra.pdf}
}

@article{rastle_morphological_2008,
  title = {Morphological Decomposition Based on the Analysis of Orthography},
  author = {Rastle, Kathleen and Davis, Matthew H.},
  year = {2008},
  month = nov,
  journal = {Language and Cognitive Processes},
  volume = {23},
  number = {7-8},
  pages = {942--971},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690960802069730},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LACXMGKE/Rastle and Davis - 2008 - Morphological decomposition based on the analysis .pdf}
}

@article{ratanmurty_visual_2020,
  title = {Visual Experience Is Not Necessary for the Development of Face-Selectivity in the Lateral Fusiform Gyrus},
  author = {Ratan Murty, N. Apurva and Teng, Santani and Beeler, David and Mynick, Anna and Oliva, Aude and Kanwisher, Nancy},
  year = {2020},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {37},
  pages = {23011--23020},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2004607117},
  abstract = {The fusiform face area responds selectively to faces and is causally involved in face perception. How does face-selectivity in the fusiform arise in development, and why does it develop so systematically in the same location across individuals? Preferential cortical responses to faces develop early in infancy, yet evidence is conflicting on the central question of whether visual experience with faces is necessary. Here, we revisit this question by scanning congenitally blind individuals with fMRI while they haptically explored 3D-printed faces and other stimuli. We found robust face-selective responses in the lateral fusiform gyrus of individual blind participants during haptic exploration of stimuli, indicating that neither visual experience with faces nor fovea-biased inputs is necessary for face-selectivity to arise in the lateral fusiform gyrus. Our results instead suggest a role for long-range connectivity in specifying the location of face-selectivity in the human brain.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9SSC2NY4/Ratan Murty et al. - 2020 - Visual experience is not necessary for the develop.pdf}
}

@article{raymond_temporary_1992,
  title = {Temporary Suppression of Visual Processing in an {{RSVP}} Task: {{An}} Attentional Blink?},
  shorttitle = {Temporary Suppression of Visual Processing in an {{RSVP}} Task},
  author = {Raymond, Jane E. and Shapiro, Kimron L. and Arnell, Karen M.},
  year = {1992},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {18},
  number = {3},
  pages = {849--860},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1277},
  doi = {10.1037/0096-1523.18.3.849},
  abstract = {Through rapid serial visual presentation (RSVP), the authors asked Ss to identify a partially specified letter (target) and then to detect the presence or absence of a fully specified letter (probe). Whereas targets are accurately identified, probes are poorly detected when they are presented during a 270-msec interval beginning 180 msec after the target. Probes presented immediately after the target or later in the RSVP stream are accurately detected. This temporary reduction in probe detection was not found in conditions in which a brief blank interval followed the target or Ss were not required to identify the target. The data suggest that the presentation of stimuli after the target but before target-identification processes are complete produces interference at a letter-recognition stage. This interference may cause the temporary suppression of visual attention mechanisms observed in the present study. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Attention,Letters (Alphabet),Visual Discrimination},
  file = {/Users/xzfang/Zotero/storage/WW449U5M/raymond (1992) temporary suppression of visual processing in an RSVP task. an attentional blink..pdf;/Users/xzfang/Zotero/storage/58XG9I25/1992-41685-001.html}
}

@article{rayner_perceptual_1975,
  title = {The Perceptual Span and Peripheral Cues in Reading},
  author = {Rayner, Keith},
  year = {1975},
  month = jan,
  journal = {Cognitive Psychology},
  volume = {7},
  number = {1},
  pages = {65--81},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(75)90005-5},
  abstract = {Skilled readers read passages that were displayed on a Cathode Ray Tube controlled by a computer. The readers' eye movements were monitored and certain critical words were changed by the computer as the eye was in motion. The experimental technique utilized in the study provided data on how wide the area is from which a reader acquires information during a fixation in silent reading. The results also delineate different types of visual information that are acquired from various areas within the perceptual span. It was found that a reader was able to make a semantic interpretation of a word that began 1\textendash 6 character spaces from his fixation point. When he fixated 7\textendash 12 character spaces prior to a word, he was able to pick up such gross visual characteristics as word shape and initial and final letters. It was concluded that the skilled reader is able to take advantage of information in the periphery. However, the size of the area from which he does is rather small.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4QMJCGQ7/0010028575900055.html}
}

@article{reagh_corticohippocampal_2021,
  title = {A Cortico-Hippocampal Scaffold for Representing and Recalling Lifelike Events},
  author = {Reagh, Zachariah M. and Ranganath, Charan},
  year = {2021},
  month = apr,
  journal = {bioRxiv},
  pages = {2021.04.16.439894},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.04.16.439894},
  abstract = {{$<$}h3{$>$}Summary{$<$}/h3{$>$} {$<$}p{$>$}Real-world events are complex, featuring elements that may be unique to, or shared across, multiple situations. In the present study, we used fMRI to identify how different event components are represented in real-time and during memory retrieval. Twenty participants viewed and recalled eight videos depicting real-world events, combining people, contexts, and context types. Multi-voxel pattern similarity analyses revealed specific `person' representations, persistent across contexts, in regions of an Anterior-Temporal Network. Conversely, we found specific `context' representations, persistent across people, in regions of a Posterior-Medial Network. We also found schema-like generalization across contexts in medial prefrontal cortex, and episodic specificity in the hippocampus. Event patterns were reinstated during recall, and hippocampal reinstatement predicted the number of details retrieved. Finally, we observed distinct representational timescales across the hippocampus and cortical regions. These findings reveal mechanisms for scaffolding different aspects of lifelike event representations in cortico-hippocampal networks as experiences are observed and recalled.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CHXEQ6GV/Reagh and Ranganath - 2021 - A cortico-hippocampal scaffold for representing an.pdf}
}

@article{regev_context_2021,
  title = {Context {{Sensitivity}} across {{Multiple Time}} Scales with a {{Flexible Frequency Bandwidth}}},
  author = {Regev, Tamar I and Markusfeld, Geffen and Deouell, Leon Y and Nelken, Israel},
  year = {2021},
  month = jul,
  journal = {Cerebral Cortex},
  number = {bhab200},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab200},
  abstract = {Everyday auditory streams are complex, including spectro-temporal content that varies at multiple timescales. Using EEG, we investigated the sensitivity of human auditory cortex to the content of past stimulation in unattended sequences of equiprobable tones. In 3 experiments including 82 participants overall, we found that neural responses measured at different latencies after stimulus onset were sensitive to frequency intervals computed over distinct timescales. Importantly, early responses were sensitive to a longer history of stimulation than later responses. To account for these results, we tested a model consisting of neural populations with frequency-specific but broad tuning that undergo adaptation with exponential recovery. We found that the coexistence of neural populations with distinct recovery rates can explain our results. Furthermore, the adaptation bandwidth of these populations depended on spectral context\textemdash it was wider when the stimulation sequence had a wider frequency range. Our results provide electrophysiological evidence as well as a possible mechanistic explanation for dynamic and multiscale context-dependent auditory processing in the human cortex.},
  file = {/Users/xzfang/Zotero/storage/M9LAILZP/Regev et al. - 2021 - Context Sensitivity across Multiple Time scales wi.pdf;/Users/xzfang/Zotero/storage/XBMJQUP9/6324862.html}
}

@misc{regev_highlevel_2021,
  title = {High-Level Language Brain Regions Are Sensitive to Sub-Lexical Regularities},
  author = {Regev, Tamar I. and Affourtit, Josef and Chen, Xuanyi and Schipper, Abigail E. and Bergen, Leon and Mahowald, Kyle and Fedorenko, Evelina},
  year = {2021},
  month = jun,
  pages = {2021.06.11.447786},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.11.447786},
  abstract = {A network of left frontal and temporal brain regions supports `high-level' language processing\textemdash{} including the processing of word meanings, as well as word-combinatorial processing\textemdash across presentation modalities. This `core' language network has been argued to store our knowledge of words and constructions as well as constraints on how those combine to form sentences. However, our linguistic knowledge additionally includes information about sounds (phonemes) and how they combine to form clusters, syllables, and words. Is this knowledge of phoneme combinatorics also represented in these language regions? Across five fMRI experiments, we investigated the sensitivity of high-level language processing brain regions to sub-lexical linguistic sound patterns by examining responses to diverse nonwords\textemdash sequences of sounds/letters that do not constitute real words (e.g., punes, silory, flope). We establish robust responses in the language network to visually (Experiment 1a, n=605) and auditorily (Experiments 1b, n=12, and 1c, n=13) presented nonwords relative to baseline. In Experiment 2 (n=16), we find stronger responses to nonwords that obey the phoneme-combinatorial constraints of English. Finally, in Experiment 3 (n=14) and a post-hoc analysis of Experiment 2, we provide suggestive evidence that the responses in Experiments 1 and 2 are not due to the activation of real words that share some phonology with the nonwords. The results suggest that knowledge of phoneme combinatorics and representations of sub-lexical linguistic sound patterns are stored within the same fronto-temporal network that stores higher-level linguistic knowledge and supports word and sentence comprehension.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J8BNF87A/Regev et al. - 2021 - High-level language brain regions are sensitive to.pdf;/Users/xzfang/Zotero/storage/37ND5PUL/2021.06.11.447786v1.html}
}

@article{reimers_adobe_2007,
  title = {Adobe {{Flash}} as a Medium for Online Experimentation: A Test of Reaction Time Measurement Capabilities},
  shorttitle = {Adobe {{Flash}} as a Medium for Online Experimentation},
  author = {Reimers, Stian and Stewart, Neil},
  year = {2007},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {39},
  number = {3},
  pages = {365--370},
  issn = {1554-351X},
  doi = {10.3758/bf03193004},
  abstract = {Adobe Flash can be used to run complex psychological experiments over the Web. We examined the reliability of using Flash to measure reaction times (RTs) using a simple binary-choice task implemented both in Flash and in a Linux-based system known to record RTs with millisecond accuracy. Twenty-four participants were tested in the laboratory using both implementations; they also completed the Flash version on computers of their own choice outside the lab. RTs from the trials run on Flash outside the lab were approximately 20 msec slower than those from trials run on Flash in the lab, which in turn were approximately 10 msec slower than RTs from the trials run on the Linux-based system (baseline condition). RT SDs were similar in all conditions, suggesting that although Flash may overestimate RTs slightly, it does not appear to add significant noise to the data recorded.},
  langid = {english},
  pmid = {17958146},
  keywords = {Aptitude,Humans,Internet,Psychological Tests,Reaction Time,Software},
  file = {/Users/xzfang/Zotero/storage/8YDRXUK2/Reimers and Stewart - 2007 - Adobe Flash as a medium for online experimentation.pdf}
}

@article{reinisch_lexically_2014,
  title = {Lexically {{Guided Phonetic Retuning}} of {{Foreign-Accented Speech}} and {{Its Generalization}}},
  author = {Reinisch, Eva and Holt, Lori L.},
  year = {2014},
  month = apr,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {40},
  number = {2},
  pages = {539--555},
  issn = {0096-1523},
  doi = {10.1037/a0034409},
  abstract = {Listeners use lexical knowledge to retune phoneme categories. When hearing an ambiguous sound between /s/ and /f/ in lexically unambiguous contexts such as gira[s/f], listeners learn to interpret the sound as /f/ because gira[f] is a real word and gira[s] is not. Later, they apply this learning even in lexically ambiguous contexts (perceiving knife rather than nice). Although such retuning could help listeners adapt to foreign-accented speech, research has focused on single phonetic contrasts artificially manipulated to create ambiguous sounds; however, accented speech varies along many dimensions. It is therefore unclear whether analogies to adaptation to accented speech are warranted. In the present studies, the to-be-adapted ambiguous sound was embedded in a global foreign accent. In addition, conditions of cross-speaker generalization were tested with focus on the extent to which perceptual similarity between 2 speakers' fricatives is a condition for generalization to occur. Results showed that listeners retune phoneme categories manipulated within the context of a global foreign accent, and that they generalize this short-term learning to the perception of phonemes from previously unheard speakers. However, generalization was observed only when exposure and test speakers' fricatives were sampled across a similar perceptual space.},
  pmcid = {PMC3962813},
  pmid = {24059846},
  file = {/Users/xzfang/Zotero/storage/6BHI78EX/Reinisch and Holt - 2014 - Lexically Guided Phonetic Retuning of Foreign-Acce.pdf}
}

@article{reinisch_uptake_2013,
  title = {The Uptake of Spectral and Temporal Cues in Vowel Perception Is Rapidly Influenced by Context},
  author = {Reinisch, Eva and Sjerps, Matthias J.},
  year = {2013},
  month = mar,
  journal = {Journal of Phonetics},
  volume = {41},
  number = {2},
  pages = {101--116},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2013.01.002},
  abstract = {Speech perception is dependent on auditory information within phonemes such as spectral or temporal cues. The perception of those cues, however, is affected by auditory information in surrounding context (e.g., a fast context sentence can make a target vowel sound subjectively longer). In a two-by-two design the current experiments investigated when these different factors influence vowel perception. Dutch listeners categorized minimal word pairs such as /tÉ‘k/\textendash/ta{$\Elzlmrk$}k/ (``branch''\textendash ``task'') embedded in a context sentence. Critically, the Dutch /É‘/\textendash/a{$\Elzlmrk$}/ contrast is cued by spectral and temporal information. We varied the second formant (F2) frequencies and durations of the target vowels. Independently, we also varied the F2 and duration of all segments in the context sentence. The timecourse of cue uptake on the targets was measured in a printed-word eye-tracking paradigm. Results show that the uptake of spectral cues slightly precedes the uptake of temporal cues. Furthermore, acoustic manipulations of the context sentences influenced the uptake of cues in the target vowel immediately. That is, listeners did not need additional time to integrate spectral or temporal cues of a target sound with auditory information in the context. These findings argue for an early locus of contextual influences in speech perception.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NA3TJIHA/Reinisch and Sjerps - 2013 - The uptake of spectral and temporal cues in vowel .pdf;/Users/xzfang/Zotero/storage/65K8DGMA/S0095447013000053.html}
}

@article{remez_auditoryphonetic_2011,
  title = {Auditory-Phonetic Projection and Lexical Structure in the Recognition of Sine-Wave Words.},
  author = {Remez, Robert E. and Dubowski, Kathryn R. and Broder, Robin S. and Davids, Morgana L. and Grossman, Yael S. and Moskalenko, Marina and Pardo, Jennifer S. and Hasbun, Sara Maria},
  year = {2011},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {37},
  number = {3},
  pages = {968--977},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/a0020734},
  abstract = {Speech remains intelligible despite the elimination of canonical acoustic correlates of phonemes from the spectrum. A portion of this perceptual flexibility can be attributed to modulation sensitivity in the auditory-to-phonetic projection, although signal-independent properties of lexical neighborhoods also affect intelligibility in utterances composed of words. Three tests were conducted to estimate the effects of exposure to natural and sine-wave samples of speech in this kind of perceptual versatility. First, sine-wave versions of the easy and hard word sets were created, modeled on the speech samples of a single talker. The performance difference in recognition of easy and hard words was used to index the perceptual reliance on signal-independent properties of lexical contrasts. Second, several kinds of exposure produced familiarity with an aspect of sine-wave speech: (a) sine-wave sentences modeled on the same talker; (b) sine-wave sentences modeled on a different talker, to create familiarity with a sine-wave carrier; and (c) natural sentences spoken by the same talker, to create familiarity with the idiolect expressed in the sine-wave words. Recognition performance with both easy and hard sine-wave words improved after exposure only to sine-wave sentences modeled on the same talker. Third, a control test showed that signal-independent uncertainty is a plausible cause of differences in recognition of easy and hard sine-wave words. The conditions of beneficial exposure reveal the specificity of attention underlying versatility in speech perception.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5WD3VYHL/Remez et al. - 2011 - Auditory-phonetic projection and lexical structure.pdf}
}

@article{remez_shortterm_2018,
  title = {Short-Term Perceptual Tuning to Talker Characteristics},
  author = {Remez, Robert E. and Thomas, Emily F. and Crank, Aislinn T. and Kostro, Katrina B. and Cheimets, Chloe B. and Pardo, Jennifer S.},
  year = {2018},
  month = oct,
  journal = {Language, Cognition and Neuroscience},
  volume = {33},
  number = {9},
  pages = {1083--1091},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2018.1442580},
  abstract = {When a listener encounters an unfamiliar talker, the ensuing perceptual accommodation to the unique characteristics of the talker has two aspects: (1) the listener assesses acoustic characteristics of speech to resolve the properties of the talker's sound production; and, (2) the listener appraises the talker's idiolect, subphonemic phonetic properties that compose the finest grain of linguistic production. A new study controlled a listener's exposure to determine whether the perceptual benefit rests on specific segmental experience. Effects of sentence exposure were measured using a spoken word identification task of Easy words (likely words drawn from sparse neighbourhoods of less likely words) and Hard words (less likely words drawn from dense neighbourhoods of more likely words). Recognition of words was facilitated by exposure to voiced obstruent consonants. Overall, these findings indicate that talker-specific perceptual tuning might depend more on exposure to phonemically marked consonants than to exposure distributed across the phoneme inventory.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HWX673RQ/Remez et al. - 2018 - Short-term perceptual tuning to talker characteris.pdf}
}

@article{remez_speech_1981,
  title = {Speech Perception without Traditional Speech Cues},
  author = {Remez, R. E. and Rubin, P. E. and Pisoni, D. B. and Carrell, T. D.},
  year = {1981},
  month = may,
  journal = {Science},
  volume = {212},
  number = {4497},
  pages = {947--949},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7233191},
  abstract = {A three-tone sinusoidal replica of a naturally produced utterance was identified by listeners, despite the readily apparent unnatural speech quality of the signal. The time-varying properties of these highly artificial acoustic signals are apparently sufficient to support perception of the linguistic message in the absence of traditional acoustic cues for phonetic segments.},
  chapter = {Reports},
  copyright = {\textcopyright{} 1981},
  langid = {english},
  pmid = {7233191},
  file = {/Users/xzfang/Zotero/storage/UKXWU2LH/Remez et al. - 1981 - Speech perception without traditional speech cues.pdf;/Users/xzfang/Zotero/storage/NE9AMTEB/947.html}
}

@article{remez_talker_1997,
  title = {Talker Identification Based on Phonetic Information},
  author = {Remez, Robert E. and Fellowes, Jennifer M. and Rubin, Philip E.},
  year = {1997},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {23},
  number = {3},
  pages = {651--666},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.23.3.651},
  abstract = {Accounts of the identification of words and talkers commonly rely on different acoustic properties. To identify a word, a perceiver discards acoustic aspects of an utterance that are talker specific, forming an abstract representation of the linguistic message with which to probe a mental lexicon. To identify a talker, a perceiver discards acoustic aspects of an utterance specific to particular phonemes, creating a representation of voice quality with which to search for familiar talkers in long-term memory. In 3 experiments, sinewave replicas of natural speech sampled from 10 talkers eliminated natural voice quality while preserving idiosyncratic phonetic variation. Listeners identified the sinewave talkers without recourse to acoustic attributes of natural voice quality. This finding supports a revised description of speech perception in which the phonetic properties of utterances serve to identify both words and talkers. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Familiarity,Phonetics,Speech Characteristics,Speech Perception,Synthetic Speech,Voice},
  file = {/Users/xzfang/Zotero/storage/LM5MIY5W/Remez et al. - 1997 - Talker identification based on phonetic informatio.pdf;/Users/xzfang/Zotero/storage/R3AI5Q2D/1997-04590-005.html}
}

@misc{ren_sleep_2020,
  title = {Sleep {{Reduces}} the {{Semantic Coherence}} of {{Memory Recall}}: {{An Application}} of {{Latent Semantic Analysis}} to {{Investigate Memory Reconstruction}}},
  shorttitle = {Sleep {{Reduces}} the {{Semantic Coherence}} of {{Memory Recall}}},
  author = {Ren, Xueying and Coutanche, Marc N.},
  year = {2020},
  month = may,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/34np8},
  abstract = {Sleep is thought to help consolidate hippocampus-dependent memories by reactivating previously encoded neural representations, promoting both quantitative and qualitative changes in memory representations. However, the qualitative nature of changes to memory representations induced by sleep remains largely uncharacterized. In this study, we investigated how memories are reconstructed by hypothesizing that semantic coherence, defined as conceptual relatedness between statements of free recall texts and quantified using latent semantic analysis (LSA), is affected by post-encoding sleep. Short naturalistic videos of events featuring six animals were presented to 115 participants that were randomly assigned to either 12- or 24-hour delay groups featuring sleep or wakefulness. The semantic coherence of participants' free recall responses was analyzed to test for an effect of sleep on semantic coherence between adjacent free recalled statements, and overall. The presence of sleep reduced both forms of coherence, compared to wakefulness, supporting the notion that sleep-dependent consolidation qualitatively changes the features of reconstructed memory representations by reducing semantic coherence.},
  keywords = {Cognitive Psychology,Memory,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/PYR4VJPQ/Ren and Coutanche - 2020 - Sleep Reduces the Semantic Coherence of Memory Rec.pdf}
}

@incollection{renzi_mental_2013,
  title = {Mental {{Imagery}} and {{Blindness}}},
  booktitle = {Multisensory {{Imagery}}},
  author = {Renzi, Chiara and Cattaneo, Zaira and Vecchi, Tomaso and Cornoldi, Cesare},
  editor = {Lacey, Simon and Lawson, Rebecca},
  year = {2013},
  pages = {115--130},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-5879-1_7},
  abstract = {Although imagery is traditionally thought to be inherently linked to visual perception, growing evidence shows that mental images can arise also from nonvisual modalities. Paradigmatic in this respect is the case of individuals born blind or that became blind soon after birth. In this chapter, we will review evidence pertaining to different aspects of cognition showing that blind individuals are able to generate analogical mental images based on haptic or auditory input. These \-representations allow blind individuals to perform efficiently in a variety of domains which require the use of imagery (such as memory, spatial and navigation abilities, numerical cognition), though exhibiting in some cases specific limitations or differences, which likely depend on the modality in which information is usually acquired in these individuals (e.g., via haptics and hearing) and the particular strategies employed.},
  isbn = {978-1-4614-5879-1},
  langid = {english},
  keywords = {Blindness,Imagery,Memory,Navigation,Spatial biases,Spatial cognition,Visual impairment},
  file = {/Users/xzfang/Zotero/storage/RGGP3G9N/Renzi et al. - 2013 - Mental Imagery and Blindness.pdf}
}

@article{reynolds_normalization_2009,
  title = {The {{Normalization Model}} of {{Attention}}},
  author = {Reynolds, John H. and Heeger, David J.},
  year = {2009},
  month = jan,
  journal = {Neuron},
  volume = {61},
  number = {2},
  pages = {168--185},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.01.002},
  abstract = {Attention has been found to have a wide variety of effects on the responses of neurons in visual cortex. We describe a model of attention that exhibits each of these different forms of attentional modulation, depending on the stimulus conditions and the spread (or selectivity) of the attention field in the model. The model helps reconcile proposals that have been taken to represent alternative theories of attention. We argue that the variety and complexity of the results reported in the literature emerge from the variety of empirical protocols that were used, such that the results observed in any one experiment depended on the stimulus conditions and the subject's attentional strategy, a notion that we define precisely in terms of the attention field in the model, but that has not typically been completely under experimental control.},
  pmcid = {PMC2752446},
  pmid = {19186161},
  file = {/Users/xzfang/Zotero/storage/FZDQQZFE/Reynolds and Heeger - 2009 - The Normalization Model of Attention.pdf}
}

@article{ricci_samedifferent_2021,
  title = {Same-Different Conceptualization: A Machine Vision Perspective},
  shorttitle = {Same-Different Conceptualization},
  author = {Ricci, Matthew and Cad{\`e}ne, R{\'e}mi and Serre, Thomas},
  year = {2021},
  month = feb,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Same-Different Conceptualization},
  volume = {37},
  pages = {47--55},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2020.08.008},
  abstract = {The goal of this review is to bring together material from cognitive psychology with recent machine vision studies to identify plausible neural mechanisms for visual same-different discrimination and relational understanding. We highlight how developments in the study of artificial neural networks provide computational evidence implicating attention and working memory in the ascertaining of visual relations, including same-different relations. We review some recent attempts to incorporate these mechanisms into flexible models of visual reasoning. Particular attention is given to recent models jointly trained on visual and linguistic information. These recent systems are promising, but they still fall short of the biological standard in several ways, which we outline in a final section.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VZCWHR5V/S2352154620301352.html}
}

@article{richardson_reduced_2020,
  title = {Reduced Neural Selectivity for Mental States in Deaf Children with Delayed Exposure to Sign Language},
  author = {Richardson, Hilary and {Koster-Hale}, Jorie and Caselli, Naomi and Magid, Rachel and Benedict, Rachel and Olson, Halie and Pyers, Jennie and Saxe, Rebecca},
  year = {2020},
  month = jun,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3246},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17004-y},
  abstract = {Language provides a rich source of information about other people's thoughts and feelings. Consequently, delayed access to language may influence conceptual development in Theory of Mind (ToM). We use functional magnetic resonance imaging and behavioral tasks to study ToM development in child (n\,=\,33, 4\textendash 12 years old) and adult (n\,=\,36) fluent signers of American Sign Language (ASL), and characterize neural ToM responses during ASL and movie-viewing tasks. Participants include deaf children whose first exposure to ASL was delayed up to 7 years (n\,=\,12). Neural responses to ToM stories (specifically, selectivity of the right temporo-parietal junction) in these children resembles responses previously observed in young children, who have similar linguistic experience, rather than those in age-matched native-signing children, who have similar biological maturation. Early linguistic experience may facilitate ToM development, via the development of a selective brain region for ToM.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Cognitive neuroscience,Psychology},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cognitive neuroscience;Psychology Subject\_term\_id: cognitive-neuroscience;psychology},
  file = {/Users/xzfang/Zotero/storage/VGFJSKEZ/Richardson et al. - 2020 - Reduced neural selectivity for mental states in de.pdf;/Users/xzfang/Zotero/storage/VG2DBLY8/s41467-020-17004-y.html}
}

@article{richardson-klavehn_measures_,
  title = {Measures of {{Memory}}},
  author = {{Richardson-Klavehn}, A and Bjork, R A},
  pages = {71},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/K77X5KLP/Richardson-Klavehn and Bjork - Measures of Memory.pdf}
}

@article{riecke_hearing_2009,
  title = {Hearing {{Illusory Sounds}} in {{Noise}}: {{The Timing}} of {{Sensory-Perceptual Transformations}} in {{Auditory Cortex}}},
  shorttitle = {Hearing {{Illusory Sounds}} in {{Noise}}},
  author = {Riecke, Lars and Esposito, Fabrizio and Bonte, Milene and Formisano, Elia},
  year = {2009},
  month = nov,
  journal = {Neuron},
  volume = {64},
  number = {4},
  pages = {550--561},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.10.016},
  abstract = {Constructive mechanisms in the auditory system may restore a fragmented sound when a gap in this sound is rendered inaudible by noise to yield a continuity illusion. Using combined psychoacoustic and electroencephalography experiments in humans, we found that the sensory-perceptual mechanisms that enable restoration suppress auditory cortical encoding of gaps in interrupted sounds. When physically interrupted tones are perceptually restored, stimulus-evoked synchronization of cortical oscillations at {$\sim$}4 Hz is suppressed as if physically uninterrupted sounds were encoded. The restoration-specific suppression is induced most strongly in primary-like regions in the right auditory cortex during illusorily filled gaps and also shortly before and after these gaps. Our results reveal that spontaneous modulations in slow evoked auditory cortical oscillations that are involved in encoding acoustic boundaries may determine the perceived continuity of sounds in noise. Such fluctuations could facilitate stable hearing of fragmented sounds in natural environments.},
  langid = {english},
  keywords = {sysneuro},
  file = {/Users/xzfang/Zotero/storage/R8G8QUNT/Riecke et al. - 2009 - Hearing Illusory Sounds in Noise The Timing of Se.pdf;/Users/xzfang/Zotero/storage/WMV5YPHC/S0896627309008459.html}
}

@article{riedl_computational_2016,
  title = {Computational {{Narrative Intelligence}}: {{A Human-Centered Goal}} for {{Artificial Intelligence}}},
  shorttitle = {Computational {{Narrative Intelligence}}},
  author = {Riedl, Mark O.},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.06484 [cs]},
  eprint = {1602.06484},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Narrative intelligence is the ability to craft, tell, understand, and respond affectively to stories. We argue that instilling artificial intelligences with computational narrative intelligence affords a number of applications beneficial to humans. We lay out some of the machine learning challenges necessary to solve to achieve computational narrative intelligence. Finally, we argue that computational narrative is a practical step towards machine enculturation, the teaching of sociocultural values to machines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/xzfang/Zotero/storage/ME22N5HK/Riedl - 2016 - Computational Narrative Intelligence A Human-Cent.pdf}
}

@article{rikhye_thalamic_2018,
  title = {Thalamic Regulation of Switching between Cortical Representations Enables Cognitive Flexibility},
  author = {Rikhye, Rajeev V. and Gilra, Aditya and Halassa, Michael M.},
  year = {2018},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {12},
  pages = {1753--1763},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0269-z},
  abstract = {Interactions between the prefrontal cortex (PFC) and mediodorsal thalamus are critical for cognitive flexibility, yet the underlying computations are unknown. To investigate frontothalamic substrates of cognitive flexibility, we developed a behavioral task in which mice switched between different sets of learned cues that guided attention toward either visual or auditory targets. We found that PFC responses reflected both the individual cues and their meaning as task rules, indicating a hierarchical cue-to-rule transformation. Conversely, mediodorsal thalamus responses reflected the statistical regularity of cue presentation and were required for switching between such experimentally specified cueing contexts. A subset of these thalamic responses sustained context-relevant PFC representations, while another suppressed the context-irrelevant ones. Through modeling and experimental validation, we find that thalamic-mediated suppression may not only reduce PFC representational interference but could also preserve unused cortical traces for future use. Overall, our study provides a computational foundation for thalamic engagement in cognitive flexibility.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CFMIAU55/Rikhye et al. - 2018 - Thalamic regulation of switching between cortical .pdf;/Users/xzfang/Zotero/storage/QYPXJDY4/s41593-018-0269-z.html}
}

@article{rimmele_proactive_2018,
  title = {Proactive {{Sensing}} of {{Periodic}} and {{Aperiodic Auditory Patterns}}},
  author = {Rimmele, Johanna M. and Morillon, Benjamin and Poeppel, David and Arnal, Luc H.},
  year = {2018},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  series = {Special {{Issue}}: {{Time}} in the {{Brain}}},
  volume = {22},
  number = {10},
  pages = {870--882},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2018.08.003},
  abstract = {The ability to predict when something will happen facilitates sensory processing and the ensuing computations. Building on the observation that neural activity entrains to periodic stimulation, leading neurophysiological models imply that temporal predictions rely on oscillatory entrainment. Although they provide a sufficient solution to predict periodic regularities, these models are challenged by a series of findings that question their suitability to account for temporal predictions based on aperiodic regularities. Aiming for a more comprehensive model of how the brain anticipates `when' in auditory contexts, we emphasize the capacity of motor and higher-order top-down systems to prepare sensory processing in a proactive and temporally flexible manner. Focusing on speech processing, we illustrate how this framework leads to new hypotheses.},
  langid = {english},
  keywords = {auditory perception,entrainment,motor,oscillation,prediction,speech},
  file = {/Users/xzfang/Desktop/Archive/rimmele_proactive_2018.pdf}
}

@article{roach_eventrelated_2008,
  title = {Event-{{Related EEG Time-Frequency Analysis}}: {{An Overview}} of {{Measures}} and {{An Analysis}} of {{Early Gamma Band Phase Locking}} in {{Schizophrenia}}},
  shorttitle = {Event-{{Related EEG Time-Frequency Analysis}}},
  author = {Roach, Brian J. and Mathalon, Daniel H.},
  year = {2008},
  month = sep,
  journal = {Schizophrenia Bulletin},
  volume = {34},
  number = {5},
  pages = {907--926},
  issn = {0586-7614},
  doi = {10.1093/schbul/sbn093},
  abstract = {An increasing number of schizophrenia studies have been examining electroencephalography (EEG) data using time-frequency analysis, documenting illness-related abnormalities in neuronal oscillations and their synchronization, particularly in the gamma band. In this article, we review common methods of spectral decomposition of EEG, time-frequency analyses, types of measures that separately quantify magnitude and phase information from the EEG, and the influence of parameter choices on the analysis results. We then compare the degree of phase locking (ie, phase-locking factor) of the gamma band (36\textendash 50 Hz) response evoked about 50 milliseconds following the presentation of standard tones in 22 healthy controls and 21 medicated patients with schizophrenia. These tones were presented as part of an auditory oddball task performed by subjects while EEG was recorded from their scalps. The results showed prominent gamma band phase locking at frontal electrodes between 20 and 60 milliseconds following tone onset in healthy controls that was significantly reduced in patients with schizophrenia (P\,=\,.03). The finding suggests that the early-evoked gamma band response to auditory stimuli is deficiently synchronized in schizophrenia. We discuss the results in terms of pathophysiological mechanisms compromising event-related gamma phase synchrony in schizophrenia and further attempt to reconcile this finding with prior studies that failed to find this effect.},
  pmcid = {PMC2632478},
  pmid = {18684772},
  file = {/Users/xzfang/Zotero/storage/3Q6IGL7S/Roach and Mathalon - 2008 - Event-Related EEG Time-Frequency Analysis An Over.pdf}
}

@article{roark_neural_2022,
  title = {A Neural Network Model of the Effect of Prior Experience with Regularities on Subsequent Category Learning},
  author = {Roark, Casey L. and Plaut, David C. and Holt, Lori L.},
  year = {2022},
  month = may,
  journal = {Cognition},
  volume = {222},
  pages = {104997},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104997},
  abstract = {Categories are often structured by the similarities of instances within the category defined across dimensions or features. Researchers typically assume that there is a direct, linear relationship between the physical input dimensions across which category exemplars are defined and the psychological representation of these dimensions. However, this assumption is not always warranted. Through a set of simulations, we demonstrate that the psychological representations of input dimensions developed through long-term prior experience can place very strong constraints on category learning. We compare the model's behavior to auditory, visual, and cross-modal human category learning and make conclusions regarding the nature of the psychological representations of the dimensions in those studies. These simulations support the conclusion that the nature of psychological representations of input dimensions is a critical aspect to understanding the mechanisms underlying category learning.},
  langid = {english},
  keywords = {Category learning,Neural network,Perception,Statistical regularities},
  file = {/Users/xzfang/Zotero/storage/SHSVPKQU/Roark et al. - 2022 - A neural network model of the effect of prior expe.pdf;/Users/xzfang/Zotero/storage/7MJKCRFL/S0010027721004200.html}
}

@article{roark_perceptual_2019,
  title = {Perceptual Dimensions Influence Auditory Category Learning},
  author = {Roark, Casey L. and Holt, Lori L.},
  year = {2019},
  month = may,
  journal = {Attention, perception \& psychophysics},
  volume = {81},
  number = {4},
  pages = {912--926},
  issn = {1943-3921},
  doi = {10.3758/s13414-019-01688-6},
  abstract = {Human category learning appears to be supported by dual learning systems. Previous research indicates the engagement of distinct neural systems in learning categories that require selective attention to dimensions versus those that require integration across dimensions. This evidence has largely come from studies of learning across perceptually separable visual dimensions, but recent research has applied dual systems models to understanding auditory and speech categorization. Since differential engagement of the dual learning systems is closely related to selective attention to input dimensions, it may be important that acoustic dimensions are quite often perceptually integral and difficult to attend to selectively. We investigated this issue across artificial auditory categories defined by center frequency and modulation frequency acoustic dimensions. Learners demonstrated a bias to integrate across the dimensions, rather than to selectively attend and the bias specifically reflected a positive correlation between the dimensions. Further, we found that the acoustic dimensions did not equivalently contribute to categorization decisions. These results demonstrate the need to reconsider the assumption that the orthogonal input dimensions used in designing an experiment are indeed orthogonal in perceptual space as there are important implications for category learning.},
  pmcid = {PMC6616009},
  pmid = {30761504},
  file = {/Users/xzfang/Zotero/storage/LV3UNK5Y/Roark and Holt - 2019 - Perceptual dimensions influence auditory category .pdf}
}

@article{roark_probabilistic_2001,
  title = {Probabilistic {{Top-Down Parsing}} and {{Language Modeling}}},
  author = {Roark, Brian},
  year = {2001},
  journal = {Computational Linguistics},
  volume = {27},
  number = {2},
  pages = {249--276},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  doi = {10.1162/089120101750300526},
  file = {/Users/xzfang/Zotero/storage/76D6QYT8/Roark - 2001 - Probabilistic Top-Down Parsing and Language Modeli.pdf}
}

@misc{roark_statistical_2020,
  title = {Statistical Learning Does Not Overrule Perceptual Priors during Category Learning},
  author = {Roark, Casey L. and Holt, Lori L.},
  year = {2020},
  month = oct,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/sdf7y},
  abstract = {Cognitive systems face a constant tension of maintaining existing representations that have been fine-tuned to long-term input regularities and adapting representations to meet the needs of short-term input that may deviate from long-term norms. Systems must balance the stability of long-term representations with plasticity to accommodate novel contexts. We investigated the interaction between perceptual biases or priors acquired across the long-term and sensitivity to statistical regularities introduced in the short-term. Participants were first passively exposed to short-term acoustic regularities and then learned categories in a supervised training task that either conflicted or aligned with long-term perceptual priors. We found that the long-term priors had robust and pervasive impact on categorization behavior. In contrast, behavior was not influenced by the nature of the short-term passive exposure. These results demonstrate that perceptual priors place strong constraints on the course of learning and that short-term passive exposure to acoustic regularities has limited impact on directing subsequent category learning.},
  langid = {american},
  keywords = {audition,Audition,category learning,Cognitive Psychology,Concepts and Categories,Learning,Perception,perceptual priors,Social and Behavioral Sciences,statistical learning,statistical regularities},
  file = {/Users/xzfang/Zotero/storage/EBPNJECV/Roark and Holt - 2020 - Statistical learning does not overrule perceptual .pdf}
}

@article{roe_visual_1992,
  title = {Visual Projections Routed to the Auditory Pathway in Ferrets: Receptive Fields of Visual Neurons in Primary Auditory Cortex},
  shorttitle = {Visual Projections Routed to the Auditory Pathway in Ferrets},
  author = {Roe, Aw and Pallas, Sl and Kwon, Yh and Sur, M},
  year = {1992},
  month = sep,
  journal = {The Journal of Neuroscience},
  volume = {12},
  number = {9},
  pages = {3651--3664},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.12-09-03651.1992},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XI9D5ZCD/Roe et al. - 1992 - Visual projections routed to the auditory pathway .pdf}
}

@article{roeske_categorical_2020,
  title = {Categorical {{Rhythms Are Shared}} between {{Songbirds}} and {{Humans}}},
  author = {Roeske, Tina C. and Tchernichovski, Ofer and Poeppel, David and Jacoby, Nori},
  year = {2020},
  month = sep,
  journal = {Current Biology},
  volume = {30},
  number = {18},
  pages = {3544-3555.e6},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2020.06.072},
  langid = {english},
  pmid = {32707062},
  keywords = {bio-musicology,birdsong,categorical rhythm,cross-cultural comparison,finger tapping,inter-species comparison,learned vocalizations,music,rhythm production},
  file = {/Users/xzfang/Zotero/storage/UNN8WS83/S0960-9822(20)30924-6.html}
}

@article{roettger_evidential_2019,
  title = {Evidential {{Strength}} of {{Intonational Cues}} and {{Rational Adaptation}} to ({{Un-}}){{Reliable Intonation}}},
  author = {Roettger, Timo B. and Franke, Michael},
  year = {2019},
  month = jul,
  journal = {Cognitive Science},
  volume = {43},
  number = {7},
  pages = {e12745},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0364-0213},
  doi = {10.1111/cogs.12745},
  abstract = {Abstract Intonation plays an integral role in comprehending spoken language. Listeners can rapidly integrate intonational information to predictively map a given pitch accent onto the speaker's likely referential intentions. We use mouse tracking to investigate two questions: (a) how listeners draw predictive inferences based on information from intonation? and (b) how listeners adapt their online interpretation of intonational cues when these are reliable or unreliable? We formulate a novel Bayesian model of rational predictive cue integration and explore predictions derived under a concrete linking hypothesis relating a quantitative notion of evidential strength of a cue to the moment in time, relative to the unfolding speech signal, at which mouse trajectories turn towards the eventually selected option. In order to capture rational belief updates after concrete observations of a speaker's behavior, we formulate and explore an extension of this model that includes the listener's hierarchical beliefs about the speaker's likely production behavior. Our results are compatible with the assumption that listeners rapidly and rationally integrate all available intonational information, that they expect reliable intonational information initially, and that they adapt these initial expectations gradually during exposition to unreliable input. All materials, data, and scripts can be retrieved here: https://osf.io/dnbuk/},
  keywords = {Intonation,Mouse tracking,Probabilistic modeling,Prosody,Rational predictive processing,Speech adaptation},
  file = {/Users/xzfang/Zotero/storage/Q76JR38L/Roettger and Franke - 2019 - Evidential Strength of Intonational Cues and Ratio.pdf;/Users/xzfang/Zotero/storage/GDJW5J5K/cogs.html}
}

@article{rolfs_visual_2013,
  title = {Visual {{Adaptation}} of the {{Perception}} of {{Causality}}},
  author = {Rolfs, Martin and Dambacher, Michael and Cavanagh, Patrick},
  year = {2013},
  month = feb,
  journal = {Current Biology},
  volume = {23},
  number = {3},
  pages = {250--254},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2012.12.017},
  abstract = {We easily recover the causal properties of visual events, enabling us to understand and predict changes in the physical world. We see a tennis racket hitting a ball and sense that it caused the ball to fly over the net; we may also have an eerie but equally compelling experience of causality if the streetlights turn on just as we slam our car's door. Both perceptual [1] and cognitive [2] processes have been proposed to explain these spontaneous inferences, but without decisive evidence one way or the other, the question remains wide open [3, 4, 5, 6, 7, 8]. Here, we address this long-standing debate using visual adaptation\textemdash a powerful tool to uncover neural populations that specialize in the analysis of specific visual features [9, 10, 11, 12]. After prolonged viewing of causal collision events called ``launches'' [1], subsequently viewed events were judged more often as noncausal. These negative aftereffects of exposure to collisions are spatially localized in retinotopic coordinates, the reference frame shared by the retina and visual cortex. They are not explained by adaptation to other stimulus features and reveal visual routines in retinotopic cortex that detect and adapt to cause and effect in simple collision stimuli.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FAQFZEW7/Rolfs et al. - 2013 - Visual Adaptation of the Perception of Causality.pdf;/Users/xzfang/Zotero/storage/YI33RYSC/S096098221201490X.html}
}

@article{rolls_sparseness_1995,
  title = {Sparseness of the Neuronal Representation of Stimuli in the Primate Temporal Visual Cortex},
  author = {Rolls, E. T. and Tovee, M. J.},
  year = {1995},
  month = feb,
  journal = {Journal of Neurophysiology},
  volume = {73},
  number = {2},
  pages = {713--726},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1995.73.2.713},
  abstract = {1. To analyze the selectivity and the sparseness of firing to visual stimuli of single neurons in the primate temporal cortical visual area, neuronal responses were measured to a set of 68 visual stimuli in macaques performing a visual fixation task. The population of neurons analyzed had responses that occurred primarily to faces. The stimuli included 23 faces, and 45 nonface images of real-world scenes, so that the function of this brain region could be analyzed when it was processing natural scenes. 2. The neurons were selected to meet the previously used criteria of face selectivity by responding more than twice as much to the optimal face as to the optimal nonface stimulus in the set. Application of information theoretic analyses to the responses of these neurons confirmed that their responses contained much more information about which of 20 face stimuli had been seen (on average 0.4 bits) than about which (of 20) nonface stimuli had been seen (on average 0.07 bits). 3. The sparseness of the representation of a scene or object provided by each of these neurons (which can be thought of as the proportion of stimuli to which the neuron responds, and which is fundamental to understanding the network operation of the system) can be defined as [formula: see text] where ri is the firing rate to the ith stimulus in the set of n stimuli. The sparseness has a maximal value of 1.0. It was found that the sparseness of the representation of the 68 stimuli by each neuron had an average across all neurons of 0.65. This indicates a rather distributed representation. 4. If the spontaneous firing rate was subtracted from the firing rate of the neuron to each stimulus, so that the changes of firing rate, i.e., the responses of the neurons, were used in the sparseness calculation, then the "response sparseness" had a lower value, with a mean of 0.33 for the population of neurons, or 0.60 if calculated over the set of faces. 5. Multidimensional scaling to produce a stimulus space represented by this population of neurons showed that the different faces were well separated in the space created, whereas the different nonface stimuli were grouped together in the space. 6. The information analyses and multidimensional scaling provided evidence that what was made explicit in the responses of these neurons was information about which face had been seen.(ABSTRACT TRUNCATED AT 400 WORDS)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4QV2AUF4/Rolls and Tovee - 1995 - Sparseness of the neuronal representation of stimu.pdf}
}

@article{romagnoli_antifungal_2010,
  title = {Antifungal Activity of Essential Oil from Fruits of {{Indian Cuminum}} Cyminum},
  author = {Romagnoli, Carlo and Andreotti, Elisa and Maietti, Silvia and Mahendra, Rai and Mares, Donatella},
  year = {2010},
  month = jul,
  journal = {Pharmaceutical Biology},
  volume = {48},
  number = {7},
  pages = {834--838},
  issn = {1388-0209},
  doi = {10.3109/13880200903283715},
  abstract = {The essential oil of fruits of Cuminum cyminum L. (Apiaceae), from India, was analyzed by GC and GC-MS, and its antifungal activity was tested on dermatophytes and phytopathogens, fungi, yeasts and some new Aspergilli. The most abundant components were cumin aldehyde, pinenes, and p-cymene, and a fraction of oxygenate compounds such as alcohol and epoxides. Because of the large amount of the highly volatile components in the cumin extract, we used a modified recent technique to evaluate the antifungal activity only of the volatile parts at doses from 5 to 20 \textmu L of pure essential oil. Antifungal testing showed that Cuminum cyminum is active in general on all fungi but in particular on the dermatophytes, where Trichophyton rubrum was the most inhibited fungus also at the lowest dose of 5 \textmu L. Less sensitive to treatment were the phytopathogens.},
  file = {/Users/xzfang/Zotero/storage/D7BAKJSD/Romagnoli et al. - 2010 - Antifungal activity of essential oil from fruits o.html}
}

@article{romei_occipital_2007,
  title = {Occipital {{Transcranial Magnetic Stimulation Has Opposing Effects}} on {{Visual}} and {{Auditory Stimulus Detection}}: {{Implications}} for {{Multisensory Interactions}}},
  shorttitle = {Occipital {{Transcranial Magnetic Stimulation Has Opposing Effects}} on {{Visual}} and {{Auditory Stimulus Detection}}},
  author = {Romei, Vincenzo and Murray, Micah M. and Merabet, Lotfi B. and Thut, Gregor},
  year = {2007},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {27},
  number = {43},
  pages = {11465--11472},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2827-07.2007},
  abstract = {Multisensory interactions occur early in time and in low-level cortical areas, including primary cortices. To test current models of early auditory\textendash visual (AV) convergence in unisensory visual brain areas, we studied the effect of transcranial magnetic stimulation (TMS) of visual cortex on behavioral responses to unisensory (auditory or visual) or multisensory (simultaneous auditory\textendash visual) stimulus presentation. Single-pulse TMS was applied over the occipital pole at short delays (30\textendash 150 ms) after external stimulus onset. Relative to TMS over a control site, reactions times (RTs) to unisensory visual stimuli were prolonged by TMS at 60\textendash 75 ms poststimulus onset (visual suppression effect), confirming stimulation of functional visual cortex. Conversely, RTs to unisensory auditory stimuli were significantly shortened when visual cortex was stimulated by TMS at the same delays (beneficial interaction effect of auditory stimulation and occipital TMS). No TMS-effect on RTs was observed for AV stimulation. The beneficial interaction effect of combined unisensory auditory and TMS-induced visual cortex stimulation matched and was correlated with the RT-facilitation after external multisensory AV stimulation without TMS, suggestive of multisensory interactions between the stimulus-evoked auditory and TMS-induced visual cortex activities. A follow-up experiment showed that auditory input enhances excitability within visual cortex itself (using phosphene-induction via TMS as a measure) over a similarly early time-window (75\textendash 120 ms). The collective data support a mechanism of early auditory\textendash visual interactions that is mediated by auditory-driven sensitivity changes in visual neurons that coincide in time with the initial volleys of visual input.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2007 Society for Neuroscience 0270-6474/07/2711465-08\$15.00/0},
  langid = {english},
  pmid = {17959789},
  keywords = {auditory,crossmodal,multisensory,primary visual cortex,TMS,transcranial magnetic stimulation,visual},
  file = {/Users/xzfang/Zotero/storage/YUPY5Y3C/Romei et al. - 2007 - Occipital Transcranial Magnetic Stimulation Has Op.pdf;/Users/xzfang/Zotero/storage/JJC9Y4H6/11465.html}
}

@article{ross_structured_2022,
  title = {A Structured {{ICA-based}} Process for Removing Auditory Evoked Potentials},
  author = {Ross, Jessica M. and Ozdemir, Recep A. and Lian, Shu Jing and Fried, Peter J. and Schmitt, Eva M. and Inouye, Sharon K. and {Pascual-Leone}, Alvaro and Shafi, Mouhsin M.},
  year = {2022},
  month = jan,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {1--19},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-05397-3},
  abstract = {Transcranial magnetic stimulation (TMS)-evoked potentials (TEPs), recorded using electroencephalography (EEG), reflect a combination of TMS-induced cortical activity and multi-sensory responses to TMS. The auditory evoked potential (AEP) is a high-amplitude sensory potential\textemdash evoked by the ``click'' sound produced by every TMS pulse\textemdash that can dominate the TEP and obscure observation of other neural components. The AEP is peripherally evoked and therefore should not be stimulation site specific. We address the problem of disentangling the peripherally evoked AEP of the TEP from components evoked by cortical stimulation and ask whether removal of AEP enables more accurate isolation of TEP. We hypothesized that isolation of the AEP using Independent Components Analysis (ICA) would reveal features that are stimulation site specific and unique individual features. In order to improve the effectiveness of ICA for removal of AEP from the TEP, and thus more clearly separate the transcranial-evoked and non-specific TMS-modulated potentials, we merged sham and active TMS datasets representing multiple stimulation conditions, removed the resulting AEP component, and evaluated performance across different sham protocols and clinical populations using reduction in Global and Local Mean Field Power (GMFP/LMFP) and cosine similarity analysis. We show that removing AEPs significantly reduced GMFP and LMFP in the post-stimulation TEP (14 to 400~ms), driven by time windows consistent with the N100 and P200 temporal characteristics of AEPs. Cosine similarity analysis supports that removing AEPs reduces TEP similarity between subjects and reduces TEP similarity between stimulation conditions. Similarity is reduced most in a mid-latency window consistent with the N100 time-course, but nevertheless remains high in this time window. Residual TEP in this window has a time-course and topography unique from AEPs, which follow-up exploratory analyses suggest could be a modulation in the alpha band that is not stimulation site specific but is unique to individual subject. We show, using two datasets and two implementations of sham, evidence in cortical topography, TEP time-course, GMFP/LMFP and cosine similarity analyses that this procedure is effective and conservative in removing the AEP from TEP, and may thus better isolate TMS-evoked activity. We show TEP remaining in early, mid and late latencies. The early response is site and subject specific. Later response may be consistent with TMS-modulated alpha activity that is not site specific but is unique to the individual. TEP remaining after removal of AEP is unique and can provide insight into TMS-evoked potentials and other modulated oscillatory dynamics.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Neuroscience,Physiology},
  file = {/Users/xzfang/Zotero/storage/AZYERPYE/Ross et al. - 2022 - A structured ICA-based process for removing audito.pdf;/Users/xzfang/Zotero/storage/ESYDSSG3/s41598-022-05397-3.html}
}

@article{roy_brainwide_2022,
  title = {Brain-Wide Mapping Reveals That Engrams for a Single Memory Are Distributed across Multiple Brain Regions},
  author = {Roy, Dheeraj S. and Park, Young-Gyun and Kim, Minyoung E. and Zhang, Ying and Ogawa, Sachie K. and DiNapoli, Nicholas and Gu, Xinyi and Cho, Jae H. and Choi, Heejin and Kamentsky, Lee and Martin, Jared and Mosto, Olivia and Aida, Tomomi and Chung, Kwanghun and Tonegawa, Susumu},
  year = {2022},
  month = apr,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {1799},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-29384-4},
  abstract = {Neuronal ensembles that hold specific memory (memory engrams) have been identified in the hippocampus, amygdala, or cortex. However, it has been hypothesized that engrams of a specific memory are distributed among multiple brain regions that are functionally connected, referred to as a unified engram complex. Here, we report a partial map of the engram complex for contextual fear conditioning memory by characterizing encoding activated neuronal ensembles in 247 regions using tissue phenotyping in mice. The mapping was aided by an engram index, which identified 117 cFos+ brain regions holding engrams with high probability, and brain-wide reactivation of these neuronal ensembles by recall. Optogenetic manipulation experiments revealed engram ensembles, many of which were functionally connected to hippocampal or amygdala engrams. Simultaneous chemogenetic reactivation of multiple engram ensembles conferred a greater level of memory recall than reactivation of a single engram ensemble, reflecting the natural memory recall process. Overall, our study supports the unified engram complex hypothesis for memory storage.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Cellular neuroscience,Fear conditioning,Long-term memory,Neural circuits},
  file = {/Users/xzfang/Zotero/storage/XGVCXAU7/Roy et al. - 2022 - Brain-wide mapping reveals that engrams for a sing.pdf;/Users/xzfang/Zotero/storage/4LJ2T5HH/s41467-022-29384-4.html}
}

@article{rue_approximate_2009,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  author = {Rue, H{\aa}vard and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {71},
  number = {2},
  pages = {319--392},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2008.00700.x},
  abstract = {Summary. Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
  langid = {english},
  keywords = {Approximate Bayesian inference,Gaussian Markov random fields,Generalized additive mixed models,Laplace approximation,Parallel computing,Sparse matrices,Structured additive regression models},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2008.00700.x},
  file = {/Users/xzfang/Zotero/storage/2D8LFCCB/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf;/Users/xzfang/Zotero/storage/G9D9QUIY/j.1467-9868.2008.00700.html}
}

@article{rugani_numberspace_2015,
  title = {Number-Space Mapping in the Newborn Chick Resembles Humans' Mental Number Line},
  author = {Rugani, Rosa and Vallortigara, Giorgio and Priftis, Konstantinos and Regolin, Lucia},
  year = {2015},
  month = jan,
  journal = {Science},
  volume = {347},
  number = {6221},
  pages = {534--536},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaa1379},
  abstract = {Even chicks may count from left to right For the most part, humans represent numbers across a mental number line, with smaller numbers on the left and larger numbers on the right. Some have argued that this is due to culture rather than being innate. Rugani et al., however, show that 3-day-old chicks share this representation of numbers, consistently seeking lower numbers to the left of a target and larger numbers to the right (see the Perspective by Brugger). These results suggest that there may be an innate spatial representation of numerical values that we share with other animals. Science, this issue p. 534; see also p. 477 Humans represent numbers along a mental number line (MNL), where smaller values are located on the left and larger on the right. The origin of the MNL and its connections with cultural experience are unclear: Pre-verbal infants and nonhuman species master a variety of numerical abilities, supporting the existence of evolutionary ancient precursor systems. In our experiments, 3-day-old domestic chicks, once familiarized with a target number (5), spontaneously associated a smaller number (2) with the left space and a larger number (8) with the right space. The same number (8), though, was associated with the left space when the target number was 20. Similarly to humans, chicks associate smaller numbers with the left space and larger numbers with the right space. Baby chicks also ``count'' from left to right. [Also see Perspective by Brugger] Baby chicks also ``count'' from left to right. [Also see Perspective by Brugger]},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  langid = {english},
  pmid = {25635096},
  file = {/Users/xzfang/Zotero/storage/7AECF4RC/Rugani et al. - 2015 - Number-space mapping in the newborn chick resemble.pdf;/Users/xzfang/Zotero/storage/TDXN5XAD/534.html}
}

@misc{ryskin_agreement_2021,
  title = {Agreement Errors Are Predicted by Rational Inference in Sentence Processing},
  author = {Ryskin, Rachel and Bergen, Leon and Gibson, Edward},
  year = {2021},
  month = oct,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/uaxsq},
  abstract = {People are able to understand language in challenging settings which often require them to correct for speaker errors, environmental noise, and perceptual unreliability.  To account for these abilities, it has recently been proposed that people are adapted to correct for noise during language comprehension, via rational Bayesian inference.  In the present research, we demonstrate that a rational noisy-channel framework for sentence comprehension can account for a well-known phenomenon\textemdash subject-verb agreement errors (e.g. The key to the cabinets are\ldots ). A series of experiments provides evidence that: a) agreement errors are associated with misrepresentations of the sentence consistent with noisy-channel inferences and b) agreement errors are rationally sensitive to environmental statistics and properties of the noise. These findings support the hypothesis that agreement errors in production result in part from a sentence comprehension mechanism that is adapted to understanding language in noisy environments.},
  langid = {american},
  keywords = {agreement errors,Linguistics,Morphology,noisy-channel,Psycholinguistics and Neurolinguistics,Social and Behavioral Sciences,Syntax},
  file = {/Users/xzfang/Zotero/storage/3GK2GJ27/Ryskin et al. - 2021 - Agreement errors are predicted by rational inferen.pdf}
}

@article{ryskin_comprehenders_2018,
  title = {Comprehenders Model the Nature of Noise in the Environment},
  author = {Ryskin, Rachel and Futrell, Richard and Kiran, Swathi and Gibson, Edward},
  year = {2018},
  month = dec,
  journal = {Cognition},
  volume = {181},
  pages = {141--150},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.08.018},
  abstract = {In everyday communication, speakers make errors and produce language in a noisy environment. Recent work suggests that comprehenders possess cognitive mechanisms for dealing with noise in the linguistic signal: a noisy-channel model. A key parameter of these models is the noise model: the comprehender's implicit model of how noise affects utterances before they are perceived. Here we examine this noise model in detail, asking whether comprehension behavior reflects a noise model that is adapted to context. We asked readers to correct sentences if they noticed errors, and manipulated context by including exposure sentences containing obvious deletions (A bystander was rescued by the fireman in the nick time.), insertions, exchanges, mixed errors, or no errors. On test sentences (The bat swung the player.), participants' corrections differed depending on the exposure condition. The results demonstrate that participants model specific types of errors and make inferences about the intentions of the speaker accordingly.},
  langid = {english},
  keywords = {Adaptation,Error correction,Noisy-channel,Rational inference,Sentence comprehension},
  file = {/Users/xzfang/Zotero/storage/9ZC7EDF8/Ryskin et al. - 2018 - Comprehenders model the nature of noise in the env.pdf;/Users/xzfang/Zotero/storage/4LJ3ZK89/S0010027718302245.html}
}

@article{ryskin_domaingeneral_2020,
  title = {Do Domain-General Executive Resources Play a Role in Linguistic Prediction? {{Re-evaluation}} of the Evidence and a Path Forward},
  shorttitle = {Do Domain-General Executive Resources Play a Role in Linguistic Prediction?},
  author = {Ryskin, Rachel and Levy, Roger P. and Fedorenko, Evelina},
  year = {2020},
  month = jan,
  journal = {Neuropsychologia},
  volume = {136},
  pages = {107258},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2019.107258},
  abstract = {Most current accounts of language comprehension agree on a role for prediction, but they disagree on the importance of domain-general executive resources in predictive behavior. In this opinion piece, we briefly review the evidence for linguistic prediction, and the findings that have been used to argue that prediction draws on domain-general executive resources. The most compelling evidence is an apparent reduction in predictive behavior during language comprehension in populations with lower executive resources, such as children, older adults, and second language (L2) learners. We propose that these between-population differences can be explained without invoking executive resources. Instead, differences in the quantity and kind of language experience that these populations bring to bear may affect the probability of engaging in predictive behavior, or simply make prediction effects more difficult to detect in paradigms designed for young adult native speakers. Thus, domain-specific prediction mechanisms remain a viable possibility. We discuss ways to further test accounts of linguistic prediction that do vs. do not require domain-general executive resources, using behavioral, computational, and brain imaging approaches.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2PQPQZT2/Ryskin et al. - 2020 - Do domain-general executive resources play a role .pdf;/Users/xzfang/Zotero/storage/6RCBJEM9/S0028393219303008.html}
}

@article{ryskin_erp_2021,
  title = {An {{ERP}} Index of Real-Time Error Correction within a Noisy-Channel Framework of Human Communication},
  author = {Ryskin, Rachel and Stearns, Laura and Bergen, Leon and Eddy, Marianna and Fedorenko, Evelina and Gibson, Edward},
  year = {2021},
  month = jul,
  journal = {Neuropsychologia},
  volume = {158},
  pages = {107855},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2021.107855},
  abstract = {Recent evidence suggests that language processing is well-adapted to noise in the input (e.g., spelling or speech errors, misreading or mishearing) and that comprehenders readily correct the input via rational inference over possible intended sentences given probable noise corruptions. In the current study, we probed the processing of noisy linguistic input, asking whether well-studied ERP components may serve as useful indices of this inferential process. In particular, we examined sentences where semantic violations could be attributed to noise\textemdash for example, in ``The storyteller could turn any incident into an amusing antidote'', where the implausible word ``antidote'' is orthographically and phonologically close to the intended ``anecdote''. We found that the processing of such sentences\textemdash where the probability that the message was corrupted by noise exceeds the probability that it was produced intentionally and perceived accurately\textemdash was associated with a reduced (less negative) N400 effect and an increased P600 effect, compared to semantic violations which are unlikely to be attributed to noise (``The storyteller could turn any incident into an amusing hearse''). Further, the magnitudes of these ERP effects were correlated with the probability that the comprehender retrieved a plausible alternative. This work thus adds to the growing body of literature that suggests that many aspects of language processing are optimized for dealing with noise in the input, and opens the door to electrophysiologic investigations of the computations that support the processing of imperfect input.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IZDC3JTE/Ryskin et al. - 2021 - An ERP index of real-time error correction within .pdf;/Users/xzfang/Zotero/storage/CC3FUVDL/S0028393221001068.html}
}

@article{ryskin_individualsubject_2020,
  title = {Individual-Subject Functional Localization Does Not Benefit {{ERP}} Analyses},
  author = {Ryskin, Rachel and Fedorenko, Evelina},
  year = {2020},
  month = oct,
  journal = {Poster at Society for the Neurobiology of Language}
}

@article{ryskin_information_2019,
  title = {Information {{Integration}} in {{Modulation}} of {{Pragmatic Inferences During Online Language Comprehension}}},
  author = {Ryskin, Rachel and Kurumada, Chigusa and Brown-Schmidt, Sarah},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {8},
  pages = {e12769},
  issn = {1551-6709},
  doi = {10.1111/cogs.12769},
  abstract = {Upon hearing a scalar adjective in a definite referring expression such as ``the big\ldots,'' listeners typically make anticipatory eye movements to an item in a contrast set, such as a big glass in the context of a smaller glass. Recent studies have suggested that this rapid, contrastive interpretation of scalar adjectives is malleable and calibrated to the speaker's pragmatic competence. In a series of eye-tracking experiments, we explore the nature of the evidence necessary for the modulation of pragmatic inferences in language comprehension, focusing on the complementary roles of top\textendash down information - (knowledge about the particular speaker's pragmatic competence) and bottom-up cues (distributional information about the use of scalar adjectives in the environment). We find that bottom-up evidence alone (e.g., the speaker says ``the big dog'' in a context with one dog), in large quantities, can be sufficient to trigger modulation of the listener's contrastive inferences, with or without top-down cues to support this adaptation. Further, these findings suggest that listeners track and flexibly combine multiple sources of information in service of efficient pragmatic communication.},
  copyright = {\textcopyright{} 2019 Cognitive Science Society, Inc},
  langid = {english},
  keywords = {Eye-tracking,Language comprehension,Pragmatics},
  file = {/Users/xzfang/Zotero/storage/ITYIN6EY/Ryskin et al. - 2019 - Information Integration in Modulation of Pragmatic.pdf;/Users/xzfang/Zotero/storage/ZVTFYMGC/cogs.html}
}

@article{ryskin_listeners_2016,
  title = {Listeners Use Speaker Identity to Access Representations of Spatial Perspective during Online Language Comprehension},
  author = {Ryskin, Rachel A. and Wang, Ranxiao Frances and {Brown-Schmidt}, Sarah},
  year = {2016},
  month = feb,
  journal = {Cognition},
  volume = {147},
  pages = {75--84},
  issn = {00100277},
  doi = {10.1016/j.cognition.2015.11.011},
  abstract = {Little is known about how listeners represent another person's spatial perspective during language processing (e.g., two people looking at a map from different angles). Can listeners use contextual cues such as speaker identity to access a representation of the interlocutor's spatial perspective? In two eye-tracking experiments, participants received auditory instructions to move objects around a screen from two randomly alternating spatial perspectives (45\textdegree{} vs. 315\textdegree{} or 135\textdegree{} vs. 225\textdegree{} rotations from the participant's viewpoint). Instructions were spoken either by one voice, where the speaker's perspective switched at random, or by two voices, where each speaker maintained one perspective. Analysis of participant eye-gaze showed that interpretation of the instructions improved when each viewpoint was associated with a different voice. These findings demonstrate that listeners can learn mappings between individual talkers and viewpoints, and use these mappings to guide online language processing.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MRLQ5IQE/Ryskin et al. - 2016 - Listeners use speaker identity to access represent.pdf}
}

@incollection{ryskin_many_2021,
  title = {The Many Timescales of Context in Language Processing},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Ryskin, Rachel and Fang, Xinzhu},
  year = {2021},
  month = oct,
  publisher = {{Academic Press}},
  doi = {10.1016/bs.plm.2021.08.001},
  abstract = {Language input is often noisy and ambiguous. Yet, humans are able to successfully communicate their thoughts to one another by leveraging contextual information to make inferences about the intended meaning of the input. Though there is broad agreement on the role and importance of context, the term is used so flexibly that it can be difficult to know what it refers to and how to make progress on studying it. Here, we propose that a key dimension which spans domains of context is the timescale over which the contextual information is sampled for the purpose of inferring meaning. We review the literature on context effects in language comprehension, organizing our discussion around a few relevant temporal windows, milliseconds, seconds, and minutes, and then propose that the individual's lifetime of language experience and historical time both constitute relevant contexts for language processing as well. We then discuss how construing of context in this way reveals gaps in the existing research landscape and propose new avenues of inquiry to address them.},
  langid = {english},
  keywords = {Context,Inference,Language,Language comprehension,Learning,Meaning,Memory,Pragmatics,Timescales}
}

@article{ryskin_talkerspecific_2020,
  title = {Talker-Specific Predictions during Language Processing},
  author = {Ryskin, Rachel and Ng, Shukhan and Mimnaugh, Katherine and {Brown-Schmidt}, Sarah and Federmeier, Kara D.},
  year = {2020},
  month = jul,
  journal = {Language, Cognition and Neuroscience},
  volume = {35},
  number = {6},
  pages = {797--812},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2019.1630654},
  abstract = {Language comprehension is shaped by world knowledge. After hearing about ``a farm animal,'' meanings of typical (``cow'') versus atypical exemplars (``ox'') are more accessible, as evidenced by N400 responses. Moreover, atypical exemplars elicit a larger post-N400 frontal positivity than typical and incongruous (``ivy'') exemplars, indexing the integration of unexpected information. Do listeners adapt this category knowledge to specific talkers? We first replicated typicality effects in the auditory modality. Then, we extended the design to a two-talker context: talkers alternated cueing (Bob: ``Susan, name a farm animal'') and answering (Susan: ``cow''). Critically, participants first heard interviews in which one talker revealed strong associations with atypical exemplars (Susan works on an ox farm). We observed increased frontal positivity to a typical exemplar (``cow'') said by Susan compared to Bob, indicating participants appreciated that the typical exemplar was atypical for Susan. These results suggest that comprehenders can tailor their expectations to the talker.},
  pmid = {33693050},
  keywords = {category typicality,Comprehension,ERP,prediction,semantics},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2019.1630654},
  file = {/Users/xzfang/Zotero/storage/JW5XJSDU/Ryskin et al. - 2020 - Talker-specific predictions during language proces.pdf;/Users/xzfang/Zotero/storage/CWNJSNA9/23273798.2019.html}
}

@article{sabour_dynamic_2017,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = nov,
  journal = {arXiv:1710.09829 [cs]},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xzfang/Zotero/storage/QSR6P8V3/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;/Users/xzfang/Zotero/storage/KKBTUTW9/1710.html}
}

@article{sadaghiani_functional_2015,
  title = {Functional {{Characterization}} of the {{Cingulo-Opercular Network}} in the {{Maintenance}} of {{Tonic Alertness}}},
  author = {Sadaghiani, Sepideh and D'Esposito, Mark},
  year = {2015},
  month = sep,
  journal = {Cerebral Cortex},
  volume = {25},
  number = {9},
  pages = {2763--2773},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhu072},
  abstract = {The complex processing architecture underlying attentional control requires delineation of the functional role of different control-related brain networks. A key component is the cingulo-opercular (CO) network composed of anterior insula/operculum, dorsal anterior cingulate cortex, and thalamus. Its function has been particularly difficult to characterize due to the network's pervasive activity and frequent co-activation with other control-related networks. We previously suggested this network to underlie intrinsically maintained tonic alertness. Here, we tested this hypothesis by separately manipulating the demand for selective attention and for tonic alertness in a two-factorial, continuous pitch discrimination paradigm. The 2 factors had independent behavioral effects. Functional imaging revealed that activity as well as functional connectivity in the CO network increased when the task required more tonic alertness. Conversely, heightened selective attention to pitch increased activity in the dorsal attention (DAT) network but not in the CO network. Across participants, performance accuracy showed dissociable correlation patterns with activity in the CO, DAT, and fronto-parietal (FP) control networks. These results support tonic alertness as a fundamental function of the CO network. They further the characterization of this function as the effortful process of maintaining cognitive faculties available for current processing requirements.},
  file = {/Users/xzfang/Zotero/storage/XFZWZ9QN/Sadaghiani and D'Esposito - 2015 - Functional Characterization of the Cingulo-Opercul.pdf;/Users/xzfang/Zotero/storage/TZ2USP7H/2926085.html}
}

@article{sadato_activation_1996,
  title = {Activation of the Primary Visual Cortex by {{Braille}} Reading in Blind Subjects},
  author = {Sadato, Norihiro and {Pascual-Leone}, Alvaro and Grafman, Jordan and Iba{\~n}ez, Vicente and Deiber, Marie-Pierre and Dold, George and Hallett, Mark},
  year = {1996},
  month = apr,
  journal = {Nature},
  volume = {380},
  number = {6574},
  pages = {526--528},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/380526a0},
  abstract = {PRIMARY visual cortex receives visual input from the eyes through the lateral geniculate nuclei, but is not known to receive input from other sensory modalities1. Its level of activity, both at rest and during auditory or tactile tasks, is higher in blind subjects than in normal controls2, suggesting that it can subserve non-visual functions; however, a direct effect of non-visual tasks on activation has not been demonstrated2\textendash 4. To determine whether the visual cortex receives input from the somatosensory system5\textendash 8, we used positron emission tomography (PET) to measure activation during tactile discrimination tasks in normal subjects and in Braille readers blinded in early life. Blind subjects showed activation of primary and secondary visual cortical areas during tactile tasks, whereas normal controls showed deactiva-tion. A simple tactile stimulus that did not require discrimination produced no activation of visual areas in either group. Thus, in blind subjects, cortical areas normally reserved for vision may be activated by other sensory modalities.},
  copyright = {1996 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/xzfang/Zotero/storage/BXKX54EY/Sadato et al. - 1996 - Activation of the primary visual cortex by Braille.pdf;/Users/xzfang/Zotero/storage/KKBGCH83/380526a0.html}
}

@techreport{saddler_deep_2020,
  type = {Preprint},
  title = {Deep Neural Network Models Reveal Interplay of Peripheral Coding and Stimulus Statistics in Pitch Perception},
  author = {Saddler, Mark R. and Gonzalez, Ray and McDermott, Josh H.},
  year = {2020},
  month = nov,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/2020.11.19.389999},
  abstract = {Computations on receptor responses enable behavior in the environment. Behavior is plausibly shaped by both the sensory receptors and the environments for which organisms are optimized, but their roles are often opaque. One classic example is pitch perception, whose properties are commonly linked to peripheral neural coding limits rather than environmental acoustic constraints. We trained artificial neural networks to estimate fundamental frequency from simulated cochlear representations of natural sounds. The best-performing networks replicated many characteristics of human pitch judgments. To probe how our ears and environment shape these characteristics, we optimized networks given altered cochleae or sound statistics. Human-like behavior emerged only when cochleae had high temporal fidelity and when models were optimized for natural sounds. The results suggest pitch perception is critically shaped by the constraints of natural environments in addition to those of the cochlea, illustrating the use of contemporary neural networks to reveal underpinnings of behavior.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3ZTUHPY9/Saddler et al. - 2020 - Deep neural network models reveal interplay of per.pdf}
}

@article{saddler_speech_2021,
  title = {Speech {{Denoising}} with {{Auditory Models}}},
  author = {Saddler, Mark R. and Francl, Andrew and Feather, Jenelle and Qian, Kaizhi and Zhang, Yang and McDermott, Josh H.},
  year = {2021},
  month = aug,
  journal = {arXiv:2011.10706 [cs, eess]},
  eprint = {2011.10706},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Contemporary speech enhancement predominantly relies on audio transforms that are trained to reconstruct a clean speech waveform. The development of high-performing neural network sound recognition systems has raised the possibility of using deep feature representations as `perceptual' losses with which to train denoising systems. We explored their utility by first training deep neural networks to classify either spoken words or environmental sounds from audio. We then trained an audio transform to map noisy speech to an audio waveform that minimized the difference in the deep feature representations between the output audio and the corresponding clean audio. The resulting transforms removed noise substantially better than baseline methods trained to reconstruct clean waveforms, and also outperformed previous methods using deep feature losses. However, a similar benefit was obtained simply by using losses derived from the filter bank inputs to the deep networks. The results show that deep features can guide speech enhancement, but suggest that they do not yet outperform simple alternatives that do not involve learned features.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/xzfang/Zotero/storage/N3C5GVXQ/Saddler et al. - 2021 - Speech Denoising with Auditory Models.pdf}
}

@article{saffran_pattern_2003,
  title = {Pattern Induction by Infant Language Learners.},
  author = {Saffran, Jenny R. and Thiessen, Erik D.},
  year = {2003},
  journal = {Developmental Psychology},
  volume = {39},
  number = {3},
  pages = {484--494},
  issn = {1939-0599, 0012-1649},
  doi = {10.1037/0012-1649.39.3.484},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/85UWCTAH/Saffran and Thiessen - 2003 - Pattern induction by infant language learners..pdf}
}

@misc{sahakyan_long_2014,
  title = {``{{A Long Time Ago}}, in a {{Context Far}}, {{Far Away}}'': {{Retrospective Time Estimates}} and {{Internal Context Change}}},
  shorttitle = {``{{A Long Time Ago}}, in a {{Context Far}}, {{Far Away}}''},
  author = {Sahakyan, Lili and Smith, James R. and Sahakyan, Lili and Smith, James R.},
  year = {2014},
  abstract = {This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly. This investigation aimed to establish retrospective time judgments as markers of internal context change across 2 memory paradigms, the effects of which have been attributed to internal context change by some researchers. Experiment 1 involved the list-method directed forgetting paradigm and established that the forget group significantly overestimated the duration of the experiment compared with the remember group. Experiment 2 involved the list-before-last paradigm, whereby participants studied 3 lists, and in},
  file = {/Users/xzfang/Zotero/storage/WZZBTX4N/Sahakyan et al. - â€œA Long Time Ago, in a Context Far, Far Awayâ€ Ret.pdf;/Users/xzfang/Zotero/storage/WRI844FV/download.html}
}

@article{salvador_when_2020,
  title = {When Norm Violations Are Spontaneously Detected: An Electrocortical Investigation},
  shorttitle = {When Norm Violations Are Spontaneously Detected},
  author = {Salvador, Cristina E and Mu, Yan and Gelfand, Michele J and Kitayama, Shinobu},
  year = {2020},
  month = mar,
  journal = {Social Cognitive and Affective Neuroscience},
  volume = {15},
  number = {3},
  pages = {319--327},
  issn = {1749-5016},
  doi = {10.1093/scan/nsaa035},
  abstract = {One fundamental function of social norms is to promote social coordination. Moreover, greater social coordination may be called for when tight norms govern social relations with others. Hence, the sensitivity to social norm violations may be jointly modulated by relational goals and a belief that the social context is tight (vs loose). We tested this analysis using an electrocortical marker of norm-violation detection (N400). Ninety-one young American adults were subliminally primed with either relational or neutral goals. Then they saw behaviors that were either norm-violating or normal. In the relational priming condition, the norm-violation N400 increased as a function of the perceived tightness of societal norms. In the control priming condition, however, the norm-violation N400 was weak regardless of perceived tightness. Thus, normative tightness was associated with increased neural processing of norm violations only when relational goals were activated. Implications for norm psychology are discussed.},
  pmcid = {PMC7235959},
  pmid = {32227086},
  file = {/Users/xzfang/Zotero/storage/H9VYDJVZ/Salvador et al. - 2020 - When norm violations are spontaneously detected a.pdf}
}

@article{salverda_tracking_2010,
  title = {Tracking the Time Course of Orthographic Information in Spoken-Word Recognition},
  author = {Salverda, Anne Pier and Tanenhaus, Michael K.},
  year = {2010},
  month = sep,
  journal = {Journal of experimental psychology. Learning, memory, and cognition},
  volume = {36},
  number = {5},
  pages = {1108--1117},
  issn = {0278-7393},
  doi = {10.1037/a0019901},
  abstract = {Two experiments evaluated the time course and use of orthographic information in spoken-word recognition in a visual world eye-tracking experiment using printed words as referents. Participants saw four words on a computer screen and listened to spoken sentences instructing them to click on one of the words (e.g., Click on the word bead). The printed words appeared 200 ms before the onset of the spoken target word. In Experiment 1, the display included the target word and a competitor with either a lower degree of phonological overlap with the target (bear) or a higher degree of phonological overlap with the target (bean). Both competitors had the same degree of orthographic overlap with the target. There were more fixations to the competitors than to unrelated distracters. Crucially, the likelihood of fixating a competitor did not vary as a function of the amount of phonological overlap between target and competitor. In Experiment 2, the display included the target word and a competitor with either a lower degree of orthographic overlap with the target (bare) or a higher degree of orthographic overlap with the target (bear). Competitors were homophonous and thus had the same degree of phonological overlap with the target. There were more fixations to higher-overlap competitors than to lower-overlap competitors, beginning during the temporal interval where initial fixations driven by the vowel are expected to occur. The authors conclude that orthographic information is rapidly activated as a spoken word unfolds and is immediately used in mapping spoken words onto potential printed referents.},
  pmcid = {PMC2933075},
  pmid = {20804288},
  file = {/Users/xzfang/Zotero/storage/4YX8VMST/Salverda and Tanenhaus - 2010 - Tracking the time course of orthographic informati.pdf}
}

@article{sanchez_lipread_2006,
  title = {Lipread Me Now, Hear Me Better Later: {{Crossmodal}} Transfer of Talker Familiarity Effects},
  shorttitle = {Lipread Me Now, Hear Me Better Later},
  author = {Sanchez, Kauyumari and Rosenblum, Lawrence D. and Miller, Rachel M.},
  year = {2006},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {120},
  number = {5},
  pages = {3248--3248},
  issn = {0001-4966},
  doi = {10.1121/1.4788294},
  abstract = {There is evidence that for both auditory and visual speech perception, familiarity with the talker facilitates speech recognition. Explanations of these effects have concentrated on the retention of talker information specific to each of these modalities. It could be, however, that some amodal, talker-specific articulatory-style information facilitates speech perception in both modalities. If this is true, then experience with a talker in one modality should facilitate perception of speech from that talker in the other modality. In a test of this prediction, subjects were given about 1 hr of experience lipreading a talker and were then asked to recover speech in noise from either this same talker or a different talker. Results revealed that subjects who lip-read and heard speech from the same talker performed better on the speech-in-noise task than did subjects who lip-read from one talker and then heard speech from a different talker.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KSWVEZU3/Sanchez et al. - 2006 - Lipread me now, hear me better later Crossmodal t.pdf}
}

@article{sanchez-alonso_model_2022,
  title = {Towards a Model of Language Neurobiology in Early Development},
  author = {{Sanchez-Alonso}, Sara and Aslin, Richard N.},
  year = {2022},
  month = jan,
  journal = {Brain and Language},
  volume = {224},
  pages = {105047},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2021.105047},
  abstract = {Understanding language neurobiology in early childhood is essential for characterizing the developmental structural and functional changes that lead to the mature adult language network. In the last two decades, the field of language neurodevelopment has received increasing attention, particularly given the rapid advances in the implementation of neuroimaging techniques and analytic approaches that allow detailed investigations into the developing brain across a variety of cognitive domains. These methodological and analytical advances hold the promise of developing early markers of language outcomes that allow diagnosis and clinical interventions at the earliest stages of development. Here, we argue that findings in language neurobiology need to be integrated within an approach that captures the dynamic nature and inherent variability that characterizes the developing brain and the interplay between behavior and (structural and functional) neural patterns. Accordingly, we describe a framework for understanding language neurobiology in early development, which minimally requires an explicit characterization of the following core domains: i) computations underlying language learning mechanisms, ii) developmental patterns of change across neural and behavioral measures, iii) environmental variables that reinforce language learning (e.g., the social context), and iv) brain maturational constraints for optimal neural plasticity, which determine the infant's sensitivity to learning from the environment. We discuss each of these domains in the context of recent behavioral and neuroimaging findings and consider the need for quantitatively modeling two main sources of variation: individual differences or trait-like patterns of variation and within-subject differences or state-like patterns of variation. The goal is to enable models that allow prediction of language outcomes from neural measures that take into account these two types of variation. Finally, we examine how future methodological approaches would benefit from the inclusion of more ecologically valid paradigms that complement and allow generalization of traditional controlled laboratory methods.},
  langid = {english},
  keywords = {Biomarkers,Development,Infant,Language,Learning,Neurobiology},
  file = {/Users/xzfang/Zotero/storage/82PBBHVD/S0093934X21001413.html}
}

@article{sanders_eventrelated_2009,
  title = {Event-Related Potentials Index Segmentation of Nonsense Sounds},
  author = {Sanders, Lisa D. and Ameral, Victoria and Sayles, Kathryn},
  year = {2009},
  month = mar,
  journal = {Neuropsychologia},
  volume = {47},
  number = {4},
  pages = {1183--1186},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2008.11.005},
  abstract = {To understand the world around us, continuous streams of information including speech must be segmented into units that can be mapped onto stored representations. Recent evidence has shown that event-related potentials (ERPs) can index the online segmentation of sound streams. In the current study, listeners were trained to recognize sequences of three nonsense sounds that could not easily be rehearsed. Beginning 40ms after onset, sequence-initial sounds elicited a larger amplitude negativity after compared to before training. This difference was not evident for medial or final sounds in the sequences. Across studies, ERP segmentation effects are remarkably similar regardless of the available segmentation cues and nature of the continuous streams. These results indicate the preferential processing of sequence-initial information is not domain specific and instead implicate a more general cognitive mechanism such as temporally selective attention.},
  langid = {english},
  keywords = {Auditory,ERP,N1,Selective attention,Speech segmentation,Statistical learning},
  file = {/Users/xzfang/Zotero/storage/Y762Q8Z4/Sanders et al. - 2009 - Event-related potentials index segmentation of non.pdf;/Users/xzfang/Zotero/storage/J4QDHMI8/S0028393208004387.html}
}

@article{sanders_segmenting_2002,
  title = {Segmenting {{Nonsense}}: {{An Event-Related Potential Index}} of {{Perceived Onsets}} in {{Continuous Speech}}},
  shorttitle = {Segmenting {{Nonsense}}},
  author = {Sanders, Lisa D. and Newport, Elissa L. and Neville, Helen J.},
  year = {2002},
  month = jul,
  journal = {Nature neuroscience},
  volume = {5},
  number = {7},
  pages = {700--703},
  issn = {1097-6256},
  doi = {10.1038/nn873},
  abstract = {Speech segmentation, determining where one word ends and the next begins in continuous speech, is necessary for auditory language processing. However, because there are few direct indices of this fast, automatic process, it has been difficult to study. We recorded event-related brain potentials (ERPs) while adult humans listened to six pronounceable nonwords presented as continuous speech and compared the responses to nonword onsets before and after participants learned the nonsense words. In subjects showing the greatest behavioral evidence of word learning, word onsets elicited a larger N100 after than before training. Thus N100 amplitude indexes speech segmentation even for recently learned words without any acoustic segmentation cues. The timing and distribution of these results suggest specific processes that may be central to speech segmentation.},
  pmcid = {PMC2532533},
  pmid = {12068301},
  file = {/Users/xzfang/Zotero/storage/HHCHE59F/Sanders et al. - 2002 - Segmenting Nonsense An Event-Related Potential In.pdf}
}

@article{sanford_depth_2002,
  title = {Depth of Processing in Language Comprehension: Not Noticing the Evidence},
  shorttitle = {Depth of Processing in Language Comprehension},
  author = {Sanford, Anthony J. and Sturt, Patrick},
  year = {2002},
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {6},
  number = {9},
  pages = {382--386},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(02)01958-7},
  abstract = {The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding.},
  langid = {english},
  keywords = {Language interpretation,Representation,Semantic anomalies,Text-change-blindness,Underspecification},
  file = {/Users/xzfang/Zotero/storage/IC7KTRMT/Sanford and Sturt - 2002 - Depth of processing in language comprehension not.pdf;/Users/xzfang/Zotero/storage/PH4BAXFN/S1364661302019587.html}
}

@article{sankaran_decoding_2018,
  title = {Decoding the Dynamic Representation of Musical Pitch from Human Brain Activity},
  author = {Sankaran, N. and Thompson, W. F. and Carlile, S. and Carlson, T. A.},
  year = {2018},
  month = jan,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {1--9},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-19222-3},
  abstract = {In music, the perception of pitch is governed largely by its tonal function given the preceding harmonic structure of the music. While behavioral research has advanced our understanding of the perceptual representation of musical pitch, relatively little is known about its representational structure in the brain. Using Magnetoencephalography (MEG), we recorded evoked neural responses to different tones presented within a tonal context. Multivariate Pattern Analysis (MVPA) was applied to ``decode'' the stimulus that listeners heard based on the underlying neural activity. We then characterized the structure of the brain's representation using decoding accuracy as a proxy for representational distance, and compared this structure to several well established perceptual and acoustic models. The observed neural representation was best accounted for by a model based on the Standard Tonal Hierarchy, whereby differences in the neural encoding of musical pitches correspond to their differences in perceived stability. By confirming that perceptual differences honor those in the underlying neuronal population coding, our results provide a crucial link in understanding the cognitive foundations of musical pitch across psychological and neural domains.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/C82C8EDZ/Sankaran et al. - 2018 - Decoding the dynamic representation of musical pit.pdf;/Users/xzfang/Zotero/storage/UUSZGGQ5/s41598-018-19222-3.html}
}

@techreport{sankaran_dynamic_2018,
  type = {Preprint},
  title = {The Dynamic Emergence of Musical Pitch Structure in Human Cortex},
  author = {Sankaran, Narayan and Carlson, Thomas A and Thompson, William Forde},
  year = {2018},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/494294},
  abstract = {Tonal music the world over is characterized by a hierarchical structuring of pitch, whereby certain tones appear stable and others unstable within their musical context. Despite its prevalence, the cortical mechanisms supporting such a percept remain poorly understood. The current study probed the neural processing dynamics underlying the representation of pitch in Western Tonal Music. Listeners were presented with tones comprising all twelve pitch-classes embedded within a musical context whilst having their magnetoencephalographic (MEG) activity recorded. Using multivariate pattern analysis (MVPA), decoders attempted to classify the identity of tones from their corresponding MEG activity at each peristimulus time sample, providing a dynamic measure of their cortical dissimilarity. Time-evolving dissimilarities between tones were then compared with the predictions of several acoustic and perceptual models. Following tone onset, we observed a temporal evolution in the neural representation. Dissimilarities between tones initially reflected their fundamental frequency separation, but beyond 200 ms reflected their status within the tonal hierarchy of perceived stability. Furthermore, when the dissimilarities corresponding to this latter period were transposed into different keys, cortical relations between keys correlated with the well-known circle of fifths. Convergent with fundamental principles of music-theory and perception, current results detail the dynamics with which the complex perceptual structure of Western tonal music emerges in human cortex within the timescale of an individual tone.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VLTRXEXL/Sankaran et al. - 2018 - The dynamic emergence of musical pitch structure i.pdf}
}

@inproceedings{santoro_simple_2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamical physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties.},
  file = {/Users/xzfang/Zotero/storage/NMCXP5AN/Santoro et al. - 2017 - A simple neural network module for relational reas.pdf}
}

@article{santoro_symbolic_2021,
  title = {Symbolic {{Behaviour}} in {{Artificial Intelligence}}},
  author = {Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.03406 [cs]},
  eprint = {2102.03406},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/QWCVXCPK/Santoro et al. - 2021 - Symbolic Behaviour in Artificial Intelligence.pdf}
}

@article{sargolini_conjunctive_2006,
  title = {Conjunctive {{Representation}} of {{Position}}, {{Direction}}, and {{Velocity}} in {{Entorhinal Cortex}}},
  author = {Sargolini, Francesca and Fyhn, Marianne and Hafting, Torkel and McNaughton, Bruce L. and Witter, Menno P. and Moser, May-Britt and Moser, Edvard I.},
  year = {2006},
  month = may,
  journal = {Science},
  volume = {312},
  number = {5774},
  pages = {758--762},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1125572},
  abstract = {Grid cells in the medial entorhinal cortex (MEC) are part of an environment-independent spatial coordinate system. To determine how information about location, direction, and distance is integrated in the grid-cell network, we recorded from each principal cell layer of MEC in rats that explored two-dimensional environments. Whereas layer II was predominated by grid cells, grid cells colocalized with head-direction cells and conjunctive grid \texttimes{} head-direction cells in the deeper layers. All cell types were modulated by running speed. The conjunction of positional, directional, and translational information in a single MEC cell type may enable grid coordinates to be updated during self-motion\textendash based navigation. In rats, one region of the cortex contains cells that code the animal's position, head direction, and speed, and may integrate this information to provide a sense of its spatial location. In rats, one region of the cortex contains cells that code the animal's position, head direction, and speed, and may integrate this information to provide a sense of its spatial location.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {16675704},
  file = {/Users/xzfang/Zotero/storage/KYH7SM9W/Sargolini et al. - 2006 - Conjunctive Representation of Position, Direction,.pdf;/Users/xzfang/Zotero/storage/9UPTJEBG/758.html}
}

@article{sassenhagen_finding_2019,
  title = {Finding the {{P3}} in the {{P600}}: {{Decoding}} Shared Neural Mechanisms of Responses to Syntactic Violations and Oddball Targets},
  shorttitle = {Finding the {{P3}} in the {{P600}}},
  author = {Sassenhagen, Jona and Fiebach, Christian J.},
  year = {2019},
  month = oct,
  journal = {NeuroImage},
  volume = {200},
  pages = {425--436},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.06.048},
  abstract = {The P600 Event-Related Brain Potential, elicited by syntactic violations in sentences, is generally interpreted as indicating language-specific structural/combinatorial processing, with far-reaching implications for models of language. P600 effects are also often taken as evidence for language-like grammars in non-linguistic domains like music or arithmetic. An alternative account, however, interprets the P600 as a P3, a domain-general brain response to salience. Using time-generalized multivariate pattern analysis, we demonstrate that P3 EEG patterns, elicited in a visual Oddball experiment, account for the P600 effect elicited in a syntactic violation experiment: P3 pattern-trained MVPA can classify P600 trials just as well as P600-trained ones. A second study replicates and generalizes this finding, and demonstrates its specificity by comparing it to face- and semantic mismatch-associated EEG responses. These results indicate that P3 and P600 share neural patterns to a substantial degree, calling into question the interpretation of P600 as a language-specific brain response and instead strengthening its association with the P3. More generally, our data indicate that observing P600-like brain responses provides no direct evidence for the presence of language-like grammars, in language or elsewhere.},
  langid = {english},
  keywords = {Domain specificity,ERP,Language,Multivariate pattern analysis (MVPA),P3,P600,Syntax},
  file = {/Users/xzfang/Zotero/storage/JKWDZQIK/Sassenhagen and Fiebach - 2019 - Finding the P3 in the P600 Decoding shared neural.pdf;/Users/xzfang/Zotero/storage/MT9ZGZ7K/S1053811919305452.html}
}

@article{sassenhagen_p600asp3_2014,
  title = {The {{P600-as-P3}} Hypothesis Revisited: {{Single-trial}} Analyses Reveal That the Late {{EEG}} Positivity Following Linguistically Deviant Material Is Reaction Time Aligned},
  shorttitle = {The {{P600-as-P3}} Hypothesis Revisited},
  author = {Sassenhagen, Jona and Schlesewsky, Matthias and {Bornkessel-Schlesewsky}, Ina},
  year = {2014},
  month = oct,
  journal = {Brain and Language},
  volume = {137},
  pages = {29--39},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2014.07.010},
  abstract = {The P600, a late positive ERP component following linguistically deviant stimuli, is commonly seen as indexing structural, high-level processes, e.g. of linguistic (re)analysis. It has also been identified with the P3 (P600-as-P3 hypothesis), which is thought to reflect a systemic neuromodulator release facilitating behavioural shifts and is usually response time aligned. We investigated single-trial alignment of the P600 to response, a critical prediction of the P600-as-P3 hypothesis. Participants heard sentences containing morphosyntactic and semantic violations and responded via a button press. The elicited P600 was perfectly response aligned, while an N400 following semantic deviations was stimulus aligned. This is, to our knowledge, the first single-trial analysis of language processing data using within-sentence behavioural responses as temporal covariates. Results support the P600-as-P3 perspective and thus constitute a step towards a neurophysiological grounding of language-related ERPs.},
  langid = {english},
  keywords = {Attention,Locus Coeruleus,N400,P3,P600,Reorienting,Semantics,Sentence processing,Single-trial analysis,Syntax},
  file = {/Users/xzfang/Zotero/storage/52QKMBK4/Sassenhagen et al. - 2014 - The P600-as-P3 hypothesis revisited Single-trial .pdf;/Users/xzfang/Zotero/storage/L5RWHG2C/S0093934X14001072.html}
}

@article{sassenhagen_traces_2020,
  title = {Traces of {{Meaning Itself}}: {{Encoding Distributional Word Vectors}} in {{Brain Activity}}},
  shorttitle = {Traces of {{Meaning Itself}}},
  author = {Sassenhagen, Jona and Fiebach, Christian J.},
  year = {2020},
  month = mar,
  journal = {Neurobiology of Language},
  volume = {1},
  number = {1},
  pages = {54--76},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00003},
  abstract = {How is semantic information stored in the human mind and brain? Some philosophers and cognitive scientists argue for vectorial representations of concepts, where the meaning of a word is represented as its position in a high-dimensional neural state space. At the intersection of natural language processing and artificial intelligence, a class of very successful distributional word vector models has developed that can account for classic EEG findings of language, that is, the ease versus difficulty of integrating a word with its sentence context. However, models of semantics have to account not only for context-based word processing, but should also describe how word meaning is represented. Here, we investigate whether distributional vector representations of word meaning can model brain activity induced by words presented without context. Using EEG activity (event-related brain potentials) collected while participants in two experiments (English and German) read isolated words, we encoded and decoded word vectors taken from the family of prediction-based Word2vec algorithms. We found that, first, the position of a word in vector space allows the prediction of the pattern of corresponding neural activity over time, in particular during a time window of 300 to 500 ms after word onset. Second, distributional models perform better than a human-created taxonomic baseline model (WordNet), and this holds for several distinct vector-based models. Third, multiple latent semantic dimensions of word meaning can be decoded from brain activity. Combined, these results suggest that empiricist, prediction-based vectorial representations of meaning are a viable candidate for the representational architecture of human semantic knowledge.},
  file = {/Users/xzfang/Zotero/storage/678UFD82/Sassenhagen and Fiebach - 2020 - Traces of Meaning Itself Encoding Distributional .pdf;/Users/xzfang/Zotero/storage/PZTERFHH/Traces-of-Meaning-Itself-Encoding-Distributional.html}
}

@article{savin_word_1963,
  title = {Word-{{Frequency Effect}} and {{Errors}} in the {{Perception}} of {{Speech}}},
  author = {Savin, Harris B.},
  year = {1963},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {35},
  number = {2},
  pages = {200--206},
  issn = {0001-4966},
  doi = {10.1121/1.1918432},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CFZU74RJ/Savin - 1963 - Wordâ€Frequency Effect and Errors in the Perception.pdf}
}

@article{saxe_divide_2006,
  title = {Divide and Conquer: {{A}} Defense of Functional Localizers},
  shorttitle = {Divide and Conquer},
  author = {Saxe, Rebecca and Brett, Matthew and Kanwisher, Nancy},
  year = {2006},
  month = may,
  journal = {NeuroImage},
  volume = {30},
  number = {4},
  pages = {1088--1096},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2005.12.062},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HBLUJHBK/Saxe et al. - 2006 - Divide and conquer A defense of functional locali.pdf}
}

@article{saxe_simulation_2005,
  title = {Against Simulation: The Argument from Error},
  shorttitle = {Against Simulation},
  author = {Saxe, Rebecca},
  year = {2005},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {9},
  number = {4},
  pages = {174--179},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2005.01.012},
  abstract = {According to Simulation Theory, to understand what is going on in another person's mind, the observer uses his or her own mind as a model of the other mind. Recently, philosophers and cognitive neuroscientists have proposed that mirror neurones (which fire in response to both executing and observing a goal directed action) provide a plausible neural substrate for simulation, a mechanism for directly perceiving, or `resonating' with, the contents of other minds. This article makes the case against Simulation Theory, using evidence from cognitive neuroscience, developmental psychology, and social psychology. In particular, the errors that adults and children make when reasoning about other minds are not consistent with the `resonance' versions of Simulation Theory.},
  langid = {english}
}

@article{scarborough_frequency_1977,
  title = {Frequency and Repetition Effects in Lexical Memory},
  author = {Scarborough, Don L. and Cortese, Charles and Scarborough, Hollis S.},
  year = {1977},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {3},
  number = {1},
  pages = {1--17},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.3.1.1},
  abstract = {Conducted 5 reaction time (RT) experiments with 75 undergraduates to explore word-frequency effects in word-nonword decision tasks and in pronunciation and memory tasks. High-frequency words were recognized substantially faster than low-frequency words in the word-nonword decision tasks. However, there was little effect of word frequency in the pronunciation and old-new memory tasks. Further, in the word-nonword lexical decision task, prior presentations of words produced substantial and apparently long-lasting reductions on the basic frequency effect. The occurrence of natural language frequency effects only in the word-nonword decision task supported the use of this task to study the organization of and retrieval from the subjective lexicon. The modification of frequency effects by repetition suggested that natural language frequency effects may be attributed partly to the recency with which words have occurred. Analysis of the response latencies using S. Sternberg's (see record 1970-11748-001) additive-factors approach indicated that frequency effects consist of both effects in encoding and in retrieval from memory. (34 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Human Information Storage,Memory,Pronunciation,Reaction Time,Recognition (Learning),Stimulus Frequency,Word Frequency,Words (Phonetic Units)},
  file = {/Users/xzfang/Zotero/storage/ZEL3F7DK/1977-22709-001.html}
}

@article{schacter_priming_1998,
  title = {Priming and the {{Brain}}},
  author = {Schacter, Daniel L and Buckner, Randy L},
  year = {1998},
  month = feb,
  journal = {Neuron},
  volume = {20},
  number = {2},
  pages = {185--195},
  issn = {08966273},
  doi = {10.1016/S0896-6273(00)80448-1},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GVBFQW8B/Schacter and Buckner - 1998 - Priming and the Brain.pdf}
}

@article{schad_how_2020,
  title = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: {{A}} Tutorial},
  shorttitle = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models},
  author = {Schad, Daniel J. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold},
  year = {2020},
  month = feb,
  journal = {Journal of Memory and Language},
  volume = {110},
  pages = {104038},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2019.104038},
  abstract = {Factorial experiments in research on memory, language, and in other areas are often analyzed using analysis of variance (ANOVA). However, for effects with more than one numerator degrees of freedom, e.g., for experimental factors with more than two levels, the ANOVA omnibus F-test is not informative about the source of a main effect or interaction. Because researchers typically have specific hypotheses about which condition means differ from each other, a priori contrasts (i.e., comparisons planned before the sample means are known) between specific conditions or combinations of conditions are the appropriate way to represent such hypotheses in the statistical model. Many researchers have pointed out that contrasts should be ``tested instead of, rather than as a supplement to, the ordinary `omnibus' F test'' (Hays, 1973, p. 601). In this tutorial, we explain the mathematics underlying different kinds of contrasts (i.e., treatment, sum, repeated, polynomial, custom, nested, interaction contrasts), discuss their properties, and demonstrate how they are applied in the R System for Statistical Computing (R Core Team, 2018). In this context, we explain the generalized inverse which is needed to compute the coefficients for contrasts that test hypotheses that are not covered by the default set of contrasts. A detailed understanding of contrast coding is crucial for successful and correct specification in linear models (including linear mixed models). Contrasts defined a priori yield far more useful confirmatory tests of experimental hypotheses than standard omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
  langid = {english},
  keywords = {A priori hypotheses,Contrasts,Linear models,Null hypothesis significance testing},
  file = {/Users/xzfang/Zotero/storage/MDWCJ4TI/Schad et al. - 2020 - How to capitalize on a priori contrasts in linear .pdf;/Users/xzfang/Zotero/storage/7D98G2KH/S0749596X19300695.html}
}

@article{schad_principled_2020,
  title = {Toward a Principled {{Bayesian}} Workflow in Cognitive Science},
  author = {Schad, Daniel J. and Betancourt, Michael and Vasishth, Shravan},
  year = {2020},
  month = feb,
  journal = {arXiv:1904.12765 [stat]},
  eprint = {1904.12765},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Experiments in research on memory, language, and in other areas of cognitive science are increasingly being analyzed using Bayesian methods. This has been facilitated by the development of probabilistic programming languages such as Stan, and easily accessible front-end packages such as brms. The utility of Bayesian methods, however, ultimately depends on the relevance of the Bayesian model, in particular whether or not it accurately captures the structure of the data and the data analyst's domain expertise. Even with powerful software, the analyst is responsible for verifying the utility of their model. To demonstrate this point, we introduce a principled Bayesian workflow (Betancourt, 2018) to cognitive science. Using a concrete working example, we describe basic questions one should ask about the model: prior predictive checks, computational faithfulness, model sensitivity, and posterior predictive checks. The running example for demonstrating the workflow is data on reading times with a linguistic manipulation of object versus subject relative clause sentences. This principled Bayesian workflow also demonstrates how to use domain knowledge to inform prior distributions. It provides guidelines and checks for valid data analysis, avoiding overfitting complex models to noise, and capturing relevant data structure in a probabilistic model. Given the increasing use of Bayesian methods, we aim to discuss how these methods can be properly employed to obtain robust answers to scientific questions. All data and code accompanying this paper are available from https://osf.io/b2vx9/.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/xzfang/Zotero/storage/LDZXRQI4/Schad et al. - 2020 - Toward a principled Bayesian workflow in cognitive.pdf;/Users/xzfang/Zotero/storage/ZMH4ZCLC/1904.html}
}

@article{schalk_facephenes_2017,
  title = {Facephenes and Rainbows: {{Causal}} Evidence for Functional and Anatomical Specificity of Face and Color Processing in the Human Brain},
  shorttitle = {Facephenes and Rainbows},
  author = {Schalk, Gerwin and Kapeller, Christoph and Guger, Christoph and Ogawa, Hiroshi and Hiroshima, Satoru and {Lafer-Sousa}, Rosa and Saygin, Zeynep M. and Kamada, Kyousuke and Kanwisher, Nancy},
  year = {2017},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {46},
  pages = {12285--12290},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1713447114},
  abstract = {Significance             Are some regions of the human brain exclusively engaged in a single specific mental process? Here we test this question in a neurosurgery patient implanted with electrodes for clinical reasons. When electrically stimulated in the fusiform face area while viewing objects, the patient reported illusory faces while the objects remained unchanged. When stimulated in nearby color-preferring sites, he reported seeing rainbows. The fact that stimulation of face-selective sites affected only face percepts and stimulation of color-preferring sites affected only color percepts, in both cases independent of the object being viewed, supports the view that some regions of cortex are indeed exclusively causally engaged in a single mental process and highlights the risks entailed in standard interpretations of neural decoding results.           ,              Neuroscientists have long debated whether some regions of the human brain are exclusively engaged in a single specific mental process. Consistent with this view, fMRI has revealed cortical regions that respond selectively to certain stimulus classes such as faces. However, results from multivoxel pattern analyses (MVPA) challenge this view by demonstrating that category-selective regions often contain information about ``nonpreferred'' stimulus dimensions. But is this nonpreferred information causally relevant to behavior? Here we report a rare opportunity to test this question in a neurosurgical patient implanted for clinical reasons with strips of electrodes along his fusiform gyri. Broadband gamma electrocorticographic responses in multiple adjacent electrodes showed strong selectivity for faces in a region corresponding to the fusiform face area (FFA), and preferential responses to color in a nearby site, replicating earlier reports. To test the causal role of these regions in the perception of nonpreferred dimensions, we then electrically stimulated individual sites while the patient viewed various objects. When stimulated in the FFA, the patient reported seeing an illusory face (or ``facephene''), independent of the object viewed. Similarly, stimulation of color-preferring sites produced illusory ``rainbows.'' Crucially, the patient reported no change in the object viewed, apart from the facephenes and rainbows apparently superimposed on them. The functional and anatomical specificity of these effects indicate that some cortical regions are exclusively causally engaged in a single specific mental process, and prompt caution about the widespread assumption that any information scientists can decode from the brain is causally relevant to behavior.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2LUJYNVQ/Schalk et al. - 2017 - Facephenes and rainbows Causal evidence for funct.pdf}
}

@article{schapiro_neural_2013,
  title = {Neural Representations of Events Arise from Temporal Community Structure},
  author = {Schapiro, Anna C. and Rogers, Timothy T. and Cordova, Natalia I. and {Turk-Browne}, Nicholas B. and Botvinick, Matthew M.},
  year = {2013},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {16},
  number = {4},
  pages = {486--492},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3331},
  abstract = {Research on event perception has focused on transient elevations in predictive uncertainty or surprise as the primary signal driving event segmentation. Here the authors report behavioral and neuroimaging evidence that suggests that event representations can emerge even in the absence of such cues. They propose that this learning occurs in a manner analogous to the learning of semantic categories.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computational neuroscience,Learning and memory},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Learning and memory Subject\_term\_id: computational-neuroscience;learning-and-memory},
  file = {/Users/xzfang/Zotero/storage/297KICZ3/Schapiro et al. - 2013 - Neural representations of events arise from tempor.pdf;/Users/xzfang/Zotero/storage/CGVM3FXN/nn.html}
}

@article{scharinger_prior_2013,
  title = {Prior Experience with Negative Spectral Correlations Promotes Information Integration during Auditory Category Learning},
  author = {Scharinger, Mathias and Henry, Molly J. and Obleser, Jonas},
  year = {2013},
  month = jul,
  journal = {Memory \& Cognition},
  volume = {41},
  number = {5},
  pages = {752--768},
  issn = {1532-5946},
  doi = {10.3758/s13421-013-0294-9},
  abstract = {Complex sounds vary along a number of acoustic dimensions. These dimensions may exhibit correlations that are familiar to listeners due to their frequent occurrence in natural sounds\textemdash namely, speech. However, the precise mechanisms that enable the integration of these dimensions are not well understood. In this study, we examined the categorization of novel auditory stimuli that differed in the correlations of their acoustic dimensions, using decision bound theory. Decision bound theory assumes that stimuli are categorized on the basis of either a single dimension (rule based) or the combination of more than one dimension (information integration) and provides tools for assessing successful integration across multiple acoustic dimensions. In two experiments, we manipulated the stimulus distributions such that in Experiment 1, optimal categorization could be accomplished by either a rule-based or an information integration strategy, while in Experiment 2, optimal categorization was possible only by using an information integration strategy. In both experiments, the pattern of results demonstrated that unidimensional strategies were strongly preferred. Listeners focused on the acoustic dimension most closely related to pitch, suggesting that pitch-based categorization was given preference over timbre-based categorization. Importantly, in Experiment 2, listeners also relied on a two-dimensional information integration strategy, if there was immediate feedback. Furthermore, this strategy was used more often for distributions defined by a negative spectral correlation between stimulus dimensions, as compared with distributions with a positive correlation. These results suggest that prior experience with such correlations might shape short-term auditory category learning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CLCGHFDM/Scharinger et al. - 2013 - Prior experience with negative spectral correlatio.pdf}
}

@article{schijndel_singlestage_2021,
  title = {Single-{{Stage Prediction Models Do Not Explain}} the {{Magnitude}} of {{Syntactic Disambiguation Difficulty}}},
  author = {van Schijndel, Marten and Linzen, Tal},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  pages = {e12988},
  issn = {1551-6709},
  doi = {10.1111/cogs.12988},
  abstract = {The disambiguation of a syntactically ambiguous sentence in favor of a less preferred parse can lead to slower reading at the disambiguation point. This phenomenon, referred to as a garden-path effect, has motivated models in which readers initially maintain only a subset of the possible parses of the sentence, and subsequently require time-consuming reanalysis to reconstruct a discarded parse. A more recent proposal argues that the garden-path effect can be reduced to surprisal arising in a fully parallel parser: words consistent with the initially dispreferred but ultimately correct parse are simply less predictable than those consistent with the incorrect parse. Since predictability has pervasive effects in reading far beyond garden-path sentences, this account, which dispenses with reanalysis mechanisms, is more parsimonious. Crucially, it predicts a linear effect of surprisal: the garden-path effect is expected to be proportional to the difference in word surprisal between the ultimately correct and ultimately incorrect interpretations. To test this prediction, we used recurrent neural network language models to estimate word-by-word surprisal for three temporarily ambiguous constructions. We then estimated the slowdown attributed to each bit of surprisal from human self-paced reading times, and used that quantity to predict syntactic disambiguation difficulty. Surprisal successfully predicted the existence of garden-path effects, but drastically underpredicted their magnitude, and failed to predict their relative severity across constructions. We conclude that a full explanation of syntactic disambiguation difficulty may require recovery mechanisms beyond predictability.},
  langid = {english},
  keywords = {Garden paths,Information theory,Neural networks,Self-paced reading,Surprisal},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12988},
  file = {/Users/xzfang/Zotero/storage/Q6UP3W75/Schijndel and Linzen - 2021 - Single-Stage Prediction Models Do Not Explain the .pdf;/Users/xzfang/Zotero/storage/H83GGHLW/cogs.html}
}

@article{schmitt_thalamic_2017,
  title = {Thalamic Amplification of Cortical Connectivity Sustains Attentional Control},
  author = {Schmitt, L. Ian and Wimmer, Ralf D. and Nakajima, Miho and Happ, Michael and Mofakham, Sima and Halassa, Michael M.},
  year = {2017},
  month = may,
  journal = {Nature},
  volume = {545},
  number = {7653},
  pages = {219--223},
  issn = {1476-4687},
  doi = {10.1038/nature22073},
  abstract = {Although interactions between the thalamus and cortex are critical for cognitive function, the exact contribution of the thalamus to these interactions remains unclear. Recent studies have shown diverse connectivity patterns across the thalamus, but whether this diversity translates to thalamic functions beyond relaying information to or between cortical regions is unknown. Here we show, by investigating the representation of two rules used to guide attention in the mouse prefrontal cortex (PFC), that the mediodorsal thalamus sustains these representations without relaying categorical information. Specifically, mediodorsal input amplifies local PFC connectivity, enabling rule-specific neural sequences to emerge and thereby maintain rule representations. Consistent with this notion, broadly enhancing PFC excitability diminishes rule specificity and behavioural performance, whereas enhancing mediodorsal excitability improves both. Overall, our results define a previously unknown principle in neuroscience; thalamic control of functional cortical connectivity. This function, which is dissociable from categorical information relay, indicates that the thalamus has a much broader role in cognition than previously thought.},
  langid = {english},
  pmcid = {PMC5570520},
  pmid = {28467827},
  keywords = {Animals,Attention,Cognition,Male,Mice,Neural Pathways,Optogenetics,Prefrontal Cortex,Thalamus},
  file = {/Users/xzfang/Zotero/storage/LPQEZ3HU/Schmitt et al. - 2017 - Thalamic amplification of cortical connectivity su.pdf}
}

@article{schneider_larger_2021,
  title = {Larger Capacity for Unconscious versus Conscious Episodic Memory},
  author = {Schneider, Else and Z{\"u}st, Marc Alain and Wuethrich, Sergej and Schmidig, Flavio and Kl{\"o}ppel, Stefan and Wiest, Roland and Ruch, Simon and Henke, Katharina},
  year = {2021},
  month = jul,
  journal = {Current Biology},
  volume = {0},
  number = {0},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2021.06.012},
  langid = {english},
  pmid = {34256016},
  keywords = {hippocampus,masking,memory capacity,subliminal,unconscious,working memory},
  file = {/Users/xzfang/Zotero/storage/Y4MN8KV7/Schneider et al. - 2021 - Larger capacity for unconscious versus conscious e.pdf;/Users/xzfang/Zotero/storage/5CMIE498/S0960-9822(21)00807-1.html}
}

@article{schoenfeld_spatiotemporal_2007,
  title = {Spatio-Temporal {{Analysis}} of {{Feature-Based Attention}}},
  author = {Schoenfeld, MA and Hopf, J-M and Martinez, A and Mai, HM and Sattler, C and Gasde, A and Heinze, H-J and Hillyard, SA},
  year = {2007},
  month = oct,
  journal = {Cerebral Cortex},
  volume = {17},
  number = {10},
  pages = {2468--2477},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhl154},
  abstract = {The cortical mechanisms of feature-selective attention to color and motion cues were studied in humans using combined electrophysiological, magnetoencephalographic, and hemodynamic (functional magnetic resonance imaging) measures of brain activity. Subjects viewed a display of random dots that periodically either changed color or moved coherently. When attention was directed to the color change it elicited enhanced neural activity in visual area V4v, previously shown to be specialized for processing color information. In contrast, when dot movement was attended it produced enhanced activity in the motion-specialized area human MT. Parallel recordings of event-related electrophysiological and magnetoencephalographic responses indicated that the attention-related facilitation of neural activity in these specialized cortical areas occurred rapidly, beginning as early as 90\textendash 120 ms after stimulus onset. We conclude that selection of an entire feature dimension (motion or color) boosts neural activity in its specialized cortical module much more rapidly than does selection of one feature value from another (e.g., one color from another), as reported in previous electrophysiological studies. By combining methods with high spatial and temporal resolution it is possible to analyze the precise time course of feature-selective processing in specialized cortical areas.},
  file = {/Users/xzfang/Zotero/storage/ZK7LUUW4/Schoenfeld et al. - 2007 - Spatio-temporal Analysis of Feature-Based Attentio.pdf;/Users/xzfang/Zotero/storage/9N4XEUNH/315616.html}
}

@incollection{scholkopf_speakers_2007a,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  editor = {Sch{\"o}lkopf, Bernhard and Platt, John and Hofmann, Thomas},
  year = {2007},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/7503.003.0111},
  abstract = {If language users are rational, they might choose to structure their utterances so as to optimize communicative properties. In particular, information-theoretic and psycholinguistic considerations suggest that this may include maximizing the uniformity of information density in an utterance. We investigate this possibility in the context of syntactic reduction, where the speaker has the option of either marking a higher-order unit (a phrase) with an extra word, or leaving it unmarked. We demonstrate that speakers are more likely to reduce less information-dense phrases. In a second step, we combine a stochastic model of structured utterance production with a logistic-regression model of syntactic reduction to study which types of cues speakers employ when estimating the predictability of upcoming elements. We demonstrate that the trend toward predictability-sensitive syntactic reduction (Jaeger, 2006) is robust in the face of a wide variety of control variables, and present evidence that speakers use both surface and structural cues for predictability estimation.},
  isbn = {978-0-262-25691-9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/R6TC4HN4/SchÃ¶lkopf et al. - 2007 - Speakers optimize information density through synt.pdf}
}

@incollection{scholkopf_speakers_2007b,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  editor = {Sch{\"o}lkopf, Bernhard and Platt, John and Hofmann, Thomas},
  year = {2007},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/7503.003.0111},
  abstract = {If language users are rational, they might choose to structure their utterances so as to optimize communicative properties. In particular, information-theoretic and psycholinguistic considerations suggest that this may include maximizing the uniformity of information density in an utterance. We investigate this possibility in the context of syntactic reduction, where the speaker has the option of either marking a higher-order unit (a phrase) with an extra word, or leaving it unmarked. We demonstrate that speakers are more likely to reduce less information-dense phrases. In a second step, we combine a stochastic model of structured utterance production with a logistic-regression model of syntactic reduction to study which types of cues speakers employ when estimating the predictability of upcoming elements. We demonstrate that the trend toward predictability-sensitive syntactic reduction (Jaeger, 2006) is robust in the face of a wide variety of control variables, and present evidence that speakers use both surface and structural cues for predictability estimation.},
  isbn = {978-0-262-25691-9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KHVNL7PY/SchÃ¶lkopf et al. - 2007 - Speakers optimize information density through synt.pdf}
}

@article{schreiber_listeners_2019,
  title = {Listeners Can Anticipate Future Segments before They Identify the Current One},
  author = {Schreiber, Kayleen E. and McMurray, Bob},
  year = {2019},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {4},
  pages = {1147--1166},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01712-9},
  abstract = {Speech unfolds rapidly over time, and the information necessary to recognize even a single phoneme may not be available simultaneously. Consequently, listeners must both integrate prior acoustic cues and anticipate future segments. Prior work on stop consonants and vowels suggests that listeners integrate asynchronous cues by partially activating lexical entries as soon as any information is available, and then updating this when later cues arrive. However, a recent study suggests that for the voiceless sibilant fricatives (/s/ and /{$\Elzesh$}/), listeners wait to initiate lexical access until all cues have arrived at the onset of the vowel. Sibilants also contain coarticulatory cues that could be used to anticipate the vowel upcoming. However, given these results, it is unclear if listeners could use them fast enough to speed vowel recognition. The current study examines anticipation by asking when listeners use coarticulatory information in the frication to predict the upcoming vowel. A visual world paradigm experiment found that listeners do not wait: they anticipate the vowel immediately from the onset of the frication, even as they wait several hundred milliseconds to identify the fricative. This finding suggests listeners do not strictly process phonemes in the order that they appear; rather the dynamics of language processing may be largely internal and only loosely coupled to the dynamics of the input.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/A7S3KJ5Q/Schreiber and McMurray - 2019 - Listeners can anticipate future segments before th.pdf}
}

@article{schrimpf_neural_2020,
  title = {The Neural Architecture of Language: {{Integrative}} Reverse-Engineering Converges on a Model for Predictive Processing},
  shorttitle = {The Neural Architecture of Language},
  author = {Schrimpf, Martin and Blank, Idan and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua and Fedorenko, Evelina},
  year = {2020},
  month = oct,
  journal = {bioRxiv},
  pages = {2020.06.26.174482},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.06.26.174482},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The neuroscience of perception has recently been revolutionized with an integrative reverse-engineering approach in which computation, brain function, and behavior are linked across many different datasets and many computational models. We here present a first systematic study taking this approach into higher-level cognition: human language processing, our species' signature cognitive skill. We find that the most powerful `transformer' networks predict neural responses at nearly 100\% and generalize across different datasets and data types (fMRI, ECoG). Across models, significant correlations are observed among all three metrics of performance: neural fit, fit to behavioral responses, and accuracy on the next-word prediction task (but not other language tasks), consistent with the long-standing hypothesis that the brain's language system is optimized for predictive processing. Model architectures with initial weights further perform surprisingly similar to final trained models, suggesting that inherent structure \textendash{} and not just experience with language \textendash{} crucially contributes to a model's match to the brain.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VGXH3A67/Schrimpf et al. - 2020 - The neural architecture of language Integrative r.pdf;/Users/xzfang/Zotero/storage/4P8NNMYB/2020.06.26.174482v2.html}
}

@article{schulte-korne_clinical_2010,
  title = {Clinical Neurophysiology of Visual and Auditory Processing in Dyslexia: {{A}} Review},
  shorttitle = {Clinical Neurophysiology of Visual and Auditory Processing in Dyslexia},
  author = {{Schulte-K{\"o}rne}, Gerd and Bruder, Jennifer},
  year = {2010},
  month = nov,
  journal = {Clinical Neurophysiology},
  volume = {121},
  number = {11},
  pages = {1794--1809},
  issn = {1388-2457},
  doi = {10.1016/j.clinph.2010.04.028},
  abstract = {Neurophysiological studies on children and adults with dyslexia provide a deeper understanding of how visual and auditory processing in dyslexia might relate to reading deficits. The goal of this review is to provide an overview of research findings in the last two decades on motion related and contrast sensitivity visual evoked potentials and on auditory event related potentials to basic tone and speech sound processing in dyslexia. These results are particularly relevant for three important theories about causality in dyslexia: the magnocellular deficit hypothesis, the temporal processing deficit hypothesis and the phonological deficit hypothesis. Support for magnocellular deficits in dyslexia are primarily provided from evidence for altered visual evoked potentials to rapidly moving stimuli presented at low contrasts. Consistently ERP findings revealed altered neurophysiological processes in individuals with dyslexia to speech stimuli, but evidence for deficits processing certain general acoustic information relevant for speech perception, such as frequency changes and temporal patterns, are also apparent.},
  langid = {english},
  keywords = {Auditory,Dyslexia,Electrophysiology,ERP,Neurophysiology,Prediction,Speech,Visual},
  file = {/Users/xzfang/Zotero/storage/6M22UWWU/Schulte-KÃ¶rne and Bruder - 2010 - Clinical neurophysiology of visual and auditory pr.pdf;/Users/xzfang/Zotero/storage/XGBLY6PY/S1388245710003810.html}
}

@article{schurgin_psychophysical_2020,
  title = {Psychophysical Scaling Reveals a Unified Theory of Visual Memory Strength},
  author = {Schurgin, Mark W. and Wixted, John T. and Brady, Timothy F.},
  year = {2020},
  month = sep,
  journal = {Nature Human Behaviour},
  pages = {1--17},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00938-0},
  abstract = {Almost all models of visual memory implicitly assume that errors in mnemonic representations are linearly related to distance in stimulus space. Here we show that neither memory nor perception are appropriately scaled in stimulus space; instead, they are based on a transformed similarity representation that is nonlinearly related to stimulus space. This result calls into question a foundational assumption of extant models of visual working memory. Once psychophysical similarity is taken into account, aspects of memory that have been thought to demonstrate a fixed working memory capacity of around three or four items and to require fundamentally different representations\textemdash across different stimuli, tasks and types of memory\textemdash can be parsimoniously explained with a unitary signal detection framework. These results have substantial implications for the study of visual memory and lead to a substantial reinterpretation of the relationship between perception, working memory and long-term memory.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IKLKTFAB/Schurgin et al. - 2020 - Psychophysical scaling reveals a unified theory of.pdf;/Users/xzfang/Zotero/storage/LLVS9NGW/s41562-020-00938-0.html}
}

@article{schuster_know_2020,
  title = {I Know What You're Probably Going to Say: {{Listener}} Adaptation to Variable Use of Uncertainty Expressions},
  shorttitle = {I Know What You're Probably Going to Say},
  author = {Schuster, Sebastian and Degen, Judith},
  year = {2020},
  month = oct,
  journal = {Cognition},
  volume = {203},
  pages = {104285},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104285},
  abstract = {Pragmatic theories of utterance interpretation share the assumption that listeners reason about alternative utterances that a speaker could have produced, but didn't. For such reasoning to be successful, listeners must have precise expectations about a speaker's production choices. This is at odds with the considerable variability across speakers that exists at all levels of linguistic representation. This tension can be reconciled by listeners adapting to the statistics of individual speakers. While linguistic adaptation is increasingly widely attested, semantic/pragmatic adaptation is underexplored. Moreover, what kind of representations listeners update during semantic/pragmatic adaptation \textendash{} estimates of the speaker's lexicon, or estimates of the speaker's utterance preferences \textendash{} remains poorly understood. In this work, we investigate semantic/pragmatic adaptation in the domain of uncertainty expressions like might and probably. In a series of web-based experiments, we find 1) that listeners vary in their expectations about a generic speaker's use of uncertainty expressions; 2) that listeners rapidly update their expectations about the use of uncertainty expressions after brief exposure to a speaker with a specific usage of uncertainty expressions; and 3) that listeners' interpretations of uncertainty expressions change after being exposed to a specific speaker. We present a novel computational model of semantic/pragmatic adaptation based on Bayesian belief updating and show, through a series of model comparisons, that semantic/pragmatic adaptation is best captured by listeners updating their beliefs both about the speaker's lexicon and their utterance preferences. This work has implications for both semantic theories of uncertainty expressions and psycholinguistic theories of adaptation: it highlights the need for dynamic semantic representations and suggests that listeners integrate their general linguistic knowledge with speaker-specific experiences to arrive at more precise interpretations.},
  langid = {english},
  keywords = {Adaptation,Bayesian cognitive modeling,Experimental pragmatics,Language comprehension,Uncertainty expressions},
  file = {/Users/xzfang/Zotero/storage/ES9F7X7D/Schuster and Degen - 2020 - I know what you're probably going to say Listener.pdf;/Users/xzfang/Zotero/storage/ARUVJIQ8/S0010027720301049.html}
}

@article{schuster_speakerspecific_2019,
  title = {Speaker-Specific Adaptation to Variable Use of Uncertainty Expressions},
  author = {Schuster, Sebastian and Degen, Judith},
  year = {2019},
  pages = {7},
  abstract = {Speakers exhibit variability in their choice between uncertainty expressions such as might and probably. Recent work has found that listeners cope with such variability by updating their expectations about how a specific speaker uses uncertainty expressions when interacting with a single speaker. However, it is still unclear to what extent listeners form speaker-specific expectations for multiple speakers and to what extent listeners are adapting to a situation independent of the speakers. Here, we take a first step towards answering these questions. In Experiment 1, listeners formed speaker-specific expectations after being exposed to two speakers whose use of uncertainty expressions differed. In Experiment 2, listeners who were exposed to two speakers with identical use of uncertainty expressions formed considerably stronger expectations than in Experiment 1. This suggests that listeners form both speaker-specific and situation-specific expectations. We discuss the implications of these results for theories of adaptation.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/W4KT8SSW/Schuster and Degen - Speaker-speciï¬c adaptation to variable use of unce.pdf}
}

@misc{schwartenbeck_generative_2021,
  title = {Generative Replay for Compositional Visual Understanding in the Prefrontal-Hippocampal Circuit},
  author = {Schwartenbeck, Philipp and Baram, Alon and Liu, Yunzhe and Mark, Shirley and Muller, Timothy and Dolan, Raymond and Botvinick, Matthew and {Kurth-Nelson}, Zeb and Behrens, Timothy},
  year = {2021},
  month = jun,
  pages = {2021.06.06.447249},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.06.447249},
  abstract = {Understanding the visual world is a constructive process. Whilst a frontal-hippocampal circuit is known to be essential for this task, little is known about the associated neuronal computations. Visual understanding appears superficially distinct from other known functions of this circuit, such as spatial reasoning and model-based planning, but recent models suggest deeper computational similarities. Here, using fMRI, we show that representations of a simple visual scene in these brain regions are relational and compositional \textendash{} key computational properties theorised to support rapid construction of hippocampal maps. Using MEG, we show that rapid sequences of representations, akin to replay in spatial navigation and planning problems, are also engaged in visual construction. Whilst these sequences have previously been proposed as mechanisms to plan possible futures or learn from the past, here they are used to understand the present. Replay sequences form constructive hypotheses about possible scene configurations. These hypotheses play out in an optimal order for relational inference, progressing from predictable to uncertain scene elements, gradually constraining possible configurations, and converging on the correct scene configuration. Together, these results suggest a computational bridge between apparently distinct functions of hippocampal-prefrontal circuitry, and a role for generative replay in constructive inference and hypothesis testing.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MW9VR36V/Schwartenbeck et al. - 2021 - Generative replay for compositional visual underst.pdf;/Users/xzfang/Zotero/storage/SCHYAP9X/2021.06.06.447249v1.html}
}

@article{schwartz_understanding_2019,
  title = {Understanding Language-Elicited {{EEG}} Data by Predicting It from a Fine-Tuned Language Model},
  author = {Schwartz, Dan and Mitchell, Tom},
  year = {2019},
  journal = {Proceedings of the 2019 Conference of the North},
  eprint = {1904.01548},
  eprinttype = {arxiv},
  pages = {43--57},
  doi = {10.18653/v1/N19-1005},
  abstract = {Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations \textendash{} known collectively as event-related potentials (ERPs) \textendash{} are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/R8F676EB/Schwartz and Mitchell - 2019 - Understanding language-elicited EEG data by predic.pdf}
}

@article{schwiedrzik_highlevel_2017,
  title = {High-{{Level Prediction Signals}} in a {{Low-Level Area}} of the {{Macaque Face-Processing Hierarchy}}},
  author = {Schwiedrzik, Caspar M. and Freiwald, Winrich A.},
  year = {2017},
  month = sep,
  journal = {Neuron},
  volume = {96},
  number = {1},
  pages = {89-97.e4},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.09.007},
  abstract = {Theories like predictive coding propose that lower-order brain areas compare their inputs to predictions derived from higher-order representations and signal their deviation as a prediction error. Here, we investigate whether the macaque face-processing system, a three-level hierarchy in the ventral stream, employs such a coding strategy. We show that after statistical learning of specific face sequences, the lower-level face area ML computes the deviation of actual from predicted stimuli. But these signals do not reflect the tuning characteristic of ML. Rather, they exhibit identity specificity and view invariance, the tuning properties of higher-level face areas AL and AM. Thus, learning appears to endow lower-level areas with the capability to test predictions at a higher level of abstraction than what is afforded by the feedforward sweep. These results provide evidence for computational architectures like predictive coding and suggest a new quality of functional organization of information-processing hierarchies beyond pure feedforward schemes.},
  langid = {english},
  keywords = {electrophysiology,face processing,functional magnetic resonance imaging,human,inferotemporal cortex,Macaque monkey,predictive coding,psychophysics,statistical learning},
  file = {/Users/xzfang/Zotero/storage/XPQDG98V/Schwiedrzik and Freiwald - 2017 - High-Level Prediction Signals in a Low-Level Area .pdf}
}

@article{schwiedrzik_pupil_2020,
  title = {Pupil {{Diameter Tracks Statistical Structure}} in the {{Environment}} to {{Increase Visual Sensitivity}}},
  author = {Schwiedrzik, Caspar M. and Sudmann, Sandrin S.},
  year = {2020},
  month = jun,
  journal = {Journal of Neuroscience},
  volume = {40},
  number = {23},
  pages = {4565--4575},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0216-20.2020},
  abstract = {Pupil diameter determines how much light hits the retina and, thus, how much information is available for visual processing. This is regulated by a brainstem reflex pathway. Here, we investigate whether this pathway is under the control of internal models about the environment. This would allow adjusting pupil dynamics to environmental statistics to augment information transmission. We present image sequences containing internal temporal structure to humans of either sex and male macaque monkeys. We then measure whether the pupil tracks this temporal structure not only at the rate of luminance variations, but also at the rate of statistics not available from luminance information alone. We find entrainment to environmental statistics in both species. This entrainment directly affects visual processing by increasing sensitivity at the environmentally relevant temporal frequency. Thus, pupil dynamics are matched to the temporal structure of the environment to optimize perception, in line with an active sensing account. SIGNIFICANCE STATEMENT When light hits the retina, the pupil reflexively constricts. This determines how much light and thus how much information is available for visual processing. We show that the rate at which the pupil constricts and dilates is matched to the temporal structure of our visual environment, although this information is not directly contained in the light variations that usually trigger reflexive pupil constrictions. Adjusting pupil diameter in accordance with environmental regularities optimizes information transmission at ecologically relevant temporal frequencies. We show that this is the case in humans and macaque monkeys, suggesting that the reflex pathways that regulate pupil diameter are under some degree of cognitive control across primate species.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2020 the authors},
  langid = {english},
  pmid = {32371603},
  keywords = {active sensing,primate,pupil,statistical learning},
  file = {/Users/xzfang/Zotero/storage/W2DYB72C/Schwiedrzik and Sudmann - 2020 - Pupil Diameter Tracks Statistical Structure in the.pdf;/Users/xzfang/Zotero/storage/M5J4UXA2/4565.html}
}

@article{scott_identification_2000,
  title = {Identification of a Pathway for Intelligible Speech in the Left Temporal Lobe},
  author = {Scott, Sophie K. and Blank, C. Catrin and Rosen, Stuart and Wise, Richard J. S.},
  year = {2000},
  month = dec,
  journal = {Brain},
  volume = {123},
  number = {12},
  pages = {2400--2406},
  issn = {0006-8950},
  doi = {10.1093/brain/123.12.2400},
  abstract = {It has been proposed that the identification of sounds, including species-specific vocalizations, by primates depends on anterior projections from the primary auditory cortex, an auditory pathway analogous to the ventral route proposed for the visual identification of objects. We have identified a similar route in the human for understanding intelligible speech. Using PET imaging to identify separable neural subsystems within the human auditory cortex, we used a variety of speech and speech-like stimuli with equivalent acoustic complexity but varying intelligibility. We have demonstrated that the left superior temporal sulcus responds to the presence of phonetic information, but its anterior part only responds if the stimulus is also intelligible. This novel observation demonstrates a left anterior temporal pathway for speech comprehension.},
  file = {/Users/xzfang/Zotero/storage/ZMFSYSSZ/Scott et al. - 2000 - Identification of a pathway for intelligible speec.pdf;/Users/xzfang/Zotero/storage/Q37EFZIN/325638.html}
}

@article{scott_neural_2009,
  title = {The Neural Processing of Masked Speech: {{Evidence}} for Different Mechanisms in the Left and Right Temporal Lobes},
  shorttitle = {The Neural Processing of Masked Speech},
  author = {Scott, Sophie K. and Rosen, Stuart and Beaman, C. Philip and Davis, Josh P. and Wise, Richard J. S.},
  year = {2009},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {125},
  number = {3},
  pages = {1737--1743},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.3050255},
  abstract = {It has been previously demonstrated that extensive activation in the dorsolateral temporal lobes associated with masking a speech target with a speech masker, consistent with the hypothesis that competition for central auditory processes is an important factor in informational masking. Here, masking from speech and two additional maskers derived from the original speech were investigated. One of these is spectrally rotated speech, which is unintelligible and has a similar (inverted) spectrotemporal profile to speech. The authors also controlled for the possibility of ``glimpsing'' of the target signal during modulated masking sounds by using speech-modulated noise as a masker in a baseline condition. Functional imaging results reveal that masking speech with speech leads to bilateral superior temporal gyrus (STG) activation relative to a speech-in-noise baseline, while masking speech with spectrally rotated speech leads solely to right STG activation relative to the baseline. This result is discussed in terms of hemispheric asymmetries for speech perception, and interpreted as showing that masking effects can arise through two parallel neural systems, in the left and right temporal lobes. This has implications for the competition for resources caused by speech and rotated speech maskers, and may illuminate some of the mechanisms involved in informational masking.}
}

@article{scott_positron_2004,
  title = {A Positron Emission Tomography Study of the Neural Basis of Informational and Energetic Masking Effects in Speech Perception},
  author = {Scott, Sophie K. and Rosen, Stuart and Wickham, Lindsay and Wise, Richard J. S.},
  year = {2004},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {115},
  number = {2},
  pages = {813--821},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.1639336}
}

@inproceedings{seetharaman_autoclip_2020,
  title = {Autoclip: {{Adaptive Gradient Clipping}} for {{Source Separation Networks}}},
  shorttitle = {Autoclip},
  booktitle = {2020 {{IEEE}} 30th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Seetharaman, Prem and Wichern, Gordon and Pardo, Bryan and Roux, Jonathan Le},
  year = {2020},
  month = sep,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Espoo, Finland}},
  doi = {10.1109/MLSP49062.2020.9231926},
  abstract = {Clipping the gradient is a known approach to improving gradient descent, but requires hand selection of a clipping threshold hyperparameter. We present AutoClip, a simple method for automatically and adaptively choosing a gradient clipping threshold, based on the history of gradient norms observed during training. Experimental results show that applying AutoClip results in improved generalization performance for audio source separation networks. Observation of the training dynamics of a separation network trained with and without AutoClip show that AutoClip guides optimization into smoother parts of the loss landscape. AutoClip is very simple to implement and can be integrated readily into a variety of applications across multiple domains.},
  isbn = {978-1-72816-662-9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/LV7H2RNW/Seetharaman et al. - 2020 - Autoclip Adaptive Gradient Clipping for Source Se.pdf}
}

@article{semmelmann_online_2017,
  title = {Online Psychophysics: Reaction Time Effects in Cognitive Experiments},
  shorttitle = {Online Psychophysics},
  author = {Semmelmann, Kilian and Weigelt, Sarah},
  year = {2017},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {49},
  number = {4},
  pages = {1241--1260},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0783-4},
  abstract = {Using the Internet to acquire behavioral data is currently on the rise. However, very basic questions regarding the feasibility of online psychophysics are still open. Here, we aimed to replicate five well-known paradigms in experimental psychology (Stroop, Flanker, visual search, masked priming, attentional blink) in three settings (classical ``lab'', ``web-in-lab'', ``web'') to account for possible changes in technology and environment. Lab and web-in-lab data were both acquired in an in-lab setting with lab using ``Gold Standard'' methods, while web-in-lab used web technology. This allowed for a direct comparison of potential differences in acquisition software. To account for additional environmental differences, the web technology experiments were published online to participate from home (setting web), thereby keeping the software and experimental design identical and only changing the environmental setting. Our main results are: First, we found an expected fixed additive timing offset when using web technology (M = 37 ms, SD = 8.14) and recording online (M = 87 ms, SD = 16.04) in comparison to lab data. Second, all task-specific effects were reproduced except for the priming paradigm, which couldn't be replicated in any setting. Third, there were no differences in error rates, which are independent of the timing offset. This finding further supports the assumption of data equality over all settings. Fourth, we found that browser type might be influencing absolute reaction times. Together, these results contribute to the slowly but steadily growing literature that online psychophysics is a suitable complement \textendash{} or even substitute \textendash{} to lab data acquisition.},
  langid = {english},
  keywords = {Cognitive psychology,Online study,Psychophysics,Reaction time,Replication,Web technology},
  file = {/Users/xzfang/Zotero/storage/MTEBYIFW/Semmelmann and Weigelt - 2017 - Online psychophysics reaction time effects in cog.pdf}
}

@article{senghas_intergenerational_2003,
  title = {Intergenerational Influence and Ontogenetic Development in the Emergence of Spatial Grammar in {{Nicaraguan Sign Language}}},
  author = {Senghas, Ann},
  year = {2003},
  month = oct,
  journal = {Cognitive Development},
  volume = {18},
  number = {4},
  pages = {511--531},
  issn = {08852014},
  doi = {10.1016/j.cogdev.2003.09.006},
  abstract = {The recent emergence of a new sign language among deaf children and adolescents in Nicaragua provides an opportunity to study how grammatical features of a language arise and spread, and how new language environments are constructed. The grammatical regularities that underlie language use reside largely outside the domain of explicit awareness. Nevertheless, knowledge of these regularities must be transmitted from one generation to the next to survive as part of the language. During this transmission, language form and use is shaped by both the characteristics of ontogenetic development within individual users and by historical changes in patterns of interaction between users. To capture this process, the present study follows the emergence of spatial modulations in Nicaraguan Sign Language (NSL). A comprehension task examining interpretations of spatially modulated verbs reveals that new form-function mappings arise among children who functionally differentiate previously equivalent forms. The new mappings are then acquired by their age peers (who are also children), and by subsequent generations of children who learn the language, but not by adult contemporaries. As a result, language emergence is characterized by a convergence on form within each age cohort, and a mismatch in form from one age cohort to the cohort that follows. In this way, each age cohort, in sequence, transforms the language environment for the next, enabling each new cohort of learners to develop further than its predecessors.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DX89ZSJI/Senghas - 2003 - Intergenerational influence and ontogenetic develo.pdf}
}

@article{serre_feedforward_2007,
  title = {A Feedforward Architecture Accounts for Rapid Categorization},
  author = {Serre, Thomas and Oliva, Aude and Poggio, Tomaso},
  year = {2007},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {15},
  pages = {6424--6429},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0700622104},
  abstract = {Primates are remarkably good at recognizing objects. The level of performance of their visual system and its robustness to image degradations still surpasses the best computer vision systems despite decades of engineering effort. In particular, the high accuracy of primates in ultra rapid object categorization and rapid serial visual presentation tasks is remarkable. Given the number of processing stages involved and typical neural latencies, such rapid visual processing is likely to be mostly feedforward. Here we show that a specific implementation of a class of feedforward theories of object recognition (that extend the Hubel and Wiesel simple-to-complex cell hierarchy and account for many anatomical and physiological constraints) can predict the level and the pattern of performance achieved by humans on a rapid masked animal vs. non-animal categorization task.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2007 by The National Academy of Sciences of the USA.                    Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {17404214},
  keywords = {computational model,natural scenes,object recognition,preattentive vision,visual cortex},
  file = {/Users/xzfang/Zotero/storage/ZULQHYQG/Serre et al. - 2007 - A feedforward architecture accounts for rapid cate.pdf;/Users/xzfang/Zotero/storage/BUA9BFBQ/6424.html}
}

@article{servant_dynamics_2019,
  title = {Dynamics of Attentional Focusing in the {{Eriksen}} Flanker Task},
  author = {Servant, Mathieu and Logan, Gordon D.},
  year = {2019},
  month = nov,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {8},
  pages = {2710--2721},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01796-3},
  abstract = {Eriksen and Eriksen (Perception \& Psychophysics, 16, 143\textendash 149, 1974) explained the flanker compatibility effect in terms of response competition. A simplified version of the original flanker task, featuring a 1-to-1 mapping of stimuli onto responses, has become prominent in the literature. Compatible flanker trials present identical items (HHHHH), whereas incompatible trials present different items (HHSHH). The 1-to-1 mapping is potentially problematic because it invites a strategy that people could use to perform the task. Subjects could first determine whether all the items are the same and focus attention on the central target only if they are not. Response times (RTs) would be longer for incompatible trials partly because they require the extra step of focusing attention. We tested this conditional focusing hypothesis by combining a 1-to-1 flanker task with a digit probe detection procedure. In half of the trials, the digit `7' appeared immediately after the response to the flanker display, at the target or a flanker location. Three experiments showed a V-shaped function of RTs to digits across locations that was not modulated by flanker compatibility. These results demonstrate that subjects focused attention on the central target regardless of the same/different configuration of the display, refuting the conditional focusing hypothesis. Our findings support Eriksen and Eriksen's original interpretation of the flanker task.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/TYN2MUXZ/Servant and Logan - 2019 - Dynamics of attentional focusing in the Eriksen fl.pdf}
}

@article{severens_timed_2005,
  title = {Timed Picture Naming Norms for 590 Pictures in {{Dutch}}},
  author = {Severens, Els and Lommel, Sven Van and Ratinckx, Elie and Hartsuiker, Robert J.},
  year = {2005},
  month = jun,
  journal = {Acta Psychologica},
  volume = {119},
  number = {2},
  pages = {159--187},
  issn = {0001-6918},
  doi = {10.1016/j.actpsy.2005.01.002},
  abstract = {The present study presents timed norms for 590 pictures in Belgian Dutch. We determined name agreement and response latencies. Furthermore, we assessed which factors influenced the naming latencies of the pictures. It appeared that age-of-acquisition, the H-statistic (an index of name agreement), and the number of syllables of the dominant response were significant predictors of the naming latencies. These results are discussed in comparison with previous findings.},
  langid = {english},
  keywords = {Age-of-acquisition effect,Name agreement,Picture naming},
  file = {/Users/xzfang/Zotero/storage/ZAE8PQZP/S0001691805000120.html}
}

@article{shahin_alpha_2012,
  title = {Alpha Activity Marking Word Boundaries Mediates Speech Segmentation},
  author = {Shahin, Antoine J. and Pitt, Mark A.},
  year = {2012},
  journal = {European Journal of Neuroscience},
  volume = {36},
  number = {12},
  pages = {3740--3748},
  issn = {1460-9568},
  doi = {10.1111/ejn.12008},
  abstract = {This study examined the neurophysiological mechanisms of speech segmentation, the process of parsing the continuous speech signal into isolated words. Individuals listened to sequences of two monosyllabic words (e.g. gas source) and non-words (e.g. nas sorf). When these phrases are spoken, talkers usually produce one continuous s-sound, not two distinct s-sounds, making it unclear where one word ends and the next one begins. This ambiguity in the signal can also result in perceptual ambiguity, causing the sequence to be heard as one word (failed to segment) or two words (segmented). We compared listeners' electroencephalogram activity when they reported hearing one word or two words, and found that bursts of fronto-central alpha activity (9\textendash 14 Hz), following the onset of the physical /s/ and end of phrase, indexed speech segmentation. Left-lateralized beta activity (14\textendash 18 Hz) following the end of phrase distinguished word from non-word segmentation. A hallmark of enhanced alpha activity is that it reflects inhibition of task-irrelevant neural populations. Thus, the current results suggest that disengagement of neural processes that become irrelevant as the words unfold marks word boundaries in continuous speech, leading to segmentation. Beta activity is likely associated with unifying word representations into coherent phrases.},
  langid = {english},
  keywords = {alpha activity,auditory object,EEG,neural inhibition,speech segmentation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.12008},
  file = {/Users/xzfang/Zotero/storage/4WEX3ZZW/Shahin and Pitt - 2012 - Alpha activity marking word boundaries mediates sp.pdf;/Users/xzfang/Zotero/storage/B6CNHQZX/ejn.html}
}

@article{shahin_neural_2018,
  title = {Neural {{Mechanisms Underlying Cross-Modal Phonetic Encoding}}},
  author = {Shahin, Antoine J. and Backer, Kristina C. and Rosenblum, Lawrence D. and Kerlin, Jess R.},
  year = {2018},
  month = feb,
  journal = {The Journal of Neuroscience},
  volume = {38},
  number = {7},
  pages = {1835--1849},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.1566-17.2017},
  abstract = {Audiovisual (AV) integration is essential for speech comprehension, especially in adverse listening situations. Divergent, but not mutually exclusive, theories have been proposed to explain the neural mechanisms underlying AV integration. One theory advocates that this process occurs via interactions between the auditory and visual cortices, as opposed to fusion of AV percepts in a multisensory integrator. Building upon this idea, we proposed that AV integration in spoken language reflects visually induced weighting of phonetic representations at the auditory cortex. EEG was recorded while male and female human subjects watched and listened to videos of a speaker uttering consonant vowel (CV) syllables /ba/ and /fa/, presented in Auditory-only, AV congruent or incongruent contexts. Subjects reported whether they heard /ba/ or /fa/. We hypothesized that vision alters phonetic encoding by dynamically weighting which phonetic representation in the auditory cortex is strengthened or weakened. That is, when subjects are presented with visual /fa/ and acoustic /ba/ and hear /fa/ (illusion-fa), the visual input strengthens the weighting of the phone /f/ representation. When subjects are presented with visual /ba/ and acoustic /fa/ and hear /ba/ (illusion-ba), the visual input weakens the weighting of the phone /f/ representation. Indeed, we found an enlarged N1 auditory evoked potential when subjects perceived illusion-ba, and a reduced N1 when they perceived illusion-fa, mirroring the N1 behavior for /ba/ and /fa/ in Auditory-only settings. These effects were especially pronounced in individuals with more robust illusory perception. These findings provide evidence that visual speech modifies phonetic encoding at the auditory cortex., SIGNIFICANCE STATEMENT The current study presents evidence that audiovisual integration in spoken language occurs when one modality (vision) acts on representations of a second modality (audition). Using the McGurk illusion, we show that visual context primes phonetic representations at the auditory cortex, altering the auditory percept, evidenced by changes in the N1 auditory evoked potential. This finding reinforces the theory that audiovisual integration occurs via visual networks influencing phonetic representations in the auditory cortex. We believe that this will lead to the generation of new hypotheses regarding cross-modal mapping, particularly whether it occurs via direct or indirect routes (e.g., via a multisensory mediator).},
  pmcid = {PMC5815461},
  pmid = {29263241},
  file = {/Users/xzfang/Zotero/storage/UG5ER8D4/Shahin et al. - 2018 - Neural Mechanisms Underlying Cross-Modal Phonetic .pdf}
}

@article{shain_fmri_2020,
  title = {{{fMRI}} Reveals Language-Specific Predictive Coding during Naturalistic Sentence Comprehension},
  author = {Shain, Cory and Blank, Idan Asher and {van Schijndel}, Marten and Schuler, William and Fedorenko, Evelina},
  year = {2020},
  month = feb,
  journal = {Neuropsychologia},
  volume = {138},
  pages = {107307},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2019.107307},
  abstract = {Much research in cognitive neuroscience supports prediction as a canonical computation of cognition across domains. Is such predictive coding implemented by feedback from higher-order domain-general circuits, or is it locally implemented in domain-specific circuits? What information sources are used to generate these predictions? This study addresses these two questions in the context of language processing. We present fMRI evidence from a naturalistic comprehension paradigm (1) that predictive coding in the brain's response to language is domain-specific, and (2) that these predictions are sensitive both to local word co-occurrence patterns and to hierarchical structure. Using a recently developed continuous-time deconvolutional regression technique that supports data-driven hemodynamic response function discovery from continuous BOLD signal fluctuations in response to naturalistic stimuli, we found effects of prediction measures in the language network but not in the domain-general multiple-demand network, which supports executive control processes and has been previously implicated in language comprehension. Moreover, within the language network, surface-level and structural prediction effects were separable. The predictability effects in the language network were substantial, with the model capturing over 37\% of explainable variance on held-out data. These findings indicate that human sentence processing mechanisms generate predictions about upcoming words using cognitive processes that are sensitive to hierarchical structure and specialized for language processing, rather than via feedback from high-level executive control mechanisms.},
  langid = {english},
  keywords = {fMRI,Language,Multiple demand network,Naturalistic,Predictive coding,Sentence processing,Surprisal,Syntactic structure},
  file = {/Users/xzfang/Zotero/storage/9XIQ92AS/Shain et al. - 2020 - fMRI reveals language-specific predictive coding d.pdf;/Users/xzfang/Zotero/storage/FXV9YSSA/S0028393219303495.html}
}

@misc{shain_robust_2021,
  title = {Robust Effects of Working Memory Demand during Naturalistic Language Comprehension in Language-Selective Cortex},
  author = {Shain, Cory and Blank, Idan A. and Fedorenko, Evelina and Gibson, Edward and Schuler, William},
  year = {2021},
  month = sep,
  pages = {2021.09.18.460917},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.09.18.460917},
  abstract = {A standard view of human language processing is that comprehenders build richly structured mental representations of natural language utterances, word by word, using computationally costly memory operations supported by domain-general working memory resources. However, three core claims of this view have been questioned, with some prior work arguing that (1) rich word-by-word structure building is not a core function of the language comprehension system, (2) apparent working memory costs are underlyingly driven by word predictability (surprisal), and/or (3) language comprehension relies primarily on domain-general rather than domain-specific working memory resources. In this work, we simultaneously evaluate all three of these claims using naturalistic comprehension in fMRI. In each participant, we functionally localize (a) a language-selective network and (b) a `multiple-demand' network that supports working memory across domains, and we analyze the responses in these two networks of interest during naturalistic story listening with respect to a range of theory-driven predictors of working memory demand under rigorous surprisal controls. Results show robust surprisal-independent effects of word-by-word memory demand in the language network and no effect of working memory demand in the multiple demand network. Our findings thus support the view that language comprehension (1) entails word-by-word structure building using (2) computationally intensive memory operations that are not explained by surprisal. However, these results challenge (3) the domain-generality of the resources that support these operations, instead indicating that working memory operations for language comprehension are carried out by the same neural resources that store linguistic knowledge. Significance Statement This study uses fMRI to investigate signatures of working memory (WM) demand during naturalistic story listening, using a broad range of theoretically motivated estimates of WM demand. Results support a strong effect of WM demand in language-selective brain regions but no effect of WM demand in ``multiple demand'' regions that have previously been associated with WM in non-linguistic domains. We further show evidence that WM effects in language regions are distinct from effects of word predictability. Our findings support a core role for WM in incremental language processing, using WM resources that are specialized for language.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SPF7PNER/Shain et al. - 2021 - Robust effects of working memory demand during nat.pdf;/Users/xzfang/Zotero/storage/RBTSKFDE/2021.09.18.460917v1.html}
}

@article{shamma_temporal_2011,
  title = {Temporal Coherence and Attention in Auditory Scene Analysis},
  author = {Shamma, Shihab A. and Elhilali, Mounya and Micheyl, Christophe},
  year = {2011},
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {34},
  number = {3},
  pages = {114--123},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2010.11.002},
  abstract = {Humans and other animals can attend to one of multiple sounds and follow it selectively over time. The neural underpinnings of this perceptual feat remain mysterious. Some studies have concluded that sounds are heard as separate streams when they activate well-separated populations of central auditory neurons, and that this process is largely pre-attentive. Here, we argue instead that stream formation depends primarily on temporal coherence between responses that encode various features of a sound source. Furthermore, we postulate that only when attention is directed towards a particular feature (e.g. pitch) do all other temporally coherent features of that source (e.g. timbre and location) become bound together as a stream that is segregated from the incoherent features of other sources.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/X8D739XA/Shamma et al. - 2011 - Temporal coherence and attention in auditory scene.pdf;/Users/xzfang/Zotero/storage/2IS3ZM2Q/S0166223610001670.html}
}

@article{shankar_scaleinvariant_2012,
  title = {A {{Scale-Invariant Internal Representation}} of {{Time}}},
  author = {Shankar, Karthik H. and Howard, Marc W.},
  year = {2012},
  month = jan,
  journal = {Neural Computation},
  volume = {24},
  number = {1},
  pages = {134--193},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00212},
  abstract = {We propose a principled way to construct an internal representation of the temporal stimulus history leading up to the present moment. A set of leaky integrators performs a Laplace transform on the stimulus function, and a linear operator approximates the inversion of the Laplace transform. The result is a representation of stimulus history that retains information about the temporal sequence of stimuli. This procedure naturally represents more recent stimuli more accurately than less recent stimuli; the decrement in accuracy is precisely scale invariant. This procedure also yields time cells that fire at specific latencies following the stimulus with a scale-invariant temporal spread. Combined with a simple associative memory, this representation gives rise to a moment-to-moment prediction that is also scale invariant in time. We propose that this scale-invariant representation of temporal stimulus history could serve as an underlying representation accessible to higher-level behavioral and cognitive mechanisms. In order to illustrate the potential utility of this scale-invariant representation in a variety of fields, we sketch applications using minimal performance functions to problems in classical conditioning, interval timing, scale-invariant learning in autoshaping, and the persistence of the recency effect in episodic memory across timescales.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MVLAHSNE/Shankar and Howard - 2012 - A Scale-Invariant Internal Representation of Time.pdf}
}

@article{shannon_mathematical_1984,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  year = {1984},
  pages = {53},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/HTWZM2BR/Shannon - A Mathematical Theory of Communication.pdf}
}

@article{sheintuch_multiple_2020,
  title = {Multiple {{Maps}} of the {{Same Spatial Context Can Stably Coexist}} in the {{Mouse Hippocampus}}},
  author = {Sheintuch, Liron and Geva, Nitzan and Baumer, Hadas and Rechavi, Yoav and Rubin, Alon and Ziv, Yaniv},
  year = {2020},
  month = apr,
  journal = {Current Biology},
  volume = {30},
  number = {8},
  pages = {1467-1476.e6},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2020.02.018},
  abstract = {Hippocampal place cells selectively fire when an animal traverses a particular location and are considered a neural substrate of spatial memory. Place cells were shown to change their activity patterns (remap) across different spatial contexts but to maintain their spatial tuning in a fixed familiar context. Here, we show that mouse hippocampal neurons can globally remap, forming multiple distinct representations (maps) of the same familiar environment, without any apparent changes in sensory input or behavior. Alternations between maps occurred only across separate visits to the environment, implying switching between distinct stable attractors in the hippocampal network. Importantly, the different maps were spatially informative and persistent over weeks, demonstrating that they can be reliably stored and retrieved from long-term memory. Taken together, our results suggest that a memory of a given spatial context could be associated with multiple distinct neuronal representations, rather than just one.},
  langid = {english},
  keywords = {attractor,calcium imaging,cognitive map,hippocampus,memory,navigation,place cells,remapping},
  file = {/Users/xzfang/Zotero/storage/ENRJPQ56/Sheintuch et al. - 2020 - Multiple Maps of the Same Spatial Context Can Stab.pdf;/Users/xzfang/Zotero/storage/BKV9NFNG/S0960982220301913.html}
}

@article{shen_comparing_,
  title = {Comparing {{Models}} of {{Associative Meaning}}: {{An Empirical Investigation}} of {{Reference}} in {{Simple Language Games}}},
  author = {Shen, Judy Hanwen and Hofer, Matthias and Felbo, Bjarke and Levy, Roger},
  pages = {10},
  abstract = {Simple reference games (Wittgenstein, 1953) are of central theoretical and empirical importance in the study of situated language use. Although language provides rich, compositional truth-conditional semantics to facilitate reference, speakers and listeners may sometimes lack the overall lexical and cognitive resources to guarantee successful reference through these means alone. However, language also has rich associational structures that can serve as a further resource for achieving successful reference. Here we investigate this use of associational information in a setting where only associational information is available: a simplified version of the popular game Codenames. Using optimal experiment design techniques, we compare a range of models varying in the type of associative information deployed and in level of pragmatic sophistication against human behavior. In this setting we find that listeners' behavior reflects direct bigram collocational associations more strongly than word-embedding or semantic knowledge graph-based associations and that there is little evidence for pragmatically sophisticated behavior by either speakers or listeners of the type that might be predicted by recursive-reasoning models such as the Rational Speech Acts theory. These results shed light on the nature of the lexical resources that speakers and listeners can bring to bear in achieving reference through associative meaning alone.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8UCSWWUG/Shen et al. - Comparing Models of Associative Meaning An Empiri.pdf}
}

@article{shen_implicit_2012,
  title = {Implicit {{Temporal Expectation Attenuates Auditory Attentional Blink}}},
  author = {Shen, Dawei and Alain, Claude},
  year = {2012},
  month = apr,
  journal = {PLoS ONE},
  volume = {7},
  number = {4},
  pages = {e36031},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0036031},
  abstract = {Attentional blink (AB) describes a phenomenon whereby correct identification of a first target impairs the processing of a second target (i.e., probe) nearby in time. Evidence suggests that explicit attention orienting in the time domain can attenuate the AB. Here, we used scalp-recorded, event-related potentials to examine whether auditory AB is also sensitive to implicit temporal attention orienting. Expectations were set up implicitly by varying the probability (i.e., 80\% or 20\%) that the probe would occur at the +2 or +8 position following target presentation. Participants showed a significant AB, which was reduced with the increased probe probability at the +2 position. The probe probability effect was paralleled by an increase in P3b amplitude elicited by the probe. The results suggest that implicit temporal attention orienting can facilitate short-term consolidation of the probe and attenuate auditory AB.},
  pmcid = {PMC3338751},
  pmid = {22558312},
  file = {/Users/xzfang/Zotero/storage/FKLXY2IA/Shen and Alain - 2012 - Implicit Temporal Expectation Attenuates Auditory .pdf}
}

@article{shen_variable_2019,
  title = {Variable Precision in Visual Perception},
  author = {Shen, Shan and Ji Ma, Wei},
  year = {2019},
  month = jan,
  journal = {Psychological review},
  volume = {126},
  number = {1},
  pages = {89--132},
  issn = {0033-295X},
  doi = {10.1037/rev0000128},
  abstract = {Given the same sensory stimuli in the same task, human observers do not always make the same response. Well-known sources of behavioral variability are sensory noise and guessing. Visual short-term memory studies have suggested that the precision of the sensory noise is itself variable. However, it is unknown whether precision is also variable in perceptual tasks without a memory component. We searched for evidence for variable precision in 11 visual perception tasks with a single relevant feature, orientation. We specifically examined the effect of distractor stimuli: distractors were absent, homogeneous and fixed across trials, homogeneous and variable, or heterogeneous and variable. We first considered four models: with and without guessing, and with and without variability in precision. We quantified the importance of both factors using six metrics: factor knock-in difference, factor knock-out difference, and log factor posterior ratio, each based on AIC or BIC. According to all six metrics, we found strong evidence for variable precision in five experiments. Next, we extended our model space to include potential confounding factors: the oblique effect and decision noise. This left strong evidence for variable precision in only one experiment, in which distractors were homogeneous but variable. Finally, when we considered suboptimal decision rules, the evidence also disappeared in this experiment. Our results provide little evidence for variable precision overall and only a hint when distractors are variable. Methodologically, the results underline the importance of including multiple factors in factorial model comparison: testing for only two factors would have yielded an incorrect conclusion.},
  pmcid = {PMC6318066},
  pmid = {30335411},
  file = {/Users/xzfang/Zotero/storage/FR2DA7HU/Shen and Ji Ma - 2019 - Variable precision in visual perception.pdf}
}

@article{shenhav_expected_2013,
  title = {The Expected Value of Control: {{An}} Integrative Theory of Anterior Cingulate Cortex Function},
  shorttitle = {The Expected Value of Control},
  author = {Shenhav, Amitai and Botvinick, Matthew M. and Cohen, Jonathan D.},
  year = {2013},
  month = jul,
  journal = {Neuron},
  volume = {79},
  number = {2},
  pages = {217--240},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.07.007},
  abstract = {The dorsal anterior cingulate cortex (dACC) has a near-ubiquitous presence in the neuroscience of cognitive control. It has been implicated in a diversity of functions, from reward processing and performance monitoring to the execution of control and action selection. Here, we propose that this diversity can be understood in terms of a single underlying function: allocation of control based on an evaluation of the expected value of control (EVC). We present a normative model of EVC that integrates three critical factors: the expected payoff from a controlled process, the amount of control that must be invested to achieve that payoff, and the cost in terms of cognitive effort. We propose that dACC integrates this information, using it to determine whether, where and how much control to allocate. We then consider how the EVC model can explain the diverse array of findings concerning dACC function.},
  pmcid = {PMC3767969},
  pmid = {23889930},
  file = {/Users/xzfang/Zotero/storage/RQ25TD83/Shenhav et al. - 2013 - The expected value of control An integrative theo.pdf}
}

@misc{sherafati_dorsolateral_2021,
  title = {Dorsolateral Prefrontal Cortex Supports Speech Perception in Listeners with Cochlear Implants},
  author = {Sherafati, Arefeh and Dwyer, Noel and Bajracharya, Aahana and Hassanpour, Mahlega S. and Eggebrecht, Adam T. and Firszt, Jill B. and Culver, Joseph P. and Peelle, Jonathan E.},
  year = {2021},
  month = oct,
  pages = {2021.10.16.464654},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.10.16.464654},
  abstract = {Cochlear implants are neuroprosthetic devices that can restore hearing in individuals with severe to profound hearing loss by electrically stimulating the auditory nerve. Because of physical limitations on the precision of this stimulation, the acoustic information delivered by a cochlear implant does not convey the same level of spectral detail as that conveyed by normal hearing. As a result, speech understanding in listeners with cochlear implants is typically poorer and more effortful than in listeners with normal hearing. The brain networks supporting speech understanding in listeners with cochlear implants are not well understood, partly due to difficulties obtaining functional neuroimaging data in this population. In the current study, we assessed the brain regions supporting spoken word understanding in adult listeners with right unilateral cochlear implants (n=20) and matched controls (n=18) using high-density diffuse optical tomography (HD-DOT), a quiet and non-invasive imaging modality with spatial resolution comparable to that of functional MRI. We found that while listening to spoken words in quiet, listeners with cochlear implants showed greater activity in the left dorsolateral prefrontal cortex, overlapping with functionally-defined domain-general processing seen in a spatial working memory task. These results suggest that listeners with cochlear implants require greater cognitive processing during speech understanding than listeners with normal hearing, supported by compensatory recruitment in the left dorsolateral prefrontal cortex.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/CSVCZPUZ/Sherafati et al. - 2021 - Dorsolateral prefrontal cortex supports speech per.pdf;/Users/xzfang/Zotero/storage/F5F3VYAA/2021.10.16.html}
}

@article{shiell_enhancement_2014,
  title = {Enhancement of {{Visual Motion Detection Thresholds}} in {{Early Deaf People}}},
  author = {Shiell, Martha M. and Champoux, Fran{\c c}ois and Zatorre, Robert J.},
  year = {2014},
  month = feb,
  journal = {PLOS ONE},
  volume = {9},
  number = {2},
  pages = {e90498},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0090498},
  abstract = {In deaf people, the auditory cortex can reorganize to support visual motion processing. Although this cross-modal reorganization has long been thought to subserve enhanced visual abilities, previous research has been unsuccessful at identifying behavioural enhancements specific to motion processing. Recently, research with congenitally deaf cats has uncovered an enhancement for visual motion detection. Our goal was to test for a similar difference between deaf and hearing people. We tested 16 early and profoundly deaf participants and 20 hearing controls. Participants completed a visual motion detection task, in which they were asked to determine which of two sinusoidal gratings was moving. The speed of the moving grating varied according to an adaptive staircase procedure, allowing us to determine the lowest speed necessary for participants to detect motion. Consistent with previous research in deaf cats, the deaf group had lower motion detection thresholds than the hearing. This finding supports the proposal that cross-modal reorganization after sensory deprivation will occur for supramodal sensory features and preserve the output functions.},
  langid = {english},
  keywords = {Auditory cortex,Deafness,Hearing disorders,Language,Motion,Sensory perception,Velocity,Vision},
  file = {/Users/xzfang/Zotero/storage/RQMMX6EX/Shiell et al. - 2014 - Enhancement of Visual Motion Detection Thresholds .pdf;/Users/xzfang/Zotero/storage/4N3WMAVZ/article.html}
}

@article{shiell_reorganization_2015,
  title = {Reorganization of {{Auditory Cortex}} in {{Early-deaf People}}: {{Functional Connectivity}} and {{Relationship}} to {{Hearing Aid Use}}},
  shorttitle = {Reorganization of {{Auditory Cortex}} in {{Early-deaf People}}},
  author = {Shiell, Martha M. and Champoux, Fran{\c c}ois and Zatorre, Robert J.},
  year = {2015},
  month = jan,
  journal = {Journal of Cognitive Neuroscience},
  volume = {27},
  number = {1},
  pages = {150--163},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_00683},
  abstract = {Cross-modal reorganization after sensory deprivation is a model for understanding brain plasticity. Although it is a well-documented phenomenon, we still know little of the mechanisms underlying it or the factors that constrain and promote it. Using fMRI, we identified visual motion-related activity in 17 early-deaf and 17 hearing adults. We found that, in the deaf, the posterior superior temporal gyrus (STG) was responsive to visual motion. We compared functional connectivity of this reorganized cortex between groups to identify differences in functional networks associated with reorganization. In the deaf more than the hearing, the STG displayed increased functional connectivity with a region in the calcarine fissure. We also explored the role of hearing aid use, a factor that may contribute to variability in cross-modal reorganization. We found that both the cross-modal activity in STG and the functional connectivity between STG and calcarine cortex correlated with duration of hearing aid use, supporting the hypothesis that residual hearing affects cross-modal reorganization. We conclude that early auditory deprivation alters not only the organization of auditory regions but also the interactions between auditory and primary visual cortex and that auditory input, as indexed by hearing aid use, may inhibit cross-modal reorganization in early-deaf people.},
  file = {/Users/xzfang/Zotero/storage/8H29PK5I/Reorganization-of-Auditory-Cortex-in-Early-deaf.html}
}

@article{shiell_right_2016,
  title = {The {{Right Hemisphere Planum Temporale Supports Enhanced Visual Motion Detection Ability}} in {{Deaf People}}: {{Evidence}} from {{Cortical Thickness}}},
  shorttitle = {The {{Right Hemisphere Planum Temporale Supports Enhanced Visual Motion Detection Ability}} in {{Deaf People}}},
  author = {Shiell, Martha M. and Champoux, Fran{\c c}ois and Zatorre, Robert J.},
  year = {2016},
  journal = {Neural Plasticity},
  volume = {2016},
  pages = {7217630},
  issn = {1687-5443},
  doi = {10.1155/2016/7217630},
  abstract = {After sensory loss, the deprived cortex can reorganize to process information from the remaining modalities, a phenomenon known as cross-modal reorganization. In blind people this cross-modal processing supports compensatory behavioural enhancements in the nondeprived modalities. Deaf people also show some compensatory visual enhancements, but a direct relationship between these abilities and cross-modally reorganized auditory cortex has only been established in an animal model, the congenitally deaf cat, and not in humans. Using T1-weighted magnetic resonance imaging, we measured cortical thickness in the planum temporale, Heschl's gyrus and sulcus, the middle temporal area MT+, and the calcarine sulcus, in early-deaf persons. We tested for a correlation between this measure and visual motion detection thresholds, a visual function where deaf people show enhancements as compared to hearing. We found that the cortical thickness of a region in the right hemisphere planum temporale, typically an auditory region, was greater in deaf individuals with better visual motion detection thresholds. This same region has previously been implicated in functional imaging studies as important for functional reorganization. The structure-behaviour correlation observed here demonstrates this area's involvement in compensatory vision and indicates an anatomical correlate, increased cortical thickness, of cross-modal plasticity.},
  langid = {english},
  pmcid = {PMC4738967},
  pmid = {26885405},
  keywords = {Adult,Auditory Cortex,Brain Mapping,Deafness,Female,Functional Laterality,Humans,Magnetic Resonance Imaging,Male,Motion Perception,Organ Size,Photic Stimulation,Visual Perception,Young Adult},
  file = {/Users/xzfang/Zotero/storage/CLYIHPPT/Shiell et al. - 2016 - The Right Hemisphere Planum Temporale Supports Enh.pdf}
}

@article{shin_memories_2021,
  title = {Memories off the Top of Your Head},
  author = {Shin, Jiyun N. and Doron, Guy and Larkum, Matthew E.},
  year = {2021},
  month = oct,
  journal = {Science},
  volume = {374},
  number = {6567},
  pages = {538--539},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.abk1859},
  file = {/Users/xzfang/Zotero/storage/3LUTD2ZH/Shin et al. - 2021 - Memories off the top of your head.pdf}
}

@article{shinn-cunningham_objectbased_2008,
  title = {Object-Based Auditory and Visual Attention},
  author = {{Shinn-Cunningham}, Barbara G.},
  year = {2008},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {12},
  number = {5},
  pages = {182--186},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2008.02.003},
  abstract = {Theories of visual attention argue that attention operates on perceptual objects, and thus that interactions between object formation and selective attention determine how competing sources interfere with perception. In auditory perception, theories of attention are less mature and no comprehensive framework exists to explain how attention influences perceptual abilities. However, the same principles that govern visual perception can explain many seemingly disparate auditory phenomena. In particular, many recent studies of `informational masking' can be explained by failures of either auditory object formation or auditory object selection. This similarity suggests that the same neural mechanisms control attention and influence perception across different sensory modalities.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GSTNI9RH/Shinn-Cunningham - 2008 - Object-based auditory and visual attention.pdf;/Users/xzfang/Zotero/storage/PYX9TMYY/S1364661308000600.html}
}

@article{shusterman_cognitive_2011,
  title = {Cognitive Effects of Language on Human Navigation},
  author = {Shusterman, Anna and Lee, Sang Ah and Spelke, Elizabeth S.},
  year = {2011},
  month = aug,
  journal = {Cognition},
  volume = {120},
  number = {2},
  pages = {186--201},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2011.04.004},
  abstract = {Language has been linked to spatial representation and behavior in humans, but the nature of this effect is debated. Here, we test whether simple verbal expressions improve 4-year-old children's performance in a disoriented search task in a small rectangular room with a single red landmark wall. Disoriented children's landmark-guided search for a hidden object was dramatically enhanced when the experimenter used certain verbal expressions to designate the landmark during the hiding event. Both a spatial expression (``I'm hiding the sticker at the red/white wall'') and a non-spatial but task-relevant expression (``The red/white wall can help you get the sticker'') enhanced children's search, relative to uncued controls. By contrast, a verbal expression that drew attention to the landmark in a task-irrelevant manner (``Look at this pretty red/white wall'') produced no such enhancement. These findings provide further evidence that language changes spatial behavior in children and illuminate one mechanism through which language exerts its effect: by helping children understand the relevance of landmarks for encoding locations.},
  pmcid = {PMC3572715},
  pmid = {21665199},
  file = {/Users/xzfang/Zotero/storage/88MK6TNL/Shusterman et al. - 2011 - Cognitive effects of language on human navigation.pdf}
}

@article{sidaras_perceptual_2009,
  title = {Perceptual Learning of Systematic Variation in {{Spanish-accented}} Speech},
  author = {Sidaras, Sabrina K. and Alexander, Jessica E. D. and Nygaard, Lynne C.},
  year = {2009},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {125},
  number = {5},
  pages = {3306--3316},
  issn = {0001-4966},
  doi = {10.1121/1.3101452},
  abstract = {Spoken language is characterized by an enormous amount of variability in how linguistic segments are realized. In order to investigate how speech perceptual processes accommodate to multiple sources of variation, adult native speakers of American English were trained with English words or sentences produced by six Spanish-accented talkers. At test, listeners transcribed utterances produced by six familiar or unfamiliar Spanish-accented talkers. With only brief exposure, listeners perceptually adapted to accent-general regularities in spoken language, generalizing to novel accented words and sentences produced by unfamiliar accented speakers. Acoustic properties of vowel production and their relation to identification performance were assessed to determine if the English listeners were sensitive to systematic variation in the realization of accented vowels. Vowels that showed the most improvement after Spanish-accented training were distinct from nearby vowels in terms of their acoustic characteristics. These findings suggest that the speech perceptual system dynamically adjusts to the acoustic consequences of changes in talker's voice and accent.},
  pmcid = {PMC2736743},
  pmid = {19425672},
  file = {/Users/xzfang/Zotero/storage/KGXXDFX3/Sidaras et al. - 2009 - Perceptual learning of systematic variation in Spa.pdf}
}

@article{silcox_costs_2021,
  title = {The Costs (and Benefits) of Effortful Listening on Context Processing: {{A}} Simultaneous Electrophysiology, Pupillometry, and Behavioral Study},
  shorttitle = {The Costs (and Benefits) of Effortful Listening on Context Processing},
  author = {Silcox, Jack W and Payne, Brennan R.},
  year = {2021},
  month = sep,
  journal = {Cortex},
  volume = {142},
  pages = {296--316},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2021.06.007},
  abstract = {There is an apparent disparity between the fields of cognitive audiology and cognitive electrophysiology as to how linguistic context is used when listening to perceptually challenging speech. To gain a clearer picture of how listening effort impacts context use, we conducted a pre-registered study to simultaneously examine electrophysiological, pupillometric, and behavioral responses when listening to sentences varying in contextual constraint and acoustic challenge in the same sample. Participants (N~=~44) listened to sentences that were highly constraining and completed with expected or unexpected sentence-final words (``The prisoners were planning their escape/party'') or were low-constraint sentences with unexpected sentence-final words (``All day she thought about the party''). Sentences were presented either in quiet or with +3~dB SNR background noise. Pupillometry and EEG were simultaneously recorded and subsequent sentence recognition and word recall were measured. While the N400 expectancy effect was diminished by noise, suggesting impaired real-time context use, we simultaneously observed a beneficial effect of constraint on subsequent recognition memory for degraded speech. Importantly, analyses of trial-to-trial coupling between pupil dilation and N400 amplitude showed that when participants' showed increased listening effort (i.e., greater pupil dilation), there was a subsequent recovery of the N400 effect, but at the same time, higher effort was related to poorer subsequent sentence recognition and word recall. Collectively, these findings suggest divergent effects of acoustic challenge and listening effort on context use: while noise impairs the rapid use of context to facilitate lexical semantic processing in general, this negative effect is attenuated when listeners show increased effort in response to noise. However, this effort-induced reliance on context for online word processing comes at the cost of poorer subsequent memory.},
  langid = {english},
  keywords = {Linguistic context,Listening effort,Memory,N400,Pupillometry,Single-trial analysis},
  file = {/Users/xzfang/Zotero/storage/MBXZF6LZ/Silcox and Payne - 2021 - The costs (and benefits) of effortful listening on.pdf;/Users/xzfang/Zotero/storage/8J2TVC49/S0010945221002203.html}
}

@misc{silston_threat_2021,
  title = {Threat Impairs Flexible Use of a Cognitive Map},
  author = {Silston, Brian and Ochsner, Kevin and Aly, Mariam},
  year = {2021},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/7p69d},
  abstract = {Goal-directed behavior requires adaptive systems that respond to environmental demands. In the absence of threat (or presence of reward), individuals are free to explore a large number of behavioral trajectories, effectively interrogating the environment across many dimensions. This leads to flexible, relational memory encoding and retrieval.  In the presence of imminent danger, motivation shifts to an imperative state characterized by a narrow focus of attention on threatening information. This impairs flexible, relational memory. Here, we test how these proposed motivational shifts (Murty \& Adcock, 2017) affect behavioral flexibility and memory. Participants learned the structure of a maze-like environment and navigated to the location of everyday objects in both safe and threatening contexts. The latter contained a predator that could `capture' participants, leading to electric shock. After learning, the path to some objects was blocked, forcing a detour for which one route was significantly shorter. We predicted that the threatening environment would push participants toward an imperative state, leading to less efficient and less flexible navigation. Across 3 studies, we found that threat caused participants to take longer paths to goal objects and less efficient detours when obstacles were encountered. Navigation was less efficient despite no impairment in recognition memory for the maps learned in safe vs threatening contexts. These results provide ecologically valid evidence that imperative states, triggered by threat, reduce the ability to flexibly use cognitive maps to guide behavior.},
  keywords = {Cognitive Psychology,Motivational Behavior,Social and Behavioral Sciences,Social and Personality Psychology},
  file = {/Users/xzfang/Zotero/storage/GI5IMLWZ/Silston et al. - 2021 - Threat impairs flexible use of a cognitive map.pdf}
}

@article{silva_rapid_2019,
  title = {Rapid {{Memory Reactivation}} at {{Movie Event Boundaries Promotes Episodic Encoding}}},
  author = {Silva, Marta and Baldassano, Christopher and Fuentemilla, Llu{\'i}s},
  year = {2019},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {43},
  pages = {8538--8548},
  file = {/Users/xzfang/Zotero/storage/9JUD2CYA/8538.html}
}

@article{silver_neuronal_2010,
  title = {Neuronal Arithmetic},
  author = {Silver, R. Angus},
  year = {2010},
  month = jul,
  journal = {Nature reviews. Neuroscience},
  volume = {11},
  number = {7},
  pages = {474--489},
  issn = {1471-003X},
  doi = {10.1038/nrn2864},
  abstract = {The vast computational power of the brain has traditionally been viewed as arising from the complex connectivity of neural networks, in which an individual neuron acts as a simple linear summation and thresholding device. However, recent studies show that individual neurons utilize a wealth of nonlinear mechanisms to transform synaptic input into output firing. These mechanisms can arise from synaptic plasticity, synaptic noise, and somatic and dendritic conductances. This tool kit of nonlinear mechanisms confers considerable computational power on both morphologically simple and more complex neurons, enabling them to perform a range of arithmetic operations on signals encoded in a variety of different ways.},
  pmcid = {PMC4750293},
  pmid = {20531421},
  file = {/Users/xzfang/Zotero/storage/XPM8E592/Silver - 2010 - Neuronal arithmetic.pdf}
}

@article{simmons_word_2018,
  title = {Word Length, Proportion of Overlap, And},
  author = {Simmons, Elizabeth Schoen and Magnuson, James S},
  year = {2018},
  journal = {Proceedings of the Cognitive Science Society},
  pages = {6},
  abstract = {We examined how phonological competition effects in spoken word recognition change with word length. Cohort effects (competition between words that overlap at onset) are strong and easily replicated. Rhyme effects (competition between words that mismatch at onset) are weaker, emerge later in the time course of spoken word recognition, and are more difficult to replicate. We conducted a simple experiment to examine cohort and rhyme competition using monosyllabic vs. bisyllabic words. Degree of competition was predicted by proportion of phonological overlap. Longer rhymes, with greater overlap in both number and proportion of shared phonemes, compete more strongly (e.g., kettle-medal [0.8 overlap] vs. cat-mat [0.67 overlap]). In contrast, long and short cohort pairs constrained to have constant (2-phoneme) overlap vary in proportion of overlap. Longer cohort pairs (e.g., camera-candle) have lower proportion of overlap (in this example, 0.33) than shorter cohorts (e.g., cat-can, with 0.67 overlap) and compete more weakly. This finding has methodological implications (rhyme effects are less likely to be observed with shorter words, while cohort effects are diminished for longer words), but also theoretical implications: degree of competition is not a simple function of overlapping phonemes; degree of competition is conditioned on proportion of overlap. Simulations with TRACE help explicate how this result might emerge.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DNN9BY4I/Simmons and Magnuson - Word length, proportion of overlap, and.pdf}
}

@article{simpson_letter_2013,
  title = {A Letter Visual-Similarity Matrix for {{Latin-based}} Alphabets},
  author = {Simpson, Ian C. and Mousikou, Petroula and Montoya, Juan Manuel and Defior, Sylvia},
  year = {2013},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {2},
  pages = {431--439},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0271-4},
  abstract = {Indicators of letter visual similarity have been used for controlling the design of empirical and neuropsychological studies and for rigorously determining the factors that underlie reading ability and literacy acquisition. Additionally, these letter similarity/confusability matrices have been useful for studies examining more general aspects of human cognition, such as perception. Despite many letter visual-similarity matrices being available, they all have two serious limitations if they are to be used by researchers in the reading domain: (1) They have been constructed using atypical reading data obtained from speeded reading-aloud tasks and/or under degraded presentation conditions; (2) they only include letters from the English alphabet. Although some letter visual-similarity matrices have been constructed using data gathered from normal reading conditions, these either are based on old fonts, which may not resemble the letters found in modern print, or were never published. For the first time, this article presents a comprehensive letter visual-similarity/confusability matrix that has been constructed based on untimed responses to clearly presented upper- and lowercase letters that are present in many languages that use Latin-based alphabets, including Catalan, Dutch, English, French, Galician, German, Italian, Portuguese, and Spanish. Such a matrix will be useful for researchers interested in the processes underpinning reading and literacy acquisition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YQYIUFGY/Simpson et al. - 2013 - A letter visual-similarity matrix for Latin-based .pdf}
}

@article{sims_ideal_2012,
  title = {An {{Ideal Observer Analysis}} of {{Visual Working Memory}}},
  author = {Sims, Chris R. and Jacobs, Robert A. and Knill, David C.},
  year = {2012},
  month = oct,
  journal = {Psychological review},
  volume = {119},
  number = {4},
  pages = {807--830},
  issn = {0033-295X},
  doi = {10.1037/a0029856},
  abstract = {Limits in visual working memory (VWM) strongly constrain human performance across many tasks. However, the nature of these limits is not well understood. In this paper we develop an ideal observer analysis of human visual working memory, by deriving the expected behavior of an optimally performing, but limited-capacity memory system. This analysis is framed around rate\textendash distortion theory, a branch of information theory that provides optimal bounds on the accuracy of information transmission subject to a fixed information capacity. The result of the ideal observer analysis is a theoretical framework that provides a task-independent and quantitative definition of visual memory capacity and yields novel predictions regarding human performance. These predictions are subsequently evaluated and confirmed in two empirical studies. Further, the framework is general enough to allow the specification and testing of alternative models of visual memory (for example, how capacity is distributed across multiple items). We demonstrate that a simple model developed on the basis of the ideal observer analysis\textemdash one which allows variability in the number of stored memory representations, but does not assume the presence of a fixed item limit\textemdash provides an excellent account of the empirical data, and further offers a principled re-interpretation of existing models of visual working memory.},
  pmcid = {PMC3646905},
  pmid = {22946744},
  file = {/Users/xzfang/Zotero/storage/SQEVMLS5/Sims et al. - 2012 - An Ideal Observer Analysis of Visual Working Memor.pdf}
}

@article{singh_accommodating_2016,
  title = {Accommodating {{Presuppositions Is Inappropriate}} in {{Implausible Contexts}}},
  author = {Singh, Raj and Fedorenko, Evelina and Mahowald, Kyle and Gibson, Edward},
  year = {2016},
  journal = {Cognitive Science},
  volume = {40},
  number = {3},
  pages = {607--634},
  issn = {1551-6709},
  doi = {10.1111/cogs.12260},
  abstract = {According to one view of linguistic information (Karttunen, 1974; Stalnaker, 1974), a speaker can convey contextually new information in one of two ways: (a) by asserting the content as new information; or (b) by presupposing the content as given information which would then have to be accommodated. This distinction predicts that it is conversationally more appropriate to assert implausible information rather than presuppose it (e.g., von Fintel, 2008; Heim, 1992; Stalnaker, 2002). A second view rejects the assumption that presuppositions are accommodated; instead, presuppositions are assimilated into asserted content and both are correspondingly open to challenge (e.g., Gazdar, 1979; van der Sandt, 1992). Under this view, we should not expect to find a difference in conversational appropriateness between asserting implausible information and presupposing it. To distinguish between these two views of linguistic information, we performed two self-paced reading experiments with an on-line stops-making-sense judgment. The results of the two experiments\textemdash using the presupposition triggers the and too\textemdash show that accommodation is inappropriate (makes less sense) relative to non-presuppositional controls when the presupposed information is implausible but not when it is plausible. These results provide support for the first view of linguistic information: the contrast in implausible contexts can only be explained if there is a presupposition-assertion distinction and accommodation is a mechanism dedicated to reasoning about presuppositions.},
  copyright = {Copyright \textcopyright{} 2015 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Accommodation,Pragmatics,Presupposition,Psycholinguistics,Semantics,Sentence processing},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12260},
  file = {/Users/xzfang/Zotero/storage/D8ZPRT6B/Singh et al. - 2016 - Accommodating Presuppositions Is Inappropriate in .pdf;/Users/xzfang/Zotero/storage/PQVXEL4U/cogs.html}
}

@article{sjerps_constraints_2011,
  title = {Constraints on the Processes Responsible for the Extrinsic Normalization of Vowels},
  author = {Sjerps, Matthias J. and Mitterer, Holger and McQueen, James M.},
  year = {2011},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {73},
  number = {4},
  pages = {1195--1215},
  issn = {1943-393X},
  doi = {10.3758/s13414-011-0096-8},
  abstract = {Listeners tune in to talkers' vowels through extrinsic normalization. We asked here whether this process could be based on compensation for the long-term average spectrum (LTAS) of preceding sounds and whether the mechanisms responsible for normalization are indifferent to the nature of those sounds. If so, normalization should apply to nonspeech stimuli. Previous findings were replicated with first-formant (F1) manipulations of speech. Targets on a [pt]\textendash [p{$\varepsilon$}t] (low\textendash high F1) continuum were labeled as [pt] more after high-F1 than after low-F1 precursors. Spectrally rotated nonspeech versions of these materials produced similar normalization. None occurred, however, with nonspeech stimuli that were less speechlike, even though precursor\textendash target LTAS relations were equivalent to those used earlier. Additional experiments investigated the roles of pitch movement, amplitude variation, formant location, and the stimuli's perceived similarity to speech. It appears that normalization is not restricted to speech but that the nature of the preceding sounds does matter. Extrinsic normalization of vowels is due, at least in part, to an auditory process that may require familiarity with the spectrotemporal characteristics of speech.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NT8943UQ/Sjerps et al. - 2011 - Constraints on the processes responsible for the e.pdf}
}

@article{sjerps_speakernormalized_2019,
  title = {Speaker-Normalized Sound Representations in the Human Auditory Cortex},
  author = {Sjerps, Matthias J. and Fox, Neal P. and Johnson, Keith and Chang, Edward F.},
  year = {2019},
  month = jun,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {2465},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10365-z},
  abstract = {The acoustic dimensions that distinguish speech sounds (like the vowel differences in ``boot'' and ``boat'') also differentiate speakers' voices. Therefore, listeners must normalize across speakers without losing linguistic information. Past behavioral work suggests an important role for auditory contrast enhancement in normalization: preceding context affects listeners' perception of subsequent speech sounds. Here, using intracranial electrocorticography in humans, we investigate whether and how such context effects arise in auditory cortex. Participants identified speech sounds that were preceded by phrases from two different speakers whose voices differed along the same acoustic dimension as target words (the lowest resonance of the vocal tract). In every participant, target vowels evoke a speaker-dependent neural response that is consistent with the listener's perception, and which follows from a contrast enhancement model. Auditory cortex processing thus displays a critical feature of normalization, allowing listeners to extract meaningful content from the voices of diverse speakers.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZB8D9GZI/Sjerps et al. - 2019 - Speaker-normalized sound representations in the hu.pdf;/Users/xzfang/Zotero/storage/KK5AXINC/s41467-019-10365-z.html}
}

@article{skerritt-davis_detecting_2018,
  title = {Detecting Change in Stochastic Sound Sequences},
  author = {{Skerritt-Davis}, Benjamin and Elhilali, Mounya},
  year = {2018},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {5},
  pages = {e1006162},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006162},
  abstract = {Our ability to parse our acoustic environment relies on the brain's capacity to extract statistical regularities from surrounding sounds. Previous work in regularity extraction has predominantly focused on the brain's sensitivity to predictable patterns in sound sequences. However, natural sound environments are rarely completely predictable, often containing some level of randomness, yet the brain is able to effectively interpret its surroundings by extracting useful information from stochastic sounds. It has been previously shown that the brain is sensitive to the marginal lower-order statistics of sound sequences (i.e., mean and variance). In this work, we investigate the brain's sensitivity to higher-order statistics describing temporal dependencies between sound events through a series of change detection experiments, where listeners are asked to detect changes in randomness in the pitch of tone sequences. Behavioral data indicate listeners collect statistical estimates to process incoming sounds, and a perceptual model based on Bayesian inference shows a capacity in the brain to track higher-order statistics. Further analysis of individual subjects' behavior indicates an important role of perceptual constraints in listeners' ability to track these sensory statistics with high fidelity. In addition, the inference model facilitates analysis of neural electroencephalography (EEG) responses, anchoring the analysis relative to the statistics of each stochastic stimulus. This reveals both a deviance response and a change-related disruption in phase of the stimulus-locked response that follow the higher-order statistics. These results shed light on the brain's ability to process stochastic sound sequences.},
  langid = {english},
  keywords = {Change detection,Electroencephalography,Entropy,Event-related potentials,Fractals,Human performance,Perception,Sensory perception},
  file = {/Users/xzfang/Zotero/storage/47ZIDZZX/Skerritt-Davis and Elhilali - 2018 - Detecting change in stochastic sound sequences.pdf;/Users/xzfang/Zotero/storage/KKJUS4J8/article.html}
}

@article{skoruppa_adaptation_2011,
  title = {Adaptation to {{Novel Accents}}: {{Feature-Based Learning}} of {{Context-Sensitive Phonological Regularities}}},
  shorttitle = {Adaptation to {{Novel Accents}}},
  author = {Skoruppa, Katrin and Peperkamp, Sharon},
  year = {2011},
  journal = {Cognitive Science},
  volume = {35},
  number = {2},
  pages = {348--366},
  issn = {1551-6709},
  doi = {10.1111/j.1551-6709.2010.01152.x},
  abstract = {This paper examines whether adults can adapt to novel accents of their native language that contain unfamiliar context-dependent phonological alternations. In two experiments, French participants listen to short stories read in accented speech. Their knowledge of the accents is then tested in a forced-choice identification task. In Experiment 1, two groups of listeners are exposed to newly created French accents in which certain vowels harmonize or disharmonize, respectively, to the rounding of the preceding vowel. Despite the cross-linguistic predominance of vowel harmony over disharmony, the two groups adapt equally well to both accents, suggesting that this typological difference is not reflected in perceptual learning. Experiment 2 further explores the mechanism underlying this type of phonological learning. Participants are exposed to an accent in which some vowels harmonize and others disharmonize, yielding an increased featural complexity. They adapt less well to this regularity, showing that adaptation to novel accents involves feature-based inferences.},
  copyright = {Copyright \textcopyright{} 2010 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Accents,Dialects,Features,Phonological learning,Speech perception},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6709.2010.01152.x},
  file = {/Users/xzfang/Zotero/storage/S4YHA5F5/Skoruppa and Peperkamp - 2011 - Adaptation to Novel Accents Feature-Based Learnin.pdf;/Users/xzfang/Zotero/storage/7XQITAXN/j.1551-6709.2010.01152.html}
}

@article{slevc_processing_2014,
  title = {Processing Structure in Language and Music: A Case for Shared Reliance on Cognitive Control},
  shorttitle = {Processing Structure in Language and Music},
  author = {Slevc, L. Robert and Okada, Brooke},
  year = {2014},
  month = aug,
  journal = {Psychonomic bulletin \& review},
  volume = {22},
  doi = {10.3758/s13423-014-0712-4},
  abstract = {The relationship between structural processing in music and language has received increasing interest in the past several years, spurred by the influential Shared Syntactic Integration Resource Hypothesis (SSIRH; Patel, Nature Neuroscience, 6, 674-681, 2003). According to this resource-sharing framework, music and language rely on separable syntactic representations but recruit shared cognitive resources to integrate these representations into evolving structures. The SSIRH is supported by findings of interactions between structural manipulations in music and language. However, other recent evidence suggests that such interactions also can arise with nonstructural manipulations, and some recent neuroimaging studies report largely nonoverlapping neural regions involved in processing musical and linguistic structure. These conflicting results raise the question of exactly what shared (and distinct) resources underlie musical and linguistic structural processing. This paper suggests that one shared resource is prefrontal cortical mechanisms of cognitive control, which are recruited to detect and resolve conflict that occurs when expectations are violated and interpretations must be revised. By this account, musical processing involves not just the incremental processing and integration of musical elements as they occur, but also the incremental generation of musical predictions and expectations, which must sometimes be overridden and revised in light of evolving musical input.},
  file = {/Users/xzfang/Zotero/storage/MERV6PB6/Slevc and Okada - 2014 - Processing structure in language and music a case.pdf}
}

@article{smith_anterior_2021,
  title = {Anterior Cingulate Inputs to Nucleus Accumbens Control the Social Transfer of Pain and Analgesia},
  author = {Smith, Monique L. and Asada, Naoyuki and Malenka, Robert C.},
  year = {2021},
  month = jan,
  journal = {Science},
  volume = {371},
  number = {6525},
  pages = {153--159},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abe3040},
  abstract = {{$<$}h3{$>$}INTRODUCTION{$<$}/h3{$>$} {$<$}p{$>$}Empathy, the adoption of another's sensory and emotional state, plays a critical role in social interactions. Although, historically, empathy was often considered to be an affective-cognitive process experienced solely by humans, it is now appreciated that many species, including rodents, display evolutionarily conserved behavioral antecedents of empathy such as observational fear. It is therefore possible to begin to define the neural mechanisms that mediate behavioral manifestations of empathy in species that are optimal for application of modern circuit neuroscience tools.{$<$}/p{$><$}h3{$>$}RATIONALE{$<$}/h3{$>$} {$<$}p{$>$}In both humans and rodents, the anterior cingulate cortex (ACC) appears to encode information about the affective state of others. However, little is known about which downstream targets of the ACC contribute to empathy-related behaviors. To address this topic, we optimized a protocol for the social transfer of pain behavior in mice and compared the ACC-dependent neural circuitry responsible for this behavior with the ACC neural circuitry required for the social transfer of two related behavioral states: analgesia and fear. These behaviors exhibit a key component of empathy, the adoption of another's sensory and affective state.{$<$}/p{$><$}h3{$>$}RESULTS{$<$}/h3{$>$} {$<$}p{$>$}A 1-hour social interaction between a bystander mouse and a cagemate experiencing inflammatory pain led to mechanical hyperalgesia in the bystander mouse, which lasted 4 hours but not 24 hours. This social transfer of pain was also evident after thermal testing and led to affective changes that were detected by a conspecific. The social interaction led to activation of neurons in the ACC and several downstream targets, including the nucleus accumbens (NAc), which was revealed by monosynaptic rabies virus tracing to be directly connected to the ACC. Bidirectional manipulation of activity in ACC-to-NAc inputs influenced the acquisition of socially transferred pain but not the expression of the mechanical sensitivity used to assay pain thresholds. A behavioral protocol revealed the rapid social transfer of analgesia, which also required activity in ACC-to-NAc inputs. By contrast, ACC-to-NAc input activity was not required for the social transfer of fear, which instead required activity in ACC projections to the basolateral amygdala (BLA).{$<$}/p{$><$}h3{$>$}CONCLUSION{$<$}/h3{$>$} {$<$}p{$>$}We established that mice rapidly adopt the sensory-affective state of a social partner, regardless of the valance of the information (that is, pain, fear, or pain relief). We find that the ACC generates specific and appropriate empathic behavioral responses through distinct downstream targets. Specifically, ACC-to-NAc input activity is necessary for the social transfer of pain and analgesia but not the social transfer of fear, which instead requires ACC-to-BLA input activity. Elucidating circuit-specific mechanisms that mediate various forms of empathy in experimentally accessible animal models is necessary for generating hypotheses that can be evaluated in human subjects using noninvasive assays. More sophisticated understanding of evolutionarily conserved brain mechanisms of empathy will also expedite the development of new therapies for the empathy-related deficits associated with a broad range of neuropsychiatric disorders.{$<$}/p{$>$}},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2021, American Association for the Advancement of Science. https://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {33414216},
  file = {/Users/xzfang/Zotero/storage/Q3962SSG/Smith et al. - 2021 - Anterior cingulate inputs to nucleus accumbens con.pdf;/Users/xzfang/Zotero/storage/5ES892J4/153.html}
}

@article{smith_chimaeric_2002,
  title = {Chimaeric Sounds Reveal Dichotomies in Auditory Perception},
  author = {Smith, Zachary M. and Delgutte, Bertrand and Oxenham, Andrew J.},
  year = {2002},
  month = mar,
  journal = {Nature},
  volume = {416},
  number = {6876},
  pages = {87--90},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/416087a},
  abstract = {By Fourier's theorem1, signals can be decomposed into a sum of sinusoids of different frequencies. This is especially relevant for hearing, because the inner ear performs a form of mechanical Fourier transform by mapping frequencies along the length of the~cochlear partition. An alternative signal decomposition, originated by Hilbert2, is to factor a signal into the product of a slowly varying envelope and a rapidly varying fine time structure. Neurons in the auditory brainstem3,4,5,6 sensitive to these features have been found in mammalian physiological studies. To investigate the relative perceptual importance of envelope and fine structure, we synthesized stimuli that we call `auditory chimaeras', which have the envelope of one sound and the fine structure of another. Here we show that the envelope is most important for speech reception, and the fine structure is most important for pitch perception and sound localization. When the two features are in conflict, the sound of speech is heard at a location determined by the fine structure, but the words are identified according to the envelope. This finding reveals a possible acoustic basis for the hypothesized `what' and `where' pathways in the auditory cortex7,8,9,10.},
  copyright = {2002 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/Users/xzfang/Zotero/storage/6RFLBSGY/Smith et al. - 2002 - Chimaeric sounds reveal dichotomies in auditory pe.pdf;/Users/xzfang/Zotero/storage/5TTE3624/416087a.html}
}

@article{smith_effect_2013,
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2013},
  month = sep,
  journal = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.013},
  abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability \textendash{} even for differences between highly unpredictable words \textendash{} and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  langid = {english},
  keywords = {Expectation,Information theory,Probabilistic models of cognition,Psycholinguistics,Reading},
  file = {/Users/xzfang/Zotero/storage/LGCZ5IRR/Smith and Levy - 2013 - The effect of word predictability on reading time .pdf;/Users/xzfang/Zotero/storage/R5PBKUT2/S0010027713000413.html}
}

@article{smith_multimodal_2017,
  title = {The Multimodal Nature of Spoken Word Processing in the Visual World: {{Testing}} the Predictions of Alternative Models of Multimodal Integration},
  shorttitle = {The Multimodal Nature of Spoken Word Processing in the Visual World},
  author = {Smith, Alastair C. and Monaghan, Padraic and Huettig, Falk},
  year = {2017},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {93},
  pages = {276--303},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2016.08.005},
  abstract = {Ambiguity in natural language is ubiquitous, yet spoken communication is effective due to integration of information carried in the speech signal with information available in the surrounding multimodal landscape. Language mediated visual attention requires visual and linguistic information integration and has thus been used to examine properties of the architecture supporting multimodal processing during spoken language comprehension. In this paper we test predictions generated by alternative models of this multimodal system. A model (TRACE) in which multimodal information is combined at the point of the lexical representations of words generated predictions of a stronger effect of phonological rhyme relative to semantic and visual information on gaze behaviour, whereas a model in which sub-lexical information can interact across modalities (MIM) predicted a greater influence of visual and semantic information, compared to phonological rhyme. Two visual world experiments designed to test these predictions offer support for sub-lexical multimodal interaction during online language processing.},
  langid = {english},
  keywords = {Connectionist modelling,Multimodal processing,Spoken word recognition,Visual attention,Visual world paradigm},
  file = {/Users/xzfang/Zotero/storage/HFAPTP96/Smith et al. - 2017 - The multimodal nature of spoken word processing in.pdf;/Users/xzfang/Zotero/storage/6KUYBVJG/S0749596X16301425.html}
}

@article{snyder_stable_2021,
  title = {A {{Stable Population Code}} for {{Attention}} in {{Prefrontal Cortex Leads}} a {{Dynamic Attention Code}} in {{Visual Cortex}}},
  author = {Snyder, Adam C. and Yu, Byron M. and Smith, Matthew A.},
  year = {2021},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {41},
  number = {44},
  pages = {9163--9176},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0608-21.2021},
  abstract = {Attention often requires maintaining a stable mental state over time while simultaneously improving perceptual sensitivity. These requirements place conflicting demands on neural populations, as sensitivity implies a robust response to perturbation by incoming stimuli, which is antithetical to stability. Functional specialization of cortical areas provides one potential mechanism to resolve this conflict. We reasoned that attention signals in executive control areas might be highly stable over time, reflecting maintenance of the cognitive state, thereby freeing up sensory areas to be more sensitive to sensory input (i.e., unstable), which would be reflected by more dynamic attention signals in those areas. To test these predictions, we simultaneously recorded neural populations in prefrontal cortex (PFC) and visual cortical area V4 in rhesus macaque monkeys performing an endogenous spatial selective attention task. Using a decoding approach, we found that the neural code for attention states in PFC was substantially more stable over time compared with the attention code in V4 on a moment-by-moment basis, in line with our guiding thesis. Moreover, attention signals in PFC predicted the future attention state of V4 better than vice versa, consistent with a top-down role for PFC in attention. These results suggest a functional specialization of attention mechanisms across cortical areas with a division of labor. PFC signals the cognitive state and maintains this state stably over time, whereas V4 responds to sensory input in a manner dynamically modulated by that cognitive state. SIGNIFICANCE STATEMENT Attention requires maintaining a stable mental state while simultaneously improving perceptual sensitivity. We hypothesized that these two demands (stability and sensitivity) are distributed between prefrontal and visual cortical areas, respectively. Specifically, we predicted attention signals in visual cortex would be less stable than in prefrontal cortex, and furthermore prefrontal cortical signals would predict attention signals in visual cortex in line with the hypothesized role of prefrontal cortex in top-down executive control. Our results are consistent with suggestions deriving from previous work using separate recordings in the two brain areas in different animals performing different tasks and represent the first direct evidence in support of this hypothesis with simultaneous multiarea recordings within individual animals.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2021 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {34583956},
  keywords = {attention,extrastriate,monkey,population,prefrontal,vision},
  file = {/Users/xzfang/Zotero/storage/UM3EWCEA/Snyder et al. - 2021 - A Stable Population Code for Attention in Prefront.pdf;/Users/xzfang/Zotero/storage/FY6MPLMT/9163.html}
}

@article{sohoglu_predictive_2012,
  title = {Predictive {{Top-Down Integration}} of {{Prior Knowledge}} during {{Speech Perception}}},
  author = {Sohoglu, E. and Peelle, J. E. and Carlyon, R. P. and Davis, M. H.},
  year = {2012},
  month = jun,
  journal = {Journal of Neuroscience},
  volume = {32},
  number = {25},
  pages = {8443--8453},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5069-11.2012},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KHL2B76P/Sohoglu et al. - 2012 - Predictive Top-Down Integration of Prior Knowledge.pdf}
}

@article{solomon_limited_2021,
  title = {Limited {{Evidence}} for {{Sensory Prediction Error Responses}} in {{Visual Cortex}} of {{Macaques}} and {{Humans}}},
  author = {Solomon, Selina S and Tang, Huizhen and Sussman, Elyse and Kohn, Adam},
  year = {2021},
  month = jun,
  journal = {Cerebral Cortex},
  volume = {31},
  number = {6},
  pages = {3136--3152},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab014},
  abstract = {A recent formulation of predictive coding theory proposes that a subset of neurons in each cortical area encodes sensory prediction errors, the difference between predictions relayed from higher cortex and the sensory input. Here, we test for evidence of prediction error responses in spiking responses and local field potentials (LFP) recorded in primary visual cortex and area V4 of macaque monkeys, and in complementary electroencephalographic (EEG) scalp recordings in human participants. We presented a fixed sequence of visual stimuli on most trials, and violated the expected ordering on a small subset of trials. Under predictive coding theory, pattern-violating stimuli should trigger robust prediction errors, but we found that spiking, LFP and EEG responses to expected and pattern-violating stimuli were nearly identical. Our results challenge the assertion that a fundamental computational motif in sensory cortex is to signal prediction errors, at least those based on predictions derived from temporal patterns of visual stimulation.},
  file = {/Users/xzfang/Zotero/storage/IJ7V7XJR/Solomon et al. - 2021 - Limited Evidence for Sensory Prediction Error Resp.pdf;/Users/xzfang/Zotero/storage/PRFCS8KA/6158064.html}
}

@misc{solomon_structure_2021,
  title = {Structure Shapes the Representation of a Novel Category},
  author = {Solomon, Sarah and Schapiro, Anna},
  year = {2021},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/59x6h},
  abstract = {Concepts contain rich structures that support flexible semantic cognition. These structures can be characterized by patterns of feature covariation: certain clusters of features tend to occur in the same items (e.g., feathers, wings, can fly). Existing computational models demonstrate how this kind of structure can be leveraged to slowly learn the distinctions between categories, on developmental timescales. It is not clear whether and how we leverage feature structure to quickly learn a novel category. We thus investigated how the internal structure of a new category is extracted from experience and what kinds of representations guide this learning. We predicted that humans can leverage feature clusters within an individual category to benefit learning and that this relies on the rapid formation of distributed representations. Novel categories were designed with patterns of feature associations determined by carefully constructed graph structures (Modular, Random, and Lattice). In Experiment 1, a feature inference task using verbal stimuli revealed that Modular categories\textemdash containing clusters of reliably covarying features\textemdash were more easily learned than non-Modular categories. Experiment 2 replicated this effect using visual categories. In Experiment 3, a temporal statistical learning paradigm revealed that this Modular benefit persisted even when category structure was incidental to the task. We found that a neural network model employing distributed representations was able to account for the effects, whereas prototype and exemplar models could not. The findings constrain theories of category learning and of structure learning more broadly, suggesting that humans quickly form distributed representations that reflect coherent feature structure.},
  langid = {american},
  keywords = {categories,Cognitive Neuroscience,Cognitive Psychology,concepts,Concepts and Categories,learning,Learning,neural network models,Neuroscience,Social and Behavioral Sciences,structure},
  file = {/Users/xzfang/Zotero/storage/X3T2FPZD/Solomon and Schapiro - 2021 - Structure shapes the representation of a novel cat.pdf}
}

@article{sonderegger_rational_,
  title = {A Rational Account of Perceptual Compensation for Coarticulation},
  author = {Sonderegger, Morgan and Yu, Alan},
  pages = {7},
  abstract = {A model is presented that explains perceptual compensation for context as a consequence of listeners optimally categorizing speech sounds given contextual variation. In using Bayes' rule to pick the most likely category, listeners' perception of speech sounds, which is biased toward the means of phonetic categories (Feldman \& Griffiths, 2007; Feldman, Griffiths, \& Morgan, 2009), is conditioned by contextual variation. The effect on the resulting identification curves of varying category frequencies and variances is discussed. A simulation case study of compensation for vowel-to-vowel coarticulation shows the predictions of the model closely correspond to human perceptual data.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/A4ER76CA/Sonderegger and Yu - A rational account of perceptual compensation for .pdf}
}

@article{song_neural_2021,
  title = {Neural Signatures of Attentional Engagement during Narratives and Its Consequences for Event Memory},
  author = {Song, Hayoung and Finn, Emily S. and Rosenberg, Monica D.},
  year = {2021},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {33},
  pages = {e2021905118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2021905118},
  abstract = {As we comprehend narratives, our attentional engagement fluctuates over time. Despite theoretical conceptions of narrative engagement as emotion-laden attention, little empirical work has characterized the cognitive and neural processes that comprise subjective engagement in naturalistic contexts or its consequences for memory. Here, we relate fluctuations in narrative engagement to patterns of brain coactivation and test whether neural signatures of engagement predict subsequent memory. In behavioral studies, participants continuously rated how engaged they were as they watched a television episode or listened to a story. Self-reported engagement was synchronized across individuals and driven by the emotional content of the narratives. In functional MRI datasets collected as different individuals watched the same show or listened to the same story, engagement drove neural synchrony, such that default mode network activity was more synchronized across individuals during more engaging moments of the narratives. Furthermore, models based on time-varying functional brain connectivity predicted evolving states of engagement across participants and independent datasets. The functional connections that predicted engagement overlapped with a validated neuromarker of sustained attention and predicted recall of narrative events. Together, our findings characterize the neural signatures of attentional engagement in naturalistic contexts and elucidate relationships among narrative engagement, sustained attention, and event memory.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SD6599MX/Song et al. - 2021 - Neural signatures of attentional engagement during.pdf}
}

@article{souza_advantage_2013,
  title = {The Advantage of Knowing the Talker},
  author = {Souza, Pamela and Gehani, Namita and Wright, Richard and McCloy, Daniel},
  year = {2013},
  month = sep,
  journal = {Journal of the American Academy of Audiology},
  volume = {24},
  number = {8},
  pages = {689--700},
  issn = {1050-0545},
  doi = {10.3766/jaaa.24.8.6},
  abstract = {BACKGROUND: Many audiologists have observed a situation where a patient appears to understand something spoken by his or her spouse or a close friend but not the same information spoken by a stranger. However, it is not clear whether this observation reflects choice of communication strategy or a true benefit derived from the talker's voice. PURPOSE: The current study measured the benefits of long-term talker familiarity for older individuals with hearing impairment in a variety of listening situations. RESEARCH DESIGN: In Experiment 1, we measured speech recognition with familiar and unfamiliar voices when the difficulty level was manipulated by varying levels of a speech-shaped background noise. In Experiment 2, we measured the benefit of a familiar voice when the background noise was other speech (informational masking). STUDY SAMPLE: A group of 31 older listeners with high-frequency sensorineural hearing loss participated in the study. Fifteen of the participants served as talkers and 16 as listeners. In each case, the talker-listener pair for the familiar condition represented a close, long-term relationship (spouse or close friend). DATA COLLECTION AND ANALYSIS: Speech-recognition scores were compared using controlled stimuli (low-context sentences) recorded by the study talkers. The sentences were presented in quiet and in two levels of speech-spectrum noise (Experiment 1) as well as in multitalker babble (Experiment 2). Repeated-measures analysis of variance was used to compare performance between the familiar and unfamiliar talkers, within and across conditions. RESULTS: Listeners performed better when speech was produced by a talker familiar to them, whether that talker was in a quiet or noisy environment. The advantage of the familiar talker was greater in a more adverse listening situation (i.e., in the highest level of background noise) but was similar for speech-spectrum noise and multitalker babble. CONCLUSIONS: The present data support a frequent clinical observation: listeners can understand their spouse better than a stranger. This effect was present for all our participants and occurred under strictly controlled conditions in which the only possible cue was the voice itself, rather than under normal communicative conditions where listener accommodation strategies on the part of the talker may confound the measurable benefit. The magnitude of the effect was larger than shown for short-term familiarity in previous work. This suggests that older listeners with hearing loss who inherently operate under deficient auditory conditions can benefit from experience with the voice characteristics of a long-term communication partner over many years of a relationship.},
  langid = {english},
  pmcid = {PMC3801269},
  pmid = {24131605},
  keywords = {Acoustic Stimulation,Aged,Aged; 80 and over,Female,Hearing Loss,Humans,Male,Middle Aged,Speech,Speech Perception,Voice},
  file = {/Users/xzfang/Zotero/storage/Z8TEWEA3/Souza et al. - 2013 - The advantage of knowing the talker.pdf}
}

@article{spampinato_deep_2019,
  title = {Deep {{Learning Human Mind}} for {{Automated Visual Classification}}},
  author = {Spampinato, Concetto and Palazzo, Simone and Kavasidis, Isaak and Giordano, Daniela and Shah, Mubarak and Souly, Nasim},
  year = {2019},
  month = oct,
  journal = {arXiv:1609.00344 [cs]},
  eprint = {1609.00344},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories. Afterwards, we train a Convolutional Neural Network (CNN)-based regressor to project images onto the learned manifold, thus effectively allowing machines to employ human brain-based features for automated visual classification. We use a 32-channel EEG to record brain activity of seven subjects while looking at images of 40 ImageNet object classes. The proposed RNN based approach for discriminating object classes using brain signals reaches an average accuracy of about 40\%, which outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain-driven approach obtains competitive performance, comparable to those achieved by powerful CNN models, both on ImageNet and CalTech 101, thus demonstrating its classification and generalization capabilities. This gives us a real hope that, indeed, human mind can be read and transferred to machines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xzfang/Zotero/storage/6T2LXPVF/Spampinato et al. - 2019 - Deep Learning Human Mind for Automated Visual Clas.pdf;/Users/xzfang/Zotero/storage/V28V5IDB/1609.html}
}

@article{spelke_core_2007,
  title = {Core Knowledge},
  author = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
  year = {2007},
  journal = {Developmental Science},
  volume = {10},
  number = {1},
  pages = {89--96},
  issn = {1467-7687},
  doi = {10.1111/j.1467-7687.2007.00569.x},
  abstract = {Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits.},
  copyright = {\textcopyright{} 2007 The Authors. Journal compilation \textcopyright{} 2007 Blackwell Publishing Ltd},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5EWBII6B/Spelke and Kinzler - 2007 - Core knowledge.pdf;/Users/xzfang/Zotero/storage/8BJ46R6Z/j.1467-7687.2007.00569.html}
}

@article{spivey_cross_1999,
  title = {Cross {{Talk Between Native}} and {{Second Languages}}: {{Partial Activation}} of an {{Irrelevant Lexicon}}},
  shorttitle = {Cross {{Talk Between Native}} and {{Second Languages}}},
  author = {Spivey, Michael J. and Marian, Viorica},
  year = {1999},
  month = may,
  journal = {Psychological Science},
  volume = {10},
  number = {3},
  pages = {281--284},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00151},
  abstract = {Bilingualism provides a unique opportunity for exploring hypotheses about how the human brain encodes language. For example, the ``input switch'' theory states that bilinguals can deactivate one language module while using the other. A new measure of spoken language comprehension, headband-mounted eyetracking, allows a firm test of this theory. When given spoken instructions to pick up an object, in a monolingual session, late bilinguals looked briefly at a distractor object whose name in the irrelevant language was initially phonetically similar to the spoken word more often than they looked at a control distractor object. This result indicates some overlap between the two languages in bilinguals, and provides support for parallel, interactive accounts of spoken word recognition in general.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9V5HB2RB/Spivey and Marian - 1999 - Cross Talk Between Native and Second Languages Pa.pdf}
}

@article{squire_onetrial_2021,
  title = {One-Trial Perceptual Learning in the Absence of Conscious Remembering and Independent of the Medial Temporal Lobe},
  author = {Squire, Larry R. and Frascino, Jennifer C. and Rivera, Charlotte S. and Heyworth, Nadine C. and He, Biyu J.},
  year = {2021},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {19},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2104072118},
  abstract = {A degraded, black-and-white image of an object, which appears meaningless on first presentation, is easily identified after a single exposure to the original, intact image. This striking example of perceptual learning reflects a rapid (one-trial) change in performance, but the kind of learning that is involved is not known. We asked whether this learning depends on conscious (hippocampus-dependent) memory for the images that have been presented or on an unconscious (hippocampus-independent) change in the perception of images, independently of the ability to remember them. We tested five memory-impaired patients with hippocampal lesions or larger medial temporal lobe (MTL) lesions. In comparison to volunteers, the patients were fully intact at perceptual learning, and their improvement persisted without decrement from 1 d to more than 5 mo. Yet, the patients were impaired at remembering the test format and, even after 1 d, were impaired at remembering the images themselves. To compare perceptual learning and remembering directly, at 7 d after seeing degraded images and their solutions, patients and volunteers took either a naming test or a recognition memory test with these images. The patients improved as much as the volunteers at identifying the degraded images but were severely impaired at remembering them. Notably, the patient with the most severe memory impairment and the largest MTL lesions performed worse than the other patients on the memory tests but was the best at perceptual learning. The findings show that one-trial, long-lasting perceptual learning relies on hippocampus-independent (nondeclarative) memory, independent of any requirement to consciously remember.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2021 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {33952702},
  keywords = {hippocampus,nondeclarative memory,perceptual learning},
  file = {/Users/xzfang/Zotero/storage/DKMU4KL5/e2104072118.html}
}

@article{stark_repetition_2000,
  title = {Repetition Priming of Words, Pseudowords, and Nonwords.},
  author = {Stark, Craig E. L. and McClelland, James L.},
  year = {2000},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {26},
  number = {4},
  pages = {945--972},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/0278-7393.26.4.945},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9XURSXHD/Stark and McClelland - 2000 - Repetition priming of words, pseudowords, and nonw.pdf}
}

@article{staub_failure_2019,
  title = {Failure to Detect Function Word Repetitions and Omissions in Reading: {{Are}} Eye Movements to Blame?},
  shorttitle = {Failure to Detect Function Word Repetitions and Omissions in Reading},
  author = {Staub, Adrian and Dodge, Sophia and Cohen, Andrew L.},
  year = {2019},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {26},
  number = {1},
  pages = {340--346},
  issn = {1531-5320},
  doi = {10.3758/s13423-018-1492-z},
  abstract = {We tested whether failure to notice repetitions of function words during reading (e.g., Amanda jumped off the the swing and landed on her feet.) is due to the eyes' tendency to skip one of the instances of the word. Eye movements were recorded during reading of sentences with repetitions of the word the or repetitions of a noun, after which readers were asked whether an error was present. A repeated the was detected on 46\% of trials overall. On trials on which both instances of the were fixated, detection was still only 66\%. A repeated noun was detected on 90\% of trials, with no significant effect of eye movement patterns. Detecting an omitted the also proved difficult, with eye movement patterns having only a small effect. Readers frequently overlook function word errors even when their eye movements provide maximal opportunity for noticing such errors, but they notice content word repetitions regardless of eye movement patterns. We propose that readers overlook function word errors because they attribute the apparent error to noise in the eye movement control system.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FA49CU4L/Staub et al. - 2019 - Failure to detect function word repetitions and om.pdf}
}

@article{steel_network_2021,
  title = {A Network Linking Scene Perception and Spatial Memory Systems in Posterior Cerebral Cortex},
  author = {Steel, Adam and Billings, Madeleine M. and Silson, Edward H. and Robertson, Caroline E.},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2632},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22848-z},
  abstract = {The neural systems supporting scene-perception and spatial-memory systems of the human brain are well-described. But how do these neural systems interact? Here, using fine-grained individual-subject fMRI, we report three cortical areas of the human brain, each lying immediately anterior to a region of the scene perception network in posterior cerebral cortex, that selectively activate when recalling familiar real-world locations. Despite their close proximity to the scene-perception areas, network analyses show that these regions constitute a distinct functional network that interfaces with spatial memory systems during naturalistic scene understanding. These ``place-memory areas'' offer a new framework for understanding how the brain implements memory-guided visual behaviors, including navigation.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Cortex,Hippocampus,Visual system},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cortex;Hippocampus;Visual system Subject\_term\_id: cortex;hippocampus;visual-system},
  file = {/Users/xzfang/Zotero/storage/ATS2GYEN/Steel et al. - 2021 - A network linking scene perception and spatial mem.pdf}
}

@article{stefanics_visual_2014,
  title = {Visual Mismatch Negativity: A Predictive Coding View},
  shorttitle = {Visual Mismatch Negativity},
  author = {Stefanics, G{\'a}bor and Kreml{\'a}{\v c}ek, Jan and Czigler, Istv{\'a}n},
  year = {2014},
  journal = {Frontiers in Human Neuroscience},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00666},
  abstract = {An increasing number of studies investigate the visual mismatch negativity (vMMN) or use the vMMN as a tool to probe various aspects of human cognition. This paper reviews the theoretical underpinnings of vMMN in the light of methodological considerations and provides recommendations for measuring and interpreting the vMMN. The following key issues are discussed from the experimentalist's point of view in a predictive coding framework: 1) experimental protocols and procedures to control `refractoriness' effects; 2) methods to control attention; 3) vMMN and veridical perception.},
  langid = {english},
  keywords = {Attention,Bayesian Brain,EEG,ERP,Perceptual Learning,Prediction error,predictive coding,refractoriness,repetition suppression,stimulus specific adaptation,visual mismatch negativity,vMMN},
  file = {/Users/xzfang/Zotero/storage/HIMRK6RM/Stefanics et al. - 2014 - Visual mismatch negativity a predictive coding vi.pdf}
}

@article{stefanucci_big_2009,
  title = {Big {{People}}, {{Little World}}: {{The Body Influences Size Perception}}},
  shorttitle = {Big {{People}}, {{Little World}}},
  author = {Stefanucci, Jeanine K. and Geuss, Michael N.},
  year = {2009},
  journal = {Perception},
  volume = {38},
  number = {12},
  pages = {1782--1795},
  issn = {0301-0066},
  abstract = {Previous research has shown that changes to the body can influence the perception of distances in near space (). In this paper, we question whether changes to the body can also influence the perception of extents in extrapersonal space, namely the perception of aperture widths. In experiment 1, broad-shouldered participants visually estimated the size of apertures to be smaller than narrow-shouldered participants. In experiment 2, we questioned whether changes to the body, which included holding a large object, wearing a large object, or simply holding out the arms would influence perceived width. Surprisingly, we found that only when participants' hands were widened was extrapersonal space rescaled. Experiment 3 explored the boundaries of the effect observed in experiment 2 by asking participants to hold their arms at different positions to locate the arm width at which apertures appeared smaller. We found that arm positions that were larger than the shoulder width made apertures appear smaller. The results suggest that dimensions of the body play a role in the scaling of environmental parameters in extrapersonal space.},
  pmcid = {PMC3298356},
  pmid = {20192128},
  file = {/Users/xzfang/Zotero/storage/QJMLUVW7/Stefanucci and Geuss - 2009 - Big People, Little World The Body Influences Size.pdf}
}

@article{stehwien_little_2020,
  title = {The {{Little Prince}} in 26 {{Languages}}: {{Towards}} a {{Multilingual Neuro-Cognitive Corpus}}},
  author = {Stehwien, Sabrina and Henke, Lena and Hale, John and Brennan, Jonathan and Meyer, Lars},
  year = {2020},
  pages = {7},
  abstract = {We present the Le Petit Prince Corpus (LPPC), a multi-lingual resource for research in (computational) psycho- and neurolinguistics. The corpus consists of the children's story The Little Prince in 26 languages. The dataset is in the process of being built using stateof-the-art methods for speech and language processing and electroencephalography (EEG). The planned release of LPPC dataset will include raw text annotated with dependency graphs in the Universal Dependencies standard, a near-natural-sounding synthetic spoken subset as well as EEG recordings. We will use this corpus for conducting neurolinguistic studies that generalize across a wide range of languages, overcoming typological constraints to traditional approaches. The planned release of the LPPC combines linguistic and EEG data for many languages using fully automatic methods, and thus constitutes a readily extendable resource that supports cross-linguistic and cross-disciplinary research.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5XAUGAFV/Stehwien et al. - The Little Prince in 26 Languages Towards a Multi.pdf}
}

@article{stein_current_2019,
  title = {The Current Status of the Magnocellular Theory of Developmental Dyslexia},
  author = {Stein, John},
  year = {2019},
  month = jul,
  journal = {Neuropsychologia},
  series = {Developmental Dyslexia: {{From}} Genes to Remediation},
  volume = {130},
  pages = {66--77},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2018.03.022},
  abstract = {Some people doubt that the concept of developmental dyslexia (DD) is useful at all because the phonological weaknesses seen in DD cannot be distinguished from those found in every person with poor reading skills, whatever their cause. Here I argue that true DD is characterised by poor temporal processing, hence impaired visual and auditory sequencing, that is caused by impaired development of transient/magnocellular (M-) systems throughout the brain. These deficits can be measured in order to distinguish the causes of the phonological weaknesses in DD from those causing similar deficits in other types of poor reading. Importantly this knowledge can be exploited to develop effective improvements in treatment. The evidence for impaired visual magnocellular function in many, if not all, people with dyslexia is now overwhelming; it is supported not only by psychophysical tests of M- function, but also by electrophysiological, eye movement, attentional, imaging, interventional and genetic findings. Analogously, auditory temporal processing is mediated by auditory transient, 'magnocellular', processing systems, and evidence is accumulating persuasively that this system is also impaired in dyslexics. I briefly introduce the idea that 'motor magnocellular systems' may also be impaired in dyslexia, then consider genetic, immunological and nutritional factors that interact to cause the impaired magnocellular phenotype. I then discuss why the dyslexic phenotype is so common by speculating about what strengths it might confer that would maintain the responsible genes in the human genome.},
  langid = {english},
  keywords = {Amplitude modulation,Audition,CAT301,Dyslexia,Embodied cognition,Genetics,Magnocellular,MCHC,Motion sensitivity,Omega 3,Talents,Vision},
  file = {/Users/xzfang/Zotero/storage/NWLEVY32/S0028393218301155.html}
}

@article{stephens_speaker_2010,
  title = {Speaker\textendash Listener Neural Coupling Underlies Successful Communication},
  author = {Stephens, Greg J. and Silbert, Lauren J. and Hasson, Uri},
  year = {2010},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {107},
  number = {32},
  pages = {14425--14430},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1008662107},
  abstract = {Verbal communication is a joint activity; however, speech production and comprehension have primarily been analyzed as independent processes within the boundaries of individual brains. Here, we applied fMRI to record brain activity from both speakers and listeners during natural verbal communication. We used the speaker's spatiotemporal brain activity to model listeners' brain activity and found that the speaker's activity is spatially and temporally coupled with the listener's activity. This coupling vanishes when participants fail to communicate. Moreover, though on average the listener's brain activity mirrors the speaker's activity with a delay, we also find areas that exhibit predictive anticipatory responses. We connected the extent of neural coupling to a quantitative measure of story comprehension and find that the greater the anticipatory speaker\textendash listener coupling, the greater the understanding. We argue that the observed alignment of production- and comprehension-based processes serves as a mechanism by which brains convey information.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {20660768},
  keywords = {functional MRI,intersubject correlation,language comprehension,language production},
  file = {/Users/xzfang/Zotero/storage/FRWKLAYQ/Stephens et al. - 2010 - Speakerâ€“listener neural coupling underlies success.pdf}
}

@article{stephenson_untangling_,
  title = {Untangling in {{Invariant Speech Recognition}}},
  author = {Stephenson, Cory and Feather, Jenelle and Padhy, Suchismita},
  pages = {21},
  abstract = {Encouraged by the success of deep neural networks on a variety of visual tasks, much theoretical and experimental work has been aimed at understanding and interpreting how vision networks operate. Meanwhile, deep neural networks have also achieved impressive performance in audio processing applications, both as subcomponents of larger systems and as complete end-to-end systems by themselves. Despite their empirical successes, comparatively little is understood about how these audio models accomplish these tasks. In this work, we employ a recently developed statistical mechanical theory that connects geometric properties of network representations and the separability of classes to probe how information is untangled within neural networks trained to recognize speech. We observe that speaker-specific nuisance variations are discarded by the network's hierarchy, whereas task-relevant properties such as words and phonemes are untangled in later layers. Higher level concepts such as parts-of-speech and context dependence also emerge in the later layers of the network. Finally, we find that the deep representations carry out significant temporal untangling by efficiently extracting task-relevant features at each time step of the computation. Taken together, these findings shed light on how deep auditory models process time dependent input signals to achieve invariant speech recognition, and show how different concepts emerge through the layers of the network.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MKRAPU42/Stephenson et al. - Untangling in Invariant Speech Recognition.pdf}
}

@article{stevens_letter_2003,
  title = {Letter Visibility and the Viewing Position Effect in Visual Word Recognition},
  author = {Stevens, Micha{\"e}l and Grainger, Jonathan},
  year = {2003},
  month = jan,
  journal = {Perception \& Psychophysics},
  volume = {65},
  number = {1},
  pages = {133--151},
  issn = {1532-5962},
  doi = {10.3758/BF03194790},
  abstract = {The ease with which printed words are recognized depends on the position at which the eyes initially fixate the word. In this study, we examined to what extent recognition performance for each fixation position depends on the average visibility of the word's constituent letters. Experiment 1 measured recognition performance to single letters embedded in strings of Xs (lengths of 5 and 7) for all combinations of letter position and initial fixation position in the string. In Experiment 2, recognition performance was measured for five-letter and seven-letter words as a function of initial fixation position in the word. Whereas average letter visibility showed a symmetric function in Experiment 1, the word recognition data of Experiment 2 showed the typical asymmetric curve. Combining the letter visibility data with measures of lexical constraint using absolute letter-in-string positions failed to capture the pattern in the word data. An alternative measure of constraint based on relative position coding of letters generated more accurate predictions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/W5GVJFCJ/Stevens and Grainger - 2003 - Letter visibility and the viewing position effect .pdf}
}

@article{stewart_adobe_,
  title = {Adobe {{Flash}} as a Medium for Online Experimentation: A Test of Reaction Time Measurement Capabilities},
  shorttitle = {Adobe {{Flash}} as a Medium for Online Experimentation},
  author = {Stewart, Neil},
  abstract = {Adobe Flash can be used to run complex psychological experiments over the Web. We examined the reliability of using Flash to measure reaction times (RTs) using a simple binary-choice task implemented both in Flash and in a Linux-based system known to record RTs with millisecond accuracy. Twenty-four participants were tested in the laboratory using both implementations; they also completed the Flash version on computers of their own choice outside the lab. RTs from the trials run on Flash outside the lab were approximately 20 msec slower than those from trials run on Flash in the lab, which in turn were approximately 10 msec slower than RTs from the trials run on the Linux-based system (baseline condition). RT SDs were similar in all conditions, suggesting that although Flash may overestimate RTs slightly, it does not appear to add significant noise to the data recorded},
  file = {/Users/xzfang/Zotero/storage/XNJTTVDX/Stewart - Adobe Flash as a medium for online experimentation.pdf;/Users/xzfang/Zotero/storage/ZZWB4XY5/48882.html}
}

@article{stilp_rapid_2010,
  title = {Rapid Efficient Coding of Correlated Complex Acoustic Properties},
  author = {Stilp, Christian E. and Rogers, Timothy T. and Kluender, Keith R.},
  year = {2010},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {107},
  number = {50},
  pages = {21914--21919},
  issn = {0027-8424},
  doi = {10.1073/pnas.1009020107},
  abstract = {Natural sounds are complex, typically changing along multiple acoustic dimensions that covary in accord with physical laws governing sound-producing sources. We report that, after passive exposure to novel complex sounds, highly correlated features initially collapse onto a single perceptual dimension, capturing covariance at the expense of unitary stimulus dimensions. Discriminability of sounds respecting the correlation is maintained, but is temporarily lost for sounds orthogonal or oblique to experienced covariation. Following extended experience, perception of variance not captured by the correlation is restored, but weighted only in proportion to total experienced covariance. A Hebbian neural network model captures some aspects of listener performance; an anti-Hebbian model captures none; but, a principal components analysis model captures the full pattern of results. Predictions from the principal components analysis model also match evolving listener performance in two discrimination tasks absent passive listening. These demonstrations of adaptation to correlated attributes provide direct behavioral evidence for efficient coding.},
  pmcid = {PMC3003067},
  pmid = {21098293},
  file = {/Users/xzfang/Zotero/storage/QKXXUIWB/Stilp et al. - 2010 - Rapid efficient coding of correlated complex acous.pdf}
}

@article{stocco_individual_2017,
  title = {Individual Differences in the {{Simon}} Effect Are Underpinned by Differences in the Competitive Dynamics in the Basal Ganglia: {{An}} Experimental Verification and a Computational Model},
  shorttitle = {Individual Differences in the {{Simon}} Effect Are Underpinned by Differences in the Competitive Dynamics in the Basal Ganglia},
  author = {Stocco, Andrea and Murray, Nicole L. and Yamasaki, Brianna L. and Renno, Taylor J. and Nguyen, Jimmy and Prat, Chantel S.},
  year = {2017},
  month = jul,
  journal = {Cognition},
  volume = {164},
  pages = {31--45},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2017.03.001},
  abstract = {Cognitive control is thought to be made possible by the activity of the prefrontal cortex, which selectively uses task-specific representations to bias the selection of task-appropriate responses over more automated, but inappropriate, ones. Recent models have suggested, however, that prefrontal representations are in turn controlled by the basal ganglia. In particular, neurophysiological considerations suggest that the basal ganglia's indirect pathway plays a pivotal role in preventing irrelevant information from being incorporated into a task, thus reducing response interference due to the processing of inappropriate stimuli dimensions. Here, we test this hypothesis by showing that individual differences in a non-verbal cognitive control task (the Simon task) are correlated with performance on a decision-making task (the Probabilistic Stimulus Selection task) that tracks the contribution of the indirect pathway. Specifically, the higher the effect of the indirect pathway, the smaller was the behavioral costs associated with suppressing interference in incongruent trials. Additionally, it was found that this correlation was driven by individual differences in incongruent trials only (with little effect on congruent ones) and specific to the indirect pathway (with almost no correlation with the effect of the direct pathways). Finally, it is shown that this pattern of results is precisely what is predicted when competitive dynamics of the basal ganglia are added to the selective attention component of a simple model of the Simon task, thus showing that our experimental results can be fully explained by our initial hypothesis.},
  langid = {english},
  keywords = {Basal ganglia,Cognitive control,Computational modeling,Prefrontal cortex,Selective attention},
  file = {/Users/xzfang/Zotero/storage/WII6T6X9/S0010027717300598.html}
}

@article{stocker_noise_2006,
  title = {Noise Characteristics and Prior Expectations in Human Visual Speed Perception},
  author = {Stocker, Alan A and Simoncelli, Eero P},
  year = {2006},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {9},
  number = {4},
  pages = {578--585},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1669},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/J9IHXZ88/Stocker and Simoncelli - 2006 - Noise characteristics and prior expectations in hu.pdf}
}

@article{stoianov_emergence_2012,
  title = {Emergence of a 'visual Number Sense' in Hierarchical Generative Models},
  author = {Stoianov, Ivilin and Zorzi, Marco},
  year = {2012},
  month = feb,
  journal = {Nature Neuroscience},
  volume = {15},
  number = {2},
  pages = {194--196},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2996},
  abstract = {This study uses computational modeling to demonstrate how a visual number sense might emerge. The results of the model successfully predict behavior from both non-human primates and human children.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Cognitive neuroscience,Network models,Object vision},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cognitive neuroscience;Network models;Object vision Subject\_term\_id: cognitive-neuroscience;network-models;object-vision},
  file = {/Users/xzfang/Zotero/storage/MY8GP38S/Stoianov and Zorzi - 2012 - Emergence of a 'visual number sense' in hierarchic.pdf;/Users/xzfang/Zotero/storage/G5SAN8SW/nn.html}
}

@article{stokes_dynamic_2013,
  title = {Dynamic {{Coding}} for {{Cognitive Control}} in {{Prefrontal Cortex}}},
  author = {Stokes, Mark~G. and Kusunoki, Makoto and Sigala, Natasha and Nili, Hamed and Gaffan, David and Duncan, John},
  year = {2013},
  month = apr,
  journal = {Neuron},
  volume = {78},
  number = {2},
  pages = {364--375},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.01.039},
  abstract = {Cognitive flexibility is fundamental to adaptive intelligent behavior. Prefrontal cortex has long been associated with flexible cognitive function, but the neurophysiological principles that enable prefrontal cells to adapt their response properties according to context-dependent rules remain poorly understood. Here, we use time-resolved population-level neural pattern analyses to explore how context is encoded and maintained in primate prefrontal cortex and used in flexible decision making. We show that an instruction cue triggers a rapid series of state transitions before settling into a stable low-activity state. The postcue state is differentially tuned according to the current task-relevant rule. During decision making, the response to a choice stimulus is characterized by an initial stimulus-specific population response but evolves to different final decision-related states depending on the current rule. These results demonstrate how neural tuning profiles in prefrontal cortex adapt to accommodate changes in behavioral context. Highly flexible tuning could be mediated via short-term synaptic plasticity.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/85PA5DTL/Stokes et al. - 2013 - Dynamic Coding for Cognitive Control in Prefrontal.pdf;/Users/xzfang/Zotero/storage/TGZWDQUB/S0896627313002237.html}
}

@article{strand_uncovering_1999,
  title = {Uncovering the {{Role}} of {{Gender Stereotypes}} in {{Speech Perception}}},
  author = {Strand, Elizabeth A.},
  year = {1999},
  month = mar,
  journal = {Journal of Language and Social Psychology},
  volume = {18},
  number = {1},
  pages = {86--100},
  publisher = {{SAGE Publications Inc}},
  issn = {0261-927X},
  doi = {10.1177/0261927X99018001006},
  abstract = {This work examines the effect of gender stereotypes on the perception of language by drawing together findings from the fields of speech perception, gender studies, and social psychology. Results from two speech perception experiments are reviewed that show that listeners' stereotypes about gender, as activated by the faces and voices of speakers, alter the listeners' perception of the fricatives /s/ and /{$\int$}/ . One experiment employs auditory-only consonant-vowel-consonant (CVC) tokens and the other employs audiovisual stimuli created from the same tokens synthesized with talking faces. This effect of stereotypes on low-level speech processing must be accounted for in models of perception, cognition, and the relationship between the physical and social environment.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/79DDW8SL/Strand - 1999 - Uncovering the Role of Gender Stereotypes in Speec.pdf}
}

@article{strange_evolving_1989,
  title = {Evolving Theories of Vowel Perception},
  author = {Strange, W.},
  year = {1989},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {85},
  number = {5},
  pages = {2081--2087},
  issn = {0001-4966},
  doi = {10.1121/1.397860},
  abstract = {Research on the perception of vowels in the last several years has given rise to new conceptions of vowels as articulatory, acoustic, and perceptual events. Starting from a "simple" target model in which vowels were characterized articulatorily as static vocal tract shapes and acoustically as points in a first and second formant (F1/F2) vowel space, this paper briefly traces the evolution of vowel theory in the 1970s and 1980s in two directions. (1) Elaborated target models represent vowels as target zones in perceptual spaces whose dimensions are specified as formant ratios. These models have been developed primarily to account for perceivers' solution of the "speaker normalization" problem. (2) Dynamic specification models emphasize the importance of formant trajectory patterns in specifying vowel identity. These models deal primarily with the problem of "target undershoot" associated with the coarticulation of vowels with consonants in natural speech and with the issue of "vowel-inherent spectral change" or diphthongization of English vowels. Perceptual studies are summarized that motivate these theoretical developments.},
  langid = {english},
  pmid = {2659637},
  keywords = {Auditory Threshold,Differential Threshold,Humans,Phonetics,Speech Perception}
}

@article{strayer_novel_2000,
  title = {Novel Popout Is an Attention-Based Phenomenon: {{An ERP}} Analysis},
  shorttitle = {Novel Popout Is an Attention-Based Phenomenon},
  author = {Strayer, David L. and Johnston, William A.},
  year = {2000},
  month = apr,
  journal = {Perception \& Psychophysics},
  volume = {62},
  number = {3},
  pages = {459--470},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03212098},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SWCT65TV/Strayer and Johnston - 2000 - Novel popout is an attention-based phenomenon An .pdf}
}

@article{strnad_multivoxel_2013,
  title = {Multivoxel {{Pattern Analysis Reveals Auditory Motion Information}} in {{MT}}+ of {{Both Congenitally Blind}} and {{Sighted Individuals}}},
  author = {Strnad, Lukas and Peelen, Marius V. and Bedny, Marina and Caramazza, Alfonso},
  year = {2013},
  month = apr,
  journal = {PLOS ONE},
  volume = {8},
  number = {4},
  pages = {e63198},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0063198},
  abstract = {Cross-modal plasticity refers to the recruitment of cortical regions involved in the processing of one modality (e.g. vision) for processing other modalities (e.g. audition). The principles determining how and where cross-modal plasticity occurs remain poorly understood. Here, we investigate these principles by testing responses to auditory motion in visual motion area MT+ of congenitally blind and sighted individuals. Replicating previous reports, we find that MT+ as a whole shows a strong and selective responses to auditory motion in congenitally blind but not sighted individuals, suggesting that the emergence of this univariate response depends on experience. Importantly, however, multivoxel pattern analyses showed that MT+ contained information about different auditory motion conditions in both blind and sighted individuals. These results were specific to MT+ and not found in early visual cortex. Basic sensitivity to auditory motion in MT+ is thus experience-independent, which may be a basis for the region's strong cross-modal recruitment in congenital blindness.},
  langid = {english},
  keywords = {Blindness,Experimental design,Functional magnetic resonance imaging,Multivariate analysis,Sensory perception,Support vector machines,Vision,Visual cortex},
  file = {/Users/xzfang/Zotero/storage/INUBM8YE/Strnad et al. - 2013 - Multivoxel Pattern Analysis Reveals Auditory Motio.pdf;/Users/xzfang/Zotero/storage/4S4RXJKC/article.html}
}

@article{sturmer_control_2002,
  title = {Control over Location-Based Response Activation in the {{Simon}} Task: {{Behavioral}} and Electrophysiological Evidence.},
  shorttitle = {Control over Location-Based Response Activation in the {{Simon}} Task},
  author = {St{\"u}rmer, Birgit and Leuthold, Hartmut and Soetens, Eric and Schr{\"o}ter, Hannes and Sommer, Werner},
  year = {2002},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {28},
  number = {6},
  pages = {1345--1363},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.28.6.1345},
  abstract = {In four Simon experiments we examined control over two routes of sensorimotor processing, i.e., response selection via the conditional route and response priming in the unconditional route. Behavioral and electrophysiological measures indicated utilization of the task irrelevant stimulus location for response selection as a function of correspondence probability. The Simon effect diminished as the frequency of noncorresponding trials increased. Importantly, location-based response priming was only observed when the stimulus followed a corresponding event but not after a noncorresponding trial. Therefore, the unconditional route appears to be suppressed whenever the task context indicates priming as potentially disadvantagious. Although it could be shown that exact repetitions of S-R sequences cause a marked speed-up of responses, presumably due to short cutting of response-selection, this third mechanism is independent of frequency-based adjustments in the conditional route and suppression of the unconditional route.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6PDKFM4J/StÃ¼rmer et al. - 2002 - Control over location-based response activation in.pdf}
}

@article{sudre_tracking_2012,
  title = {Tracking Neural Coding of Perceptual and Semantic Features of Concrete Nouns},
  author = {Sudre, Gustavo and Pomerleau, Dean and Palatucci, Mark and Wehbe, Leila and Fyshe, Alona and Salmelin, Riitta and Mitchell, Tom},
  year = {2012},
  month = aug,
  journal = {NeuroImage},
  volume = {62},
  number = {1},
  pages = {451--463},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2012.04.048},
  abstract = {We present a methodological approach employing magnetoencephalography (MEG) and machine learning techniques to investigate the flow of perceptual and semantic information decodable from neural activity in the half second during which the brain comprehends the meaning of a concrete noun. Important information about the cortical location of neural activity related to the representation of nouns in the human brain has been revealed by past studies using fMRI. However, the temporal sequence of processing from sensory input to concept comprehension remains unclear, in part because of the poor time resolution provided by fMRI. In this study, subjects answered 20 questions (e.g. is it alive?) about the properties of 60 different nouns prompted by simultaneous presentation of a pictured item and its written name. Our results show that the neural activity observed with MEG encodes a variety of perceptual and semantic features of stimuli at different times relative to stimulus onset, and in different cortical locations. By decoding these features, our MEG-based classifier was able to reliably distinguish between two different concrete nouns that it had never seen before. The results demonstrate that there are clear differences between the time course of the magnitude of MEG activity and that of decodable semantic information. Perceptual features were decoded from MEG activity earlier in time than semantic features, and features related to animacy, size, and manipulability were decoded consistently across subjects. We also observed that regions commonly associated with semantic processing in the fMRI literature may not show high decoding results in MEG. We believe that this type of approach and the accompanying machine learning methods can form the basis for further modeling of the flow of neural information during language processing and a variety of other cognitive processes.},
  langid = {english},
  keywords = {Knowledge representation,Language comprehension,Magnetoencephalography,Semantics},
  file = {/Users/xzfang/Zotero/storage/IZLQAJXB/Sudre et al. - 2012 - Tracking neural coding of perceptual and semantic .pdf;/Users/xzfang/Zotero/storage/X2I7GHTI/S1053811912004442.html}
}

@article{summerfield_neural_2008,
  title = {A {{Neural Representation}} of {{Prior Information}} during {{Perceptual Inference}}},
  author = {Summerfield, Christopher and Koechlin, Etienne},
  year = {2008},
  month = jul,
  journal = {Neuron},
  volume = {59},
  number = {2},
  pages = {336--347},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.05.021},
  abstract = {Perceptual inference is biased by foreknowledge about what is probable or possible. How prior expectations are neurally represented during visual perception, however, remains unknown. We used functional magnetic resonance imaging to measure brain activity in humans judging simple visual stimuli. Perceptual decisions were either biased in favor of a single alternative (A/{$\sim$}A decisions) or taken without bias toward either choice (A/B decisions). Extrastriate and anterior temporal lobe regions were more active during A/{$\sim$}A than A/B decisions, suggesting multiple representations of prior expectations within the visual hierarchy. Forward connectivity was increased when expected and observed perception diverged (``prediction error'' signals), whereas prior expectations fed backward from higher to lower regions. Finally, the coincidence between expected and observed perception activated orbital prefrontal regions, perhaps reflecting the reinforcement of prior expectations. These data support computational and quantitative models proposing that a visual percept emerges from converging bottom-up and top-down signals.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/Users/xzfang/Zotero/storage/CDADKIIH/Summerfield and Koechlin - 2008 - A Neural Representation of Prior Information durin.pdf}
}

@article{sumner_effects_2013,
  title = {Effects of Phonetically-Cued Talker Variation on Semantic           Encoding},
  author = {Sumner, Meghan and Kataoka, Reiko},
  year = {2013},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {134},
  number = {6},
  pages = {EL485-EL491},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4826151},
  abstract = {This study reports equivalence in recognition for variable productions of spoken words           that differ greatly in frequency. General American (GA) listeners participated in either a           semantic priming or a false-memory task, each with three talkers with different accents:           GA, New York City (NYC), and Southern Standard British English (BE). GA/BE induced strong           semantic priming and low false recall rates. NYC induced no semantic priming but high           false recall rates. These results challenge current theory and illuminate encoding-based           differences sensitive to phonetically-cued talker variation. The findings highlight the           central role of phonetic variation in the spoken word recognition process.},
  file = {/Users/xzfang/Zotero/storage/6CWHAGF7/Sumner and Kataoka - 2013 - Effects of phonetically-cued talker variation on s.pdf;/Users/xzfang/Zotero/storage/UXE6JVFF/1.html}
}

@article{sumner_role_2011,
  title = {The Role of Variation in the Perception of Accented Speech},
  author = {Sumner, Meghan},
  year = {2011},
  month = apr,
  journal = {Cognition},
  volume = {119},
  number = {1},
  pages = {131--136},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2010.10.018},
  abstract = {Phonetic variation has been considered a barrier that listeners must overcome in speech perception, but has been proved beneficial in category learning. In this paper, I show that listeners use within-speaker variation to accommodate gross categorical variation. Within the perceptual learning paradigm, listeners are exposed to p-initial words in English produced by a native speaker of French. Critically, listeners are trained on these words with either invariant or highly-variable VOTs. While a gross boundary shift is made for participants exposed to the variable VOTs, no such shift is observed after exposure to the invariant stimuli. These data suggest that increasing variation improves the mapping of perceptually mismatched stimuli.},
  langid = {english},
  keywords = {Accented speech,Perceptual learning,Phonetic categories,Phonetic variation,Speech perception,VOT},
  file = {/Users/xzfang/Zotero/storage/L989CYKA/S0010027710002556.html}
}

@article{sun_curious_2021,
  title = {Curious {{Objects}}: {{How Visual Complexity Guides Attention}} and {{Engagement}}},
  shorttitle = {Curious {{Objects}}},
  author = {Sun, Zekun and Firestone, Chaz},
  year = {2021},
  month = apr,
  journal = {Cognitive Science},
  volume = {45},
  number = {4},
  issn = {0364-0213, 1551-6709},
  doi = {10.1111/cogs.12933},
  abstract = {Some things look more complex than others. For example, a crenulate and richly organized leaf may seem more complex than a plain stone. What is the nature of this experience\textemdash and why do we have it in the first place? Here, we explore how object complexity serves as an efficiently extracted visual signal that the object merits further exploration. We algorithmically generated a library of geometric shapes and determined their complexity by computing the cumulative surprisal of their internal skeletons\textemdash essentially quantifying the ``amount of information'' within each shape\textemdash and then used this approach to ask new questions about the perception of complexity. Experiments 1\textendash 3 asked what kind of mental process extracts visual complexity: a slow, deliberate, reflective process (as when we decide that an object is expensive or popular) or a fast, effortless, and automatic process (as when we see that an object is big or blue)? We placed simple and complex objects in visual search arrays and discovered that complex objects were easier to find among simple distractors than simple objects are among complex distractors\textemdash a classic search asymmetry indicating that complexity is prioritized in visual processing. Next, we explored the function of complexity: Why do we represent object complexity in the first place? Experiments 4\textendash 5 asked subjects to study serially presented objects in a self-paced manner (for a later memory test); subjects dwelled longer on complex objects than simple objects\textemdash even when object shape was completely task-irrelevant\textemdash suggesting a connection between visual complexity and exploratory engagement. Finally, Experiment 6 connected these implicit measures of complexity to explicit judgments. Collectively, these findings suggest that visual complexity is extracted efficiently and automatically, and even arouses a kind of ``perceptual curiosity'' about objects that encourages subsequent attentional engagement.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QNH936L7/Sun and Firestone - 2021 - Curious Objects How Visual Complexity Guides Atte.pdf}
}

@article{sun_decision_2017,
  title = {Decision Ambiguity Is Mediated by a Late Positive Potential Originating from Cingulate Cortex},
  author = {Sun, Sai and Zhen, Shanshan and Fu, Zhongzheng and Wu, Daw-An and Shimojo, Shinsuke and Adolphs, Ralph and Yu, Rongjun and Wang, Shuo},
  year = {2017},
  month = aug,
  journal = {NeuroImage},
  volume = {157},
  pages = {400--414},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2017.06.003},
  abstract = {People often make decisions in the face of ambiguous information, but it remains unclear how ambiguity is represented in the brain. We used three types of ambiguous stimuli and combined EEG and fMRI to examine the neural representation of perceptual decisions under ambiguity. We identified a late positive potential, the LPP, which differentiated levels of ambiguity, and which was specifically associated with behavioral judgments about choices that were ambiguous, rather than passive perception of ambiguous stimuli. Mediation analyses together with two further control experiments confirmed that the LPP was generated only when decisions are made (not during mere perception of ambiguous stimuli), and only when those decisions involved choices on a dimension that is ambiguous. A further control experiment showed that a stronger LPP arose in the presence of ambiguous stimuli compared to when only unambiguous stimuli were present. Source modeling suggested that the LPP originated from multiple loci in cingulate cortex, a finding we further confirmed using fMRI and fMRI-guided ERP source prediction. Taken together, our findings argue for a role of an LPP originating from cingulate cortex in encoding decisions based on task-relevant perceptual ambiguity, a process that may in turn influence confidence judgment, response conflict, and error correction.},
  pmcid = {PMC6911707},
  pmid = {28606805},
  file = {/Users/xzfang/Zotero/storage/HJ2VD8QF/Sun et al. - 2017 - Decision ambiguity is mediated by a late positive .pdf}
}

@article{sun_face_2013,
  title = {Face {{Contour}} Is {{Crucial}} to the {{Fat Face Illusion}}},
  author = {Sun, Yu-Hao and Quinn, Paul C and Wang, Zhe and Shi, Huimin and Zhong, Ming and Jin, Haiyang and Ge, Liezhong and Pascalis, Olivier and Tanaka, James W and Lee, Kang},
  year = {2013},
  month = may,
  journal = {Perception},
  volume = {42},
  number = {5},
  pages = {488--494},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0301-0066},
  doi = {10.1068/p7439},
  abstract = {In 2010 Thompson reported a ``fat face thin'' illusion that, when next to an inverted face, an upright face looks ``fatter''. Sun et al (2012 Perception41 117\textendash 120) observed that one of the faces need not be inverted for the illusion to emerge: When two identical faces are presented one above the other, the face at the bottom appears ``fatter'' than the top one. Neither inverted faces nor clocks induced the illusion. Here we conducted three experiments probing the role that face contour plays in producing the fat face illusion. In experiment 1 line drawing faces were found to induce the illusion, suggesting that face contour is important for producing the illusion. In experiment 2 line drawing faces with scrambled internal features and empty line drawing faces devoid of internal features were found to induce the illusion. In experiment 3 internal face features arranged in their canonical face layout, but not in a scrambled layout, were found to induce the illusion. However, the magnitude of the effect was significantly weaker than the effect obtained for empty face contour in experiment 2. Collectively, these results suggest that a fat face illusion is obtained when there is sufficient information in the stimulus to activate an internal face schema.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UGI9H2A3/Sun et al. - 2013 - Face Contour is Crucial to the Fat Face Illusion.pdf}
}

@article{sun_hippocampal_2020,
  title = {Hippocampal Neurons Represent Events as Transferable Units of Experience},
  author = {Sun, Chen and Yang, Wannan and Martin, Jared and Tonegawa, Susumu},
  year = {2020},
  month = may,
  journal = {Nature Neuroscience},
  volume = {23},
  number = {5},
  pages = {651--663},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-020-0614-x},
  abstract = {The brain codes continuous spatial, temporal and sensory changes in daily experience. Recent studies suggest that the brain also tracks experience as segmented subdivisions (events), but the neural basis for encoding events remains unclear. Here, we designed a maze for mice, composed of four materially indistinguishable lap events, and identify hippocampal CA1 neurons whose activity are modulated not only by spatial location but also lap number. These `event-specific rate remapping' (ESR) cells remain lap-specific even when the maze length is unpredictably altered within trials, which suggests that ESR cells treat lap events as fundamental units. The activity pattern of ESR cells is reused to represent lap events when the maze geometry is altered from square to circle, which suggests that it helps transfer knowledge between experiences. ESR activity is separately manipulable from spatial activity, and may therefore constitute an independent hippocampal code: an `event code' dedicated to organizing experience by events as discrete and transferable units.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4KWJ2UIF/Sun et al. - 2020 - Hippocampal neurons represent events as transferab.pdf;/Users/xzfang/Zotero/storage/VMSN82GX/s41593-020-0614-x.html}
}

@article{sun_new_2012,
  title = {A {{New}} ``{{Fat Face}}'' {{Illusion}}:},
  shorttitle = {A {{New}} ``{{Fat Face}}'' {{Illusion}}},
  author = {Sun, Yu-Hao and Ge, Liezhong and Quinn, Paul C. and Wang, Zhe and Xiao, Naiqi G. and Pascalis, Olivier and Tanaka, James and Lee, Kang},
  year = {2012},
  month = jan,
  journal = {Perception},
  publisher = {{SAGE PublicationsSage UK: London, England}},
  doi = {10.1068/p6906},
  abstract = {We report a novel fat face illusion that when two identical images of the same face are aligned vertically, the face at the bottom appears `fatter'. This illusi...},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KBA26XM7/Sun et al. - 2012 - A New â€œFat Faceâ€ Illusion.pdf;/Users/xzfang/Zotero/storage/URZ5Y2KY/p6906.html}
}

@article{swets_role_2007,
  title = {The Role of Working Memory in Syntactic Ambiguity Resolution: {{A}} Psychometric Approach},
  shorttitle = {The Role of Working Memory in Syntactic Ambiguity Resolution},
  author = {Swets, Benjamin and Desmet, Timothy and Hambrick, David Z. and Ferreira, Fernanda},
  year = {2007},
  journal = {Journal of Experimental Psychology: General},
  pages = {64--81},
  abstract = {In 2 studies, the authors used a combination of psychometric and experimental techniques to investigate the effects of domain-general and domain-specific working memory factors on offline decisions concerning attachment of an ambiguous relative clause. Both studies used English and Dutch stimuli presented to English- and Dutch-speaking participants, respectively. In Study 1, readers with low working memory spans were less likely to use recency strategies for disambiguation than were readers with high spans. This finding is inconsistent with predictions of locality- and resource-based accounts of attachment. Psychometric analyses showed that both domain-specific (verbal) and domain-general working memory accounted for the effect. Study 2 found support for the hypothesis that segmentation strategies imposed during silent reading can account for the counterintuitive relationship. Results suggest that readers with low spans have a greater tendency to break up large segments of text because of their limited working memory, leading to high attachment of the ambiguous relative clause.},
  file = {/Users/xzfang/Zotero/storage/2FKZJWYX/Swets et al. - 2007 - The role of working memory in syntactic ambiguity .pdf;/Users/xzfang/Zotero/storage/L29II47I/summary.html}
}

@article{swinney_lexical_1979,
  title = {Lexical Access during Sentence Comprehension: ({{Re}})Consideration of Context Effects},
  shorttitle = {Lexical Access during Sentence Comprehension},
  author = {Swinney, David A. and Robinowitz, Janet Dorfzahn Sara},
  year = {1979},
  journal = {Journal of Verbal Learning and Verbal Behavior},
  pages = {546--659},
  abstract = {The effects of prior semantic context upon lexical access during sentence comprehension were examined in two experiments. In both studies, subjects comprehended auditorily presented sentences containing lexical ambiguities and simultaneously performed a lexical decision task upon visually presented letter strings. Lexical decisions for visual words related to each of the meanings of the ambiguity were facilitated when these words were presented simultaneous with the end of the ambiguity (Experiment 1). This effect held even when a strong biasing context was present ` When presented four syllables following the ambiguity, only lexical decisions for visual words related to the contextually appropriate meaning of the ambiguity were facilitated (Experiment 2). Arguments are made for autonomy of the lexical access process of a model of semantic context effects is offered. Sentence comprehension requires the integration of information derived from a number of ongoing cognitive processes. It is clear, for example, that semantic and syntactic contexts interact with moment-to-moment comprehension processes to affect our interpretation of individual words and sentences; observations that contexts act to determine sentential interpretations abound in the literature. However, while this effect is well documented, the process by which it occurs is not. Until the manner in which contexts exert their effects (i.e., the nature of information interaction) can be detailed, claims relying on the concept of "contextual determination" are empty and merely beg the question.},
  file = {/Users/xzfang/Zotero/storage/RLYGK8M9/Swinney and Robinowitz - 1979 - Lexical access during sentence comprehension (Re).pdf;/Users/xzfang/Zotero/storage/REF42IZR/summary.html}
}

@article{szewczyk_contextbased_2022,
  title = {Context-Based Facilitation of Semantic Access Follows Both Logarithmic and Linear Functions of Stimulus Probability},
  author = {Szewczyk, Jakub M. and Federmeier, Kara D.},
  year = {2022},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {123},
  pages = {104311},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2021.104311},
  abstract = {Stimuli are easier to process when context makes them predictable, but does context-based facilitation arise from preactivation of a limited set of relatively probable upcoming stimuli (with facilitation then linearly related to probability) or, instead, because the system maintains and updates a probability distribution across all items (with facilitation logarithmically related to probability)? We measured the N400, an index of semantic access, to words of varying probability, including unpredictable words. Word predictability was measured using both cloze probabilities and a state-of-the-art machine learning language model (GPT-2). We reanalyzed five datasets (n~=~138) to demonstrate and then replicate that context-based facilitation on the N400 is graded, even among unpredictable words. Furthermore, we established that the relationship between word predictability and context-based facilitation combines linear and logarithmic functions. We argue that this composite function reveals properties of the mapping between words and semantic features and how feature- and word-related information is activated on-line.},
  langid = {english},
  keywords = {Context-based facilitation,GPT-2,N400,Semantic access}
}

@article{tafazoli_transformationtolerant_2012,
  title = {Transformation-{{Tolerant Object Recognition}} in {{Rats Revealed}} by {{Visual Priming}}},
  author = {Tafazoli, S. and Di Filippo, A. and Zoccolan, D.},
  year = {2012},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {32},
  number = {1},
  pages = {21--34},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3932-11.2012},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8XTZUL4B/Tafazoli et al. - 2012 - Transformation-Tolerant Object Recognition in Rats.pdf}
}

@article{takahashi_precise_1997,
  title = {Precise Measurement of Individual Rapid Eye Movements in {{REM}} Sleep of Humans},
  author = {Takahashi, K. and Atsumi, Y.},
  year = {1997},
  month = sep,
  journal = {Sleep},
  volume = {20},
  number = {9},
  pages = {743--752},
  issn = {0161-8105},
  doi = {10.1093/sleep/20.9.743},
  abstract = {An automated analyzer for individual eye movements (EMs) has been developed that enables precise analyses of their incidence. Three new parameters for each EM are obtained: EM magnitude, the angle and speed of eyeball rotation, and the energy of each EM. All rapid eye movement (REM) sleep EMs from 40 nights of polysomnography for 20 healthy young men were analyzed. The mean frequency of eye movement (EM frequency) was 15.9 per minute. Compared to conventionally analyzed rapid eye movement (REM) density, EM frequency was more sensitive to differences among sleep cycles, nights, and individuals. The mean EM rotation was 6.27 +/- 0.021 degrees, the mean speed of rotation was 58.73 +/- 0.18 degrees/second, and mean energy was 525.85 +/- 3.82 degrees2/second. The distribution of changes in these new parameters differed from conventional measures across REM episodes. The conventional measures, REM episode duration, and REM density increased progressively in successive REM episodes in an ascent-to-right pattern. However, the new parameters peaked in the second, followed by relatively low values, producing an inverted V pattern. This discrepancy could indicate physiological mechanisms of EM that are not revealed in conventional measures of REM sleep intensity.},
  langid = {english},
  pmid = {9406327},
  keywords = {Adult,Humans,Male,Polysomnography,Sleep; REM},
  file = {/Users/xzfang/Zotero/storage/YAZRLCHL/Takahashi and Atsumi - 1997 - Precise measurement of individual rapid eye moveme.pdf}
}

@article{tal_redundancy_2022,
  title = {Redundancy Can Benefit Learning: {{Evidence}} from Word Order and Case Marking},
  shorttitle = {Redundancy Can Benefit Learning},
  author = {Tal, Shira and Arnon, Inbal},
  year = {2022},
  month = jul,
  journal = {Cognition},
  volume = {224},
  pages = {105055},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105055},
  abstract = {The prevalence of redundancy in the world languages has long puzzled language researchers. It is especially surprising in light of the growing evidence on speakers' tendency to avoid redundant elements in production (omitting or reducing more predictable elements). Here, we propose that redundancy can be functional for learning. In particular, we argue that redundant cues can facilitate learning, even when they make the language system more complicated. This prediction is further motivated by the Linguistic Niche Hypothesis (Lupyan \& Dale, 2010), which suggests that morphological complexity can arise due to the advantage redundancy might confer for child learners. We test these hypotheses in an artificial language learning study with children and adults, where either word order alone or both word order and case marking serve as cues for thematic assignment in a novel construction. We predict, and find, that children learning the redundant language learn to produce it, and show better comprehension of the novel thematic assignment than children learning the non-redundant language, despite having to learn an additional morpheme. Children in both conditions were similarly accurate in producing the novel word order, suggesting redundancy might have a differential effect on comprehension and production. Adults did not show better learning in the redundant condition, most likely because they were at ceiling in both conditions. We discuss implications for theories of language learning and language change.},
  langid = {english},
  keywords = {Artificial language learning,Language acquisition,Learning biases,Redundancy},
  file = {/Users/xzfang/Zotero/storage/Z8JRUDA2/Tal and Arnon - 2022 - Redundancy can benefit learning Evidence from wor.pdf;/Users/xzfang/Zotero/storage/KC4G4D84/S0010027722000439.html}
}

@article{taler_lexical_2010,
  title = {Lexical {{Neighborhood Density Effects}} on {{Spoken Word Recognition}} and {{Production}} in {{Healthy Aging}}},
  author = {Taler, Vanessa and Aaron, Geoffrey P. and Steinmetz, Lauren G. and Pisoni, David B.},
  year = {2010},
  month = sep,
  journal = {The Journals of Gerontology Series B: Psychological Sciences and Social Sciences},
  volume = {65B},
  number = {5},
  pages = {551--560},
  issn = {1079-5014},
  doi = {10.1093/geronb/gbq039},
  abstract = {We examined the effects of lexical competition and word frequency on spoken word recognition and production in healthy aging. Older (n = 16) and younger adults (n = 21) heard and repeated meaningful English sentences presented in the presence of multitalker babble at two signal-to-noise ratios, +10 and -3 dB. Each sentence contained three keywords of high or low word frequency and phonological neighborhood density (ND). Both participant groups responded less accurately to high- than low-ND stimuli; response latencies (from stimulus offset to response onset) were longer for high- than low-ND sentences, whereas response durations\textemdash time from response onset to response offset\textemdash were longer for low- than high-ND stimuli. ND effects were strongest for older adults in the most difficult conditions, and ND effects in accuracy were related to inhibitory function. The results suggest that the sentence repetition task described here taps the effects of lexical competition in both perception and production and that these effects are similar across the life span, but that accuracy in the lexical discrimination process is affected by declining inhibitory function in older adults.},
  pmcid = {PMC2920945},
  pmid = {20542997},
  file = {/Users/xzfang/Zotero/storage/ESMUKMMV/Taler et al. - 2010 - Lexical Neighborhood Density Effects on Spoken Wor.pdf}
}

@article{tanenhaus_integration_1995,
  title = {Integration of Visual and Linguistic Information in Spoken Language Comprehension},
  author = {Tanenhaus, M. K. and {Spivey-Knowlton}, M. J. and Eberhard, K. M. and Sedivy, J. C.},
  year = {1995},
  month = jun,
  journal = {Science},
  volume = {268},
  number = {5217},
  pages = {1632--1634},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7777863},
  abstract = {Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing.},
  chapter = {Reports},
  copyright = {\textcopyright{} 1995},
  langid = {english},
  pmid = {7777863},
  file = {/Users/xzfang/Zotero/storage/I2JLGNEP/1632.html}
}

@article{tang_intonational_2017,
  title = {Intonational Speech Prosody Encoding in the Human Auditory Cortex},
  author = {Tang, C. and Hamilton, L. S. and Chang, E. F.},
  year = {2017},
  month = aug,
  journal = {Science},
  volume = {357},
  number = {6353},
  pages = {797--801},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aam8577},
  abstract = {Brain mechanisms of pitch perception To emphasize a word, we briefly raise our pitch; this alone can change the meaning of a sentence. Tang et al. performed high-density brain recordings on clinically monitored neurosurgical patients. They discovered that intonational pitch is represented by a highly specialized and dedicated neural population in the auditory cortex. Discrete cortical sites extracted intonational information in real time from the speech signal. These sites were overlapping with, but functionally independent from, sites that encode other critical aspects of speech, such as the phonemes and information about the speaker. Science, this issue p. 797 Speakers of all human languages regularly use intonational pitch to convey linguistic meaning, such as to emphasize a particular word. Listeners extract pitch movements from speech and evaluate the shape of intonation contours independent of each speaker's pitch range. We used high-density electrocorticography to record neural population activity directly from the brain surface while participants listened to sentences that varied in intonational pitch contour, phonetic content, and speaker. Cortical activity at single electrodes over the human superior temporal gyrus selectively represented intonation contours. These electrodes were intermixed with, yet functionally distinct from, sites that encoded different information about phonetic features or speaker identity. Furthermore, the representation of intonation contours directly reflected the encoding of speaker-normalized relative pitch but not absolute pitch. Selective neuronal populations in the human temporal gyrus represent speech intonation through the encoding of relative pitch. Selective neuronal populations in the human temporal gyrus represent speech intonation through the encoding of relative pitch.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2017 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {28839071},
  file = {/Users/xzfang/Zotero/storage/2LKUHR8B/Tang et al. - 2017 - Intonational speech prosody encoding in the human .pdf;/Users/xzfang/Zotero/storage/XVVV9LAY/797.html}
}

@article{tanner_robust_2019,
  title = {Robust Neurocognitive Individual Differences in Grammatical Agreement Processing: {{A}} Latent Variable Approach},
  shorttitle = {Robust Neurocognitive Individual Differences in Grammatical Agreement Processing},
  author = {Tanner, Darren},
  year = {2019},
  month = feb,
  journal = {Cortex},
  volume = {111},
  pages = {210--237},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.10.011},
  abstract = {Many neurocognitive accounts of language processing presume that neural responses detected in grand mean analyses of cortical electrophysiological activity reflect the normative brain response in the population under investigation. However, emerging work now shows that individuals' brain responses can vary systematically in both the size and type of effect elicited. The present research therefore examined individual differences in neural activity elicited by grammatical agreement anomalies during language comprehension in a large cohort of highly literate, monolingual English speakers (N~=~114), a population generally assumed to be relatively homogenous in terms of linguistic knowledge and processing. Results showed systematic variability in event-related brain potentials (ERPs) elicited by subject-verb agreement anomalies, with brain responses varying on a continuum between N400 and P600 dominant responses. Similar variation was found both when agreement was realized via inflectional morphology or via lexical alternations. Individuals' brain response type correlated strongly across these two conditions. Similar variation was also found for ERPs elicited during rapid serial visual presentation and when self-paced ERPs were recorded. Multilevel latent variable regression showed that variation in brain response amplitude and type was not related to individual differences in language experience or verbal working memory capacity, despite high statistical power. These findings indicate that descriptions of processing dynamics predicated solely on grand mean analyses of central tendency can fail to provide an accurate, generalizable account of how processing unfolds in many or most individual members of the population studied. Furthermore, these findings show that systematic individual variation in engagement of neural system supporting grammatical processing is found even in language users at the highest end of the proficiency spectrum and in grammatically simple sentences. This study therefore has implications for studies of language processing in atypical populations.},
  langid = {english},
  keywords = {ERP,Grammatical agreement,Individual differences,LAN,N400,P600},
  file = {/Users/xzfang/Zotero/storage/4B665SGA/Tanner - 2019 - Robust neurocognitive individual differences in gr.pdf;/Users/xzfang/Zotero/storage/848CD8BT/S0010945218303447.html}
}

@article{tavakoli_antimicrobial_2015,
  title = {Antimicrobial {{Activities}} of the {{Combined Use}} of {{Cuminum Cyminum L}}. {{Essential Oil}}, {{Nisin}} and {{Storage Temperature Against Salmonella}} Typhimurium and {{Staphylococcus}} Aureus {{In Vitro}}},
  author = {Tavakoli, Hamid Reza and Mashak, Zohreh and Moradi, Bizhan and Sodagari, Hamid Reza},
  year = {2015},
  month = apr,
  journal = {Jundishapur Journal of Microbiology},
  volume = {8},
  number = {4},
  issn = {2008-3645},
  doi = {10.5812/jjm.8(4)2015.24838},
  abstract = {Background: Foodborne diseases are considered as major health problems in different countries. Concerns over the safety of some chemical preservatives and negative consumer reactions to them have prompted interest in natural alternatives for the maintenance or extension of food shelf life. In this respect, the combination of a plant essential oil and nisin has used for controlling the growth of foodborne pathogens as natural food preservative using the mathematical model. Objectives: The purpose of this study was to determine the effect of different concentrations of Cuminum cyminum L. essential oil (0, 15, 30 and 45 \textmu L/100 mL) and nisin (0, 0.5 and 1.5 \textmu g/mL) combination at different temperatures (10, 25 and 35\textdegree C) on growth of Salmonella typhimurium and Staphylococcus aureus in the Brain-Heart Infusion (BHI) broth model. The concentrations of 0 \textmu L/100 mL for essential oil and 0 \textmu g/mL for nisin imply the negative control. Materials and Methods: A multivariate variance experiment was performed. To assess the effect of essential oil, nisin and the incubation temperature on growth probability (log P\%) of S. typhimurium and S. aureus, four concentrations of C. cyminum L. essential oil (0, 15, 30 and 45 \textmu L/100 mL), three concentrations of nisin (0, 0.5 and 1.5 \textmu g/mL) and three storage temperatures (10, 25 and 35\textdegree C) were considered. Results: The growth of S. typhimurium was significantly decreased by the concentration of essential oil {$\geq$} 30 \textmu L/100 mL in combination with nisin {$\geq$} 0.5 \textmu g/mL at temperature = 10\textdegree C (P {$<$} 0.05). Also, in combination of the essential oil {$\geq$} 15 \textmu L/100 mL and nisin {$\geq$} 0.5 \textmu g/mL at temperature {$\leq$} 25\textdegree C, the growth of S. aureus was significantly reduced (P {$<$} 0.05). Conclusions: These results indicate that the combination of essential oil and nisin inhibits the growth of S. typhimurium and S. aureus bacteria and there is the possibility of using them as substitutes for chemical food preservatives. Moreover, the model (log P\%) in this study can be a good tool for the reduction of microbiological hazards in food industry.},
  pmcid = {PMC4449852},
  pmid = {26034554},
  file = {/Users/xzfang/Zotero/storage/R9VRG3TJ/Tavakoli ç­‰. - 2015 - Antimicrobial Activities of the Combined Use of Cu.pdf}
}

@article{teichmann_influence_2020,
  title = {The {{Influence}} of {{Object-Color Knowledge}} on {{Emerging Object Representations}} in the {{Brain}}},
  author = {Teichmann, Lina and Quek, Genevieve L. and Robinson, Amanda K. and Grootswagers, Tijl and Carlson, Thomas A. and Rich, Anina N.},
  year = {2020},
  month = aug,
  journal = {Journal of Neuroscience},
  volume = {40},
  number = {35},
  pages = {6779--6789},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0158-20.2020},
  abstract = {The ability to rapidly and accurately recognize complex objects is a crucial function of the human visual system. To recognize an object, we need to bind incoming visual features, such as color and form, together into cohesive neural representations and integrate these with our preexisting knowledge about the world. For some objects, typical color is a central feature for recognition; for example, a banana is typically yellow. Here, we applied multivariate pattern analysis on time-resolved neuroimaging (MEG) data to examine how object-color knowledge affects emerging object representations over time. Our results from 20 participants (11 female) show that the typicality of object-color combinations influences object representations, although not at the initial stages of object and color processing. We find evidence that color decoding peaks later for atypical object-color combinations compared with typical object-color combinations, illustrating the interplay between processing incoming object features and stored object knowledge. Together, these results provide new insights into the integration of incoming visual information with existing conceptual object knowledge. SIGNIFICANCE STATEMENT To recognize objects, we have to be able to bind object features, such as color and shape, into one coherent representation and compare it with stored object knowledge. The MEG data presented here provide novel insights about the integration of incoming visual information with our knowledge about the world. Using color as a model to understand the interaction between seeing and knowing, we show that there is a unique pattern of brain activity for congruently colored objects (e.g., a yellow banana) relative to incongruently colored objects (e.g., a red banana). This effect of object-color knowledge only occurs after single object features are processed, demonstrating that conceptual knowledge is accessed relatively late in the visual processing hierarchy.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2020 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {32703903},
  keywords = {color,decoding,MEG,MVPA,object-color knowledge},
  file = {/Users/xzfang/Zotero/storage/TUSAY736/Teichmann et al. - 2020 - The Influence of Object-Color Knowledge on Emergin.pdf;/Users/xzfang/Zotero/storage/G82WWE84/6779.html}
}

@article{temperley_information_2015,
  title = {Information {{Density}} and {{Syntactic Repetition}}},
  author = {Temperley, David and Gildea, Daniel},
  year = {2015},
  month = nov,
  journal = {Cognitive Science},
  volume = {39},
  number = {8},
  pages = {1802--1823},
  issn = {03640213},
  doi = {10.1111/cogs.12215},
  abstract = {In noun phrase (NP) coordinate constructions (e.g., NP and NP), there is a strong tendency for the syntactic structure of the second conjunct to match that of the first; the second conjunct in such constructions is therefore low in syntactic information. The theory of uniform information density predicts that low-information syntactic constructions will be counterbalanced by high information in other aspects of that part of the sentence, and high-information constructions will be counterbalanced by other low-information components. Three predictions follow: (a) lexical probabilities (measured by N-gram probabilities and head-dependent probabilities) will be lower in second conjuncts than first conjuncts; (b) lexical probabilities will be lower in matching second conjuncts (those whose syntactic expansions match the first conjunct) than nonmatching ones; and (c) syntactic repetition should be especially common for low-frequency NP expansions. Corpus analysis provides support for all three of these predictions.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IH423YFT/Temperley and Gildea - 2015 - Information Density and Syntactic Repetition.pdf}
}

@article{temperley_probabilistic_2008,
  title = {A {{Probabilistic Model}} of {{Melody Perception}}},
  author = {Temperley, David},
  year = {2008},
  journal = {Cognitive Science},
  volume = {32},
  number = {2},
  pages = {418--444},
  issn = {1551-6709},
  doi = {10.1080/03640210701864089},
  abstract = {This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. The model uses Bayesian reasoning: For any ``surface'' pattern and underlying ``structure,'' we can infer the structure maximizing P(structure|surface) based on knowledge of P(surface, structure). The probability of the surface can then be calculated as {$\sum$} P(surface, structure), summed over all structures. In this case, the surface is a pattern of notes; the structure is a key. A generative model is proposed, based on three principles: (a) melodies tend to remain within a narrow pitch range; (b) note-to-note intervals within a melody tend to be small; and (c) notes tend to conform to a distribution (or key profile) that depends on the key. The model is tested in three ways. First, it is tested on its ability to identify the keys of a set of folksong melodies. Second, it is tested on a melodic expectation task in which it must judge the probability of different notes occurring given a prior context; these judgments are compared with perception data from a melodic expectation experiment. Finally, the model is tested on its ability to detect incorrect notes in melodies by assigning them lower probabilities than the original versions.},
  langid = {english},
  keywords = {Expectation,Key perception,Music cognition,Probabilistic modeling},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1080/03640210701864089},
  file = {/Users/xzfang/Zotero/storage/S8HAL8CD/Temperley - 2008 - A Probabilistic Model of Melody Perception.pdf;/Users/xzfang/Zotero/storage/7VCS2F4V/03640210701864089.html}
}

@article{temperley_probabilistic_2014,
  title = {Probabilistic {{Models}} of {{Melodic Interval}}},
  author = {Temperley, David},
  year = {2014},
  month = sep,
  journal = {Music Perception},
  volume = {32},
  number = {1},
  pages = {85--99},
  issn = {0730-7829, 1533-8312},
  doi = {10.1525/mp.2014.32.1.85},
  abstract = {Two probabilistic models of melodic interval are compared. In the Markov model, the ``interval probability'' of a note is defined by the corpus frequency of its melodic interval (the interval to the previous note), conditioned on the previous one or two intervals; in the Gaussian model, the interval probability is a simple mathematical function of the size of the note's melodic interval and its position in relation to the range of the melody. In both models, this interval probability is then multiplied by the probability of the note's scale degree to yield its actual probability. The two models were tested on four corpora of tonal melodies using cross-entropy. The Markov model yielded a somewhat lower (better) cross-entropy than the Gaussian model, but is also much more complex, requiring far more parameters. The models were also tested on melodic expectation data, and on their ability to predict the distribution of intervals in a corpus. Possible ways of improving the models are discussed, as well as their broader implications for music cognition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/L6XHWMBD/Temperley - 2014 - Probabilistic Models of Melodic Interval.pdf}
}

@article{temperley_uniform_2019,
  title = {Uniform {{Information Density}} in {{Music}}},
  author = {Temperley, David},
  year = {2019},
  month = jul,
  journal = {Music Theory Online},
  volume = {25},
  number = {2},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IVLFNBIX/mto.19.25.2.temperley.html}
}

@article{temple_disrupted_2001,
  title = {Disrupted Neural Responses to Phonological and Orthographic Processing in Dyslexic Children: An {{fMRI}} Study},
  shorttitle = {Disrupted Neural Responses to Phonological and Orthographic Processing in Dyslexic Children},
  author = {Temple, Elise and Poldrack, Russell A. and Salidis, Joanna and Deutsch, Gayle K. and Tallal, Paula and Merzenich, Michael M. and Gabrieli, John D. E.},
  year = {2001},
  month = feb,
  journal = {NeuroReport},
  volume = {12},
  number = {2},
  pages = {299--307},
  issn = {0959-4965},
  abstract = {Developmental dyslexia, characterized by difficulty in reading, has been associated with phonological and orthographic processing deficits. fMRI was performed on dyslexic and normal-reading children (8\textendash 12 years old) during phonological and orthographic tasks of rhyming and matching visually presented letter pairs. During letter rhyming, both normal and dyslexic reading children had activity in left frontal brain regions, whereas only normal-reading children had activity in left temporo-parietal cortex. During letter matching, normal-reading children showed activity throughout extrastriate cortex, especially in occipito-parietal regions, whereas dyslexic children had little activity in extrastriate cortex during this task. These results indicate dyslexia may be characterized in childhood by disruptions in the neural bases of both phonological and orthographic processes important for reading.},
  langid = {american},
  file = {/Users/xzfang/Zotero/storage/G8HEPTVQ/Disrupted_neural_responses_to_phonological_and.24.html}
}

@article{teng_constrained_2020,
  title = {Constrained {{Structure}} of {{Ancient Chinese Poetry Facilitates Speech Content Grouping}}},
  author = {Teng, Xiangbin and Ma, Min and Yang, Jinbiao and Blohm, Stefan and Cai, Qing and Tian, Xing},
  year = {2020},
  month = apr,
  journal = {Current Biology},
  volume = {30},
  number = {7},
  pages = {1299-1305.e7},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2020.01.059},
  abstract = {Ancient Chinese poetry is constituted by structured language that deviates from ordinary language usage [1, 2]; its poetic genres impose unique combinatory constraints on linguistic elements [3]. How does the constrained poetic structure facilitate speech segmentation when common linguistic [4, 5, 6, 7, 8] and statistical cues [5, 9] are unreliable to listeners in poems? We generated artificial Jueju, which arguably has the most constrained structure in ancient Chinese poetry, and presented each poem twice as an isochronous sequence of syllables to native Mandarin speakers while conducting magnetoencephalography (MEG) recording. We found that listeners deployed their prior knowledge of Jueju to build the line structure and to establish the conceptual flow of Jueju. Unprecedentedly, we found a phase precession phenomenon indicating predictive processes of speech segmentation\textemdash the neural phase advanced faster after listeners acquired knowledge of incoming speech. The statistical co-occurrence of monosyllabic words in Jueju negatively correlated with speech segmentation, which provides an alternative perspective on how statistical cues facilitate speech segmentation. Our findings suggest that constrained poetic structures serve as a temporal map for listeners to group speech contents and to predict incoming speech signals. Listeners can parse speech streams by using not only grammatical and statistical cues but also their prior knowledge of the form of language. Video Abstract},
  langid = {english},
  keywords = {artificial intelligence,brain rhythms,empirical aesthetics,integration,natural language processing,neural oscillations and entrainment,neural phase precession,prediction,speech,time windows and constants},
  file = {/Users/xzfang/Zotero/storage/S94AQVHB/Teng et al. - 2020 - Constrained Structure of Ancient Chinese Poetry Fa.pdf}
}

@article{teng_segmenting_2021,
  title = {Segmenting and {{Predicting Musical Phrase Structure Exploits Neural Gain Modulation}} and {{Phase Precession}}},
  author = {Teng, Xiangbin and {Larrouy-Maestri}, Pauline and Poeppel, David},
  year = {2021},
  month = jul,
  journal = {bioRxiv},
  pages = {2021.07.15.452556},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.07.15.452556},
  abstract = {{$<$}p{$>$}Music, like language, is characterized by hierarchically organized structure that unfolds over time. Music listening therefore requires not only the tracking of notes and beats but also internally constructing high-level musical structures or phrases and anticipating incoming contents. Unlike for language, mechanistic evidence for online musical segmentation and prediction at a structural level is sparse. We recorded neurophysiological data from participants listening to music in its original forms as well as in manipulated versions with locally or globally reversed harmonic structures. We discovered a low-frequency neural component that modulated the neural rhythms of beat tracking and reliably parsed musical phrases. We next identified phrasal phase precession, suggesting that listeners established structural predictions from ongoing listening experience to track phrasal boundaries. The data point to brain mechanisms that listeners use to segment continuous music at the phrasal level and to predict abstract structural features of music.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/T27P894V/Teng et al. - 2021 - Segmenting and Predicting Musical Phrase Structure.pdf;/Users/xzfang/Zotero/storage/93JI5BYQ/2021.07.15.452556v1.html}
}

@article{tenoever_oscillating_2021,
  title = {An Oscillating Computational Model Can Track Pseudo-Rhythmic Speech by Using Linguistic Predictions},
  author = {{ten Oever}, Sanne and Martin, Andrea E},
  editor = {King, Andrew J and K{\"o}sem, Anne and K{\"o}sem, Anne and Rimmele, Johanna and Doelling, Keith},
  year = {2021},
  month = aug,
  journal = {eLife},
  volume = {10},
  pages = {e68066},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.68066},
  abstract = {Neuronal oscillations putatively track speech in order to optimize sensory processing. However, it is unclear how isochronous brain oscillations can track pseudo-rhythmic speech input. Here we propose that oscillations can track pseudo-rhythmic speech when considering that speech time is dependent on content-based predictions flowing from internal language models. We show that temporal dynamics of speech are dependent on the predictability of words in a sentence. A computational model including oscillations, feedback, and inhibition is able to track pseudo-rhythmic speech input. As the model processes, it generates temporal phase codes, which are a candidate mechanism for carrying information forward in time. The model is optimally sensitive to the natural temporal speech dynamics and can explain empirical data on temporal speech illusions. Our results suggest that speech tracking does not have to rely only on the acoustics but could also exploit ongoing interactions between oscillations and constraints flowing from internal language models.},
  keywords = {language,oscillations,prediction,speech,temporal processing},
  file = {/Users/xzfang/Zotero/storage/XWPQWL9C/ten Oever and Martin - 2021 - An oscillating computational model can track pseud.pdf}
}

@article{tenoever_oscillatory_2015,
  title = {Oscillatory Phase Shapes Syllable Perception},
  author = {{ten Oever}, Sanne and Sack, Alexander T.},
  year = {2015},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {52},
  pages = {15833--15837},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1517519112},
  abstract = {The role of oscillatory phase for perceptual and cognitive processes is being increasingly acknowledged. To date, little is known about the direct role of phase in categorical perception. Here we show in two separate experiments that the identification of ambiguous syllables that can either be perceived as /da/ or /ga/ is biased by the underlying oscillatory phase as measured with EEG and sensory entrainment to rhythmic stimuli. The measured phase difference in which perception is biased toward /da/ or /ga/ exactly matched the different temporal onset delays in natural audiovisual speech between mouth movements and speech sounds, which last 80 ms longer for /ga/ than for /da/. These results indicate the functional relationship between prestimulus phase and syllable identification, and signify that the origin of this phase relationship could lie in exposure and subsequent learning of unique audiovisual temporal onset differences.},
  chapter = {Social Sciences},
  copyright = {\textcopyright{}  . http://www.pnas.org/preview\_site/misc/userlicense.xhtml},
  langid = {english},
  pmid = {26668393},
  keywords = {audiovisual,oscillations,phase,speech,temporal processing},
  file = {/Users/xzfang/Zotero/storage/RE3FZ4JX/Oever and Sack - 2015 - Oscillatory phase shapes syllable perception.pdf;/Users/xzfang/Zotero/storage/DW4CDX4D/15833.html}
}

@article{tessler_warm_2022,
  title = {Warm (for {{Winter}}): {{Inferring Comparison Classes}} in {{Communication}}},
  shorttitle = {Warm (for {{Winter}})},
  author = {Tessler, Michael Henry and Goodman, Noah D.},
  year = {2022},
  journal = {Cognitive Science},
  volume = {46},
  number = {3},
  pages = {e13095},
  issn = {1551-6709},
  doi = {10.1111/cogs.13095},
  abstract = {The meanings of natural language utterances depend heavily on context. Yet, what counts as context is often only implicit in conversation. The utterance it's warm outside signals that the temperature outside is relatively high, but the temperature could be high relative to a number of different comparison classes: other days of the year, other weeks, other seasons, etc. Theories of context sensitivity in language agree that the comparison class is a crucial variable for understanding meaning, but little is known about how a listener decides upon the comparison class. Using the case study of gradable adjectives (e.g., warm), we extend a Bayesian model of pragmatic inference to reason flexibly about the comparison class and test its qualitative predictions in a large-scale free-production experiment. We find that human listeners infer the comparison class by reasoning about the kinds of observations that would be remarkable enough for a speaker to mention, given the speaker and listener's shared knowledge of the world. Further, we quantitatively synthesize the model and data using Bayesian data analysis, which reveals that usage frequency and a preference for basic-level categories are two main factors in comparison class inference. This work presents new data and reveals the mechanisms by which human listeners recover the relevant aspects of context when understanding language.},
  langid = {english},
  keywords = {Bayesian cognitive model,Bayesian data analysis,Comparison class,Context | Adjectives,Pragmatics,Rational Speech Act,Reference class},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13095},
  file = {/Users/xzfang/Zotero/storage/2R7ER4AZ/Tessler and Goodman - 2022 - Warm (for Winter) Inferring Comparison Classes in.pdf}
}

@article{thakral_reinstatement_2020,
  title = {Reinstatement of {{Event Details}} during {{Episodic Simulation}} in the {{Hippocampus}}},
  author = {Thakral, Preston P. and Madore, Kevin P. and Addis, Donna Rose and Schacter, Daniel L.},
  year = {2020},
  month = apr,
  journal = {Cerebral Cortex},
  volume = {30},
  number = {4},
  pages = {2321--2337},
  publisher = {{Oxford Academic}},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhz242},
  abstract = {Abstract.  According to the constructive episodic simulation hypothesis, episodic simulation (i.e., imagining specific novel future episodes) draws on some of t},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RPCGREGY/Thakral et al. - 2020 - Reinstatement of Event Details during Episodic Sim.pdf;/Users/xzfang/Zotero/storage/WDZ33X86/5614446.html}
}

@article{theodore_characteristics_2010,
  title = {Characteristics of Listener Sensitivity to Talker-Specific Phonetic Detail},
  author = {Theodore, Rachel M. and Miller, Joanne L.},
  year = {2010},
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {128},
  number = {4},
  pages = {2090--2099},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.3467771},
  abstract = {Previous research shows that listeners are sensitive to talker differences in phonetic properties of speech, including voice-onset-time (VOT) in word-initial voiceless stop consonants, and that learning how a talker produces one voiceless stop transfers to another word with the same voiceless stop [Allen, J. S., and Miller, J. L. (2004). J. Acoust. Soc. Am. 115, 3171\textendash 3183]. The present experiments examined whether transfer extends to words that begin with different voiceless stops. During training, listeners heard two talkers produce a given voiceless-initial word (e.g., pain). VOTs were manipulated such that one talker produced the voiceless stop with relatively short VOTs and the other with relatively long VOTs. At test, listeners heard a short- and long-VOT variant of the same word (e.g., pain) or a word beginning with a different voiceless stop (e.g., cane or coal), and were asked to select which of the two VOT variants was most representative of a given talker. In all conditions, which variant was selected at test was in line with listeners' exposure during training, and the effect was equally strong for the novel word and the training word. These findings suggest that accommodating talker-specific phonetic detail does not require exposure to each individual phonetic segment.},
  file = {/Users/xzfang/Zotero/storage/8IKTHLGJ/Theodore and Miller - 2010 - Characteristics of listener sensitivity to talker-.pdf;/Users/xzfang/Zotero/storage/FRX8HBDB/1.html}
}

@article{theodore_distributional_2019,
  title = {Distributional Learning for Speech Reflects Cumulative Exposure to a Talker's Phonetic Distributions},
  author = {Theodore, Rachel M. and Monto, Nicholas R.},
  year = {2019},
  month = jun,
  journal = {Psychonomic bulletin \& review},
  volume = {26},
  number = {3},
  pages = {985--992},
  issn = {1069-9384},
  doi = {10.3758/s13423-018-1551-5},
  abstract = {Efficient speech perception requires listeners to maintain an exquisite tension between stability of the language architecture and flexibility to accommodate variation in the input, such as that associated with individual talker differences in speech production. Achieving this tension can be guided by top-down learning mechanisms, wherein lexical information constrains interpretation of speech input, and by bottom-up learning mechanisms, in which distributional information in the speech signal used to optimize the mapping to speech sound categories. An open question for theories of perceptual learning concerns the nature of the representations that are built for individual talkers: do these representations reflect long-term, global exposure to a talker or rather only short-term, local exposure? Recent research suggests that when lexical knowledge is used to resolve a talker's ambiguous productions, listeners disregard previous experience with a talker and instead rely on only recent experience, a finding that is contrary to predictions of Bayesian belief-updating accounts of perceptual adaptation. Here we use a distributional learning paradigm in which lexical information is not explicitly required to resolve ambiguous input to provide an additional test of global versus local exposure accounts. Listeners completed two blocks of phonetic categorization for stimuli that differed in voice-onset-time, a probabilistic cue to the voicing contrast in English stop consonants. In each block, two distributions were presented, one specifying /g/ and one specifying /k/. Across the two blocks, variance of the distributions was manipulated to be either narrow or wide. The critical manipulation was order of the two blocks; half of the listeners were first exposed to the narrow distributions followed by the wide distributions, with the order reversed for the other half of the listeners. The results showed that for earlier trials, the identification slope was steeper for the narrow-wide group compared to the wide-narrow group, but this difference was attenuated for later trials. The between-group convergence was driven by an asymmetry in learning between the two orders such that only those in the narrow-wide group showed slope movement during exposure, a pattern that was mirrored by computational simulations in which the distributional statistics of the present talker were integrated with prior experience with English. This pattern of results suggests that listeners did not disregard all prior experience with the talker, and instead used cumulative exposure to guide phonetic decisions, which raises the possibility that maximally accommodating a talker's phonetic signature entails maintaining representations that reflect global experience.},
  pmcid = {PMC6559869},
  pmid = {30604404},
  file = {/Users/xzfang/Zotero/storage/F7WQ6G8C/Theodore and Monto - 2019 - Distributional learning for speech reflects cumula.pdf}
}

@article{theodore_individual_2009,
  title = {Individual Talker Differences in Voice-Onset-Time: {{Contextual}} Influences},
  shorttitle = {Individual Talker Differences in Voice-Onset-Time},
  author = {Theodore, Rachel M. and Miller, Joanne L. and DeSteno, David},
  year = {2009},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {125},
  number = {6},
  pages = {3974--3982},
  issn = {0001-4966},
  doi = {10.1121/1.3106131},
  abstract = {Previous research indicates that talkers differ in phonetically relevant properties of speech, including voice-onset-time (VOT) in word-initial stop consonants; some talkers have characteristically shorter VOTs than others. Previous research also indicates that VOT is robustly affected by contextual influences, including speaking rate and place of articulation. This paper examines whether these contextual influences on VOT are themselves talker-specific. Many tokens of alveolar /ti/ (experiment 1) or labial /pi/ and velar /ki/ (experiment 2) were elicited from talkers across a range of rates. VOT and vowel duration (a metric of rate) were measured for each token. Hierarchical linear modeling analyses showed that (1) VOT increased as rate decreased for all talkers, but the magnitude of the increase varied significantly across talkers; thus the effect of rate on VOT was talker-specific; (2) the talker-specific effect of rate was stable across a change in place of articulation; and (3) for all talkers VOTs were shorter for labial than velar stops, and there was no significant variability in the magnitude of this displacement across talkers; thus the effect of place on VOT was not talker-specific. The implications of these findings for how listeners might accommodate talker differences in VOT during speech perception are discussed.},
  pmcid = {PMC2806434},
  pmid = {19507979},
  file = {/Users/xzfang/Zotero/storage/QSHURUIT/Theodore et al. - 2009 - Individual talker differences in voice-onset-time.pdf}
}

@article{thierry_eventrelated_2008,
  title = {Event-Related Potential Characterisation of the {{Shakespearean}} Functional Shift in Narrative Sentence Structure},
  author = {Thierry, Guillaume and Martin, Clara D. and {Gonzalez-Diaz}, Victorina and Rezaie, Roozbeh and Roberts, Neil and Davis, Philip M.},
  year = {2008},
  month = apr,
  journal = {NeuroImage},
  volume = {40},
  number = {2},
  pages = {923--931},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2007.12.006},
  abstract = {Neurolinguistic studies have scrutinised the physiological consequences of disruptions in the flow of language comprehension produced by violations of meaning, syntax, or both. Some 400 years ago, Shakespeare already crafted verses in which the functional status of words was changed, as in ``to lip a wanton in a secure couch''. Here, we tested the effect of word class conversion as used by Shakespeare \textendash{} the functional shift \textendash{} on event-related brain potential waves traditionally reported in neurophysiolinguistics: the left anterior negativity (LAN), the N400, and the P600. Participants made meaningfulness decisions to sentences containing (a) a semantic incongruity, (b) a functional shift, (c) a double violation, or (d) neither a semantic incongruity nor a syntactic violation. The Shakespearean functional shift elicited significant LAN and P600 modulations but failed to modulate the N400 wave. This provides evidence that words which had their functional status changed triggered both an early syntactic evaluation process thought to be mainly automatic and a delayed re-evaluation/repair process that is more controlled, but semantic integration required no additional processing. We propose that this dissociation between syntactic and semantic evaluation enabled Shakespeare to create dramatic effects without diverting his public away from meaning.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VLGG9P2W/Thierry et al. - 2008 - Event-related potential characterisation of the Sh.pdf;/Users/xzfang/Zotero/storage/RXV968IU/S1053811907010968.html}
}

@article{thierry_unconscious_2009,
  title = {Unconscious Effects of Language-Specific Terminology on Preattentive Color Perception},
  author = {Thierry, Guillaume and Athanasopoulos, Panos and Wiggett, Alison and Dering, Benjamin and Kuipers, Jan-Rouke},
  year = {2009},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {106},
  number = {11},
  pages = {4567--4570},
  issn = {0027-8424},
  doi = {10.1073/pnas.0811155106},
  abstract = {It is now established that native language affects one's perception of the world. However, it is unknown whether this effect is merely driven by conscious, language-based evaluation of the environment or whether it reflects fundamental differences in perceptual processing between individuals speaking different languages. Using brain potentials, we demonstrate that the existence in Greek of 2 color terms\textemdash ghalazio and ble\textemdash distinguishing light and dark blue leads to greater and faster perceptual discrimination of these colors in native speakers of Greek than in native speakers of English. The visual mismatch negativity, an index of automatic and preattentive change detection, was similar for blue and green deviant stimuli during a color oddball detection task in English participants, but it was significantly larger for blue than green deviant stimuli in native speakers of Greek. These findings establish an implicit effect of language-specific terminology on human color perception.},
  pmcid = {PMC2657373},
  pmid = {19240215},
  file = {/Users/xzfang/Zotero/storage/WEETXHFJ/Thierry et al. - 2009 - Unconscious effects of language-specific terminolo.pdf}
}

@article{thill_theories_2013,
  title = {Theories and Computational Models of Affordance and Mirror Systems: {{An}} Integrative Review},
  shorttitle = {Theories and Computational Models of Affordance and Mirror Systems},
  author = {Thill, Serge and Caligiore, Daniele and Borghi, Anna M. and Ziemke, Tom and Baldassarre, Gianluca},
  year = {2013},
  month = mar,
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {37},
  number = {3},
  pages = {491--521},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2013.01.012},
  abstract = {Neuroscientific and psychological data suggest a close link between affordance and mirror systems in the brain. However, we still lack a full understanding of both the individual systems and their interactions. Here, we propose that the architecture and functioning of the two systems is best understood in terms of two challenges faced by complex organisms, namely: (a) the need to select among multiple affordances and possible actions dependent on context and high-level goals and (b) the exploitation of the advantages deriving from a hierarchical organisation of behaviour based on actions and action-goals. We first review and analyse the psychological and neuroscientific literature on the mechanisms and processes organisms use to deal with these challenges. We then analyse existing computational models thereof. Finally we present the design of a computational framework that integrates the reviewed knowledge. The framework can be used both as a theoretical guidance to interpret empirical data and design new experiments, and to design computational models addressing specific problems debated in the literature.},
  langid = {english},
  keywords = {Affordance processing,Canonical neurons,Computational modelling,Embodied cognition,Integration,Mirror system,Neurophysiology,Neuroscience,Psychology},
  file = {/Users/xzfang/Zotero/storage/U6EVGMDA/Thill et al. - 2013 - Theories and computational models of affordance an.pdf;/Users/xzfang/Zotero/storage/G62BFRPS/S0149763413000134.html}
}

@article{thompson_cultural_2020,
  title = {Cultural Influences on Word Meanings Revealed through Large-Scale Semantic Alignment},
  author = {Thompson, Bill and Roberts, Se{\'a}n G. and Lupyan, Gary},
  year = {2020},
  month = oct,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {10},
  pages = {1029--1038},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0924-8},
  abstract = {If the structure of language vocabularies mirrors the structure of natural divisions that are universally perceived, then the meanings of words in different languages should closely align. By contrast, if shared word meanings are a product of shared culture, history and geography, they may differ between languages in substantial but predictable ways. Here, we analysed the semantic neighbourhoods of 1,010 meanings in 41 languages. The most-aligned words were from semantic domains with high internal structure (number, quantity and kinship). Words denoting natural kinds, common actions and artefacts aligned much less well. Languages that are more geographically proximate, more historically related and/or spoken by more-similar cultures had more aligned word meanings. These results provide evidence that the meanings of common words vary in ways that reflect the culture, history and geography of their users.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Evolution of language,Human behaviour,Language and linguistics},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Evolution of language;Human behaviour;Language and linguistics Subject\_term\_id: evolution-of-language;human-behaviour;language-and-linguistics},
  file = {/Users/xzfang/Zotero/storage/CIQ93AF8/Thompson et al. - 2020 - Cultural influences on word meanings revealed thro.pdf;/Users/xzfang/Zotero/storage/PNYYUYFQ/s41562-020-0924-8.html}
}

@article{thompson_why_2012,
  title = {Why {{Do Most Faces Look Thinner Upside Down}}?},
  author = {Thompson, Peter and Wilson, Jennie},
  year = {2012},
  month = dec,
  journal = {i-Perception},
  volume = {3},
  number = {10},
  pages = {765--774},
  publisher = {{SAGE Publications}},
  issn = {2041-6695},
  doi = {10.1068/i0554},
  abstract = {Faces are found generally to be perceived as thinner when viewed upside down. When a face is viewed upright, the internal features are thought to influence the perception of face shape. However, when inverted, it has been proposed that disruption to holistic processing means that these factors can no longer be used to judge the shape of a face. We show that it is not the case that an inverted face reverts to some average shape whereby fat faces appear thinner upside down whereas thin faces appear fatter. The fact that the illusion appears to occur for most face shapes is discussed with regard to the horizontal\textendash vertical illusion.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BYTP26UW/Thompson and Wilson - 2012 - Why Do Most Faces Look Thinner Upside Down.pdf}
}

@article{thornton_people_2021,
  title = {People Accurately Predict the Transition Probabilities between Actions},
  author = {Thornton, Mark A. and Tamir, Diana I.},
  year = {2021},
  journal = {Science Advances},
  volume = {7},
  number = {9},
  pages = {eabd4995},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.abd4995},
  file = {/Users/xzfang/Zotero/storage/X22RQSIZ/Thornton and Tamir - People accurately predict the transition probabili.pdf}
}

@article{thornton_sensorimotor_2018,
  title = {Sensorimotor Activity Measured via Oscillations of {{EEG}} Mu Rhythms in Speech and Non-Speech Discrimination Tasks with and without Segmentation Demands},
  author = {Thornton, David and Harkrider, Ashley W. and Jenson, David and Saltuklaroglu, Tim},
  year = {2018},
  month = dec,
  journal = {Brain and Language},
  volume = {187},
  pages = {62--73},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2017.03.011},
  abstract = {Better understanding of the role of sensorimotor processing in speech and non-speech segmentation can be achieved with more temporally precise measures. Twenty adults made same/different discriminations of speech and non-speech stimuli pairs, with and without segmentation demands. Independent component analysis of 64-channel EEG data revealed clear sensorimotor mu components, with characteristic alpha and beta peaks, localized to premotor regions in 70\% of participants.Time-frequency analyses of mu components from accurate trials showed that (1) segmentation tasks elicited greater event-related synchronization immediately following offset of the first stimulus, suggestive of inhibitory activity; (2) strong late event-related desynchronization in all conditions, suggesting that working memory/covert replay contributed substantially to sensorimotor activity in all conditions; (3) stronger beta desynchronization in speech versus non-speech stimuli during stimulus presentation, suggesting stronger auditory-motor transforms for speech versus non-speech stimuli. Findings support the continued use of oscillatory approaches for helping understand segmentation and other cognitive tasks.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/4Q8VM8KD/S0093934X16301274.html}
}

@article{thorpe_seeking_2001,
  title = {Seeking {{Categories}} in the {{Brain}}},
  author = {Thorpe, Simon J. and {Fabre-Thorpe}, Mich{\`e}le},
  year = {2001},
  month = jan,
  journal = {Science},
  volume = {291},
  number = {5502},
  pages = {260--263},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1058249},
  abstract = {{$<$}p{$>$} Categorizing the objects that we see is a fundamental property of our brain and the brains of other mammals. In their enlightening Perspective, Thorpe and Fabre-Thorpe discuss exciting new findings in monkeys ( Freedman \emph{et al}.) that identify neurons in the prefrontal cortex that are primarily responsible for categorizing objects. {$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {\textcopyright{} 2001 American Association for the Advancement of Science},
  langid = {english},
  pmid = {11253215},
  file = {/Users/xzfang/Zotero/storage/MKK84J39/260.html}
}

@article{thorpe_seeking_2001a,
  title = {Seeking {{Categories}} in the {{Brain}}},
  author = {Thorpe, Simon J. and {Fabre-Thorpe}, Mich{\`e}le},
  year = {2001},
  month = jan,
  journal = {Science},
  volume = {291},
  number = {5502},
  pages = {260--263},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1058249},
  abstract = {{$<$}p{$>$} Categorizing the objects that we see is a fundamental property of our brain and the brains of other mammals. In their enlightening Perspective, Thorpe and Fabre-Thorpe discuss exciting new findings in monkeys ( Freedman \emph{et al}.) that identify neurons in the prefrontal cortex that are primarily responsible for categorizing objects. {$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {\textcopyright{} 2001 American Association for the Advancement of Science},
  langid = {english},
  pmid = {11253215},
  file = {/Users/xzfang/Zotero/storage/AW5WHCMB/260.html}
}

@article{tian_imagined_2018,
  title = {Imagined Speech Influences Perceived Loudness of Sound},
  author = {Tian, Xing and Ding, Nai and Teng, Xiangbin and Bai, Fan and Poeppel, David},
  year = {2018},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {225--234},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0305-8},
  abstract = {The way top-down and bottom-up processes interact to shape our perception and behaviour is a fundamental question and remains highly controversial. How early in a processing stream do such interactions occur, and what factors govern such interactions? The degree of abstractness of a perceptual attribute (for example, orientation versus shape in vision, or loudness versus sound identity in hearing) may determine the locus of neural processing and interaction between bottom-up and internal information. Using an imagery-perception repetition paradigm, we find that imagined speech affects subsequent auditory perception, even for a low-level attribute such as loudness. This effect is observed in early auditory responses in magnetoencephalography and electroencephalography that correlate with behavioural loudness ratings. The results suggest that the internal reconstruction of neural representations without external stimulation is flexibly regulated by task demands, and that such top-down processes can interact with bottom-up information at an early perceptual stage to modulate perception.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JW7SKQ97/Tian et al. - 2018 - Imagined speech influences perceived loudness of s.pdf}
}

@article{tillmann_congenital_2009,
  title = {Congenital Amusia: {{A}} Short-Term Memory Deficit for Non-Verbal, but Not Verbal Sounds},
  shorttitle = {Congenital Amusia},
  author = {Tillmann, Barbara and Schulze, Katrin and Foxton, Jessica M.},
  year = {2009},
  month = dec,
  journal = {Brain and Cognition},
  volume = {71},
  number = {3},
  pages = {259--264},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2009.08.003},
  abstract = {Congenital amusia refers to a lifelong disorder of music processing and is linked to pitch-processing deficits. The present study investigated congenital amusics' short-term memory for tones, musical timbres and words. Sequences of five events (tones, timbres or words) were presented in pairs and participants had to indicate whether the sequences were the same or different. The performance of congenital amusics confirmed a memory deficit for tone sequences, but showed normal performance for word sequences. For timbre sequences, amusics' memory performance was impaired in comparison to matched controls. Overall timbre performance was found to be correlated with melodic contour processing (as assessed by the Montreal Battery of Evaluation of Amusia). The present findings show that amusics' deficits extend to non-verbal sound material other than pitch, in this case timbre, while not affecting memory for verbal material. This is in line with previous suggestions about the domain-specificity of congenital amusia.},
  langid = {english},
  keywords = {Auditory scene analysis,Congenital amusia,Contour,Pitch,Short-term memory,Timbre,Words},
  file = {/Users/xzfang/Zotero/storage/SNR7N86L/Tillmann et al. - 2009 - Congenital amusia A short-term memory deficit for.pdf}
}

@article{tillmann_congenital_2009a,
  title = {Congenital Amusia: {{A}} Short-Term Memory Deficit for Non-Verbal, but Not Verbal Sounds},
  shorttitle = {Congenital Amusia},
  author = {Tillmann, Barbara and Schulze, Katrin and Foxton, Jessica M.},
  year = {2009},
  month = dec,
  journal = {Brain and Cognition},
  volume = {71},
  number = {3},
  pages = {259--264},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2009.08.003},
  abstract = {Congenital amusia refers to a lifelong disorder of music processing and is linked to pitch-processing deficits. The present study investigated congenital amusics' short-term memory for tones, musical timbres and words. Sequences of five events (tones, timbres or words) were presented in pairs and participants had to indicate whether the sequences were the same or different. The performance of congenital amusics confirmed a memory deficit for tone sequences, but showed normal performance for word sequences. For timbre sequences, amusics' memory performance was impaired in comparison to matched controls. Overall timbre performance was found to be correlated with melodic contour processing (as assessed by the Montreal Battery of Evaluation of Amusia). The present findings show that amusics' deficits extend to non-verbal sound material other than pitch, in this case timbre, while not affecting memory for verbal material. This is in line with previous suggestions about the domain-specificity of congenital amusia.},
  langid = {english},
  keywords = {Auditory scene analysis,Congenital amusia,Contour,Pitch,Short-term memory,Timbre,Words},
  file = {/Users/xzfang/Zotero/storage/VEALLPDQ/Tillmann et al. - 2009 - Congenital amusia A short-term memory deficit for.pdf;/Users/xzfang/Zotero/storage/LIRG48TH/S0278262609001560.html}
}

@article{tomasello_instantaneous_2022,
  title = {Instantaneous {{Neural Processing}} of {{Communicative Functions Conveyed}} by {{Speech Prosody}}},
  author = {Tomasello, Rosario and Grisoni, Luigi and Boux, Isabella and Sammler, Daniela and Pulverm{\"u}ller, Friedemann},
  year = {2022},
  month = feb,
  journal = {Cerebral Cortex},
  pages = {bhab522},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab522},
  abstract = {During conversations, speech prosody provides important clues about the speaker's communicative intentions. In many languages, a rising vocal pitch at the end of a sentence typically expresses a question function, whereas a falling pitch suggests a statement. Here, the neurophysiological basis of intonation and speech act understanding were investigated with high-density electroencephalography (EEG) to determine whether prosodic features are reflected at the neurophysiological level. Already approximately\,100~ms after the sentence-final word differing in prosody, questions, and statements expressed with the same sentences led to different neurophysiological activity recorded in the event-related potential. Interestingly, low-pass filtered sentences and acoustically matched nonvocal musical signals failed to show any neurophysiological dissociations, thus suggesting that the physical intonation alone cannot explain this modulation. Our results show rapid neurophysiological indexes of prosodic communicative information processing that emerge only when pragmatic and lexico-semantic information are fully expressed. The early enhancement of question-related activity compared with statements was due to sources in the articulatory-motor region, which may reflect the richer action knowledge immanent to questions, namely the expectation of the partner action of answering the question. The present findings demonstrate a neurophysiological correlate of prosodic communicative information processing, which enables humans to rapidly detect and understand speaker intentions in linguistic interactions.},
  file = {/Users/xzfang/Zotero/storage/BLBDNZMI/Tomasello et al. - 2022 - Instantaneous Neural Processing of Communicative F.pdf;/Users/xzfang/Zotero/storage/4Z64V45N/6524018.html}
}

@article{tomasello_thirty_2019,
  title = {Thirty Years of Great Ape Gestures},
  author = {Tomasello, Michael and Call, Josep},
  year = {2019},
  journal = {Animal Cognition},
  volume = {22},
  number = {4},
  issn = {1435-9448},
  doi = {10.1007/s10071-018-1167-1},
  abstract = {We and our colleagues have been doing studies of great ape gestural communication for more than 30 years. Here we attempt to spell out what we have learned. Some aspects of the process have been reliably established by multiple researchers, for example, its intentional structure and its sensitivity to the attentional state of the recipient. Other aspects are more controversial. We argue here that it is a mistake to assimilate great ape gestures to the species-typical displays of other mammals by claiming that they are fixed action patterns, as there are many differences, including the use of attention-getters. It is also a mistake, we argue, to assimilate great ape gestures to human gestures by claiming that they are used referentially and declaratively in a human-like manner, as apes' ``pointing'' gesture has many limitations and they do not gesture iconically. Great ape gestures constitute a unique form of primate communication with their own unique qualities.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2NLG36MR/Tomasello and Call - 2019 - Thirty years of great ape gestures.pdf}
}

@article{toneva_interpreting_2019,
  title = {Interpreting and Improving Natural-Language Processing (in Machines) with Natural Language-Processing (in the Brain)},
  author = {Toneva, Mariya and Wehbe, Leila},
  year = {2019},
  month = nov,
  journal = {arXiv:1905.11833 [cs, q-bio]},
  eprint = {1905.11833},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/xzfang/Zotero/storage/TI5HUX4H/Toneva and Wehbe - 2019 - Interpreting and improving natural-language proces.pdf;/Users/xzfang/Zotero/storage/CI3TVPTP/1905.html}
}

@article{torralba_contextual_2006,
  title = {Contextual Guidance of Eye Movements and Attention in Real-World Scenes: {{The}} Role of Global Features in Object Search.},
  shorttitle = {Contextual Guidance of Eye Movements and Attention in Real-World Scenes},
  author = {Torralba, Antonio and Oliva, Aude and Castelhano, Monica S. and Henderson, John M.},
  year = {2006},
  month = oct,
  journal = {Psychological Review},
  volume = {113},
  number = {4},
  pages = {766--786},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.113.4.766},
  abstract = {Many experiments have shown that the human visual system makes extensive use of contextual information for facilitating object search in natural scenes. However, the question of how to formally model contextual influences is still open. On the basis of a Bayesian framework, the authors present an original approach of attentional guidance by global scene context. The model comprises 2 parallel pathways; one pathway computes local features (saliency) and the other computes global (scenecentered) features. The contextual guidance model of attention combines bottom-up saliency, scene context, and top-down mechanisms at an early stage of visual processing and predicts the image regions likely to be fixated by human observers performing natural search tasks in real-world scenes.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KA45WI25/Torralba et al. - 2006 - Contextual guidance of eye movements and attention.pdf}
}

@article{toscano_continuous_2010,
  title = {Continuous Perception and Graded Categorization: {{Electrophysiological}} Evidence for a Linear Relationship between the Acoustic Signal and Perceptual Encoding of Speech},
  shorttitle = {Continuous Perception and Graded Categorization},
  author = {Toscano, Joseph C. and McMurray, Bob and Dennhardt, Joel and Luck, Steven. J.},
  year = {2010},
  month = oct,
  journal = {Psychological science},
  volume = {21},
  number = {10},
  pages = {1532--1540},
  issn = {0956-7976},
  doi = {10.1177/0956797610384142},
  abstract = {Speech sounds are highly variable, yet listeners readily extract information from them and transform continuous acoustic signals into meaningful categories during language comprehension. A central question is whether perceptual encoding captures continuous acoustic detail in a one-to-one fashion or whether it is affected by categories. We addressed this in an event-related potential (ERP) experiment in which listeners categorized spoken words that varied along a continuous acoustic dimension (voice onset time; VOT) in an auditory oddball task. We found that VOT effects were present through a late stage of perceptual processing (N1 component, ca. 100 ms poststimulus) and were independent of categories. In addition, effects of within-category differences in VOT were present at a post-perceptual categorization stage (P3 component, ca. 450 ms poststimulus). Thus, at perceptual levels, acoustic information is encoded continuously, independent of phonological information. Further, at phonological levels, fine-grained acoustic differences are preserved along with category information.},
  pmcid = {PMC3523688},
  pmid = {20935168},
  file = {/Users/xzfang/Zotero/storage/V8BQPZGN/Toscano et al. - 2010 - Continuous perception and graded categorization E.pdf}
}

@article{toscano_cue_2010,
  title = {Cue {{Integration With Categories}}: {{Weighting Acoustic Cues}} in {{Speech Using Unsupervised Learning}} and {{Distributional Statistics}}},
  shorttitle = {Cue {{Integration With Categories}}},
  author = {Toscano, Joseph C. and McMurray, Bob},
  year = {2010},
  journal = {Cognitive Science},
  volume = {34},
  number = {3},
  pages = {434--464},
  issn = {1551-6709},
  doi = {10.1111/j.1551-6709.2009.01077.x},
  abstract = {During speech perception, listeners make judgments about the phonological category of sounds by taking advantage of multiple acoustic cues for each phonological contrast. Perceptual experiments have shown that listeners weight these cues differently. How do listeners weight and combine acoustic cues to arrive at an overall estimate of the category for a speech sound? Here, we present several simulations using a mixture of Gaussians models that learn cue weights and combine cues on the basis of their distributional statistics. We show that a cue-weighting metric in which cues receive weight as a function of their reliability at distinguishing phonological categories provides a good fit to the perceptual data obtained from human listeners, but only when these weights emerge through the dynamics of learning. These results suggest that cue weights can be readily extracted from the speech signal through unsupervised learning processes.},
  copyright = {Copyright \textcopyright{} 2009 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Categorization,Cue weighting,Mixture of Gaussians,Reliability,Speech development,Speech perception,Statistical learning,Unsupervised learning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6709.2009.01077.x},
  file = {/Users/xzfang/Zotero/storage/N4942NIL/Toscano and McMurray - 2010 - Cue Integration With Categories Weighting Acousti.pdf;/Users/xzfang/Zotero/storage/8IELPEGB/j.1551-6709.2009.01077.html}
}

@article{toscano_cueintegration_2012,
  title = {Cue-Integration and Context Effects in Speech: {{Evidence}} against Speaking-Rate Normalization},
  shorttitle = {Cue-Integration and Context Effects in Speech},
  author = {Toscano, Joseph C. and McMurray, Bob},
  year = {2012},
  month = aug,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {74},
  number = {6},
  pages = {1284--1301},
  issn = {1943-393X},
  doi = {10.3758/s13414-012-0306-z},
  abstract = {Listeners are able to accurately recognize speech despite variation in acoustic cues across contexts, such as different speaking rates. Previous work has suggested that listeners use rate information (indicated by vowel length; VL) to modify their use of context-dependent acoustic cues, like voice-onset time (VOT), a primary cue to voicing. We present several experiments and simulations that offer an alternative explanation: that listeners treat VL as a phonetic cue rather than as an indicator of speaking rate, and that they rely on general cue-integration principles to combine information from VOT and VL. We demonstrate that listeners use the two cues independently, that VL is used in both naturally produced and synthetic speech, and that the effects of stimulus naturalness can be explained by a cue-integration model. Together, these results suggest that listeners do not interpret VOT relative to rate information provided by VL and that the effects of speaking rate can be explained by more general cue-integration principles.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XGZ7CDCE/Toscano and McMurray - 2012 - Cue-integration and context effects in speech Evi.pdf}
}

@article{toscano_timecourse_2015,
  title = {The Time-Course of Speaking Rate Compensation: Effects of Sentential Rate and Vowel Length on Voicing Judgments},
  shorttitle = {The Time-Course of Speaking Rate Compensation},
  author = {Toscano, Joseph C. and McMurray, Bob},
  year = {2015},
  month = may,
  journal = {Language, Cognition and Neuroscience},
  volume = {30},
  number = {5},
  pages = {529--543},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2014.946427},
  abstract = {Many sources of context information in speech (such as speaking rate) occur either before or after the phonetic cues they influence, yet there is little work examining the time-course of these effects. Here, we investigate how listeners compensate for preceding sentence rate and subsequent vowel length (VL; a secondary cue that has been used as a proxy for speaking rate) when categorising words varying in voice-onset time (VOT). Participants selected visual objects in a display while their eye-movements were recorded, allowing us to examine when each source of information had an effect on lexical processing. We found that the effect of VOT preceded that of VL, suggesting that each cue is used as it becomes available. In a second experiment, we found that, in contrast, the effect of preceding sentence rate occurred simultaneously with VOT, suggesting that listeners interpret VOT relative to preceding rate.},
  pmid = {25780801},
  keywords = {context effects,speaking rate,speech perception,spoken word recognition,visual world paradigm},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2014.946427},
  file = {/Users/xzfang/Zotero/storage/DEN4H3US/Toscano and McMurray - 2015 - The time-course of speaking rate compensation eff.pdf;/Users/xzfang/Zotero/storage/CP6KR56U/23273798.2014.html}
}

@article{town_neural_2013,
  title = {Neural and Behavioral Investigations into Timbre Perception},
  author = {Town, Stephen M. and Bizley, Jennifer K.},
  year = {2013},
  month = nov,
  journal = {Frontiers in Systems Neuroscience},
  volume = {7},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2013.00088},
  abstract = {Timbre is the attribute that distinguishes sounds of equal pitch, loudness and duration. It contributes to our perception and discrimination of different vowels and consonants in speech, instruments in music and environmental sounds. Here we begin by reviewing human timbre perception and the spectral and temporal acoustic features that give rise to timbre in speech, musical and environmental sounds. We also consider the perception of timbre by animals, both in the case of human vowels and non-human vocalizations. We then explore the neural representation of timbre, first within the peripheral auditory system and later at the level of the auditory cortex. We examine the neural networks that are implicated in timbre perception and the computations that may be performed in auditory cortex to enable listeners to extract information about timbre. We consider whether single neurons in auditory cortex are capable of representing spectral timbre independently of changes in other perceptual attributes and the mechanisms that may shape neural sensitivity to timbre. Finally, we conclude by outlining some of the questions that remain about the role of neural mechanisms in behavior and consider some potentially fruitful avenues for future research.},
  pmcid = {PMC3826062},
  pmid = {24312021},
  file = {/Users/xzfang/Zotero/storage/3MG7UEV7/Town and Bizley - 2013 - Neural and behavioral investigations into timbre p.pdf}
}

@article{tracy_spontaneous_2008,
  title = {The Spontaneous Expression of Pride and Shame: {{Evidence}} for Biologically Innate Nonverbal Displays},
  shorttitle = {The Spontaneous Expression of Pride and Shame},
  author = {Tracy, Jessica L. and Matsumoto, David},
  year = {2008},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {33},
  pages = {11655--11660},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0802686105},
  abstract = {The present research examined whether the recognizable nonverbal expressions associated with pride and shame may be biologically innate behavioral responses to success and failure. Specifically, we tested whether sighted, blind, and congenitally blind individuals across cultures spontaneously display pride and shame behaviors in response to the same success and failure situations\textemdash victory and defeat at the Olympic or Paralympic Games. Results showed that sighted, blind, and congenitally blind individuals from {$>$}30 nations displayed the behaviors associated with the prototypical pride expression in response to success. Sighted, blind, and congenitally blind individuals from most cultures also displayed behaviors associated with shame in response to failure. However, culture moderated the shame response among sighted athletes: it was less pronounced among individuals from highly individualistic, self-expression-valuing cultures, primarily in North America and West Eurasia. Given that congenitally blind individuals across cultures showed the shame response to failure, findings overall are consistent with the suggestion that the behavioral expressions associated with both shame and pride are likely to be innate, but the shame display may be intentionally inhibited by some sighted individuals in accordance with cultural norms.},
  chapter = {Social Sciences},
  copyright = {\textcopyright{} 2008 by The National Academy of Sciences of the USA},
  langid = {english},
  pmid = {18695237},
  keywords = {emotion,innate behavioral response,nonverbal expression,self-conscious emotion},
  file = {/Users/xzfang/Zotero/storage/U2NPKS3M/Tracy and Matsumoto - 2008 - The spontaneous expression of pride and shame Evi.pdf;/Users/xzfang/Zotero/storage/E42LMTJT/11655.html}
}

@techreport{traer_causal_2020,
  type = {Preprint},
  title = {Causal Inference in Environmental Sound Recognition},
  author = {Traer, James and {Norman-Haignere}, Sam V. and McDermott, Josh H.},
  year = {2020},
  month = jul,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/2020.07.13.200949},
  abstract = {Sound is caused by physical events in the world. Do humans infer these causes when recognizing sound sources? We tested whether the recognition of common environmental sounds depends on the inference of a basic physical variable \textendash{} the source intensity (i.e. the power that produces a sound). A source's intensity can be inferred from the intensity it produces at the ear and its distance, which is normally conveyed by reverberation. Listeners could thus use intensity at the ear and reverberation to constrain recognition by inferring the underlying source intensity. Alternatively, listeners might separate these acoustic cues from their representation of a sound's identity in the interest of invariant recognition. We compared these two hypotheses by measuring recognition accuracy for sounds with typically low or high source intensity (e.g. pepper grinders vs. trucks) that were presented across a range of intensities at the ear or with reverberation cues to distance. The recognition of low-intensity sources (e.g. pepper grinders) was impaired by high presentation intensities or reverberation that conveyed distance, either of which imply high source intensity. Neither effect occurred for high-intensity sources. The results suggest that listeners implicitly use the intensity at the ear along with distance cues to infer a source's power and constrain its identity. The recognition of real-world sounds thus appears to depend upon the inference of their physical generative parameters, even generative parameters whose cues might otherwise be separated from the representation of a sound's identity.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5AG7S3B5/Traer et al. - 2020 - Causal inference in environmental sound recognitio.pdf}
}

@article{traxler_trends_2014,
  title = {Trends in Syntactic Parsing: Anticipation, {{Bayesian}} Estimation, and Good-Enough Parsing},
  shorttitle = {Trends in Syntactic Parsing},
  author = {Traxler, Matthew J.},
  year = {2014},
  month = nov,
  journal = {Trends in cognitive sciences},
  volume = {18},
  number = {11},
  pages = {605--611},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2014.08.001},
  abstract = {Syntactic parsing processes establish dependencies between words in a sentence. These dependencies affect how comprehenders assign meaning to sentence constituents. Classical approaches to parsing describe it entirely as a bottom-up signal analysis. More recent approaches assign the comprehender a more active role, allowing the comprehender's individual experience, knowledge, and beliefs to influence his or her interpretation. This review describes developments in three related aspects of sentence processing research: anticipatory processing, Bayesian/noisy-channel approaches to sentence processing, and the `good-enough' parsing hypothesis.},
  pmcid = {PMC6814003},
  pmid = {25200381},
  file = {/Users/xzfang/Zotero/storage/XPNWRWRG/Traxler - 2014 - Trends in syntactic parsing anticipation, Bayesia.pdf}
}

@misc{treisman__,
  title = {And},
  author = {Treisman, Anne M.},
  abstract = {A new hypothesis about the role of focused attention is proposed. The feature-integration theory of attention suggests that attention must be directed serially to each stimulus in a display whenever conjunctions of more than one separable feature are needed to characterize or distinguish the possible objects presented. A number of predictions were tested in a variety of paradigms including visual search, texture segregation, identification and localization, and using both separable dimensions (shape and color) and local elements or parts of figures (lines, curves, etc. in letters) as the features to be integrated into complex wholes. The results were in general consistent with the hypothesis. They offer a new set of criteria for distinguishing separable from integral features and a new rationale for predicting which tasks will show attention limits and which will not. When we open our eyes on a familiar scene, we form an immediate impression of recognizable objects, organized coherently in a spatial framework. Analysis of our experience into more elementary sensations is difficult, and appears subjectively to require an unusual type of perceptual activity. In contrast, the physiological evidence suggests that the visual scene is analyzed at an early stage by specialized populations of receptors that respond selectively to such properties as orientation, color, spatial frequency, or movement, and map these properties in different areas of the brain (Zeki, 1976). The controversy between analytic and synthetic theories of perception goes back many years: the Associationists asserted that the experience of complex wholes is built by combining more elementary sensations, while the Gestalt psychologists claimed that the whole precedes its parts, that we initially register unitary objects and relationships, and only later, if necessary, analyze these objects into their component parts or properties. This view is still active now},
  file = {/Users/xzfang/Zotero/storage/657LPAK9/Treisman - And.pdf;/Users/xzfang/Zotero/storage/UF3HF86F/summary.html}
}

@article{treisman_feature_,
  title = {Feature {{Analysis}} in {{Early Vision}}: {{Evidence From Search Asymmetries}}},
  author = {Treisman, Anne and Gormican, Stephen},
  pages = {34},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZRA269IR/Treisman and Gormican - Feature Analysis in Early Vision Evidence From Se.pdf}
}

@article{treisman_featureintegration_1980,
  title = {A Feature-Integration Theory of Attention},
  author = {Treisman, Anne M. and Gelade, Garry},
  year = {1980},
  month = jan,
  journal = {Cognitive Psychology},
  volume = {12},
  number = {1},
  pages = {97--136},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(80)90005-5},
  abstract = {A new hypothesis about the role of focused attention is proposed. The feature-integration theory of attention suggests that attention must be directed serially to each stimulus in a display whenever conjunctions of more than one separable feature are needed to characterize or distinguish the possible objects presented. A number of predictions were tested in a variety of paradigms including visual search, texture segregation, identification and localization, and using both separable dimensions (shape and color) and local elements or parts of figures (lines, curves, etc. in letters) as the features to be integrated into complex wholes. The results were in general consistent with the hypothesis. They offer a new set of criteria for distinguishing separable from integral features and a new rationale for predicting which tasks will show attention limits and which will not.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/UY8Q5WMS/Treisman and Gelade - 1980 - A feature-integration theory of attention.pdf}
}

@article{treisman_preattentive_1985,
  title = {Preattentive {{Processing}} in {{Vision}}},
  author = {Treisman, Anne},
  year = {1985},
  pages = {22},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/246LMYEG/Treisman - Preattentive Processing in Vision.pdf}
}

@article{treisman_search_1985,
  title = {Search Asymmetry: {{A}} Diagnostic for Preattentive Processing of Separable Features},
  shorttitle = {Search Asymmetry},
  author = {Treisman, Anne and Souther, Janet},
  year = {1985},
  journal = {Journal of Experimental Psychology: General},
  volume = {114},
  number = {3},
  pages = {285--310},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/0096-3445.114.3.285},
  abstract = {Five experiments, with 42 university students, showed that the search rate for a target among distractors varied dramatically depending on which stimulus played the role of target and which that of distractors. For example, the time required to find a circle distinguished by an intersecting line was independent of the number of regular circles in the display, whereas the time to find a regular circle among circles with lines increased linearly with the number of distractors. The pattern of performance suggests parallel processing when the target had a unique distinguishing feature and serial self-terminating search when the target was distinguished only by the absence of a feature that was present in all the distractors. Results are consistent with A. Treisman and G. Gelade's (see record 1980-04685-001) feature-integration theory, which predicts that a single feature should be detected by the mere presence of activity in the relevant feature map, whereas tasks that require Ss to locate multiple instances of a feature demand focused attention. Search asymmetries may therefore offer a new diagnostic to identify the primitive features of early vision. Several candidate features are examined. Colors, line ends or terminators, and closure (in the sense of a partly or wholly enclosed area) appear to be functional features; connectedness, intactness (absence of an intersecting line), and acute angles do not. (40 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Maps,Distraction,Pictorial Stimuli,Visual Discrimination,Visual Search},
  file = {/Users/xzfang/Zotero/storage/B2UZ44XH/1986-13592-001.html}
}

@article{tripathy_extent_2002,
  title = {The Extent of Crowding in Peripheral Vision Does Not Scale with Target Size},
  author = {Tripathy, Srimant P and Cavanagh, Patrick},
  year = {2002},
  month = sep,
  journal = {Vision Research},
  volume = {42},
  number = {20},
  pages = {2357--2369},
  issn = {0042-6989},
  doi = {10.1016/S0042-6989(02)00197-9},
  abstract = {Identifying a target is more difficult when distracters are present within a zone of interaction around the target. We investigated whether the spatial extent of the zone of interaction scales with the size of the target. Our target was a letter T in one-of-four orientations. Our distracters were four squared-thetas in one-of-two orientations, presented one in each of the four cardinal directions, equidistant from the target. Target\textendash distracter separation was varied and the proportion of correct responses at each separation was determined. From these the extent of interaction was estimated. This procedure was repeated for different target sizes spread over a 5-fold range. In each case, the contrast of the target was adjusted so that its visibility was constant across target sizes. The experiment was performed in the luminance domain (grey targets on grey background) and in the chromatic domain (green target on equiluminant grey background). In the luminance domain, target size had only a small effect on the extent of interaction; these interactions did not scale with target size. The extents of interaction for chromatic stimuli were similar to those for luminance stimuli. For a fixed target visibility, decreasing the duration of the stimulus resulted in an increase in the extent of interaction. The relevance of our findings is discussed with regard to a variety of proposed explanations for crowding. Our results are consistent with an attention-based explanation for crowding.},
  langid = {english}
}

@article{trott_languages_2022,
  title = {Languages Are Efficient, but for Whom?},
  author = {Trott, Sean and Bergen, Benjamin},
  year = {2022},
  month = aug,
  journal = {Cognition},
  volume = {225},
  pages = {105094},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105094},
  abstract = {Human languages evolve to make communication more efficient. But efficiency creates trade-offs: what is efficient for speakers is not always efficient for comprehenders. How do languages balance these competing pressures? We focus on Zipf's meaning-frequency law, the observation that frequent wordforms have more meanings. On the one hand, this law could reflect a speaker-oriented pressure to reuse frequent wordforms. Yet human languages still maintain thousands of distinct wordforms, suggesting a countervailing, comprehender-oriented pressure. What balance of these pressures produces Zipf's meaning-frequency law? Using a neutral baseline, we find that frequent wordforms in real lexica have fewer homophones than predicted by their phonotactic structure: real lexica favor a comprehender-oriented pressure to reduce the cost of frequent disambiguation. These results help clarify the evolutionary drive for efficiency: human languages are subject to competing pressures for efficient communication, the relative magnitudes of which reveal how individual-level cognitive constraints shape languages over time.},
  langid = {english},
  keywords = {Ambiguity,Efficiency,Frequency,Homophones,Language evolution},
  file = {/Users/xzfang/Zotero/storage/KBANMPID/S0010027722000828.html}
}

@article{trude_limitations_2013,
  title = {Limitations on Adaptation to Foreign Accents},
  author = {Trude, Alison M. and Tremblay, Annie and {Brown-Schmidt}, Sarah},
  year = {2013},
  month = oct,
  journal = {Journal of memory and language},
  volume = {69},
  number = {3},
  pages = {349--367},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2013.05.002},
  abstract = {Although foreign accents can be highly dissimilar to native speech, existing research suggests that listeners readily adapt to foreign accents after minimal exposure. However, listeners often report difficulty understanding non-native accents, and the time-course and specificity of adaptation remain unclear. Across five experiments, we examined whether listeners could use a newly learned feature of a foreign accent to eliminate lexical competitors during online speech perception. Participants heard the speech of a native English speaker and a native speaker of Qu\'ebec French who, in English, pronounces /i/ as [i] (e.g., weak as wick) before all consonants except voiced fricatives. We examined whether listeners could learn to eliminate a shifted /i/-competitor (e.g., weak) when interpreting the accented talker produce an unshifted word (e.g., wheeze). In four experiments, adaptation was strikingly limited, though improvement across the course of the experiment and with stimulus variations indicates learning was possible. In a fifth experiment, adaptation was not improved when a native English talker produced the critical vowel shift, demonstrating that the limitation is not simply due to the fact the accented talker was non-native. These findings suggest that although listeners can arrive at the correct interpretation of a foreign accent, this process can pose significant difficulty.},
  pmcid = {PMC3763963},
  pmid = {24014935},
  file = {/Users/xzfang/Zotero/storage/APU3M67J/Trude et al. - 2013 - Limitations on adaptation to foreign accents.pdf}
}

@article{trude_talkerspecific_2012,
  title = {Talker-Specific Perceptual Adaptation during Online Speech Perception},
  author = {Trude, Alison M. and {Brown-Schmidt}, Sarah},
  year = {2012},
  month = sep,
  journal = {Language and Cognitive Processes},
  volume = {27},
  number = {7-8},
  pages = {979--1001},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690965.2011.597153},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/U2LFJUEZ/Trude and Brown-Schmidt - 2012 - Talker-specific perceptual adaptation during onlin.pdf}
}

@article{trude_talkerspecific_2014,
  title = {Talker-Specific Learning in Amnesia: {{Insight}} into Mechanisms of Adaptive Speech Perception},
  shorttitle = {Talker-Specific Learning in Amnesia},
  author = {Trude, Alison M. and Duff, Melissa C. and {Brown-Schmidt}, Sarah},
  year = {2014},
  month = may,
  journal = {Cortex; a journal devoted to the study of the nervous system and behavior},
  volume = {54},
  pages = {117--123},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2014.01.015},
  abstract = {A hallmark of human speech perception is the ability to comprehend speech quickly and effortlessly despite enormous variability across talkers. However, current theories of speech perception do not make specific claims about the memory mechanisms involved in this process. To examine whether declarative memory is necessary for talker-specific learning, we tested the ability of amnesic patients with severe declarative memory deficits to learn and distinguish the accents of two unfamiliar talkers by monitoring their eye-gaze as they followed spoken instructions. Analyses of the time-course of eye fixations showed that amnesic patients rapidly learned to distinguish these accents and tailored perceptual processes to the voice of each talker. These results demonstrate that declarative memory is not necessary for this ability and points to the involvement of non-declarative memory mechanisms. These results are consistent with findings that other social and accommodative behaviors are preserved in amnesia and contribute to our understanding of the interactions of multiple memory systems in the use and understanding of spoken language.},
  pmcid = {PMC3995838},
  pmid = {24657480},
  file = {/Users/xzfang/Zotero/storage/S82XU2ZC/Trude et al. - 2014 - Talker-specific learning in amnesia Insight into .pdf}
}

@article{tsantani_faces_2019,
  title = {Faces and Voices in the Brain: {{A}} Modality-General Person-Identity Representation in Superior Temporal Sulcus},
  shorttitle = {Faces and Voices in the Brain},
  author = {Tsantani, Maria and Kriegeskorte, Nikolaus and McGettigan, Carolyn and Garrido, L{\'u}cia},
  year = {2019},
  month = nov,
  journal = {NeuroImage},
  volume = {201},
  pages = {116004},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.07.017},
  abstract = {Face-selective and voice-selective brain regions have been shown to represent face-identity and voice-identity, respectively. Here we investigated whether there are modality-general person-identity representations in the brain that can be driven by either a face or a voice, and that invariantly represent naturalistically varying face videos and voice recordings of the same identity. Models of face and voice integration suggest that such representations could exist in multimodal brain regions, and in unimodal regions via direct coupling between face- and voice-selective regions. Therefore, in this study we used fMRI to measure brain activity patterns elicited by the faces and voices of familiar people in face-selective, voice-selective, and person-selective multimodal brain regions. We used representational similarity analysis to (1) compare representational geometries (i.e. representational dissimilarity matrices) of face- and voice-elicited identities, and to (2) investigate the degree to which pattern discriminants for pairs of identities generalise from one modality to the other. We did not find any evidence of similar representational geometries across modalities in any of our regions of interest. However, our results showed that pattern discriminants that were trained to discriminate pairs of identities from their faces could also discriminate the respective voices (and vice-versa) in the right posterior superior temporal sulcus (rpSTS). Our findings suggest that the rpSTS is a person-selective multimodal region that shows a modality-general person-identity representation and integrates face and voice identity information.},
  langid = {english},
  keywords = {Face recognition,Multisensory processing,Person-identity recognition,Representational similarity analysis,Voice recognition},
  file = {/Users/xzfang/Zotero/storage/EWCP53KK/Tsantani et al. - 2019 - Faces and voices in the brain A modality-general .pdf;/Users/xzfang/Zotero/storage/RI24QVG5/S1053811919305853.html}
}

@article{tsao_comparing_2008,
  title = {Comparing Face Patch Systems in Macaques and Humans},
  author = {Tsao, D. Y. and Moeller, S. and Freiwald, W. A.},
  year = {2008},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {49},
  pages = {19514--19519},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0809662105},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JT4QNTWK/Tsao et al. - 2008 - Comparing face patch systems in macaques and human.pdf}
}

@article{tsao_integrating_2018,
  title = {Integrating Time from Experience in the Lateral Entorhinal Cortex},
  author = {Tsao, Albert and Sugar, J{\o}rgen and Lu, Li and Wang, Cheng and Knierim, James J. and Moser, May-Britt and Moser, Edvard I.},
  year = {2018},
  month = sep,
  journal = {Nature},
  volume = {561},
  number = {7721},
  pages = {57--62},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0459-6},
  abstract = {The encoding of time and its binding to events are crucial for episodic memory, but how these processes are carried out in hippocampal\textendash entorhinal circuits is unclear. Here we show in freely foraging rats that temporal information is robustly encoded across time scales from seconds to hours within the overall population state of the lateral entorhinal cortex. Similarly pronounced encoding of time was not present in the medial entorhinal cortex or in hippocampal areas CA3\textendash CA1. When animals' experiences were constrained by behavioural tasks to become similar across repeated trials, the encoding of temporal flow across trials was reduced, whereas the encoding of time relative to the start of trials was improved. The findings suggest that populations of lateral entorhinal cortex neurons represent time inherently through the encoding of experience. This representation of episodic time may be integrated with spatial inputs from the medial entorhinal cortex in the hippocampus, allowing the hippocampus to store a unified representation of what, where and when.},
  copyright = {2018 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Hippocampus;Neural encoding Subject\_term\_id: hippocampus;neural-encoding},
  file = {/Users/xzfang/Zotero/storage/DEX38C57/Tsao et al. - 2018 - Integrating time from experience in the lateral en.pdf;/Users/xzfang/Zotero/storage/3J57IC9E/s41586-018-0459-6.html}
}

@article{tsao_topological_2021,
  title = {A Topological Solution to Object Segmentation and Tracking},
  author = {Tsao, Thomas and Tsao, Doris Y.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02036 [cs]},
  eprint = {2107.02036},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The world is composed of objects, the ground, and the sky. Visual perception of objects requires solving two fundamental challenges: segmenting visual input into discrete units, and tracking identities of these units despite appearance changes due to object deformation, changing perspective, and dynamic occlusion. Current computer vision approaches to segmentation and tracking that approach human performance all require learning, raising the question: can objects be segmented and tracked without learning? Here, we show that the mathematical structure of light rays reflected from environment surfaces yields a natural representation of persistent surfaces, and this surface representation provides a solution to both the segmentation and tracking problems. We describe how to generate this surface representation from continuous visual input, and demonstrate that our approach can segment and invariantly track objects in cluttered synthetic video despite severe appearance changes, without requiring learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/xzfang/Zotero/storage/D9MRQENP/Tsao and Tsao - 2021 - A topological solution to object segmentation and .pdf;/Users/xzfang/Zotero/storage/2KSEVR7V/2107.html}
}

@article{tune_impact_2020,
  title = {The Impact of Neural Attentional Filters on Listening Behavior in a Large Cohort of Aging Individuals},
  author = {Tune, Sarah and Alavash, Mohsen and Fiedler, Lorenz and Obleser, Jonas},
  year = {2020},
  month = dec,
  journal = {bioRxiv},
  pages = {2020.05.20.105874},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.05.20.105874},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Successful listening crucially depends on intact attentional filters that separate relevant from irrelevant information. Research into their neurobiological implementation has focused on one of two auditory filter strategies: the lateralization of alpha power and selective neural speech tracking. However, the functional interplay of the two neural filter strategies and their potency to index listening success in an aging population remains unclear. Using electroencephalography and a dual-talker task in a representative sample of aging listeners (N=155; age=39\textendash 80 years), we here demonstrate an often-missed link from single-trial behavioral outcomes back to trial-by-trial changes in neural attentional filtering. First, we observed preserved attentional\textendash{} cue-driven modulation of both neural filters across chronological age and hearing levels. Second, neural filter states varied independently of one another, demonstrating a functional trade-off between distinct neurobiological attentional filter mechanisms. Stronger neural speech tracking but not alpha lateralization boosted trial-to-trial behavioral performance. Our results highlight the translational potential of neural speech tracking as an individualized neural marker of adaptive listening behavior.{$<$}/p{$><$}h3{$>$}Significance statement{$<$}/h3{$>$} {$<$}p{$>$}Successful listening requires a form of attentional filtering into behaviorally relevant and irrelevant acoustic information. Most previous studies have focused on one of two candidate neural filter strategies: the lateralization of alpha power and selective neural speech tracking. Closing the gap between hitherto separate lines of research, we used electroencephalography and a dual-talker task in a large sample of aging listeners to directly probe the functional relevance of state- and trait-level changes in these neural filter strategies to listening success. We demonstrate the co-existence of largely independent neural filters that establish alternating regimes of strong alpha lateralization versus neural speech tracking. Additionally, our results emphasize the utility of neural speech tracking over alpha lateralization as a potential neural marker of an individual's adaptive listening behavior.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/79ZVK7ZY/Tune et al. - 2020 - The impact of neural attentional filters on listen.pdf;/Users/xzfang/Zotero/storage/CJPE5HL9/2020.05.20.105874v3.html}
}

@article{turner_unraveling_2022,
  title = {Unraveling the {{Neural Mechanisms Which Encode Rapid Streams}} of {{Visual Input}}},
  author = {Turner, William},
  year = {2022},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {42},
  number = {7},
  pages = {1170--1172},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2013-21.2021},
  abstract = {As you read these words, light is striking your retinas, triggering chemical and electrical cascades. For the information contained within these wavelengths to be of use to you, your brain must encode, process, and transform it into more abstract representations ([Treisman, 1986][1]). Importantly,},
  chapter = {Journal Club},
  copyright = {Copyright \textcopyright{} 2022 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {35173038},
  file = {/Users/xzfang/Zotero/storage/D35ZC9C5/Turner - 2022 - Unraveling the Neural Mechanisms Which Encode Rapi.pdf;/Users/xzfang/Zotero/storage/F9FQJCGW/1170.html}
}

@article{tzeng_role_2016,
  title = {The Role of Training Structure in Perceptual Learning of Accented Speech.},
  author = {Tzeng, Christina Y. and Alexander, Jessica E. D. and Sidaras, Sabrina K. and Nygaard, Lynne C.},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {42},
  number = {11},
  pages = {1793--1805},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/xhp0000260},
  abstract = {Foreign-accented speech contains multiple sources of variation that listeners learn to accommodate. Extending previous findings showing that exposure to high-variation training facilitates perceptual learning of accented speech, the current study examines to what extent the structure of training materials affects learning. During training, native adult speakers of American English transcribed sentences spoken in English by native Spanish-speaking adults. In Experiment 1, training stimuli were blocked by speaker, sentence, or randomized with respect to speaker and sentence (Variable training). At test, listeners transcribed novel English sentences produced by Spanish-accented speakers. Listeners' transcription accuracy was highest in the Variable condition, suggesting that varying both speaker identity and sentence across training trials enabled listeners to generalize their learning to novel speakers and linguistic content. Experiment 2 assessed the extent to which ordering of training tokens by a single factor, speaker intelligibility, would facilitate speaker-independent accent learning, finding that listeners' test performance did not reliably differ across conditions. Overall, these results suggest that the structure of training exposure, specifically trial-by-trial variation on both speaker's voice and linguistic content, facilitates learning of the systematic properties of accented speech. The current findings suggest a crucial role of training structure in optimizing perceptual learning. Beyond characterizing the types of variation listeners encode in their representations of spoken utterances, theories of spoken language processing should incorporate the role of training structure in learning lawful variation in speech.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/2S4NUKPW/Tzeng et al. - 2016 - The role of training structure in perceptual learn.pdf}
}

@article{uddin_cortical_2020,
  title = {Cortical Mechanisms of Talker Normalization in Fluent Sentences},
  author = {Uddin, Sophia and Reis, Katherine S. and Heald, Shannon L. M. and Van Hedger, Stephen C. and Nusbaum, Howard C.},
  year = {2020},
  month = feb,
  journal = {Brain and Language},
  volume = {201},
  pages = {104722},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2019.104722},
  abstract = {Adjusting to the vocal characteristics of a new talker is important for speech recognition. Previous research has indicated that adjusting to talker differences is an active cognitive process that depends on attention and working memory (WM). These studies have not examined how talker variability affects perception and neural responses in fluent speech. Here we use source analysis from high-density EEG to show that perceiving fluent speech in which the talker changes recruits early involvement of parietal and temporal cortical areas, suggesting functional involvement of WM and attention in talker normalization. We extend these findings to acoustic source change in general by examining understanding environmental sounds in spoken sentence context. Though there may be differences in cortical recruitment to processing demands for non-speech sounds versus a changing talker, the underlying mechanisms are similar, supporting the view that shared cognitive-general mechanisms assist both talker normalization and speech-to-nonspeech transitions.},
  langid = {english},
  keywords = {Acoustic,Attention,Environmental sounds,Fluent speech,Source analysis,Talker normalization,Working memory},
  file = {/Users/xzfang/Zotero/storage/L38KX9P8/Uddin et al. - 2020 - Cortical mechanisms of talker normalization in flu.pdf;/Users/xzfang/Zotero/storage/AEFRFG54/S0093934X19303384.html}
}

@article{uddin_hearing_2018,
  title = {Hearing Sounds as Words: {{Neural}} Responses to Environmental Sounds in the Context of Fluent Speech},
  shorttitle = {Hearing Sounds as Words},
  author = {Uddin, Sophia and Heald, Shannon L. M. and Van Hedger, Stephen C. and Nusbaum, Howard C.},
  year = {2018},
  month = apr,
  journal = {Brain and Language},
  volume = {179},
  pages = {51--61},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2018.02.004},
  abstract = {Environmental sounds (ES) can be understood easily when substituted for words in sentences, suggesting that linguistic context benefits may be mediated by processes more general than some language-specific theories assert. However, the underlying neural processing is not understood. EEG was recorded for spoken sentences ending in either a spoken word or a corresponding ES. Endings were either congruent or incongruent with the sentence frame, and thus were expected to produce N400 activity. However, if ES and word meanings are combined with language context by different mechanisms, different N400 responses would be expected. Incongruent endings (both words and ES) elicited frontocentral negativities corresponding to the N400 typically observed to incongruent spoken words. Moreover, sentential constraint had similar effects on N400 topographies to ES and words. Comparison of speech and ES responses suggests that understanding meaning in speech context may be mediated by similar neural mechanisms for these two types of stimuli.},
  langid = {english},
  keywords = {Context,Environmental sounds,Event-related potential,Language processing,N400,Sentence understanding},
  file = {/Users/xzfang/Zotero/storage/3LVDTEDT/Uddin et al. - 2018 - Hearing sounds as words Neural responses to envir.pdf}
}

@inproceedings{unni_coupled_2020,
  title = {Coupled {{Training}} of {{Sequence-to-Sequence Models}} for {{Accented Speech Recognition}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Unni, Vinit and Joshi, Nitish and Jyothi, Preethi},
  year = {2020},
  month = may,
  pages = {8254--8258},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9052912},
  abstract = {Accented speech poses significant challenges for state-of-the-art automatic speech recognition (ASR) systems. Accent is a property of speech that lasts throughout an utterance in varying degrees of strength. This makes it hard to isolate the influence of accent on individual speech sounds. We propose coupled training for encoder-decoder ASR models that acts on pairs of utterances corresponding to the same text spoken by speakers with different accents. This training regime introduces an L2 loss between the attention-weighted representations corresponding to pairs of utterances with the same text, thus acting as a regularizer and encouraging representations from the encoder to be more accent-invariant. We focus on recognizing accented English samples from the Mozilla Common Voice corpus. We obtain significant error rate reductions on accented samples from a large set of diverse accents using coupled training. We also show consistent improvements in performance on heavily accented samples (as determined by a standalone accent classifier).},
  keywords = {Accented speech recognition,Conferences,Context modeling,coupled training,Error analysis,sequence-to-sequence models with attention,Signal processing,Speech processing,Speech recognition,Training},
  file = {/Users/xzfang/Zotero/storage/QQ3CPV36/Unni et al. - 2020 - Coupled Training of Sequence-to-Sequence Models fo.pdf;/Users/xzfang/Zotero/storage/6MDTWAUY/9052912.html}
}

@article{uyar_retinotopic_2016,
  title = {Retinotopic Information Interacts with Category Selectivity in Human Ventral Cortex},
  author = {Uyar, Fatma and Shomstein, Sarah and Greenberg, Adam S. and Behrmann, Marlene},
  year = {2016},
  month = nov,
  journal = {Neuropsychologia},
  volume = {92},
  pages = {90--106},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2016.05.022},
  abstract = {Until recently, the general consensus with respect to the organization of ventral visual cortex is that early, retinotopic regions are sensitive to the spatial position of the input stimuli whereas later, higher-order regions are sensitive to the category of the input stimuli. Growing recognition of the bidirectional connectivity of the visual system has challenged this view and recent empirical evidence suggests a more interactive and graded system. Here, based on findings from functional MRI in adult observers, in which meridians and category selective regions are localized and their activation sampled, we support this latter perspective by showing that category effects are present in retinotopic cortical areas and spatial position effects are present in higher-order regions. Furthermore, the results indicate that the retinotopic and later areas are functionally connected suggesting a possible mechanism by which these seemingly disparate effects come to be intermixed in both early and later regions of the visual system.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MAAE66XT/Uyar et al. - 2016 - Retinotopic information interacts with category se.pdf}
}

@article{vanbergen_sensory_2015,
  title = {Sensory Uncertainty Decoded from Visual Cortex Predicts Behavior},
  author = {{van Bergen}, Ruben S. and Ji Ma, Wei and Pratte, Michael S. and Jehee, Janneke F. M.},
  year = {2015},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {18},
  number = {12},
  pages = {1728--1730},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4150},
  abstract = {Using functional MRI and a novel model-based analysis, the authors find that the uncertainty in sensory representations can reliably be estimated from trial-by-trial activity in human visual cortex. Moreover, this uncertainty represented in cortical activity affects the way people make decisions.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Extrastriate cortex,Neural decoding,Perception,Striate cortex},
  file = {/Users/xzfang/Zotero/storage/63KFUYNQ/van Bergen et al. - 2015 - Sensory uncertainty decoded from visual cortex pre.pdf;/Users/xzfang/Zotero/storage/SLB2XDWQ/nn.html}
}

@article{vandecappelle_eegbased_2021,
  title = {{{EEG-based}} Detection of the Locus of Auditory Attention with Convolutional Neural Networks},
  author = {Vandecappelle, Servaas and Deckers, Lucas and Das, Neetha and Ansari, Amir Hossein and Bertrand, Alexander and Francart, Tom},
  editor = {{Shinn-Cunningham}, Barbara G and O'Sullivan, James and Dimitrijevic, Andrew},
  year = {2021},
  month = apr,
  journal = {eLife},
  volume = {10},
  pages = {e56481},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.56481},
  abstract = {In a multi-speaker scenario, the human auditory system is able to attend to one particular speaker of interest and ignore the others. It has been demonstrated that it is possible to use electroencephalography (EEG) signals to infer to which speaker someone is attending by relating the neural activity to the speech signals. However, classifying auditory attention within a short time interval remains the main challenge. We present a convolutional neural network-based approach to extract the locus of auditory attention (left/right) without knowledge of the speech envelopes. Our results show that it is possible to decode the locus of attention within 1\textendash 2 s, with a median accuracy of around 81\%. These results are promising for neuro-steered noise suppression in hearing aids, in particular in scenarios where per-speaker envelopes are unavailable.},
  keywords = {auditory attention detection,brain-computer interfaces,convolutional neural networks,electroencephalography,neuro-steered auditory prosthesis},
  file = {/Users/xzfang/Zotero/storage/9TN9TZ58/Vandecappelle et al. - 2021 - EEG-based detection of the locus of auditory atten.pdf}
}

@article{vandemeerendonk_monitoring_2011,
  title = {Monitoring in Language Perception: {{Electrophysiological}} and Hemodynamic Responses to Spelling Violations},
  shorttitle = {Monitoring in Language Perception},
  author = {{van de Meerendonk}, Nan and Indefrey, Peter and Chwilla, Dorothee J. and Kolk, Herman H. J.},
  year = {2011},
  month = feb,
  journal = {NeuroImage},
  volume = {54},
  number = {3},
  pages = {2350--2363},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2010.10.022},
  abstract = {The monitoring theory of language perception proposes that competing representations that are caused by strong expectancy violations can trigger a conflict which elicits reprocessing of the input to check for possible processing errors. This monitoring process is thought to be reflected by the P600 component in the EEG. The present study further investigated this monitoring process by comparing syntactic and spelling violations in an EEG and an fMRI experiment. To assess the effect of conflict strength, misspellings were embedded in sentences that were weakly or strongly predictive of a critical word. In support of the monitoring theory, syntactic and spelling violations elicited similarly distributed P600 effects. Furthermore, the P600 effect was larger to misspellings in the strongly compared to the weakly predictive sentences. The fMRI results showed that both syntactic and spelling violations increased activation in the left inferior frontal gyrus (lIFG), while only the misspellings activated additional areas. Conflict strength did not affect the hemodynamic response to spelling violations. These results extend the idea that the lIFG is involved in implementing cognitive control in the presence of representational conflicts in general to the processing of errors in language perception.},
  langid = {english},
  keywords = {Cognitive control,Conflict,Left inferior frontal gyrus,P600,Reprocessing},
  file = {/Users/xzfang/Zotero/storage/GJFASTGW/van de Meerendonk et al. - 2011 - Monitoring in language perception Electrophysiolo.pdf;/Users/xzfang/Zotero/storage/32NQLKKN/S1053811910013145.html}
}

@article{vandenberg_optimal_2012,
  title = {Optimal Inference of Sameness},
  author = {{van den Berg}, R. and Vogel, M. and Josic, K. and Ma, W. J.},
  year = {2012},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {8},
  pages = {3178--3183},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1108790109},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/N2GAPXME/van den Berg et al. - 2012 - Optimal inference of sameness.pdf}
}

@article{vandenberg_visual_2016,
  title = {Visual Search Performance Is Predicted by Both Prestimulus and Poststimulus Electrical Brain Activity},
  author = {{van den Berg}, Berry and Appelbaum, Lawrence G. and Clark, Kait and Lorist, Monicque M. and Woldorff, Marty G.},
  year = {2016},
  month = nov,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {37718},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep37718},
  abstract = {An individual's performance on cognitive and perceptual tasks varies considerably across time and circumstances. We investigated neural mechanisms underlying such performance variability using regression-based analyses to examine trial-by-trial relationships between response times (RTs) and different facets of electrical brain activity. Thirteen participants trained five days on a color-popout visual-search task, with EEG recorded on days one and five. The task was to find a color-popout target ellipse in a briefly presented array of ellipses and discriminate its orientation. Later within a session, better preparatory attention (reflected by less prestimulus Alpha-band oscillatory activity) and better poststimulus early visual responses (reflected by larger sensory N1 waves) correlated with faster RTs. However, N1 amplitudes decreased by half throughout each session, suggesting adoption of a more efficient search strategy within a session. Additionally, fast RTs were preceded by earlier and larger lateralized N2pc waves, reflecting faster and stronger attentional orienting to the targets. Finally, SPCN waves associated with target-orientation discrimination were smaller for fast RTs in the first but not the fifth session, suggesting optimization with practice. Collectively, these results delineate variations in visual search processes that change over an experimental session, while also pointing to cortical mechanisms underlying performance in visual search.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Attention,Cognitive control,Electroencephalography â€“ EEG},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Attention;Cognitive control;Electroencephalography \textendash{} EEG Subject\_term\_id: attention;cognitive-control;electroencephalography-eeg},
  file = {/Users/xzfang/Zotero/storage/4WRDZAY5/van den Berg et al. - 2016 - Visual search performance is predicted by both pre.pdf;/Users/xzfang/Zotero/storage/E438Z7NX/srep37718.html}
}

@article{vanderheijden_cortical_2019,
  title = {Cortical Mechanisms of Spatial Hearing},
  author = {{van der Heijden}, Kiki and Rauschecker, Josef P. and {de Gelder}, Beatrice and Formisano, Elia},
  year = {2019},
  month = oct,
  journal = {Nature Reviews Neuroscience},
  volume = {20},
  number = {10},
  pages = {609--623},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-019-0206-5},
  abstract = {Humans and other animals use spatial hearing to rapidly localize events in the environment. However, neural encoding of sound location is a complex process involving the~computation and integration of multiple spatial cues that are not represented directly in the sensory organ (the cochlea). Our understanding of these mechanisms has increased enormously in the past few years. Current research is focused on the contribution of animal models for understanding human spatial audition, the effects of behavioural demands on neural sound location encoding, the emergence of a cue-independent location representation in the auditory cortex, and the relationship between single-source and concurrent location encoding in~complex auditory scenes. Furthermore, computational modelling seeks to unravel how neural representations of sound source locations are derived from the complex binaural waveforms of real-life sounds. In this article, we review and integrate the latest insights from neurophysiological, neuroimaging and computational modelling studies of mammalian spatial hearing. We propose that the cortical representation of sound location emerges from recurrent processing taking place in a dynamic, adaptive network of early (primary) and higher-order (posterior\textendash dorsal and dorsolateral prefrontal) auditory regions. This cortical network accommodates changing behavioural requirements and is especially relevant for processing the location of real-life, complex sounds and complex auditory scenes.},
  copyright = {2019 Springer Nature Limited},
  langid = {english},
  keywords = {Auditory system,Cortex,Sensory processing},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Auditory system;Cortex;Sensory processing Subject\_term\_id: auditory-system;cortex;sensory-processing},
  file = {/Users/xzfang/Zotero/storage/D99KGVZZ/van der Heijden et al. - 2019 - Cortical mechanisms of spatial hearing.pdf;/Users/xzfang/Zotero/storage/IK6WQTUI/s41583-019-0206-5.html}
}

@inproceedings{vanschijndel_hierarchic_2015,
  title = {Hierarchic Syntax Improves Reading Time Prediction},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {{van Schijndel}, Marten and Schuler, William},
  year = {2015},
  pages = {1597--1605},
  publisher = {{Association for Computational Linguistics}},
  address = {{Denver, Colorado}},
  doi = {10.3115/v1/N15-1183},
  abstract = {Previous work has debated whether humans make use of hierarchic syntax when processing language (Frank and Bod, 2011; Fossum and Levy, 2012). This paper uses an eye-tracking corpus to demonstrate that hierarchic syntax significantly improves reading time prediction over a strong n-gram baseline. This study shows that an interpolated 5-gram baseline can be made stronger by combining n-gram statistics over entire eye-tracking regions rather than simply using the last n-gram in each region, but basic hierarchic syntactic measures are still able to achieve significant improvements over this improved baseline.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EZGQE33W/van Schijndel and Schuler - 2015 - Hierarchic syntax improves reading time prediction.pdf}
}

@article{vanschijndel_model_2013,
  title = {A {{Model}} of {{Language Processing}} as {{Hierarchic Sequential Prediction}}},
  author = {{van Schijndel}, Marten and Exley, Andy and Schuler, William},
  year = {2013},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {522--540},
  issn = {17568757},
  doi = {10.1111/tops.12034},
  abstract = {Computational models of memory are often expressed as hierarchic sequence models, but the hierarchies in these models are typically fairly shallow, reflecting the tendency for memories of superordinate sequence states to become increasingly conflated. This article describes a broad-coverage probabilistic sentence processing model that uses a variant of a left-corner parsing strategy to flatten sentence processing operations in parsing into a similarly shallow hierarchy of learned sequences. The main result of this paper is that a model with these kinds of constraints can process broad coverage newspaper text with the same accuracy as a state-ofthe-art parser not defined in terms of sequential working memory operations.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7K8W7Z6I/van Schijndel et al. - 2013 - A Model of Language Processing as Hierarchic Seque.pdf}
}

@article{vanschijndel_modeling_2018,
  title = {Modeling Garden Path Effects without Explicit Hierarchical Syntax},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2018},
  journal = {Neural Network Models},
  pages = {6},
  abstract = {The disambiguation of syntactically ambiguous sentences can lead to reading difficulty, often referred to as a garden path effect. The surprisal hypothesis suggests that this difficulty can be accounted for using word predictability. We tested this hypothesis using predictability estimates derived from two families of language models: grammar-based models, which explicitly encode the syntax of the language; and recurrent neural network (RNN) models, which do not. Both classes of models correctly predicted increased difficulty in ambiguous sentences compared to controls, suggesting that the syntactic representations induced by RNNs are sufficient for this purpose. At the same time, surprisal estimates derived from all models systematically underestimated the magnitude of the effect, and failed to predict the difference between easier (NP/S) and harder (NP/Z) ambiguities. This suggests that it may not be possible to reduce garden path effects to predictability.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YFSA3RYT/van Schijndel and Linzen - Modeling garden path effects without explicit hier.pdf}
}

@article{vanschijndel_neural_2018,
  title = {A {{Neural Model}} of {{Adaptation}} in {{Reading}}},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2018},
  month = oct,
  journal = {arXiv:1808.09930 [cs]},
  eprint = {1808.09930},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context. We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model. We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/X69C2ZZB/van Schijndel and Linzen - 2018 - A Neural Model of Adaptation in Reading.pdf}
}

@article{vanschijndel_single_2021,
  title = {Single-{{Stage Prediction Models Do Not Explain}} the {{Magnitude}} of {{Syntactic Disambiguation Difficulty}}},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2021},
  month = jun,
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  issn = {0364-0213, 1551-6709},
  doi = {10.1111/cogs.12988},
  abstract = {The disambiguation of a syntactically ambiguous sentence in favor of a less preferred parse can lead to slower reading at the disambiguation point. This phenomenon, referred to as a garden-path effect, has motivated models in which readers initially maintain only a subset of the possible parses of the sentence, and subsequently require time-consuming reanalysis to reconstruct a discarded parse. A more recent proposal argues that the garden-path effect can be reduced to surprisal arising in a fully parallel parser: words consistent with the initially dispreferred but ultimately correct parse are simply less predictable than those consistent with the incorrect parse. Since predictability has pervasive effects in reading far beyond garden-path sentences, this account, which dispenses with reanalysis mechanisms, is more parsimonious. Crucially, it predicts a linear effect of surprisal: the garden-path effect is expected to be proportional to the difference in word surprisal between the ultimately correct and ultimately incorrect interpretations. To test this prediction, we used recurrent neural network language models to estimate word-by-word surprisal for three temporarily ambiguous constructions. We then estimated the slowdown attributed to each bit of surprisal from human self-paced reading times, and used that quantity to predict syntactic disambiguation difficulty. Surprisal successfully predicted the existence of garden-path effects, but drastically underpredicted their magnitude, and failed to predict their relative severity across constructions. We conclude that a full explanation of syntactic disambiguation difficulty may require recovery mechanisms beyond predictability.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/MNMD3Q2W/van Schijndel and Linzen - 2021 - Singleâ€Stage Prediction Models Do Not Explain the .pdf}
}

@article{varley_evidence_2000,
  title = {Evidence for Cognition without Grammar from Causal Reasoning and `Theory of Mind' in an Agrammatic Aphasic Patient},
  author = {Varley, Rosemary and Siegal, Michael},
  year = {2000},
  month = jun,
  journal = {Current Biology},
  volume = {10},
  number = {12},
  pages = {723--726},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/S0960-9822(00)00538-8},
  langid = {english},
  pmid = {10873809},
  file = {/Users/xzfang/Zotero/storage/529V4REC/Varley and Siegal - 2000 - Evidence for cognition without grammar from causal.pdf;/Users/xzfang/Zotero/storage/AFRLGGTX/S0960-9822(00)00538-8.html}
}

@misc{vasishth_sample_2021,
  title = {Sample Size Determination for {{Bayesian}} Hierarchical Models Commonly Used in Psycholinguistics},
  author = {Vasishth, Shravan and Yadav, Himanshu and Schad, Daniel and Nicenboim, Bruno},
  year = {2021},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/u8yvc},
  abstract = {Although Bayesian data analysis has the great advantage that one need not specify the sample size in advance of running an experiment, there are nevertheless situations where it becomes necessary to have at least an initial ballpark estimate for a target sample size. An example where this becomes necessary is grant applications. In this paper, we adapt a simulation-based method proposed by Wang and Gelfand, 2002 (A simulation-based approach to Bayesian sample size determination for performance under a given model and for separating models. Statistical Science, 193-208) for a Bayes-factor based design analysis. We demonstrate how relatively complex hierarchical models (which are commonly used in psycholinguistics) can be used to determine approximate sample sizes for planning experiments. The code is available for researchers to adapt for their own purposes and applications at https://osf.io/hjgrm/.},
  keywords = {Bayesian data analysis,hierarchical models,Linguistics,power analysis,Psycholinguistics and Neurolinguistics,Sample size determination,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/SIWI5IBV/Vasishth et al. - 2021 - Sample size determination for Bayesian hierarchica.pdf}
}

@article{vaz_coupled_2019,
  title = {Coupled Ripple Oscillations between the Medial Temporal Lobe and Neocortex Retrieve Human Memory},
  author = {Vaz, Alex P. and Inati, Sara K. and Brunel, Nicolas and Zaghloul, Kareem A.},
  year = {2019},
  month = mar,
  journal = {Science},
  volume = {363},
  number = {6430},
  pages = {975--978},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aau8956},
  abstract = {Coupled ripples in memory Short-lived, high-frequency oscillations in the brain called ripples have been implicated as substrates for memory formation. There is, however, little evidence linking ripple activity with awake memory retrieval in humans. Vaz et al. analyzed intracranial recordings in human subjects (see the Perspective by Gelinas). They found that ripple oscillations in the brain's medial temporal lobe were coupled with ripple oscillations in the temporal cortex. This coupling was enhanced just before successful memory retrieval. During successful retrievals with ripples, patterns of oscillations were recapitulated across multiple electrodes, consistent with the initial encoding. The observation that ripple oscillations occur before successful memory retrieval suggests that they may play a mechanistic role in the retrieval process. Science, this issue p. 975; see also p. 927 Episodic memory retrieval relies on the recovery of neural representations of waking experience. This process is thought to involve a communication dynamic between the medial temporal lobe memory system and the neocortex. How this occurs is largely unknown, however, especially as it pertains to awake human memory retrieval. Using intracranial electroencephalographic recordings, we found that ripple oscillations were dynamically coupled between the human medial temporal lobe (MTL) and temporal association cortex. Coupled ripples were more pronounced during successful verbal memory retrieval and recover the cortical neural representations of remembered items. Together, these data provide direct evidence that coupled ripples between the MTL and association cortex may underlie successful memory retrieval in the human brain. Intracranial EEG recordings successfully reveal the link between ripple oscillations and memory retrieval in the awake human brain. Intracranial EEG recordings successfully reveal the link between ripple oscillations and memory retrieval in the awake human brain.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {30819961},
  file = {/Users/xzfang/Zotero/storage/IEGFSPZC/Vaz et al. - 2019 - Coupled ripple oscillations between the medial tem.pdf;/Users/xzfang/Zotero/storage/AQW2DRWM/975.html}
}

@article{vaz_replay_2020,
  title = {Replay of Cortical Spiking Sequences during Human Memory Retrieval},
  author = {Vaz, Alex P. and Wittig, John H. and Inati, Sara K. and Zaghloul, Kareem A.},
  year = {2020},
  month = mar,
  journal = {Science},
  volume = {367},
  number = {6482},
  pages = {1131--1134},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aba0672},
  abstract = {Human brain activity during memory Animal studies suggest that sequence replay of neuronal activity may underlie memory retrieval and consolidation. However, there is no direct evidence that the replay of spiking activity sequences is important for these processes in the human brain. Vaz et al. simultaneously recorded single-unit spikes, local field potential, and intracranial electroencephalography signals in the brain while participants performed a memory task. Sharp wave ripple oscillations in the temporal lobe cortex reflected bursts of neural spiking, and these bursts of spikes organized into sequences during memory formation. These sequences were replayed during successful memory retrieval. The extent of sequence replay during correct recall was related to the extent to which cortical spiking activity was coupled with ripples in the medial temporal lobe. Science, this issue p. 1131 Episodic memory retrieval is thought to rely on the replay of past experiences, yet it remains unknown how human single-unit activity is temporally organized during episodic memory encoding and retrieval. We found that ripple oscillations in the human cortex reflect underlying bursts of single-unit spiking activity that are organized into memory-specific sequences. Spiking sequences occurred repeatedly during memory formation and were replayed during successful memory retrieval, and this replay was associated with ripples in the medial temporal lobe. Together, these data demonstrate that human episodic memory is encoded by specific sequences of neural activity and that memory recall involves reinstating this temporal order of activity. Human single-unit and local field potential recordings of encoding-related activity sequences during long-term memory formation and retrieval are investigated. Human single-unit and local field potential recordings of encoding-related activity sequences during long-term memory formation and retrieval are investigated.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {32139543},
  file = {/Users/xzfang/Zotero/storage/7PF8HWRN/Vaz et al. - 2020 - Replay of cortical spiking sequences during human .pdf;/Users/xzfang/Zotero/storage/PR899T5F/1131.html}
}

@article{vergara-martinez_erp_2013,
  title = {{{ERP}} Correlates of Letter Identity and Letter Position Are Modulated by Lexical Frequency},
  author = {{Vergara-Mart{\'i}nez}, Marta and Perea, Manuel and G{\'o}mez, Pablo and Swaab, Tamara Y.},
  year = {2013},
  month = apr,
  journal = {Brain and language},
  volume = {125},
  number = {1},
  pages = {11--27},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2012.12.009},
  abstract = {The encoding of letter position is a key aspect in all recently proposed models of visual-word recognition. We analyzed the impact of lexical frequency on letter position assignment by examining the temporal dynamics of lexical activation induced by pseudowords extracted from words of different frequencies. For each word (e.g., BRIDGE), we created two pseudowords: A transposed-letter (TL: BRIGDE) and a replaced-letter pseudoword (RL: BRITGE). ERPs were recorded while participants read words and pseudowords in two tasks: Semantic categorization (Experiment 1) and lexical decision (Experiment 2). For high-frequency stimuli, similar ERPs were obtained for words and TL-pseudowords, but the N400 component to words was reduced relative to RL-pseudowords, indicating less lexical/semantic activation. In contrast, TL- and RL-pseudowords created from low-frequency stimuli elicited similar ERPs. Behavioral responses in the lexical decision task paralleled this asymmetry. The present findings impose constraints on computational and neural models of visual-word recognition.},
  pmcid = {PMC3612367},
  pmid = {23454070},
  file = {/Users/xzfang/Zotero/storage/DTWD9EAL/Vergara-MartÃ­nez et al. - 2013 - ERP correlates of letter identity and letter posit.pdf}
}

@article{viebahn_increased_2018,
  title = {Increased Exposure and Phonetic Context Help Listeners Recognize Allophonic Variants},
  author = {Viebahn, Malte C. and Luce, Paul A.},
  year = {2018},
  month = aug,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {80},
  number = {6},
  pages = {1539--1558},
  issn = {1943-393X},
  doi = {10.3758/s13414-018-1525-8},
  abstract = {This study examines the influence of increased exposure and phonetic context on the recognition of words that are produced with nasal flaps in American English (e.g., the word center produced as cenner). Previous work has shown that despite their high frequency of occurrence, words produced with nasal flaps are recognized more slowly and less accurately compared with canonical pronunciation variants produced with /nt/, which occur less frequently. We conducted two experiments in order to investigate how exposure and phonetic context influence this reported processing disadvantage for flapped variants. Experiment 1 demonstrated that the time to recognize flapped variants presented in isolation decreased over the course of the experiment, while accuracy increased. Experiment 2 replicated this finding and showed further that flapped variants that were presented in a casually produced sentence context were recognized faster compared with flapped variants presented in a carefully produced sentence context. Interestingly, the effect of context emerged only in late responses and was present only for flapped but not for canonical variants. Our results thus show that increased exposure and phonetic context help listeners recognize allophonic variants. This finding provides further support for the notion that listeners are flexible and adapt to phonetic variation in speech.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/YXHP8FCK/Viebahn and Luce - 2018 - Increased exposure and phonetic context help liste.pdf}
}

@article{viebahn_speaking_2017,
  title = {Speaking {{Style Influences}} the {{Brain}}'s {{Electrophysiological Response}} to {{Grammatical Errors}} in {{Speech Comprehension}}},
  author = {Viebahn, Malte C. and Ernestus, Mirjam and McQueen, James M.},
  year = {2017},
  month = jul,
  journal = {Journal of Cognitive Neuroscience},
  volume = {29},
  number = {7},
  pages = {1132--1146},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01095},
  abstract = {This electrophysiological study asked whether the brain processes grammatical gender violations in casual speech differently than in careful speech. Native speakers of Dutch were presented with utterances that contained adjective\textendash noun pairs in which the adjective was either correctly inflected with a word-final schwa (e.g., een spannende roman, ``a suspenseful novel'') or incorrectly uninflected without that schwa (een spannend roman). Consistent with previous findings, the uninflected adjectives elicited an electrical brain response sensitive to syntactic violations when the talker was speaking in a careful manner. When the talker was speaking in a casual manner, this response was absent. A control condition showed electrophysiological responses for carefully as well as casually produced utterances with semantic anomalies, showing that listeners were able to understand the content of both types of utterance. The results suggest that listeners take information about the speaking style of a talker into account when processing the acoustic\textendash phonetic information provided by the speech signal. Absent schwas in casual speech are effectively not grammatical gender violations. These changes in syntactic processing are evidence of contextually driven neural flexibility.},
  file = {/Users/xzfang/Zotero/storage/BH86AQKB/Viebahn et al. - 2017 - Speaking Style Influences the Brain's Electrophysi.pdf;/Users/xzfang/Zotero/storage/SIEVCHFY/Speaking-Style-Influences-the-Brain-s.html}
}

@article{vigano_hippocampalentorhinal_,
  title = {The Hippocampal-Entorhinal System Represents Nested Hierarchical Relations between Words during Concept Learning},
  author = {Vigan{\`o}, Simone and Piazza, Manuela},
  journal = {Hippocampus},
  volume = {n/a},
  number = {n/a},
  issn = {1098-1063},
  doi = {10.1002/hipo.23320},
  abstract = {A fundamental skill of an intelligent mind is that of being able to rapidly discover the structural organization underlying the relations across the objects or the events in the world. Humans, thanks to language, master this skill. For example, a child learning that dolphins and cats can also be referred to as mammals, not only will infer the presence of a hierarchical organization for which dolphins and cats are subordinate exemplars of the category mammals, but will also derive that dolphins are, at least at one conceptual level, more similar to cats than to sharks, despite their indisputable higher perceptual similarity to the latter. The hippocampal-entorhinal system, classically known for its involvement in relational and inferential memory, is a likely candidate to construct and hold these complex relational structures between concepts. To test this hypothesis, we trained healthy human adults to organize a novel audio-visual object space into categories labeled with novel words. Crucially, a hierarchical taxonomy existed between the object categories, and participants discovered it via inference during a simple associative object-to-word training. Using functional MRI after learning, and a combination of ROI-based multivariate analyses, we found that both the mid-anterior hippocampus and the entorhinal cortex represented the inferred hierarchical structure between words: subordinate-level words were represented more similarly to their related superordinate than to unrelated ones. This was paired, in the entorhinal cortex, by an additional signature of internalized structural representation of nested hierarchy: words referring to subordinate concepts belonging to the same superordinate category were represented more similarly compared with those not belonging to the same superordinate level: interestingly, this similarity was never directly taught to subjects nor it was made explicit during the task, but only indirectly derived through a logical inferential process and, crucially, contrasted the evidence coming from the definitional perceptual properties of the concepts. None of these results were observed before learning, when the same words were not yet semantically organized. A whole-brain searchlight revealed that the effect in the entorhinal cortex extends to a wider network of areas, encompassing the prefrontal, temporal, and parietal cortices, partially overlapping with the semantic network.},
  copyright = {\textcopyright{} 2021 Wiley Periodicals LLC},
  langid = {english},
  keywords = {concepts,hierarchies,hippocampal-entorhinal system,inference,relations,words},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.23320},
  file = {/Users/xzfang/Zotero/storage/M9KJQSXB/ViganÃ² and Piazza - The hippocampal-entorhinal system represents neste.pdf;/Users/xzfang/Zotero/storage/5XX7HQTZ/hipo.html}
}

@article{vissers_monitoring_2006,
  title = {Monitoring in Language Perception: {{The}} Effect of Misspellings of Words in Highly Constrained Sentences},
  shorttitle = {Monitoring in Language Perception},
  author = {Vissers, Constance Th.W.M. and Chwilla, Dorothee J. and Kolk, Herman H.J.},
  year = {2006},
  month = aug,
  journal = {Brain Research},
  volume = {1106},
  number = {1},
  pages = {150--163},
  issn = {00068993},
  doi = {10.1016/j.brainres.2006.05.012},
  abstract = {We present evidence for a monitoring process in language perception at the word level, reflected by a P600. This P600 is triggered when a conflict evolves because the brain encounters an unexpected linguistic item when another item is highly expected. To resolve this conflict between representations, the brain monitors the input to check for possible processing errors. A P600 was hypothesized to occur after orthographic anomalies, like pseudohomophones, in particular when the word from which the pseudohomophone is derived is highly expected. This hypothesis was tested by recording ERPs while participants read high-cloze sentences (`In that library the pupils borrow books \ldots.') and low-cloze sentences (`The pillows are stuffed with books \ldots.'). In a pretest, the high-cloze sentences were produced by more than 90\% of the subjects, while the low-cloze sentences were never produced. In half of the sentences, the critical word books was replaced by a pseudohomophone (e.g., bouks), which in the high-cloze sentences orthographically and phonologically resembles the highly expected word. Consistent with the monitoring hypothesis, only pseudohomophones in high-cloze sentences elicited a widely distributed P600 effect while pseudohomophones in low-cloze sentences did not. A standard N400 effect of cloze probability occurred both for words and pseudohomophones. The present ERP results support the view that there is a process of monitoring that takes place in language perception which is reflected by the P600. It occurs whenever a conflict between a strong tendency to accept and one to reject a word brings the cognitive system in state of indecision.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/XK5P23CZ/Vissers et al. - 2006 - Monitoring in language perception The effect of m.pdf}
}

@article{vissers_monitoring_2008,
  title = {Monitoring in Language Perception: {{Evidence}} from {{ERPs}} in a Picture\textendash Sentence Matching Task},
  shorttitle = {Monitoring in Language Perception},
  author = {Vissers, Constance Th. W. M. and Kolk, Herman H. J. and {van de Meerendonk}, Nan and Chwilla, Dorothee J.},
  year = {2008},
  month = jan,
  journal = {Neuropsychologia},
  volume = {46},
  number = {4},
  pages = {967--982},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2007.11.027},
  abstract = {P600 effects have been observed after syntactic ambiguous sentences, after several types of syntactic and semantic anomalies and after orthographic anomalies. On the basis of these findings, several investigators propose the P600 effect to reflect syntactic repair or syntactic restructuring. According to our Monitoring Theory the P600 effect reflects more general sentence reanalysis, to check whether the input sentence has been perceived appropriately. When the brain encounters a highly unexpected linguistic event, a conflict arises between the expected representation and the representation derived from the input. This conflict is proposed to trigger a process of reanalysis. In the present study, expectancy was manipulated by varying the truth-value of a sentence in relation to a picture. ERPs were recorded from 27 electrodes while we presented participants (N=30) pictures of spatial arrays followed by a sentence giving a correct or incorrect description of the picture. The mismatches were predicted to create a conflict between the conceptual representation on the basis of the picture and the actual sentence and should therefore lead to a P600. A P600 effect was indeed observed after both intra-dimensional {$\square$} {$\bigtriangleup$}\textemdash `the triangle stands in front of the square.', and extra-dimensional {$\square$} {$\bigtriangleup$}\textemdash `the triangle stands below the square.' mismatches. The present results support our Monitoring Theory; that is, the function of reprocessing reflected by the P600 effect is not purely syntactic repair or restructuring but is more general in nature, to check for possible processing errors.},
  langid = {english},
  keywords = {Event-related potential,Functional significance of the P600,Language,Pictureâ€“sentence mismatches,Sentence processing},
  file = {/Users/xzfang/Zotero/storage/6BHPL7X2/S0028393207004046.html}
}

@article{vong_crosssituational_2022,
  title = {Cross-{{Situational Word Learning With Multimodal Neural Networks}}},
  author = {Vong, Wai Keen and Lake, Brenden M.},
  year = {2022},
  journal = {Cognitive Science},
  volume = {46},
  number = {4},
  pages = {e13122},
  issn = {1551-6709},
  doi = {10.1111/cogs.13122},
  abstract = {In order to learn the mappings from words to referents, children must integrate co-occurrence information across individually ambiguous pairs of scenes and utterances, a challenge known as cross-situational word learning. In machine learning, recent multimodal neural networks have been shown to learn meaningful visual-linguistic mappings from cross-situational data, as needed to solve problems such as image captioning and visual question answering. These networks are potentially appealing as cognitive models because they can learn from raw visual and linguistic stimuli, something previous cognitive models have not addressed. In this paper, we examine whether recent machine learning approaches can help explain various behavioral phenomena from the psychological literature on cross-situational word learning. We consider two variants of a multimodal neural network architecture and look at seven different phenomena associated with cross-situational word learning and word learning more generally. Our results show that these networks can learn word-referent mappings from a single epoch of training, mimicking the amount of training commonly found in cross-situational word learning experiments. Additionally, these networks capture some, but not all of the phenomena we studied, with all of the failures related to reasoning via mutual exclusivity. These results provide insight into the kinds of phenomena that arise naturally from relatively generic neural network learning algorithms, and which word learning phenomena require additional inductive biases.},
  langid = {english},
  keywords = {Concept learning,Cross-situational word learning,Multimodal neural networks,Mutual exclusivity,Word learning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13122},
  file = {/Users/xzfang/Zotero/storage/Z93G7W5X/Vong and Lake - 2022 - Cross-Situational Word Learning With Multimodal Ne.pdf;/Users/xzfang/Zotero/storage/9Z7Y5IED/cogs.html}
}

@article{vroomen_selective_2004,
  title = {Selective Adaptation and Recalibration of Auditory Speech by Lipread Information: Dissipation},
  shorttitle = {Selective Adaptation and Recalibration of Auditory Speech by Lipread Information},
  author = {Vroomen, Jean and {van Linden}, Sabine and Keetels, Mirjam and {de Gelder}, B{\'e}atrice and Bertelson, Paul},
  year = {2004},
  month = oct,
  journal = {Speech Communication},
  volume = {44},
  number = {1-4},
  pages = {55--61},
  issn = {01676393},
  doi = {10.1016/j.specom.2004.03.009},
  abstract = {Recently, we have shown that lipread speech can recalibrate auditory speech identification when there is a conflict between the auditory and visual information (Bertelson, P., Vroomen, J., De Gelder, B, 2003. Visual recalibration of auditory speech identification: a McGurk aftereffect. Psychol. Sci. 14 (2003) 592\textendash 597). When an ambiguous sound intermediate between /aba/ and /ada/ was dubbed onto a face articulating /aba/ (or /ada/), the proportion of responses consistent with the visual stimulus increased in subsequent unimodal auditory sound identification trials, revealing recalibration. In contrast, when an unambiguous /aba/ or /ada/ sound was dubbed onto the face (with no conflict between vision and audition), the proportion of responses decreased, revealing selective adaptation. In the present study we show that recalibration and selective adaptation not only differ in the direction of their aftereffects, but also that they dissipate at different rates, confirming that the effects are caused by different mechanisms.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QXH2C93T/Vroomen et al. - 2004 - Selective adaptation and recalibration of auditory.pdf}
}

@article{vroomen_visual_2007,
  title = {Visual Recalibration and Selective Adaptation in Auditory\textendash Visual Speech Perception: {{Contrasting}} Build-up Courses},
  shorttitle = {Visual Recalibration and Selective Adaptation in Auditory\textendash Visual Speech Perception},
  author = {Vroomen, Jean and {van Linden}, Sabine and {de Gelder}, B{\'e}atrice and Bertelson, Paul},
  year = {2007},
  month = jan,
  journal = {Neuropsychologia},
  series = {Advances in {{Multisensory Processes}}},
  volume = {45},
  number = {3},
  pages = {572--577},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2006.01.031},
  abstract = {Exposure to incongruent auditory and visual speech produces both visual recalibration and selective adaptation of auditory speech identification. In an earlier study, exposure to an ambiguous auditory utterance (intermediate between /aba/ and /ada/) dubbed onto the video of a face articulating either /aba/ or /ada/, recalibrated the perceived identity of auditory targets in the direction of the visual component, while exposure to congruent non-ambiguous /aba/ or /ada/ pairs created selective adaptation, i.e. a shift of perceived identity in the opposite direction [Bertelson, P., Vroomen, J., \& de Gelder, B. (2003). Visual recalibration of auditory speech identification: a McGurk aftereffect. Psychological Science, 14, 592\textendash 597]. Here, we examined the build-up course of the after-effects produced by the same two types of bimodal adapters, over a 1\textendash 256 range of presentations. The (negative) after-effects of non-ambiguous congruent adapters increased monotonically across that range, while those of ambiguous incongruent adapters followed a curvilinear course, going up and then down with increasing exposure. This pattern is discussed in terms of an asynchronous interaction between recalibration and selective adaptation processes.},
  langid = {english},
  keywords = {After-effect,Auditoryâ€“visual speech,McGurk effect,Perceptual learning,Recalibration,Selective adaptation,Speechreading},
  file = {/Users/xzfang/Zotero/storage/596MEEZW/Vroomen et al. - 2007 - Visual recalibration and selective adaptation in a.pdf;/Users/xzfang/Zotero/storage/EGA2P3SE/S0028393206000455.html}
}

@article{wallmark_corpus_2019,
  title = {A Corpus Analysis of Timbre Semantics in Orchestration Treatises},
  author = {Wallmark, Zachary},
  year = {2019},
  month = jul,
  journal = {Psychology of Music},
  volume = {47},
  number = {4},
  pages = {585--605},
  publisher = {{SAGE Publications Ltd}},
  issn = {0305-7356},
  doi = {10.1177/0305735618768102},
  abstract = {What does the common descriptive lexicon for instrumental sound tell us about how we conceptualize musical timbre? Perceptual studies have revealed a number of verbal attributes that reliably map onto timbral qualities, but the conventions of timbre description in spoken and written discourse remain poorly understood. Books on orchestration provide a valuable source of natural language about instrumental timbre. This article uses methods from corpus linguistics to explore the semantic features of timbre through a quantitative analysis of 11 orchestration treatises and manuals. Findings reveal a relatively constrained vocabulary for timbre: about 50 adjectives account for half of all descriptions in the corpus. The timbre lexicon can be categorized according to affect, matter, metaphor, mimesis, action, acoustics, and onomatopoeia, and further reduced to three latent conceptual dimensions, which are labeled and discussed. Descriptive patterns vary systematically by instrument and instrument family, suggesting certain regularities and consistencies to timbre description in the orchestral tradition. This study helps test the long-held assumption that conventions of timbre description are vague and unsystematic, and offers a cognitive linguistic account of the timbre-language connection.},
  langid = {english},
  keywords = {corpus methods,musicology,orchestration,semantics,text analysis,Timbre}
}

@article{walsh_evaluating_2020,
  title = {Evaluating the Neurophysiological Evidence for Predictive Processing as a Model of Perception},
  author = {Walsh, Kevin S. and McGovern, David P. and Clark, Andy and O'Connell, Redmond G.},
  year = {2020},
  month = mar,
  journal = {Annals of the New York Academy of Sciences},
  volume = {1464},
  number = {1},
  pages = {242--268},
  issn = {0077-8923},
  doi = {10.1111/nyas.14321},
  abstract = {For many years, the dominant theoretical framework guiding research into the neural origins of perceptual experience has been provided by hierarchical feedforward models, in which sensory inputs are passed through a series of increasingly complex feature detectors. However, the long-standing orthodoxy of these accounts has recently been challenged by a radically different set of theories that contend that perception arises from a purely inferential process supported by two distinct classes of neurons: those that transmit predictions about sensory states and those that signal sensory information that deviates from those predictions. Although these predictive processing (PP) models have become increasingly influential in cognitive neuroscience, they are also criticized for lacking the empirical support to justify their status. This limited evidence base partly reflects the considerable methodological challenges that are presented when trying to test the unique predictions of these models. However, a confluence of technological and theoretical advances has prompted a recent surge in human and nonhuman neurophysiological research seeking to fill this empirical gap. Here, we will review this new research and evaluate the degree to which its findings support the key claims of PP., Predictive processing models have become increasingly influential in cognitive neuroscience as a possible explanation for the neural origins of perceptual experience, but have been criticized for lacking adequate empirical support. However, there has been a recent surge in human and nonhuman neurophysiological research seeking to fill this empirical gap. Here, we will review this new research and evaluate the degree to which its findings support the key claims of predictive processing.},
  pmcid = {PMC7187369},
  pmid = {32147856},
  file = {/Users/xzfang/Zotero/storage/LKL8ZQJV/Walsh et al. - 2020 - Evaluating the neurophysiological evidence for pre.pdf}
}

@article{wammes_increasing_2021,
  title = {Increasing Stimulus Similarity Drives Nonmonotonic Representational Change in Hippocampus},
  author = {Wammes, Jeffrey D. and Norman, Kenneth A. and {Turk-Browne}, Nicholas B.},
  year = {2021},
  month = mar,
  journal = {bioRxiv},
  pages = {2021.03.13.435275},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.03.13.435275},
  abstract = {{$<$}p{$>$}Studies of hippocampal learning have obtained seemingly contradictory results, with manipulations that increase coactivation of memories sometimes leading to differentiation of these memories, but sometimes not. These results could potentially be reconciled using the nonmonotonic plasticity hypothesis, which posits that representational change (memories moving apart or together) is a U-shaped function of the coactivation of these memories during learning. Testing this hypothesis requires manipulating coactivation over a wide enough range to reveal the full U-shape. To accomplish this, we used a novel neural network image synthesis procedure to create pairs of stimuli that varied parametrically in their similarity in high-level visual regions that provide input to the hippocampus. Sequences of these pairs were shown to human participants during high-resolution fMRI. As predicted, learning changed the representations of paired images in the dentate gyrus as a U-shaped function of image similarity, with neural differentiation occurring only for moderately similar images.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/NPKP8SUM/Wammes et al. - 2021 - Increasing stimulus similarity drives nonmonotonic.pdf;/Users/xzfang/Zotero/storage/67CKT3TK/2021.03.13.435275v1.html}
}

@article{wandell_learning_2012,
  title = {Learning to {{See Words}}},
  author = {Wandell, Brian A. and Rauschecker, Andreas M. and Yeatman, Jason D.},
  year = {2012},
  month = jan,
  journal = {Annual review of psychology},
  volume = {63},
  pages = {31--53},
  issn = {0066-4308},
  doi = {10.1146/annurev-psych-120710-100434},
  abstract = {Skilled reading requires recognizing written words rapidly; functional neuroimaging research has clarified how the written word initiates a series of responses in visual cortex. These responses are communicated to circuits in ventral occipitotemporal (VOT) cortex that learn to identify words rapidly. Structural neuroimaging has further clarified aspects of the white matter pathways that communicate reading signals between VOT and language systems. We review this circuitry, its development, and its deficiencies in poor readers. This review emphasizes data that measure the cortical responses and white matter pathways in individual subjects rather than group differences. Such methods have the potential to clarify why a child has difficulty learning to read and to offer guidance about the interventions that may be useful for that child.},
  pmcid = {PMC3228885},
  pmid = {21801018},
  file = {/Users/xzfang/Zotero/storage/HP7SBTKT/Wandell et al. - 2012 - Learning to See Words.pdf}
}

@article{wang_flexible_2018,
  title = {Flexible Timing by Temporal Scaling of Cortical Responses},
  author = {Wang, Jing and Narain, Devika and Hosseini, Eghbal A. and Jazayeri, Mehrdad},
  year = {2018},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {1},
  pages = {102--110},
  issn = {1546-1726},
  doi = {10.1038/s41593-017-0028-6},
  abstract = {Humans can deliberately control the timing of their actions but the neural mechanisms underlying such control are largely unknown. In this article, Wang, Narain and their colleagues report that such flexibility emerges in rhesus monkeys from the ability of their brain to flexibly control the speed at which cortical responses unfold in time.},
  copyright = {2017 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/IID4Y9EU/Wang et al. - 2018 - Flexible timing by temporal scaling of cortical re.pdf;/Users/xzfang/Zotero/storage/LWXUGHH6/s41593-017-0028-6.html}
}

@article{wang_identifying_2016,
  title = {Identifying Thematic Roles from Neural Representations Measured by Functional Magnetic Resonance Imaging},
  author = {Wang, Jing and Cherkassky, Vladimir L. and Yang, Ying and Chang, Kai-min Kevin and Vargas, Robert and Diana, Nicholas and Just, Marcel Adam},
  year = {2016},
  month = may,
  journal = {Cognitive Neuropsychology},
  volume = {33},
  number = {3-4},
  pages = {257--264},
  issn = {0264-3294},
  doi = {10.1080/02643294.2016.1182480},
  abstract = {The generativity and complexity of human thought stem in large part from the ability to represent relations among concepts and form propositions. The current study reveals how a given object such as rabbit is neurally encoded differently and identifiably depending on whether it is an agent (``the rabbit punches the monkey'') or a patient (``the monkey punches the rabbit''). Machine-learning classifiers were trained on functional magnetic resonance imaging (fMRI) data evoked by a set of short videos that conveyed agent\textendash verb\textendash patient propositions. When tested on a held-out video, the classifiers were able to reliably identify the thematic role of an object from its associated fMRI activation pattern. Moreover, when trained on one subset of the study participants, classifiers reliably identified the thematic roles in the data of a left-out participant (mean accuracy = .66), indicating that the neural representations of thematic roles were common across individuals.},
  pmid = {27314175},
  keywords = {Functional magnetic resonance imaging,multivariate pattern analysis,propositional representation,thematic roles},
  file = {/Users/xzfang/Zotero/storage/5N9XV9P6/Wang et al. - 2016 - Identifying thematic roles from neural representat.pdf;/Users/xzfang/Zotero/storage/ZXBDI3J5/02643294.2016.html}
}

@article{wang_idiosyncratic_2021,
  title = {Idiosyncratic {{Tower}} of {{Babel}}: {{Individual Differences}} in {{Word-Meaning Representation Increase}} as {{Word Abstractness Increases}}},
  shorttitle = {Idiosyncratic {{Tower}} of {{Babel}}},
  author = {Wang, Xiaosha and Bi, Yanchao},
  year = {2021},
  month = sep,
  journal = {Psychological Science},
  pages = {09567976211003877},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/09567976211003877},
  abstract = {Humans primarily rely on language to communicate, on the basis of a shared understanding of the basic building blocks of communication: words. Do we mean the same things when we use the same words? Although cognitive neural research on semantics has revealed the common principles of word-meaning representation, the factors underlying the potential individual variations in word meanings are unknown. Here, we empirically characterized the intersubject consistency of 90 words across 20 adult subjects (10 female) using both behavioral measures (rating-based semantic-relationship patterns) and neuroimaging measures (word-evoked brain activity patterns). Across both the behavioral and neuroimaging experiments, we showed that the magnitude of individual disagreements on word meanings could be modeled on the basis of how much language or sensory experience is associated with a word and that this variation increases with word abstractness. Uncovering the cognitive and neural origins of word-meaning disagreements across individuals has implications for potential mechanisms to modulate such disagreements.},
  langid = {english},
  keywords = {functional MRI,intersubject consistency,language,open data,open materials,sensory experience,word meaning},
  file = {/Users/xzfang/Zotero/storage/6SD25BWY/Wang and Bi - 2021 - Idiosyncratic Tower of Babel Individual Differenc.pdf}
}

@article{wang_predictive_2021,
  title = {Predictive {{Neural Computations Support Spoken Word Recognition}}: {{Evidence}} from {{MEG}} and {{Competitor Priming}}},
  shorttitle = {Predictive {{Neural Computations Support Spoken Word Recognition}}},
  author = {Wang, Yingcan Carol and Sohoglu, Ediz and Gilbert, Rebecca A. and Henson, Richard N. and Davis, Matthew H.},
  year = {2021},
  month = aug,
  journal = {The Journal of Neuroscience},
  volume = {41},
  number = {32},
  pages = {6919--6932},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1685-20.2021},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SIUUZEJT/Wang et al. - 2021 - Predictive Neural Computations Support Spoken Word.pdf}
}

@article{wang_speaking_2018,
  title = {Speaking Rhythmically Improves Speech Recognition under ``Cocktail-Party'' Conditions},
  author = {Wang, Mengyuan and Kong, Lingzhi and Zhang, Changxin and Wu, Xihong and Li, Liang},
  year = {2018},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {143},
  number = {4},
  pages = {EL255-EL259},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.5030518},
  abstract = {This study examines whether speech rhythm affects speech recognition under ``cocktail-party'' conditions. Against a two-talker masker, but not a speech-spectrum noise masker, recognition of the last (third) keyword in a normal rhythmic sentence was significantly better than that of the first keyword. However, this word-position-related speech-recognition improvement disappeared for rhythmically hybrid target sentences that were constructed by grouping parts from different sentences with different artificially modulated rhythms (rates) (fast, normal, or slow). Thus, the normal rhythm with a constant rate plays a role in improving speech recognition against informational speech masking, probably through a build-up of temporal prediction for target words.},
  file = {/Users/xzfang/Zotero/storage/65SR5VAS/Wang et al. - 2018 - Speaking rhythmically improves speech recognition .pdf}
}

@article{ward_general_2018,
  title = {General {{Transformations}} of {{Object Representations}} in {{Human Visual Cortex}}},
  author = {Ward, Emily J. and Isik, Leyla and Chun, Marvin M.},
  year = {2018},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {40},
  pages = {8526--8537},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2800-17.2018},
  abstract = {The brain actively represents incoming information, but these representations are only useful to the extent that they flexibly reflect changes in the environment. How does the brain transform representations across changes, such as in size or viewing angle? We conducted a fMRI experiment and a magnetoencephalography experiment in humans (both sexes) in which participants viewed objects before and after affine viewpoint changes (rotation, translation, enlargement). We used a novel approach, representational transformation analysis, to derive transformation functions that linked the distributed patterns of brain activity evoked by an object before and after an affine change. Crucially, transformations derived from one object could predict a postchange representation for novel objects. These results provide evidence of general operations in the brain that are distinct from neural representations evoked by particular objects and scenes. SIGNIFICANCE STATEMENT The dominant focus in cognitive neuroscience has been on how the brain represents information, but these representations are only useful to the extent that they flexibly reflect changes in the environment. How does the brain transform representations, such as linking two states of an object, for example, before and after an object undergoes a physical change? We used a novel method to derive transformations between the brain activity evoked by an object before and after an affine viewpoint change. We show that transformations derived from one object undergoing a change generalized to a novel object undergoing the same change. This result shows that there are general perceptual operations that transform object representations from one state to another.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2018 the authors 0270-6474/18/388526-12\$15.00/0},
  langid = {english},
  pmid = {30126975},
  keywords = {object recognition,perception,transformation,viewpoint invariance,visual representations},
  file = {/Users/xzfang/Zotero/storage/Z8Q69FPX/Ward et al. - 2018 - General Transformations of Object Representations .pdf;/Users/xzfang/Zotero/storage/6TJ2X7CX/8526.html}
}

@article{warren_rational_2017,
  title = {A Rational Inference Approach to Group and Individual-Level Sentence Comprehension Performance in Aphasia},
  author = {Warren, Tessa and Dickey, Michael Walsh and Liburd, Teljer L.},
  year = {2017},
  month = jul,
  journal = {Cortex; a journal devoted to the study of the nervous system and behavior},
  volume = {92},
  pages = {19--31},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2017.02.015},
  abstract = {The rational inference, or noisy channel, account of language comprehension predicts that comprehenders are sensitive to the probabilities of different interpretations for a given sentence and adapt as these probabilities change (). This account provides an important new perspective on aphasic sentence comprehension: aphasia may increase the likelihood of sentence distortion, leading people with aphasia (PWA) to rely more on the prior probability of an interpretation and less on the form or structure of the sentence (). We report the results of a sentence-picture matching experiment that tested the predictions of the rational inference account and other current models of aphasic sentence comprehension across a variety of sentence structures. Consistent with the rational inference account, PWA showed similar sensitivity to the probability of particular kinds of form distortions as age-matched controls, yet overall their interpretations relied more on prior probability and less on sentence form. As predicted by rational inference, but not by other models of sentence comprehension in aphasia, PWA's interpretations were more faithful to the form for active and passive sentences than for direct object and prepositional object sentences. However contra rational inference, there was no evidence that individual PWA's severity of syntactic or semantic impairment predicted their sensitivity to form versus the prior probability of a sentence, as cued by semantics. These findings confirm and extend previous findings that suggest the rational inference account holds promise for explaining aphasic and neurotypical comprehension, but they also raise new challenges for the account.},
  pmcid = {PMC5485413},
  pmid = {28391038},
  file = {/Users/xzfang/Zotero/storage/58GBZPJT/Warren et al. - 2017 - A rational inference approach to group and individ.pdf}
}

@article{wassenaar_thematic_2007,
  title = {Thematic Role Assignment in Patients with {{Broca}}'s Aphasia: {{Sentence}}\textendash Picture Matching Electrified},
  shorttitle = {Thematic Role Assignment in Patients with {{Broca}}'s Aphasia},
  author = {Wassenaar, Marlies and Hagoort, Peter},
  year = {2007},
  month = jan,
  journal = {Neuropsychologia},
  volume = {45},
  number = {4},
  pages = {716--740},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2006.08.016},
  abstract = {An event-related brain potential experiment was carried out to investigate on-line thematic role assignment during sentence\textendash picture matching in patients with Broca's aphasia. Subjects were presented with a picture that was followed by an auditory sentence. The sentence either matched the picture or mismatched the visual information depicted. Sentences differed in complexity, and ranged from simple active semantically irreversible sentences to passive semantically reversible sentences. ERPs were recorded while subjects were engaged in sentence\textendash picture matching. In addition, reaction time and accuracy were measured. Three groups of subjects were tested: Broca patients (N=10), non-aphasic patients with a right hemisphere (RH) lesion (N=8), and healthy aged-matched controls (N=15). The results of this study showed that, in neurologically unimpaired individuals, thematic role assignment in the context of visual information was an immediate process. This in contrast to patients with Broca's aphasia who demonstrated no signs of on-line sensitivity to the picture\textendash sentence mismatches. The syntactic contribution to the thematic role assignment process seemed to be diminished given the reduction and even absence of P600 effects. Nevertheless, Broca patients showed some off-line behavioral sensitivity to the sentence\textendash picture mismatches. The long response latencies of Broca's aphasics make it likely that off-line response strategies were used.},
  langid = {english},
  keywords = {Broca's aphasia,Event-related brain potentials,P600,Sentenceâ€“picture matching,Thematic role assignment},
  file = {/Users/xzfang/Zotero/storage/8IQ4GRBQ/Wassenaar and Hagoort - 2007 - Thematic role assignment in patients with Broca's .pdf;/Users/xzfang/Zotero/storage/A67HQ6AC/S0028393206003204.html}
}

@article{watrous_oscillatory_2020,
  title = {The {{Oscillatory ReConstruction Algorithm}} Adaptively Identifies Frequency Bands to Improve Spectral Decomposition in Human and Rodent Neural Recordings},
  author = {Watrous, Andrew J. and Buchanan, Robert J.},
  year = {2020},
  month = dec,
  journal = {Journal of Neurophysiology},
  volume = {124},
  number = {6},
  pages = {1914--1922},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00292.2020},
  abstract = {Neural oscillations show substantial variability within and across individuals and brain regions, yet most existing studies analyze oscillations using canonical, fixed-frequency bands. Thus, there is an ongoing need for tools that capture oscillatory variability in neural signals. Toward this end, Oscillatory ReConstruction Algorithm is a novel and adaptive analytic tool that allows researchers to measure neural oscillations with more precision and less researcher bias.           ,              Neural oscillations are routinely analyzed using methods that measure activity in fixed frequency bands (e.g., alpha, 8\textendash 12 Hz), although the frequency of neural signals varies within and across individuals based on numerous factors including neuroanatomy, behavioral demands, and species. Furthermore, band-limited activity is an often assumed, typically unmeasured model of neural activity, and band definitions vary considerably across studies. Together, these factors mask individual differences and can lead to noisy spectral estimates and interpretational problems when linking electrophysiology to behavior. We developed the Oscillatory ReConstruction Algorithm (``ORCA''), an unsupervised method to measure the spectral characteristics of neural signals in adaptively identified bands, which incorporates two new methods for frequency band identification. ORCA uses the instantaneous amplitude, phase, and frequency of activity in each band to reconstruct the signal and directly quantify spectral decomposition performance using each of four different models. To reduce researcher bias, ORCA provides spectral estimates derived from the best model and requires minimal hyperparameterization. Analyzing human scalp EEG data during eyes-open and eyes-closed ``resting'' conditions, we first identify variability in the frequency content of neural signals across subjects and electrodes. We demonstrate that ORCA significantly improves spectral decomposition compared with conventional methods and captures the well-known increase in low-frequency activity during eye closure in electrode- and subject-specific frequency bands. We further illustrate the utility of our method in rodent CA1 recordings. ORCA is a novel analytic tool that allows researchers to investigate how nonstationary neural oscillations vary across behaviors, brain regions, individuals, and species.             NEW \& NOTEWORTHY Neural oscillations show substantial variability within and across individuals and brain regions, yet most existing studies analyze oscillations using canonical, fixed-frequency bands. Thus, there is an ongoing need for tools that capture oscillatory variability in neural signals. Toward this end, Oscillatory ReConstruction Algorithm is a novel and adaptive analytic tool that allows researchers to measure neural oscillations with more precision and less researcher bias.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/SFEJA7KX/Watrous and Buchanan - 2020 - The Oscillatory ReConstruction Algorithm adaptivel.pdf}
}

@article{watson_crossmodal_2014,
  title = {Crossmodal {{Adaptation}} in {{Right Posterior Superior Temporal Sulcus}} during {{Face}}\textendash{{Voice Emotional Integration}}},
  author = {Watson, Rebecca and Latinus, Marianne and Noguchi, Takao and Garrod, Oliver and Crabbe, Frances and Belin, Pascal},
  year = {2014},
  month = may,
  journal = {Journal of Neuroscience},
  volume = {34},
  number = {20},
  pages = {6813--6821},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4478-13.2014},
  abstract = {The integration of emotional information from the face and voice of other persons is known to be mediated by a number of ``multisensory'' cerebral regions, such as the right posterior superior temporal sulcus (pSTS). However, whether multimodal integration in these regions is attributable to interleaved populations of unisensory neurons responding to face or voice or rather by multimodal neurons receiving input from the two modalities is not fully clear. Here, we examine this question using functional magnetic resonance adaptation and dynamic audiovisual stimuli in which emotional information was manipulated parametrically and independently in the face and voice via morphing between angry and happy expressions. Healthy human adult subjects were scanned while performing a happy/angry emotion categorization task on a series of such stimuli included in a fast event-related, continuous carryover design. Subjects integrated both face and voice information when categorizing emotion\textemdash although there was a greater weighting of face information\textemdash and showed behavioral adaptation effects both within and across modality. Adaptation also occurred at the neural level: in addition to modality-specific adaptation in visual and auditory cortices, we observed for the first time a crossmodal adaptation effect. Specifically, fMRI signal in the right pSTS was reduced in response to a stimulus in which facial emotion was similar to the vocal emotion of the preceding stimulus. These results suggest that the integration of emotional information from face and voice in the pSTS involves a detectable proportion of bimodal neurons that combine inputs from visual and auditory cortices.},
  copyright = {Copyright \textcopyright{} 2014 the authors 0270-6474/14/346813-09\$15.00/0},
  langid = {english},
  pmid = {24828635},
  keywords = {emotion perception,functional magnetic resonance adaptation,multisensory integration},
  file = {/Users/xzfang/Zotero/storage/HAC5UH66/Watson et al. - 2014 - Crossmodal Adaptation in Right Posterior Superior .pdf;/Users/xzfang/Zotero/storage/ED3X6RJ3/6813.html}
}

@misc{weatherholtz_speech_2016,
  title = {Speech {{Perception}} and {{Generalization Across Talkers}} and {{Accents}}},
  author = {Weatherholtz, Kodi and Jaeger, T. Florian},
  year = {2016},
  month = dec,
  journal = {Oxford Research Encyclopedia of Linguistics},
  doi = {10.1093/acrefore/9780199384655.013.95},
  abstract = {"Speech Perception and Generalization Across Talkers and Accents" published on  by Oxford University Press.},
  howpublished = {https://oxfordre.com/linguistics/view/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-95},
  isbn = {9780199384655},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5YDNXB9F/Weatherholtz and Jaeger - 2016 - Speech Perception and Generalization Across Talker.pdf;/Users/xzfang/Zotero/storage/CCIQDI8S/acrefore-9780199384655-e-95.html}
}

@article{weber_consonant_2003,
  title = {Consonant {{And Vowel Confusion Patterns By American English Listeners}}},
  author = {Weber, Andrea and Smits, Roel},
  year = {2003},
  pages = {4},
  abstract = {This study investigated the perception of American English phonemes by native listeners. Listeners identified either the consonant or the vowel in all possible English CV and VC syllables. The syllables were embedded in multispeaker babble at three signalto-noise ratios (0 dB, 8 dB, and 16 dB). Effects of syllable position, signal-to-noise ratio, and articulatory features on vowel and consonant identification are discussed. The results constitute the largest source of data that is currently available on phoneme confusion patterns of American English phonemes by native listeners.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/KXM9BADP/Weber and Smits - 2003 - Consonant And Vowel Confusion Patterns By American.pdf}
}

@inproceedings{wehbe_aligning_2014,
  title = {Aligning Context-Based Statistical Models of Language with Brain Activity during Reading},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wehbe, Leila and Vaswani, Ashish and Knight, Kevin and Mitchell, Tom},
  year = {2014},
  month = oct,
  pages = {233--243},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1030},
  file = {/Users/xzfang/Zotero/storage/WJYL5HT4/Wehbe et al. - 2014 - Aligning context-based statistical models of langu.pdf}
}

@article{wehbe_incremental_2021,
  title = {Incremental {{Language Comprehension Difficulty Predicts Activity}} in the {{Language Network}} but {{Not}} the {{Multiple Demand Network}}},
  author = {Wehbe, Leila and Blank, Idan Asher and Shain, Cory and Futrell, Richard and Levy, Roger and {von der Malsburg}, Titus and Smith, Nathaniel and Gibson, Edward and Fedorenko, Evelina},
  year = {2021},
  month = apr,
  journal = {Cerebral Cortex},
  number = {bhab065},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhab065},
  abstract = {What role do domain-general executive functions play in human language comprehension? To address this question, we examine the relationship between behavioral measures of comprehension and neural activity in the domain-general ``multiple demand'' (MD) network, which has been linked to constructs like attention, working memory, inhibitory control, and selection, and implicated in diverse goal-directed behaviors. Specifically, functional magnetic resonance imaging data collected during naturalistic story listening are compared with theory-neutral measures of online comprehension difficulty and incremental processing load (reading times and eye-fixation durations). Critically, to ensure that variance in these measures is driven by features of the linguistic stimulus rather than reflecting participant- or trial-level variability, the neuroimaging and behavioral datasets were collected in nonoverlapping samples. We find no behavioral-neural link in functionally localized MD regions; instead, this link is found in the domain-specific, fronto-temporal ``core language network,'' in both left-hemispheric areas and their right hemispheric homotopic areas. These results argue against strong involvement of domain-general executive circuits in language comprehension.},
  file = {/Users/xzfang/Zotero/storage/7746JHCA/Wehbe et al. - 2021 - Incremental Language Comprehension Difficulty Pred.pdf;/Users/xzfang/Zotero/storage/H2PVETG8/6249820.html}
}

@article{wei_bayesian_2015a,
  title = {A {{Bayesian}} Observer Model Constrained by Efficient Coding Can Explain 'anti-{{Bayesian}}' Percepts},
  author = {Wei, Xue-Xin and Stocker, Alan A.},
  year = {2015},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {18},
  number = {10},
  pages = {1509--1517},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4105},
  abstract = {The authors present a new observer model that combines efficient (en)coding and Bayesian decoding. The model makes the seemingly `anti-Bayesian' prediction that perception can be biased away from an observer's prior expectations. Psychophysical data that previously were difficult to explain are well-matched by the model's prediction.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Neural decoding,Neural encoding,Perception},
  file = {/Users/xzfang/Zotero/storage/HLC9JMEE/Wei and Stocker - 2015 - A Bayesian observer model constrained by efficient.pdf;/Users/xzfang/Zotero/storage/HKRSG8S9/nn.html}
}

@article{weissbart_cortical_2020,
  title = {Cortical {{Tracking}} of {{Surprisal}} during {{Continuous Speech Comprehension}}},
  author = {Weissbart, Hugo and Kandylaki, Katerina D. and Reichenbach, Tobias},
  year = {2020},
  month = jan,
  journal = {Journal of Cognitive Neuroscience},
  volume = {32},
  number = {1},
  pages = {155--166},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01467},
  abstract = {Speech comprehension requires rapid online processing of a continuous acoustic signal to extract structure and meaning. Previous studies on sentence comprehension have found neural correlates of the predictability of a word given its context, as well as of the precision of such a prediction. However, they have focused on single sentences and on particular words in those sentences. Moreover, they compared neural responses to words with low and high predictability, as well as with low and high precision. However, in speech comprehension, a listener hears many successive words whose predictability and precision vary over a large range. Here, we show that cortical activity in different frequency bands tracks word surprisal in continuous natural speech and that this tracking is modulated by precision. We obtain these results through quantifying surprisal and precision from naturalistic speech using a deep neural network and through relating these speech features to EEG responses of human volunteers acquired during auditory story comprehension. We find significant cortical tracking of surprisal at low frequencies, including the delta band as well as in the higher frequency beta and gamma bands, and observe that the tracking is modulated by the precision. Our results pave the way to further investigate the neurobiology of natural speech comprehension.},
  file = {/Users/xzfang/Zotero/storage/PF6TR39T/Weissbart et al. - 2020 - Cortical Tracking of Surprisal during Continuous S.pdf;/Users/xzfang/Zotero/storage/CLJE5MLE/Cortical-Tracking-of-Surprisal-during-Continuous.html}
}

@article{wen_transposedword_2021,
  title = {The Transposed-Word Effect Revisited: The Role of Syntax in Word Position Coding},
  shorttitle = {The Transposed-Word Effect Revisited},
  author = {Wen, Yun and Mirault, Jonathan and Grainger, Jonathan},
  year = {2021},
  month = jun,
  journal = {Language, Cognition and Neuroscience},
  volume = {36},
  number = {5},
  pages = {668--673},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2021.1880608},
  abstract = {Skilled readers may misinterpret ``you that read wrong'' for ``you read that wrong'': a transposed-word effect. This relatively novel finding, which supports parallel word processing during sentence reading, is attributed to a combination of noisy bottom-up word position coding and top-down syntactic constraints. The present study focussed on the contribution of syntactic constraints in driving transposed-word effects. In a speeded grammatical decision experiment, two types of ungrammatical transposed-word sequences were compared, namely a transposition either across a syntactic phrase (``the have girls gone home'') or within a syntactic phrase (``the girls gone have home''). We found longer response times and lower accuracy rates for within-phrase transpositions than across-phrase transpositions, demonstrating a direct influence of syntactic structures on the transposed-word effect. We conclude that the assignment of words to positions in a sentence is guided by top-down syntactic constraints.},
  keywords = {grammatical decision task,parallel processing,reading,Transposed words,word position coding},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2021.1880608},
  file = {/Users/xzfang/Zotero/storage/KYJRCIGR/Wen et al. - 2021 - The transposed-word effect revisited the role of .pdf;/Users/xzfang/Zotero/storage/7JQ8K5Y8/23273798.2021.html}
}

@article{werker_crosslanguage_1984,
  title = {Cross-Language Speech Perception: {{Evidence}} for Perceptual Reorganization during the First Year of Life},
  shorttitle = {Cross-Language Speech Perception},
  author = {Werker, Janet F. and Tees, Richard C.},
  year = {1984},
  month = jan,
  journal = {Infant Behavior and Development},
  volume = {7},
  number = {1},
  pages = {49--63},
  issn = {0163-6383},
  doi = {10.1016/S0163-6383(84)80022-3},
  abstract = {Previous work in which we compared English infants, English adults, and Hindi adults on their ability to discriminate two pairs of Hindi (non-English) speech contrasts has indicated that infants discriminate speech sounds according to phonetic category without prior specific language experience (Werker, Gilbert, Humphrey, \& Tees, 1981), whereas adults and children as young as age 4 (Werker \& Tees, in press), may lose this ability as a function of age and or linguistic experience. The present work was designed to (a) determine the generalizability of such a decline by comparing adult English, adult Salish, and English infant subjects on their perception of a new non-English (Salish) speech contrast, and (b) delineate the time course of the developmental decline in this ability. The results of these experiments replicate our original findings by showing that infants can discriminate nonnative speech contrasts without relevant experience, and that there is a decline in this ability during ontogeny. Furthermore, data from both cross-sectional and longitudinal studies shows that this decline occurs within the first year of life, and that it is a function of specific language experience.},
  langid = {english},
  keywords = {cross-language,decline,infants,speech perception},
  file = {/Users/xzfang/Zotero/storage/YCT8QHQD/Werker and Tees - 1984 - Cross-language speech perception Evidence for per.pdf}
}

@article{werker_developmental_1981,
  ids = {werker_developmental_1981a},
  title = {Developmental {{Aspects}} of {{Cross-Language Speech Perception}}},
  author = {Werker, Janet F. and Gilbert, John H. V. and Humphrey, Keith and Tees, Richard C.},
  year = {1981},
  journal = {Child Development},
  volume = {52},
  number = {1},
  pages = {349--355},
  publisher = {{[Wiley, Society for Research in Child Development]}},
  issn = {0009-3920},
  doi = {10.2307/1129249},
  abstract = {Previous research has suggested that infants discriminate many speech sounds according to phonemic category regardless of language exposure, while adults of one language group may have difficulty discriminating nonnative linguistic contrasts. Our study attempted to address directly questions about infant perceptual ability and the possibility of its decline as a function of development in the absence of specific experience by comparing English-speaking adults, Hindi-speaking adults, and 7-month-old infants on their ability to discriminate 2 pairs of natural Hindi (non-English) speech contrasts. To do this, infants were tested in a "visually reinforced infant speech discrimination" paradigm, while a variant of this paradigm was used to test adults. Support was obtained for the above hypotheses. Infants were shown to be able to discriminate both Hindi sound pairs, and support for the idea of a decrease in speech perceptual abilities with age and experience was clearly evident with the rarer of the 2 non-English contrasts. The results were then discussed with respect to the possible nature and purpose of these abilities.},
  file = {/Users/xzfang/Zotero/storage/YDW6W6U4/Werker et al. - 2021 - Developmental Aspects of Cross-Language Speech Per.pdf}
}

@article{whitney_ensemble_2018,
  title = {Ensemble {{Perception}}},
  author = {Whitney, David and Yamanashi Leib, Allison},
  year = {2018},
  journal = {Annual Review of Psychology},
  volume = {69},
  number = {1},
  pages = {105--129},
  doi = {10.1146/annurev-psych-010416-044232},
  abstract = {To understand visual consciousness, we must understand how the brain represents ensembles of objects at many levels of perceptual analysis. Ensemble perception refers to the visual system's ability to extract summary statistical information from groups of similar objects\textemdash often in a brief glance. It defines foundational limits on cognition, memory, and behavior. In this review, we provide an operational definition of ensemble perception and demonstrate that ensemble perception spans across multiple levels of visual analysis, incorporating both low-level visual features and high-level social information. Further, we investigate the functional usefulness of ensemble perception and its efficiency, and we consider possible physiological and cognitive mechanisms that underlie an individual's ability to make accurate and rapid assessments of crowds of objects.},
  pmid = {28892638},
  keywords = {consciousness,crowding,object recognition,scene,summary statistics,texture,vision},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-psych-010416-044232},
  file = {/Users/xzfang/Zotero/storage/44MK4QN5/Whitney and Yamanashi Leib - 2018 - Ensemble Perception.pdf}
}

@article{whitney_visual_2011,
  title = {Visual Crowding: A Fundamental Limit on Conscious Perception and Object Recognition},
  shorttitle = {Visual Crowding},
  author = {Whitney, David and Levi, Dennis M.},
  year = {2011},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {15},
  number = {4},
  pages = {160--168},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2011.02.005},
  abstract = {Crowding, the inability to recognize objects in clutter, sets a fundamental limit on conscious visual perception and object recognition throughout most of the visual field. Despite how widespread and essential it is to object recognition, reading and visually guided action, a solid operational definition of what crowding is has only recently become clear. The goal of this review is to provide a broad-based synthesis of the most recent findings in this area, to define what crowding is and is not, and to set the stage for future work that will extend our understanding of crowding well beyond low-level vision. Here we define six diagnostic criteria for what counts as crowding, and further describe factors that both escape and break crowding. All of these lead to the conclusion that crowding occurs at multiple stages in the visual hierarchy.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/GSEXHSG5/Whitney and Levi - 2011 - Visual crowding a fundamental limit on conscious .pdf}
}

@article{wiggs_properties_1998,
  title = {Properties and Mechanisms of Perceptual Priming},
  author = {Wiggs, Cheri L and Martin, Alex},
  year = {1998},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  volume = {8},
  number = {2},
  pages = {227--233},
  issn = {09594388},
  doi = {10.1016/S0959-4388(98)80144-X},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ENJGT7SC/Wiggs and Martin - 1998 - Properties and mechanisms of perceptual priming.pdf}
}

@article{wilcox_targeted_2021,
  title = {A {{Targeted Assessment}} of {{Incremental Processing}} in {{Neural LanguageModels}} and {{Humans}}},
  author = {Wilcox, Ethan Gotlieb and Vani, Pranali and Levy, Roger P.},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.03232 [cs]},
  eprint = {2106.03232},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model `accuracy' scores (a` la Marvin and Linzen (2018)) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter syntactic violations they fail to accurately predict the longer reaction times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xzfang/Zotero/storage/F92S6TEJ/Wilcox et al. - 2021 - A Targeted Assessment of Incremental Processing in.pdf}
}

@article{wilder_differences_2019,
  title = {Differences between Morphological and Repetition Priming in Auditory Lexical Decision: {{Implications}} for Decompositional Models},
  shorttitle = {Differences between Morphological and Repetition Priming in Auditory Lexical Decision},
  author = {Wilder, Robert J. and Goodwin Davies, Amy and Embick, David},
  year = {2019},
  month = jul,
  journal = {Cortex},
  series = {Structure in Words: The Present and Future of Morphological Processing in a Multidisciplinary Perspective},
  volume = {116},
  pages = {122--142},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.10.007},
  abstract = {A natural way of probing the effects of morphology on lexical processing is to directly compare morphological priming, for which primes and targets share a stem but are mismatched in morphological structure (e.g., frogs~\textrightarrow ~frog), with outright repetition priming (e.g., frog~\textrightarrow ~frog). However, work making this comparison has reported no difference between these two types of priming. Importantly, the reported non-differences have been found in the visual domain. Here, we investigate morphological (Morph) versus~repetition (Rep) priming in two auditory primed lexical decision experiments. Using the English plural suffix -/z/, we compare Rep priming with Morph priming for both singular and plural target conditions (e.g., frog/frogs~\textrightarrow ~frog, frog/frogs~\textrightarrow ~frogs). Overall, we find robust priming in both Rep and Morph conditions. However, for both singular and plural targets, there is consistent evidence that Rep priming is greater than Morph priming at early lags of 0 and 1 intervening items. This facilitation decreases with an increasing number of intervening items. Comparisons with phonological and semantic controls demonstrate that this pattern cannot be attributed solely to shared form or meaning. We interpret these findings in a decompositional model of morphological processing. The robust facilitation in Morph and Rep conditions is attributed to the activation of a shared stem representation. The convergence of Morph and Rep is attributed to a diminishing episodic trace related to morphological recombination.},
  langid = {english},
  keywords = {Decomposition process,Lexical representation,Morphological priming,Recombination process,Repetition priming},
  file = {/Users/xzfang/Zotero/storage/8C4KNQ5L/Wilder et al. - 2019 - Differences between morphological and repetition p.pdf;/Users/xzfang/Zotero/storage/T56GRDNQ/S0010945218303332.html}
}

@article{willems_prediction_2016,
  title = {Prediction {{During Natural Language Comprehension}}},
  author = {Willems, Roel M. and Frank, Stefan L. and Nijhof, Annabel D. and Hagoort, Peter and {van den Bosch}, Antal},
  year = {2016},
  month = jun,
  journal = {Cerebral Cortex},
  volume = {26},
  number = {6},
  pages = {2506--2516},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhv075},
  abstract = {The notion of prediction is studied in cognitive neuroscience with increasing intensity. We investigated the neural basis of 2 distinct aspects of word prediction, derived from information theory, during story comprehension. We assessed the effect of entropy of next-word probability distributions as well as surprisal. A computational model determined entropy and surprisal for each word in 3 literary stories. Twenty-four healthy participants listened to the same 3 stories while their brain activation was measured using fMRI. Reversed speech fragments were presented as a control condition. Brain areas sensitive to entropy were left ventral premotor cortex, left middle frontal gyrus, right inferior frontal gyrus, left inferior parietal lobule, and left supplementary motor area. Areas sensitive to surprisal were left inferior temporal sulcus (``visual word form area''), bilateral superior temporal gyrus, right amygdala, bilateral anterior temporal poles, and right inferior frontal sulcus. We conclude that prediction during language comprehension can occur at several levels of processing, including at the level of word form. Our study exemplifies the power of combining computational linguistics with cognitive neuroscience, and additionally underlines the feasibility of studying continuous spoken language materials with fMRI.},
  file = {/Users/xzfang/Zotero/storage/RANE4DUT/Willems et al. - 2016 - Prediction During Natural Language Comprehension.pdf;/Users/xzfang/Zotero/storage/ZGDQLWL7/1754078.html}
}

@article{williams_highorder_2022,
  title = {High-{{Order Areas}} and {{Auditory Cortex Both Represent}} the {{High-Level Event Structure}} of {{Music}}},
  author = {Williams, Jamal A. and Margulis, Elizabeth H. and Nastase, Samuel A. and Chen, Janice and Hasson, Uri and Norman, Kenneth A. and Baldassano, Christopher},
  year = {2022},
  month = mar,
  journal = {Journal of Cognitive Neuroscience},
  volume = {34},
  number = {4},
  pages = {699--714},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01815},
  abstract = {Recent fMRI studies of event segmentation have found that default mode regions represent high-level event structure during movie watching. In these regions, neural patterns are relatively stable during events and shift at event boundaries. Music, like narratives, contains hierarchical event structure (e.g., sections are composed of phrases). Here, we tested the hypothesis that brain activity patterns in default mode regions reflect the high-level event structure of music. We used fMRI to record brain activity from 25 participants (male and female) as they listened to a continuous playlist of 16 musical excerpts and additionally collected annotations for these excerpts by asking a separate group of participants to mark when meaningful changes occurred in each one. We then identified temporal boundaries between stable patterns of brain activity using a hidden Markov model and compared the location of the model boundaries to the location of the human annotations. We identified multiple brain regions with significant matches to the observer-identified boundaries, including auditory cortex, medial prefrontal cortex, parietal cortex, and angular gyrus. From these results, we conclude that both higher-order and sensory areas contain information relating to the high-level event structure of music. Moreover, the higher-order areas in this study overlap with areas found in previous studies of event perception in movies and audio narratives, including regions in the default mode network.},
  file = {/Users/xzfang/Zotero/storage/UDYG6RT4/Williams et al. - 2022 - High-Order Areas and Auditory Cortex Both Represen.pdf}
}

@article{wills_attractor_2005,
  title = {Attractor {{Dynamics}} in the {{Hippocampal Representation}} of the {{Local Environment}}},
  author = {Wills, Tom J. and Lever, Colin and Cacucci, Francesca and Burgess, Neil and O'Keefe, John},
  year = {2005},
  month = may,
  journal = {Science},
  volume = {308},
  number = {5723},
  pages = {873--876},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1108905},
  abstract = {Memories are thought to be attractor states of neuronal representations, with the hippocampus a likely substrate for context-dependent episodic memories. However, such states have not been directly observed. For example, the hippocampal place cell representation of location was previously found to respond continuously to changes in environmental shape alone. We report that exposure to novel square and circular environments made of different materials creates attractor representations for both shapes: Place cells abruptly and simultaneously switch between representations as environmental shape changes incrementally. This enables study of attractor dynamics in a cognitive representation and may correspond to the formation of distinct contexts in context-dependent memory. Neurons in the hippocampus code smooth changes in the shape of a room by an abrupt change from a firing pattern characteristic of one distinct shape category to another. Neurons in the hippocampus code smooth changes in the shape of a room by an abrupt change from a firing pattern characteristic of one distinct shape category to another.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {15879220},
  file = {/Users/xzfang/Zotero/storage/Z6CP88Q9/Wills et al. - 2005 - Attractor Dynamics in the Hippocampal Representati.pdf;/Users/xzfang/Zotero/storage/CLSPVZBX/873.html}
}

@article{wilsch_spatial_2020,
  title = {Spatial {{Attention}} and {{Temporal Expectation Exert Differential Effects}} on {{Visual}} and {{Auditory Discrimination}}},
  author = {Wilsch, Anna and Mercier, Manuel R. and Obleser, Jonas and Schroeder, Charles E. and Haegens, Saskia},
  year = {2020},
  month = aug,
  journal = {Journal of cognitive neuroscience},
  volume = {32},
  number = {8},
  pages = {1562--1576},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01567},
  abstract = {Anticipation of an impending stimulus shapes the state of the sensory systems, optimizing neural and behavioral responses. Here, we studied the role of brain oscillations in mediating spatial and temporal anticipations. Because spatial attention and temporal expectation are often associated with visual and auditory processing, respectively, we directly contrasted the visual and auditory modalities and asked whether these anticipatory mechanisms are similar in both domains. We recorded the magnetoencephalogram in healthy human participants performing an auditory and visual target discrimination task, in which cross-modal cues provided both temporal and spatial information with regard to upcoming stimulus presentation. Motivated by prior findings, we were specifically interested in delta (1\textendash 3 Hz) and alpha (8\textendash 13 Hz) band oscillatory state in anticipation of target presentation and their impact on task performance. Our findings support the view that spatial attention has a stronger effect in the visual domain, whereas temporal expectation effects are more prominent in the auditory domain. For the spatial attention manipulation, we found a typical pattern of alpha lateralization in the visual system, which correlated with response speed. Providing a rhythmic temporal cue led to increased postcue synchronization of low-frequency rhythms, although this effect was more broadband in nature, suggesting a general phase reset rather than frequency-specific neural entrainment. In addition, we observed delta-band synchronization with a frontal topography, which correlated with performance, especially in the auditory task. Combined, these findings suggest that spatial and temporal anticipations operate via a top-down modulation of the power and phase of low-frequency oscillations, respectively.},
  pmcid = {PMC8078477},
  pmid = {32319865},
  file = {/Users/xzfang/Zotero/storage/XYMZJIZV/Wilsch et al. - 2020 - Spatial Attention and Temporal Expectation Exert D.pdf}
}

@article{wilson_neuroplasticity_2020,
  title = {Neuroplasticity in {{Post-Stroke Aphasia}}: {{A Systematic Review}} and {{Meta-Analysis}} of {{Functional Imaging Studies}} of {{Reorganization}} of {{Language Processing}}},
  shorttitle = {Neuroplasticity in {{Post-Stroke Aphasia}}},
  author = {Wilson, Stephen M. and Schneck, Sarah M.},
  year = {2020},
  month = dec,
  journal = {Neurobiology of Language},
  volume = {2},
  number = {1},
  pages = {22--82},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00025},
  abstract = {Recovery from aphasia is thought to depend on neural plasticity, that is, the functional reorganization of surviving brain regions such that they take on new or expanded roles in language processing. We carried out a systematic review and meta-analysis of all articles published between 1995 and early 2020 that have described functional imaging studies of six or more individuals with post-stroke aphasia, and have reported analyses bearing on neuroplasticity of language processing. Each study was characterized and appraised in detail, with particular attention to three critically important methodological issues: task performance confounds, contrast validity, and correction for multiple comparisons. We identified 86 studies describing a total of 561 relevant analyses. We found that methodological limitations related to task performance confounds, contrast validity, and correction for multiple comparisons have been pervasive. Only a few claims about language processing in individuals with aphasia are strongly supported by the extant literature: First, left hemisphere language regions are less activated in individuals with aphasia than in neurologically normal controls; and second, in cohorts with aphasia, activity in left hemisphere language regions, and possibly a temporal lobe region in the right hemisphere, is positively correlated with language function. There is modest, equivocal evidence for the claim that individuals with aphasia differentially recruit right hemisphere homotopic regions, but no compelling evidence for differential recruitment of additional left hemisphere regions or domain-general networks. There is modest evidence that left hemisphere language regions return to function over time, but no compelling longitudinal evidence for dynamic reorganization of the language network.},
  file = {/Users/xzfang/Zotero/storage/K9N9M6VY/Wilson and Schneck - 2020 - Neuroplasticity in Post-Stroke Aphasia A Systemat.pdf;/Users/xzfang/Zotero/storage/PDBIDN52/Neuroplasticity-in-Post-Stroke-Aphasia-A.html}
}

@article{winawer_russian_2007,
  title = {Russian Blues Reveal Effects of Language on Color Discrimination},
  author = {Winawer, Jonathan and Witthoft, Nathan and Frank, Michael C. and Wu, Lisa and Wade, Alex R. and Boroditsky, Lera},
  year = {2007},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {19},
  pages = {7780--7785},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0701644104},
  abstract = {English and Russian color terms divide the color spectrum differently. Unlike English, Russian makes an obligatory distinction between lighter blues (``goluboy'') and darker blues (``siniy''). We investigated whether this linguistic difference leads to differences in color discrimination. We tested English and Russian speakers in a speeded color discrimination task using blue stimuli that spanned the siniy/goluboy border. We found that Russian speakers were faster to discriminate two colors when they fell into different linguistic categories in Russian (one siniy and the other goluboy) than when they were from the same linguistic category (both siniy or both goluboy). Moreover, this category advantage was eliminated by a verbal, but not a spatial, dual task. These effects were stronger for difficult discriminations (i.e., when the colors were perceptually close) than for easy discriminations (i.e., when the colors were further apart). English speakers tested on the identical stimuli did not show a category advantage in any of the conditions. These results demonstrate that (i) categories in language affect performance on simple perceptual color tasks and (ii) the effect of language is online (and can be disrupted by verbal interference).},
  copyright = {\textcopyright{} 2007 by The National Academy of Sciences of the USA},
  langid = {english},
  pmid = {17470790},
  keywords = {categorization,cross-linguistic,Whorf},
  file = {/Users/xzfang/Zotero/storage/4RG5AG6H/Winawer et al. - 2007 - Russian blues reveal effects of language on color .pdf;/Users/xzfang/Zotero/storage/2S8ALZEN/7780.html}
}

@incollection{winkler_auditory_2013,
  title = {Auditory {{Event-related Potentials}}},
  booktitle = {Encyclopedia of {{Computational Neuroscience}}},
  author = {Winkler, Istvan and Denham, Susan and Escera, Carles},
  editor = {Jaeger, Dieter and Jung, Ranu},
  year = {2013},
  pages = {1--29},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7320-6_99-1},
  isbn = {978-1-4614-7320-6},
  langid = {english},
  keywords = {Auditory Evoke Brainstem Response,Involuntary Attention,Middle Latency Response,Mismatch Negativity,Sound Onset}
}

@article{winkler_modeling_2009,
  title = {Modeling the Auditory Scene: Predictive Regularity Representations and Perceptual Objects},
  shorttitle = {Modeling the Auditory Scene},
  author = {Winkler, Istv{\'a}n and Denham, Susan L. and Nelken, Israel},
  year = {2009},
  month = dec,
  journal = {Trends in Cognitive Sciences},
  volume = {13},
  number = {12},
  pages = {532--540},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2009.09.003},
  abstract = {Predictive processing of information is essential for goal-directed behavior. We offer an account of auditory perception suggesting that representations of predictable patterns, or `regularities', extracted from the incoming sounds serve as auditory perceptual objects. The auditory system continuously searches for regularities within the acoustic signal. Primitive regularities may be encoded by neurons adapting their response to specific sounds. Such neurons have been observed in many parts of the auditory system. Representations of the detected regularities produce predictions of upcoming sounds as well as alternative solutions for parsing the composite input into coherent sequences potentially emitted by putative sound sources. Accuracy of the predictions can be utilized for selecting the most likely interpretation of the auditory input. Thus in our view, perception generates hypotheses about the causal structure of the world.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/VDNRNZ9I/Winkler et al. - 2009 - Modeling the auditory scene predictive regularity.pdf;/Users/xzfang/Zotero/storage/W4GVA2WE/S1364661309002095.html}
}

@article{winn_manipulation_2020,
  title = {Manipulation of Voice Onset Time in Speech Stimuli: {{A}} Tutorial and Flexible {{Praat}} Script},
  shorttitle = {Manipulation of Voice Onset Time in Speech Stimuli},
  author = {Winn, Matthew B.},
  year = {2020},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {147},
  number = {2},
  pages = {852--866},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/10.0000692},
  abstract = {Voice onset time (VOT) is an acoustic property of stop consonants that is commonly manipulated in studies of phonetic perception. This paper contains a thorough description of the ``progressive cutback and replacement'' method of VOT manipulation, and comparison with other VOT manipulation techniques. Other acoustic properties that covary with VOT\textemdash such as fundamental frequency and formant transitions\textemdash are also discussed, along with considerations for testing VOT perception and its relationship to various other measures of auditory temporal or spectral processing. An implementation of the progressive cutback and replacement method in the Praat scripting language is presented, which is suitable for modifying natural speech for perceptual experiments involving VOT and/or related covarying F0 and intensity cues. Justifications are provided for the stimulus design choices and constraints implemented in the script.},
  file = {/Users/xzfang/Zotero/storage/XS7AN98L/Winn - 2020 - Manipulation of voice onset time in speech stimuli.pdf}
}

@article{winn_speech_,
  title = {Speech: {{It}}'s {{Not}} as {{Acoustic}} as {{You Think}}},
  author = {Winn, Matthew B},
  pages = {7},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/5DQ3XS5J/Winn - Speech Itâ€™s Not as Acoustic as You Think.pdf}
}

@article{winn_using_2015,
  title = {Using Speech Sounds to Test Functional Spectral Resolution in Listeners with Cochlear Implants},
  author = {Winn, Matthew B. and Litovsky, Ruth Y.},
  year = {2015},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {137},
  number = {3},
  pages = {1430--1442},
  issn = {0001-4966},
  doi = {10.1121/1.4908308},
  abstract = {In this study, spectral properties of speech sounds were used to test functional spectral resolution in people who use cochlear implants (CIs). Specifically, perception of the /ba/-/da/ contrast was tested using two spectral cues: Formant transitions (a fine-resolution cue) and spectral tilt (a coarse-resolution cue). Higher weighting of the formant cues was used as an index of better spectral cue perception. Participants included 19 CI listeners and 10 listeners with normal hearing (NH), for whom spectral resolution was explicitly controlled using a noise vocoder with variable carrier filter widths to simulate electrical current spread. Perceptual weighting of the two cues was modeled with mixed-effects logistic regression, and was found to systematically vary with spectral resolution. The use of formant cues was greatest for NH listeners for unprocessed speech, and declined in the two vocoded conditions. Compared to NH listeners, CI listeners relied less on formant transitions, and more on spectral tilt. Cue-weighting results showed moderately good correspondence with word recognition scores. The current approach to testing functional spectral resolution uses auditory cues that are known to be important for speech categorization, and can thus potentially serve as the basis upon which CI processing strategies and innovations are tested.},
  pmcid = {PMC4368591},
  pmid = {25786954},
  file = {/Users/xzfang/Zotero/storage/SEUUP73Y/Winn and Litovsky - 2015 - Using speech sounds to test functional spectral re.pdf}
}

@article{winter_power_2020,
  title = {Power, {{Gender}}, and {{Individual Differences}} in {{Spatial Metaphor}}: {{The Role}} of {{Perceptual Stereotypes}} and {{Language Statistics}}},
  shorttitle = {Power, {{Gender}}, and {{Individual Differences}} in {{Spatial Metaphor}}},
  author = {Winter, Bodo and Duffy, Sarah E. and Littlemore, Jeannette},
  year = {2020},
  month = jul,
  journal = {Metaphor and Symbol},
  volume = {35},
  number = {3},
  pages = {188--205},
  publisher = {{Routledge}},
  issn = {1092-6488},
  doi = {10.1080/10926488.2020.1794319},
  abstract = {English speakers use vertical language to talk about power, such as when speaking of people being ``at the bottom of the social hierarchy'' or ``rising to the top.'' Experimental research has shown that people automatically associate higher spatial positions with more powerful social groups, such as doctors and army generals, compared to lower spatial positions, which are associated with relatively less powerful groups, such as nurses and soldiers. However, power as a social dimension is also associated with gender. Here, by means of a reaction-time study and a corpus study, we show that professions that display greater gender asymmetries, such as doctor/nurse, exhibit stronger vertical associations. Moreover, we show that people's perception of vertical metaphors for power depends on their own gender, with male participants having stronger vertical biases than female participants, which we propose is due to the fact that men are more prone to thinking about power in bodily terms, and to associate it with physical dominance. Our results provide clear evidence for individual differences in metaphor comprehension, thus demonstrating empirically that the same metaphor is understood differently by different people.},
  annotation = {\_eprint: https://doi.org/10.1080/10926488.2020.1794319},
  file = {/Users/xzfang/Zotero/storage/CY9CBKWR/10926488.2020.html}
}

@article{witkowski_learned_2019,
  title = {Learned Feature Variance Is Encoded in the Target Template and Drives Visual Search},
  author = {Witkowski, Phillip and Geng, Joy J.},
  year = {2019},
  month = sep,
  journal = {Visual Cognition},
  volume = {27},
  number = {5-8},
  pages = {487--501},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/13506285.2019.1645779},
  abstract = {Real-world visual search targets are frequently imperfect perceptual matches to our internal templates. For example, a friend on different occasions will have different clothes, hairstyles, and accessories, but some of these may vary more than others. The ability to deal with template-totarget variability is important to visual search in natural environments, but we know relatively little about how this is handled by the attentional system. Here, we test the hypothesis that topdown attentional biases are sensitive to the variance of target features and prioritize lessvariable dimensions. Subjects were shown target cues composed of coloured dots moving in a specific direction followed by a working memory probe (30\%) or visual search display (70\%). Critically, the target features in the visual search display differed from the cue, with one feature drawn from a narrow distribution (low-variance dimension), and the other sampled from a broader distribution (high-variance dimension). The results demonstrate that subjects used knowledge of the likely cue-to-target variance to set template precision and bias attentional selection. Our results suggest that observers are sensitive to the variance of feature dimensions within a target and use this information to weight mechanisms of attentional selection.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/Y5GXYKY4/Witkowski and Geng - 2019 - Learned feature variance is encoded in the target .pdf}
}

@article{wlotko_predict_2012,
  title = {To Predict or Not to Predict: {{Age-related}} Differences in the Use of Sentential Context},
  shorttitle = {To Predict or Not to Predict},
  author = {Wlotko, Edward W. and Federmeier, Kara D. and Kutas, Marta},
  year = {2012},
  month = dec,
  journal = {Psychology and aging},
  volume = {27},
  number = {4},
  pages = {975--988},
  issn = {0882-7974},
  doi = {10.1037/a0029206},
  abstract = {Older adults (as a group) are less likely than younger adults to engage in an anticipatory mode of language comprehension, failing to successfully pre-activate information about upcoming likely (predictable) words during online processing. To assess (within one set of materials) age-related changes in the use of sentential context to affect processing of predictable words and in the consequences of violating predictions, event-related brain potentials were recorded while older adults read sentences that varied in sentence-level constraint and expectancy of sentence-final words. Strongly constraining sentences were completed by their most expected, predictable words and weakly constraining sentences were completed by their most expected, less predictable words. Both types of sentences also were completed by unexpected (but plausible) words. Older adults showed reduced and delayed effects of sentential context on processing predictable words. Whereas younger adults elicit an enhanced positive ERP (starting around 500 ms post-stimulus onset, largest over prefrontal electrode sites), specifically for unexpected words that violate strong expectancies for a different word, older adults as a group did not exhibit this neural consequence of disconfirmed predictions. Older adults were instead more likely to show a left-lateralized frontal negativity for predictable items. This ERP response has been attributed to processes needed to revisit contextual material in forming an interpretation of message-level meaning, which may be more likely when anticipatory modes of comprehension are not engaged. Taken together, the results suggest that normal aging can affect allocation of resources to different cognitive and neural pathways in achieving comprehension outcomes.},
  pmcid = {PMC3685629},
  pmid = {22775363},
  file = {/Users/xzfang/Zotero/storage/B97DN2U8/Wlotko et al. - 2012 - To predict or not to predict Age-related differen.pdf}
}

@article{wolfe_asymmetries_2001,
  title = {Asymmetries in Visual Search: {{An}} Introduction},
  shorttitle = {Asymmetries in Visual Search},
  author = {Wolfe, Jeremy M.},
  year = {2001},
  month = apr,
  journal = {Perception \& Psychophysics},
  volume = {63},
  number = {3},
  pages = {381--389},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03194406},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/RR6V4L2S/Wolfe - 2001 - Asymmetries in visual search An introduction.pdf}
}

@article{wolfe_hybrid_2016,
  title = {Hybrid Foraging Search: {{Searching}} for Multiple Instances of Multiple Types of Target},
  shorttitle = {Hybrid Foraging Search},
  author = {Wolfe, Jeremy M. and Aizenman, Avigael M. and Boettcher, Sage E. P. and Cain, Matthew S.},
  year = {2016},
  month = feb,
  journal = {Vision Research},
  volume = {119},
  pages = {50--59},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2015.12.006},
  abstract = {This paper introduces the ``hybrid foraging'' paradigm. In typical visual search tasks, observers search for one instance of one target among distractors. In hybrid search, observers search through visual displays for one instance of any of several types of target held in memory. In foraging search, observers collect multiple instances of a single target type from visual displays. Combining these paradigms, in hybrid foraging tasks observers search visual displays for multiple instances of any of several types of target (as might be the case in searching the kitchen for dinner ingredients or an X-ray for different pathologies). In the present experiment, observers held 8\textendash 64 target objects in memory. They viewed displays of 60\textendash 105 randomly moving photographs of objects and used the computer mouse to collect multiple targets before choosing to move to the next display. Rather than selecting at random among available targets, observers tended to collect items in runs of one target type. Reaction time (RT) data indicate searching again for the same item is more efficient than searching for any other targets, held in memory. Observers were trying to maximize collection rate. As a result, and consistent with optimal foraging theory, they tended to leave 25\textendash 33\% of targets uncollected when moving to the next screen/patch. The pattern of RTs shows that while observers were collecting a target item, they had already begun searching memory and the visual display for additional targets, making the hybrid foraging task a useful way to investigate the interaction of visual and memory search.},
  langid = {english},
  keywords = {Attention,Human search,Hybrid foraging,Hybrid search,Multiple targets,Visual search},
  file = {/Users/xzfang/Zotero/storage/HXYI3EAP/Wolfe et al. - 2016 - Hybrid foraging search Searching for multiple ins.pdf;/Users/xzfang/Zotero/storage/GEWSFW4K/S0042698915003739.html}
}

@article{wolfe_role_1992,
  title = {On the {{Role}} of {{Symmetry}} in {{Visual Search}}},
  author = {Wolfe, Jeremy M. and {Friedman-Hill}, Stacia R.},
  year = {1992},
  month = may,
  journal = {Psychological Science},
  volume = {3},
  number = {3},
  pages = {194--198},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1992.tb00026.x},
  abstract = {It is known that the efficiency of visual search for a target item among distractor items increases when distractors are similar to each other and decreases when target and distractors are similar. Here we show that symmetry relations between targets and distractors can alter search efficiency. When distractors form a background texture symmetrical about a vertical axis, search is easier than when they do not. In contrast, when some distractors are symmetrical with the target, search is more difficult than when they are not. These results suggest (1) that symmetry relations are processed in parallel and can help to distinguish a target from a distracting background and (2) that stimulus similarity can have several components even for a single feature (here, orientation).},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EP4YWNCH/Wolfe and Friedman-Hill - 1992 - On the Role of Symmetry in Visual Search.pdf}
}

@article{wolfe_varying_2010,
  title = {Varying {{Target Prevalence Reveals Two Dissociable Decision Criteria}} in {{Visual Search}}},
  author = {Wolfe, Jeremy M. and Van Wert, Michael J.},
  year = {2010},
  month = jan,
  journal = {Current Biology},
  volume = {20},
  number = {2},
  pages = {121--124},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2009.11.066},
  abstract = {Target prevalence powerfully influences visual search behavior. In most visual search experiments, targets appear on at least 50\% of trials 1, 2, 3. However, when targets are rare~(as in medical or airport screening), observers shift response criteria, leading to elevated miss error rates 4, 5. Observers also speed target-absent responses and may make more motor errors [6]. This could be a speed/accuracy tradeoff with fast, frequent absent responses producing more miss errors. Disproving this hypothesis, our experiment one shows that very high target prevalence (98\%) shifts response criteria in the opposite direction, leading to elevated false alarms in a simulated baggage search. However, the very frequent target-present responses are not speeded. Rather, rare target-absent responses are greatly slowed. In experiment two, prevalence was varied sinusoidally over 1000 trials as observers' accuracy and reaction times (RTs) were measured. Observers' criterion and target-absent RTs tracked prevalence. Sensitivity (d{${'}$}) and target-present RTs did not vary with prevalence 7, 8, 9. These results support a model in which prevalence influences two parameters: a decision criterion governing the series of perceptual decisions about each attended item, and a quitting threshold that governs the timing of target-absent responses. Models in which target prevalence only influences an overall decision criterion are not supported.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/Users/xzfang/Zotero/storage/5TR82E8H/Wolfe and Van Wert - 2010 - Varying Target Prevalence Reveals Two Dissociable .pdf;/Users/xzfang/Zotero/storage/EG5BHQUP/S0960982209021228.html}
}

@article{wolfe_why_1998,
  title = {Why Are There Eccentricity Effects in Visual Search? {{Visual}} and Attentional Hypotheses},
  shorttitle = {Why Are There Eccentricity Effects in Visual Search?},
  author = {Wolfe, Jeremy M. and O'Neill, Patricia and Bennett, Sara C.},
  year = {1998},
  month = jan,
  journal = {Perception \& Psychophysics},
  volume = {60},
  number = {1},
  pages = {140--156},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03211924},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ZKLNNPHP/Wolfe et al. - 1998 - Why are there eccentricity effects in visual searc.pdf}
}

@article{wolff_linguistic_2011,
  title = {Linguistic Relativity},
  author = {Wolff, Phillip and Holmes, Kevin J.},
  year = {2011},
  journal = {WIREs Cognitive Science},
  volume = {2},
  number = {3},
  pages = {253--265},
  issn = {1939-5086},
  doi = {10.1002/wcs.104},
  abstract = {The central question in research on linguistic relativity, or the Whorfian hypothesis, is whether people who speak different languages think differently. The recent resurgence of research on this question can be attributed, in part, to new insights about the ways in which language might impact thought. We identify seven categories of hypotheses about the possible effects of language on thought across a wide range of domains, including motion, color, spatial relations, number, and false belief understanding. While we do not find support for the idea that language determines the basic categories of thought or that it overwrites preexisting conceptual distinctions, we do find support for the proposal that language can make some distinctions difficult to avoid, as well as for the proposal that language can augment certain types of thinking. Further, we highlight recent evidence suggesting that language may induce a relatively schematic mode of thinking. Although the literature on linguistic relativity remains contentious, there is growing support for the view that language has a profound effect on thought. WIREs Cogni Sci 2011 2 253\textendash 265 DOI: 10.1002/wcs.104 This article is categorized under: Psychology {$>$} Language},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.104},
  file = {/Users/xzfang/Zotero/storage/HWZ7H8ZC/Wolff and Holmes - 2011 - Linguistic relativity.pdf;/Users/xzfang/Zotero/storage/AZGX35VU/wcs.html}
}

@article{wong_neural_2004,
  title = {Neural {{Bases}} of {{Talker Normalization}}},
  author = {Wong, Patrick C. M. and Nusbaum, Howard C. and Small, Steven L.},
  year = {2004},
  month = sep,
  journal = {Journal of Cognitive Neuroscience},
  volume = {16},
  number = {7},
  pages = {1173--1184},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/0898929041920522},
  abstract = {Abstract             To recognize phonemes across variation in talkers, listeners can use information about vocal characteristics, a process referred to as ``talker normalization.'' The present study investigates the cortical mechanisms underlying talker normalization using fMRI. Listeners recognized target words presented in either a spoken list produced by a single talker or a mix of different talkers. It was found that both conditions activate an extensive cortical network. However, recognizing words in the mixed-talker condition, relative to the blocked-talker condition, activated middle/superior temporal and superior parietal regions to a greater degree. This temporal\textendash{} parietal network is possibly associated with selectively attending and processing spectral and spatial acoustic cues required in recognizing speech in a mixed-talker condition.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/QG3M6SKQ/Wong et al. - 2004 - Neural Bases of Talker Normalization.pdf}
}

@article{wonnacott_balancing_2011,
  title = {Balancing Generalization and Lexical Conservatism: {{An}} Artificial Language Study with Child Learners},
  shorttitle = {Balancing Generalization and Lexical Conservatism},
  author = {Wonnacott, Elizabeth},
  year = {2011},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {65},
  number = {1},
  pages = {1--14},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2011.03.001},
  abstract = {Successful language acquisition involves generalization, but learners must balance this against the acquisition of lexical constraints. Such learning has been considered problematic for theories of acquisition: if learners generalize abstract patterns to new words, how do they learn lexically-based exceptions? One approach claims that learners use distributional statistics to make inferences about when generalization is appropriate, a hypothesis which has recently received support from Artificial Language Learning experiments with adult learners (Wonnacott, Newport, \& Tanenhaus, 2008). Since adult and child language learning may be different (Hudson Kam \& Newport, 2005), it is essential to extend these results to child learners. In the current work, four groups of children (6years) were each exposed to one of four semi-artificial languages. The results demonstrate that children are sensitive to linguistic distributions at and above the level of particular lexical items, and that these statistics influence the balance between generalization and lexical conservatism. The data are in line with an approach which models generalization as rational inference and in particular with the predictions of the domain general hierarchical Bayesian model developed in Kemp, Perfors \& Tenenbaum, 2006. This suggests that such models have relevance for theories of language acquisition.},
  langid = {english},
  keywords = {Artificial language learning,Bakerâ€™s Paradox,Language acquisition,Over-generalization,Statistical learning},
  file = {/Users/xzfang/Zotero/storage/V7V6C6U9/Wonnacott - 2011 - Balancing generalization and lexical conservatism.pdf;/Users/xzfang/Zotero/storage/I6BP5BNW/S0749596X11000209.html}
}

@article{woods_attentive_2015,
  title = {Attentive {{Tracking}} of {{Sound Sources}}},
  author = {Woods, Kevin~J.P. and McDermott, Josh~H.},
  year = {2015},
  month = aug,
  journal = {Current Biology},
  volume = {25},
  number = {17},
  pages = {2238--2246},
  issn = {09609822},
  doi = {10.1016/j.cub.2015.07.043},
  abstract = {Auditory scenes often contain concurrent sound sources, but listeners are typically interested in just one of these and must somehow select it for further processing. One challenge is that real-world sounds such as speech vary over time and as a consequence often cannot be separated or selected based on particular values of their features (e.g., high pitch). Here we show that human listeners can circumvent this challenge by tracking sounds with a movable focus of attention. We synthesized pairs of voices that changed in pitch and timbre over random, intertwined trajectories, lacking distinguishing features or linguistic information. Listeners were cued beforehand to attend to one of the voices. We measured their ability to extract this cued voice from the mixture by subsequently presenting the ending portion of one voice and asking whether it came from the cued voice. We found that listeners could perform this task but that performance was mediated by attention\textemdash listeners who performed best were also more sensitive to perturbations in the cued voice than in the uncued voice. Moreover, the task was impossible if the source trajectories did not maintain sufficient separation in feature space. The results suggest a locus of attention that can follow a sound's trajectory through a feature space, likely aiding selection and segregation amid similar distractors.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7VGR6WKQ/Woods and McDermott - 2015 - Attentive Tracking of Sound Sources.pdf}
}

@article{woolnough_spatiotemporal_2021,
  title = {Spatiotemporal Dynamics of Orthographic and Lexical Processing in the Ventral Visual Pathway},
  author = {Woolnough, Oscar and Donos, Cristian and Rollo, Patrick S. and Forseth, Kiefer J. and Lakretz, Yair and Crone, Nathan E. and {Fischer-Baum}, Simon and Dehaene, Stanislas and Tandon, Nitin},
  year = {2021},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {3},
  pages = {389--398},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00982-w},
  abstract = {Reading is a rapid, distributed process that engages multiple components of the ventral visual stream. To understand the neural constituents and their interactions that allow us to identify written words, we performed direct intra-cranial recordings in a large cohort of humans. This allowed us to isolate the spatiotemporal dynamics of visual word recognition across the entire left ventral occipitotemporal cortex. We found that mid-fusiform cortex is the first brain region sensitive to lexicality, preceding the traditional visual word form area. The magnitude and duration of its activation are driven by the statistics of natural language. Information regarding lexicality and word frequency propagates posteriorly from this region to visual word form regions and to earlier visual cortex, which, while active earlier, show sensitivity to words later. Further, direct electrical stimulation of this region results in reading arrest, further illustrating its crucial role in reading. This unique sensitivity of mid-fusiform cortex to sub-lexical and lexical characteristics points to its central role as the orthographic lexicon\textemdash the long-term memory representations of visual word forms. Using intracranial recordings and stimulation, Woolnough et al. map in space and time the neural systems that enable us to read efficiently.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Language,Reading},
  annotation = {Primary\_atype: Research Subject\_term: Language;Reading Subject\_term\_id: language;reading},
  file = {/Users/xzfang/Zotero/storage/9PLX5RWL/Woolnough et al. - 2021 - Spatiotemporal dynamics of orthographic and lexica.pdf;/Users/xzfang/Zotero/storage/BJJVYX3M/s41562-020-00982-w.html}
}

@article{wostmann_spatiotemporal_2016,
  title = {Spatiotemporal Dynamics of Auditory Attention Synchronize with Speech},
  author = {W{\"o}stmann, Malte and Herrmann, Bj{\"o}rn and Maess, Burkhard and Obleser, Jonas},
  year = {2016},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {14},
  pages = {3873--3878},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1523357113},
  abstract = {Attention plays a fundamental role in selectively processing stimuli in our environment despite distraction. Spatial attention induces increasing and decreasing power of neural alpha oscillations (8\textendash 12 Hz) in brain regions ipsilateral and contralateral to the locus of attention, respectively. This study tested whether the hemispheric lateralization of alpha power codes not just the spatial location but also the temporal structure of the stimulus. Participants attended to spoken digits presented to one ear and ignored tightly synchronized distracting digits presented to the other ear. In the magnetoencephalogram, spatial attention induced lateralization of alpha power in parietal, but notably also in auditory cortical regions. This alpha power lateralization was not maintained steadily but fluctuated in synchrony with the speech rate and lagged the time course of low-frequency (1\textendash 5 Hz) sensory synchronization. Higher amplitude of alpha power modulation at the speech rate was predictive of a listener's enhanced performance of stream-specific speech comprehension. Our findings demonstrate that alpha power lateralization is modulated in tune with the sensory input and acts as a spatiotemporal filter controlling the read-out of sensory content.},
  chapter = {Biological Sciences},
  langid = {english},
  pmid = {27001861},
  keywords = {alpha lateralization,attention,neural oscillations,speech,synchronization},
  file = {/Users/xzfang/Zotero/storage/CZ6HWJKX/WÃ¶stmann et al. - 2016 - Spatiotemporal dynamics of auditory attention sync.pdf;/Users/xzfang/Zotero/storage/8VNCGB8G/3873.html}
}

@article{wright_comparing_2012,
  title = {Comparing Identification of Standardized and Regionally-Valid Vowels},
  author = {Wright, Richard and Souza, Pamela},
  year = {2012},
  month = feb,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {55},
  number = {1},
  pages = {182--193},
  issn = {1092-4388},
  doi = {10.1044/1092-4388(2011/10-0278)},
  abstract = {Purpose In perception studies, it is common to use vowel stimuli from standardized recordings or synthetic stimuli created using values from well-known published research. Although the use of standardized stimuli is convenient, unconsidered dialect and regional accent differences may introduce confounding effects. The goal of this study was to examine the effect of regional accent variation on vowel identification. Method We analyzed formant values of 8 monophthong vowels produced by 12 talkers from the region where the research took place and compared them to standardized vowels. Fifteen listeners with normal hearing identified synthesized vowels presented in varying levels of noise and at varying spectral distances from the local-dialect values. Results Acoustically, local vowels differed from standardized vowels, and distance varied across vowels. Perceptually, there was a robust effect of accent similarity such that identification was reduced for vowels at greater distances from local values. Conclusions Researchers and clinicians should take care in choosing stimuli for perception experiments. It is recommended that regionally validated vowels be used rather than relying on standardized vowels in vowel perception tasks.},
  pmcid = {PMC3288672},
  pmid = {22199181},
  file = {/Users/xzfang/Zotero/storage/KTGC8INQ/Wright and Souza - 2012 - Comparing identification of standardized and regio.pdf}
}

@misc{wu_cortical_2019,
  title = {Cortical Transformation of Stimulus-Space in Order to Linearize a Linearly Inseparable Task},
  author = {Wu, Meng-Huan and Kleinschmidt, Dave and Emberson, Lauren and Doko, Donias and Edelman, Shimon and Jacobs, Robert and Raizada, Rajeev},
  year = {2019},
  month = dec,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/ctgy6},
  abstract = {Author's final accepted version of paper accepted for publication in Journal of Cognitive Neuroscience.   The human brain is able to learn difficult categorization tasks, even ones that have linearly inseparable boundaries; however, it is currently unknown how it achieves this computational feat. We investigated this by training participants on an animal categorization task with a linearly inseparable prototype structure in a morph shape space. Participants underwent fMRI scans before and after four days of behavioral training. Widespread representational changes were found throughout the brain, including an untangling of the categories' neural patterns that made them more linearly separable after behavioral training. These neural changes were task-dependent, as they were only observed while participants were performing the categorization task, not during passive viewing. Moreover, they were found to occur in frontal and parietal areas, rather than ventral temporal cortices, suggesting that they reflected attentional and decisional reweighting, rather than changes in object recognition templates. These results illustrate how the brain can flexibly transform neural representational space in order to solve computationally challenging tasks.},
  keywords = {Attention,Cognitive Neuroscience,Cognitive Psychology,Concepts and Categories,Learning,Neuroscience,Social and Behavioral Sciences},
  file = {/Users/xzfang/Zotero/storage/CFIGCS5W/Wu et al. - 2019 - Cortical transformation of stimulus-space in order.pdf}
}

@article{wu_improving_2017,
  title = {Improving Handwritten {{Chinese}} Text Recognition Using Neural Network Language Models and Convolutional Neural Network Shape Models},
  author = {Wu, Yi-Chao and Yin, Fei and Liu, Cheng-Lin},
  year = {2017},
  month = may,
  journal = {Pattern Recognition},
  volume = {65},
  pages = {251--264},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2016.12.026},
  abstract = {Handwritten Chinese text recognition based on over-segmentation and path search integrating multiple contexts has been demonstrated successful, wherein the language model (LM) and character shape models play important roles. Although back-off N-gram LMs (BLMs) have been used dominantly for decades, they suffer from the data sparseness problem, especially for high-order LMs. Recently, neural network LMs (NNLMs) have been applied to handwriting recognition with superiority to BLMs. With the aim of improving Chinese handwriting recognition, this paper evaluates the effects of two types of character-level NNLMs, namely, feedforward neural network LMs (FNNLMs) and recurrent neural network LMs (RNNLMs). Both FNNLMs and RNNLMs are also combined with BLMs to construct hybrid LMs. For fair comparison with BLMs and a state-of-the-art system, we evaluate in a system with the same character over-segmentation and classification techniques as before, and compare various LMs using a small text corpus used before. Experimental results on the Chinese handwriting database CASIA-HWDB validate that NNLMs improve the recognition performance, and hybrid RNNLMs outperform the other LMs. To report a new benchmark, we also evaluate selected LMs on a large corpus, and replace the baseline character classifier, over-segmentation, and geometric context models with convolutional neural network (CNN) based models. The performance on both the CASIA-HWDB and the ICDAR-2013 competition dataset are improved significantly. On the CASIA-HWDB test set, the character-level accurate rate (AR) and correct rate (CR) achieve 95.88\% and 95.95\%, respectively.},
  langid = {english},
  keywords = {Convolutional neural network shape models,Feedforward neural network language model,Handwritten Chinese text recognition,Hybrid language model,Recurrent neural network language model},
  file = {/Users/xzfang/Zotero/storage/XTL6P5VD/Wu et al. - 2017 - Improving handwritten Chinese text recognition usi.pdf;/Users/xzfang/Zotero/storage/N28D5VAY/S0031320316304472.html}
}

@article{wurm_distinct_2019,
  title = {Distinct Roles of Temporal and Frontoparietal Cortex in Representing Actions across Vision and Language},
  author = {Wurm, Moritz F. and Caramazza, Alfonso},
  year = {2019},
  month = jan,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1--10},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08084-y},
  abstract = {Temporal and frontoparietal brain areas both encode representations of actions, but whether they do so in different ways is unclear. Here, the authors show that only lateral posterior temporal cortex (LPTC) encodes representations that generalize across directly observed action scenes and written descriptions.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8AB52GRC/Wurm and Caramazza - 2019 - Distinct roles of temporal and frontoparietal cort.pdf;/Users/xzfang/Zotero/storage/CTGD9A9D/s41467-018-08084-y.html}
}

@article{xiang_reversing_2015,
  title = {Reversing Expectations during Discourse Comprehension},
  author = {Xiang, Ming and Kuperberg, Gina},
  year = {2015},
  month = jul,
  journal = {Language, cognition and neuroscience},
  volume = {30},
  number = {6},
  pages = {648--672},
  issn = {2327-3798},
  doi = {10.1080/23273798.2014.995679},
  abstract = {In two ERP experiments, we asked whether comprehenders used the concessive connective, even so, to predict upcoming events. Participants read coherent and incoherent scenarios, with and without even so, e.g. ``Elizabeth had a history exam on Monday. She took the test and aced/failed it. (Even so), she went home and celebrated wildly.'', as they rated coherence (Experiment 1) or simply answered intermittent comprehension questions (Experiment 2). The semantic function of even so was used to reverse real-world knowledge predictions, leading to an attenuated N400 to coherent versus incoherent target words (``celebrated''). Moreover, its pragmatic communicative function enhanced predictive processing, leading to more N400 attenuation to coherent targets in scenarios with than without even so. This benefit however, did not come for free: the detection of failed event predictions triggered a later posterior positivity and/or an anterior negativity effect, and costs of maintaining alternative likelihood relations manifest as a sustained negativity effect on sentence-final words.},
  pmcid = {PMC4405243},
  pmid = {25914891},
  file = {/Users/xzfang/Zotero/storage/H2UN28MU/Xiang and Kuperberg - 2015 - Reversing expectations during discourse comprehens.pdf}
}

@article{xie_comparing_2020,
  title = {Comparing Non-Native and Native Speech: {{Are L2}} Productions More Variable?},
  shorttitle = {Comparing Non-Native and Native Speech},
  author = {Xie, Xin and Jaeger, T. Florian},
  year = {2020},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {147},
  number = {5},
  pages = {3322--3347},
  issn = {0001-4966},
  doi = {10.1121/10.0001141},
  abstract = {Foreign-accented speech of second language learners is often difficult to understand for native listeners of that language. Part of this difficulty has been hypothesized to be caused by increased within-category variability of nonnative speech. However, until recently, there have been few direct tests for this hypothesis. The realization of vowels and word-final stops in productions of native-English L1 speakers and native-Mandarin speakers of L2 English is compared. With the largest sample size to date, it is shown that at least proficient non-native speakers exhibit little or no difference in category variability compared to native speakers. This is shown while correcting for the effects of phonetic context. The same non-native speakers show substantial deviations from native speech in the central tendencies (means) of categories, as well as in the correlations among cues they produce. This relativizes a common and a priori plausible assumption that competition between first and second language representations necessarily leads to increased variability\textemdash or, equivalently, decreased precision, consistency, and stability\textemdash of non-native speech. Instead, effects of non-nativeness on category variability are category- and cue-specific.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9BPWFKKG/Xie and Jaeger - 2020 - Comparing non-native and native speech Are L2 pro.pdf}
}

@article{xie_crosstalker_2021,
  title = {Cross-Talker Generalization in the Perception of Nonnative Speech: {{A}} Large-Scale Replication},
  shorttitle = {Cross-Talker Generalization in the Perception of Nonnative Speech},
  author = {Xie, Xin and Liu, Linda and Jaeger, T. Florian},
  year = {2021},
  journal = {Journal of Experimental Psychology: General},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/xge0001039},
  abstract = {Speech perception depends on the ability to generalize previously experienced input effectively across talkers. How such cross-talker generalization is achieved has remained an open question. In a seminal study, Bradlow \& Bent (2008, henceforth BB08) found that exposure to just 5 min of accented speech can elicit improved recognition that generalizes to an unfamiliar talker of the same accent (N = 70 participants). Cross-talker generalization was, however, only observed after exposure to multiple talkers of the accent, not after exposure to a single accented talker. This contrast between single- and multitalker exposure has been highly influential beyond research on speech perception, suggesting a critical role of exposure variability in learning and generalization. We assess the replicability of BB08's findings in two large-scale perception experiments (total N = 640) including 20 unique combinations of exposure and test talkers. Like BB08, we find robust evidence for cross-talker generalization after multitalker exposure. Unlike BB08, we also find evidence for generalization after single-talker exposure. The degree of cross-talker generalization depends on the specific combination of exposure and test talker. This and other recent findings suggest that exposure to cross-talker variability is not necessary for cross-talker generalization. Variability during exposure might affect generalization only indirectly, mediated through the informativeness of exposure about subsequent speech during test: Similarity-based inferences can explain both the original BB08 and the present findings. We present Bayesian data analysis, including Bayesian meta-analyses and replication tests for generalized linear mixed models. All data, stimuli, and reproducible literate (R markdown) code are shared via OSF. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Adaptation,Bayesian Analysis,Exposure,Generalization (Learning),Linguistics,Oral Communication,Speech Perception,Statistical Probability},
  file = {/Users/xzfang/Zotero/storage/8MMCHSDQ/Xie et al. - 2021 - Cross-talker generalization in the perception of n.pdf;/Users/xzfang/Zotero/storage/WYBI5ITL/Supplementary-information.pdf;/Users/xzfang/Zotero/storage/KIYF8YJ3/2021-71835-001.html}
}

@article{xie_encoding_2021,
  title = {Encoding and Decoding of Meaning through Structured Variability in Intonational Speech Prosody},
  author = {Xie, Xin and {Bux{\'o}-Lugo}, Andr{\'e}s and Kurumada, Chigusa},
  year = {2021},
  month = jun,
  journal = {Cognition},
  volume = {211},
  pages = {104619},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104619},
  abstract = {Speech prosody plays an important role in communication of meaning. The cognitive and computational mechanisms supporting this communication remain to be understood, however. Prosodic cues vary across talkers and speaking conditions, creating ambiguity in the sound-to-meaning mapping. We hypothesize that listeners ameliorate this ambiguity in part by learning talker-specific statistics of prosodic cues. To test this hypothesis, we investigate the production and recognition of question vs. statement prosody in American English. Experiment 1 elicits productions of questions and statements from 65 talkers to examine the distributional statistics characterizing within- and cross-talker variability in these productions. We use Bayesian ideal observer models to assess the predicted consequences of cross-talker variability on listeners' recognition of prosody. We find that learning of talker-specific distributional statistics is predicted to facilitate recognition, above and beyond what can be achieved via commonly assumed normalizations of prosodic cues. Experiment 2 tests this prediction in a comprehension experiment. We expose different groups of listeners to different prosodic input statistics and assess listeners' recognition of questions and statements both prior to, and following, exposure. Prior to exposure, ideal observer-derived predictions based on Experiment 1 provide a good qualitative fit against listeners' recognition of prosodic contours in Experiment 2. Following exposure, listeners shift the categorization boundary between questions and statements in ways consistent with learning of talker-specific statistics.},
  langid = {english},
  keywords = {Adaptation,Intonation,Language comprehension,Language production,Meaning,Prosody,Variability},
  file = {/Users/xzfang/Zotero/storage/NWY5KIGX/Xie et al. - 2021 - Encoding and decoding of meaning through structure.pdf;/Users/xzfang/Zotero/storage/Q2TM44QD/S001002772100038X.html}
}

@article{xie_learning_2017,
  title = {Learning a {{Talker}} or {{Learning}} an {{Accent}}: {{Acoustic Similarity Constrains Generalization}} of {{Foreign Accent Adaptation}} to {{New Talkers}}},
  shorttitle = {Learning a {{Talker}} or {{Learning}} an {{Accent}}},
  author = {Xie, Xin and Myers, Emily B.},
  year = {2017},
  month = dec,
  journal = {Journal of memory and language},
  volume = {97},
  pages = {30--46},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2017.07.005},
  abstract = {Past research has revealed that native listeners use top-down information to adjust the mapping from speech sounds to phonetic categories. Such phonetic adjustments help listeners adapt to foreign-accented speech. However, the mechanism by which talker-specific adaptation generalizes to other talkers is poorly understood. Here we asked what conditions induce crosstalker generalization in talker accent adaptation. Native-English listeners were exposed to Mandarin-accented words, produced by a single talker or multiple talkers. Following exposure, adaptation to the accent was tested by recognition of novel words in a task that assesses online lexical access. Crucially, test words were novel words and were produced by a novel Mandarin-accented talker. Results indicated that regardless of exposure condition (single or multiple talker exposure), generalization was greatest when the talkers were acoustically similar to one another, suggesting that listeners were not developing an accent-wide schema for Mandarin talkers, but rather attuning to the specific acoustic-phonetic properties of the talkers. Implications for general mechanisms of talker generalization in speech adaptation are discussed.},
  pmcid = {PMC5589144},
  pmid = {28890602},
  file = {/Users/xzfang/Zotero/storage/6TTQQKSQ/Xie and Myers - 2017 - Learning a Talker or Learning an Accent Acoustic .pdf}
}

@article{xie_rapid_2018,
  title = {Rapid Adaptation to Foreign-Accented Speech and Its Transfer to an Unfamiliar Talker},
  author = {Xie, Xin and Weatherholtz, Kodi and Bainton, Larisa and Rowe, Emily and Burchill, Zachary and Liu, Linda and Jaeger, T. Florian},
  year = {2018},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {143},
  number = {4},
  pages = {2013--2031},
  issn = {0001-4966},
  doi = {10.1121/1.5027410},
  abstract = {How fast can listeners adapt to unfamiliar foreign accents? Clarke and Garrett [J. Acoust. Soc. Am. 116, 3647\textendash 3658 (2004)] (CG04) reported that native-English listeners adapted to foreign-accented English within a minute, demonstrating improved processing of spoken words. In two web-based experiments that closely follow the design of CG04, the effects of rapid accent adaptation are examined and its generalization is explored across talkers. Experiment 1 replicated the core finding of CG04 that initial perceptual difficulty with foreign-accented speech can be attenuated rapidly by a brief period of exposure to an accented talker. Importantly, listeners showed both faster (replicating CG04) and more accurate (extending CG04) comprehension of this talker. Experiment 2 revealed evidence that such adaptation transferred to a different talker of a same accent. These results highlight the rapidity of short-term accent adaptation and raise new questions about the underlying mechanism. It is suggested that the web-based paradigm provides a useful tool for investigations in speech adaptation.},
  pmcid = {PMC5895469},
  pmid = {29716296},
  file = {/Users/xzfang/Zotero/storage/E2MGURN7/Xie et al. - 2018 - Rapid adaptation to foreign-accented speech and it.pdf}
}

@article{xu_activity_2012,
  title = {Activity Recall in a Visual Cortical Ensemble},
  author = {Xu, Shengjin and Jiang, Wanchen and Poo, Mu-ming and Dan, Yang},
  year = {2012},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {15},
  number = {3},
  pages = {449--455},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3036},
  abstract = {Recording in the rat primary visual cortex, this study finds that after repeated exposure to a light spot moving along the same path, just seeing the static spot at its start position is sufficient to cause the sequence of activity associated with the movements of the spot along its path. This activity may contribute to cue-triggered recall of learned sequences.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Learning and memory;Spike-timing-dependent plasticity;Visual system Subject\_term\_id: learning-and-memory;spike-timing-dependent-plasticity;visual-system},
  file = {/Users/xzfang/Zotero/storage/4WY7MI5W/Xu et al. - 2012 - Activity recall in a visual cortical ensemble.pdf;/Users/xzfang/Zotero/storage/BS363CLW/nn.html}
}

@article{xu_perceived_2020,
  title = {Perceived Language Competence Modulates Criteria for Speech Error Processing: Evidence from Event-Related Potentials},
  shorttitle = {Perceived Language Competence Modulates Criteria for Speech Error Processing},
  author = {Xu, Jue and Rahman, Rasha Abdel and Sommer, Werner},
  year = {2020},
  month = jul,
  journal = {Language, Cognition and Neuroscience},
  volume = {35},
  number = {6},
  pages = {752--765},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1562558},
  abstract = {With event-related potentials we examined how speaker identity affects the processing of speech errors. In two experiments with probe verification and sentence correctness judgement tasks, respectively, grammatical agreement violations and slips of the tongue were embedded in German sentences spoken in native or Chinese accent. Portraits of European or Asian persons served as cues for speaker's identity. In Experiment 1, only a P600 was elicited by grammatical agreement errors in native speech in the second presentations. In Experiment 2, grammatical errors again elicited a P600 only in native speech. Slips of the tongue, however, elicited a P600 in both native and non-native speech and a N400 for native speech. Hence, perceived speaker nativeness seems to modulate the integration of grammatical agreement violations into the utterance. Slips of the tongue induced (re)interpretation processes (P600) for both native and non-native speech, whereas retrieval of lexico-semantic information (N400) is reduced in non-native speech.},
  keywords = {Grammatical agreement violation,N400,P600,slips of the tongue,speaker identity},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2018.1562558},
  file = {/Users/xzfang/Zotero/storage/WKK8WWJA/Xu et al. - 2020 - Perceived language competence modulates criteria f.pdf;/Users/xzfang/Zotero/storage/SR5S7EJR/23273798.2018.html}
}

@article{xu_revisiting_2005,
  title = {Revisiting the {{Role}} of the {{Fusiform Face Area}} in {{Visual Expertise}}},
  author = {Xu, Yaoda},
  year = {2005},
  month = aug,
  journal = {Cerebral Cortex},
  volume = {15},
  number = {8},
  pages = {1234--1242},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhi006},
  abstract = {It has previously been reported (Gauthier et al., 2000, Nat. Neurosci., 3:191\textendash 197) in a functional magnetic resonance imaging (fMRI) study that objects of visual expertise (cars and birds) activate the right fusiform face area (FFA) more strongly than non-expertise stimuli, and it was argued that the right FFA is involved in expertise specific rather than face specific visual processing. This expertise effect, however, may be due to experts taking advantage of the `faceness' of the stimuli: birds have faces and three-quarter frontal views of cars resemble faces. This expertise effect may also be caused by a biased attentional modulation: with a blocked fMRI design, experts may attend more to a block of expertise than a block of non-expertise stimuli. In this study, using both side-view car images that do not resemble faces and bird images in an event-related fMRI design that minimizes attentional modulation, an expertise effect in the right FFA is observed in both car and bird experts (although a baseline bias makes the bird expertise effect less reliable). These results are consistent with those of Gauthier et al., and suggest the involvement of the right FFA in processing non-face expertise visual stimuli.},
  file = {/Users/xzfang/Zotero/storage/DEWRIG6R/Xu - 2005 - Revisiting the Role of the Fusiform Face Area in V.pdf;/Users/xzfang/Zotero/storage/LS4V793K/304700.html}
}

@article{xu_scaleinvariant_2014,
  title = {Scale-{{Invariant Convolutional Neural Networks}}},
  author = {Xu, Yichong and Xiao, Tianjun and Zhang, Jiaxing and Yang, Kuiyuan and Zhang, Zheng},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.6369 [cs]},
  eprint = {1411.6369},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a modeldesigned to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/xzfang/Zotero/storage/X9428ZZV/Xu et al. - 2014 - Scale-Invariant Convolutional Neural Networks.pdf;/Users/xzfang/Zotero/storage/U7WJCW9V/1411.html}
}

@article{xu_sequential_2021,
  title = {Sequential Adaptation Effects Reveal Proactive Control in Processing Spoken Sentences: {{Evidence}} from Event-Related Potentials},
  shorttitle = {Sequential Adaptation Effects Reveal Proactive Control in Processing Spoken Sentences},
  author = {Xu, Jue and Abdel Rahman, Rasha and Sommer, Werner},
  year = {2021},
  month = mar,
  journal = {Brain and Language},
  volume = {214},
  pages = {104904},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2020.104904},
  abstract = {How domain-general cognitive control is engaged in language processing remains debated. We address how linguistic processes are monitored and regulated by analyzing the effects of previous-trial sentence correctness on the P600 component of the event-related potential (ERP) in the current-trial. In data from a previous experiment about processing spoken sentences, P600 amplitudes to both correct and incorrect words in current sentences were smaller after incorrect as compared to correct previous sentences. Therefore, the detection of speech errors may initiate sustained proactive control over the monitoring demands for upcoming sentences. No sequential adaptation was found in the difference between P600 amplitudes to incorrect and correct current conditions. We propose that the P600 reflects the reactive reanalysis of speech processing and/or the resolution of linguistic conflicts, but is also sensitive to proactive speech monitoring, an important aspect of cognitive control.},
  langid = {english},
  keywords = {Cognitive control,P600,Proactive control,Sequential effects,Speech errors},
  file = {/Users/xzfang/Zotero/storage/H7JCNVSZ/Xu et al. - 2021 - Sequential adaptation effects reveal proactive con.pdf;/Users/xzfang/Zotero/storage/PREZ7Z77/S0093934X20301632.html}
}

@article{yamins_eight_2016,
  title = {Eight Open Questions in the Computational Modeling of Higher Sensory Cortex},
  author = {Yamins, Daniel LK and DiCarlo, James J},
  year = {2016},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  series = {Neurobiology of Cognitive Behavior},
  volume = {37},
  pages = {114--120},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2016.02.001},
  abstract = {Propelled by advances in biologically inspired computer vision and artificial intelligence, the past five years have seen significant progress in using deep neural networks to model response patterns of neurons in visual cortex. In this paper, we briefly review this progress and then discuss eight key `open questions' that we believe will drive research in computational models of sensory systems over the next five years, both in visual cortex and beyond.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/H8UM8P5Q/S0959438816300022.html}
}

@article{yamins_performanceoptimized_2014,
  title = {Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex},
  author = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
  year = {2014},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {23},
  pages = {8619--8624},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1403112111},
  abstract = {The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model's categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model's intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization\textemdash applied in a biologically appropriate model class\textemdash can be used to build quantitative predictive models of neural processing.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {24812127},
  keywords = {array electrophysiology,computational neuroscience,computer vision},
  file = {/Users/xzfang/Zotero/storage/KM6W7EL8/Yamins et al. - 2014 - Performance-optimized hierarchical models predict .pdf;/Users/xzfang/Zotero/storage/23EU3XEV/8619.html}
}

@article{yang_apparent_,
  title = {Is Apparent Instability a Guiding Feature in Visual Search?},
  author = {Yang, Yung-Hao and Wolfe, Jeremy M},
  journal = {VISUAL COGNITION},
  pages = {22},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DH5FSWXV/Yang and Wolfe - Is apparent instability a guiding feature in visua.pdf}
}

@article{yang_one_2022,
  title = {One Model for the Learning of Language},
  author = {Yang, Yuan and Piantadosi, Steven T.},
  year = {2022},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {5},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2021865119},
  abstract = {A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g., an, (ab)n(ab)n{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{$><$}mml:mrow{$><$}mml:msup{$><$}mml:mrow{$><$}mml:mo stretchy="false"{$>$}({$<$}/mml:mo{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mi{$>$}b{$<$}/mml:mi{$><$}mml:mo stretchy="false"{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}/mml:mrow{$><$}/mml:math{$>$}, and \{a,b\}+\{a,b\}+{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{$><$}mml:mrow{$><$}mml:msup{$><$}mml:mrow{$><$}mml:mo{$>\lbrace<$}/mml:mo{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mi{$>$}b{$<$}/mml:mi{$><$}mml:mo{$>\rbrace<$}/mml:mo{$><$}/mml:mrow{$><$}mml:mo{$>$}+{$<$}/mml:mo{$><$}/mml:msup{$><$}/mml:mrow{$><$}/mml:math{$>$}), context-free (e.g., anbn,anbn+manbn, anbn+m{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{$><$}mml:mrow{$><$}mml:msup{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}b{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mo{$>$} {$<$}/mml:mo{$><$}mml:msup{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}b{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}mml:mo{$>$}+{$<$}/mml:mo{$><$}mml:mi{$>$}m{$<$}/mml:mi{$><$}/mml:mrow{$><$}/mml:msup{$><$}/mml:mrow{$><$}/mml:math{$>$}, and xxRxxR{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{$><$}mml:mrow{$><$}mml:mi{$>$}x{$<$}/mml:mi{$><$}mml:msup{$><$}mml:mi{$>$}x{$<$}/mml:mi{$><$}mml:mi{$>$}R{$<$}/mml:mi{$><$}/mml:msup{$><$}/mml:mrow{$><$}/mml:math{$>$}), and context-sensitive (e.g., anbncn,anbmcndmanbncn, anbmcndm{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{$><$}mml:mrow{$><$}mml:msup{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}b{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}c{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mo{$>$} {$<$}/mml:mo{$><$}mml:msup{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}b{$<$}/mml:mi{$><$}mml:mi{$>$}m{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}c{$<$}/mml:mi{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}mml:mi{$>$}m{$<$}/mml:mi{$><$}/mml:msup{$><$}/mml:mrow{$><$}/mml:math{$>$}, and xx) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
  chapter = {Social Sciences},
  copyright = {Copyright \textcopyright{} 2022 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {35074868},
  keywords = {computational linguistics,formal language theory,learning theory,program induction},
  file = {/Users/xzfang/Zotero/storage/ZB6RGDFC/Yang and Piantadosi - 2022 - One model for the learning of language.pdf;/Users/xzfang/Zotero/storage/DZMPHJZX/e2021865119.html}
}

@article{yao_reading_2021,
  title = {Reading Direct Speech Quotes Increases Theta Phase-Locking: {{Evidence}} for Cortical Tracking of Inner Speech?},
  shorttitle = {Reading Direct Speech Quotes Increases Theta Phase-Locking},
  author = {Yao, Bo and Taylor, Jason R. and Banks, Briony and Kotz, Sonja A.},
  year = {2021},
  month = oct,
  journal = {NeuroImage},
  volume = {239},
  pages = {118313},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2021.118313},
  abstract = {Growing evidence shows that theta-band (4\textendash 7~Hz) activity in the auditory cortex phase-locks to rhythms of overt speech. Does theta activity also encode the rhythmic dynamics of inner speech? Previous research established that silent reading of direct speech quotes (e.g., Mary said: ``This dress is lovely!'') elicits more vivid inner speech than indirect speech quotes (e.g., Mary said that the dress was lovely). As we cannot directly track the phase alignment between theta activity and inner speech over time, we used EEG to measure the brain's phase-locked responses to the onset of speech quote reading. We found that direct (vs. indirect) quote reading was associated with increased theta phase synchrony over trials at 250\textendash 500~ms post-reading onset, with sources of the evoked activity estimated in the speech processing network. An eye-tracking control experiment confirmed that increased theta phase synchrony in direct quote reading was not driven by eye movement patterns, and more likely reflects synchronous phase resetting at the onset of inner speech. These findings suggest a functional role of theta phase modulation in reading-induced inner speech.},
  langid = {english},
  keywords = {Inner speech,Neural oscillations,Phase synchrony,Phase-locking,Reading,Theta activity},
  file = {/Users/xzfang/Zotero/storage/6FLFB8YV/S1053811921005899.html}
}

@misc{yarkoni_generalizability_2019,
  title = {The {{Generalizability Crisis}}},
  author = {Yarkoni, Tal},
  year = {2019},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/jqw35},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned\textemdash that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology\textemdash the linear mixed model\textemdash I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that whereas the "random effect" formalism is used pervasively in psychology to model inter-subject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that the failure to  problems many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  keywords = {generalization,inference,philosophy of science,prediction,psychology,Quantitative Methods,random effects,Social and Behavioral Sciences,statistics,Theory and Philosophy of Science},
  file = {/Users/xzfang/Zotero/storage/XYEU97H8/Yarkoni - 2019 - The Generalizability Crisis.pdf}
}

@article{yarkoni_moving_2008,
  title = {Moving beyond {{Coltheart}}'s {{N}}: {{A}} New Measure of Orthographic Similarity},
  shorttitle = {Moving beyond {{Coltheart}}'s {{N}}},
  author = {Yarkoni, Tal and Balota, David and Yap, Melvin},
  year = {2008},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {15},
  number = {5},
  pages = {971--979},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.15.5.971},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/JJSEC6B5/Yarkoni et al. - 2008 - Moving beyond Coltheartâ€™s N A new measure of orth.pdf}
}

@article{yashar_intertrial_2010,
  title = {Intertrial {{Repetition Facilitates Selection}} in {{Time}}: {{Common Mechanisms Underlie Spatial}} and {{Temporal Search}}},
  shorttitle = {Intertrial {{Repetition Facilitates Selection}} in {{Time}}},
  author = {Yashar, Amit and Lamy, Dominique},
  year = {2010},
  month = feb,
  journal = {Psychological Science},
  volume = {21},
  number = {2},
  pages = {243--251},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797609357928},
  abstract = {Recent research has demonstrated that what observers attend to at a given time affects how their attention is deployed in the few moments that follow. When an observer searches for a discrepant target, repetition of the target feature from the previous trial speeds search, an effect known as priming of pop-out (PoP). Previous PoP studies have relied exclusively on spatial search tasks. Here, using a rapid serial visual presentation task, we show that PoP also occurs when temporal uncertainty makes search necessary, and that when spatial and temporal search trials are interleaved, the PoP effect transfers from one task to the other. The results suggest that common mechanisms of target-feature activation and distractor-feature inhibition underlie spatial and temporal visual search. They elucidate the role of PoP in visual search by showing that it speeds engagement of attention to the selected target, rather than earlier stages involving target localization and attention focusing.},
  langid = {english},
  keywords = {attention,intertrial priming,priming of pop-out,temporal search,visual search}
}

@article{yashar_intertrial_2010a,
  title = {Intertrial {{Repetition Facilitates Selection}} in {{Time}}: {{Common Mechanisms Underlie Spatial}} and {{Temporal Search}}},
  shorttitle = {Intertrial {{Repetition Facilitates Selection}} in {{Time}}},
  author = {Yashar, Amit and Lamy, Dominique},
  year = {2010},
  month = feb,
  journal = {Psychological Science},
  volume = {21},
  number = {2},
  pages = {243--251},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797609357928},
  abstract = {Recent research has demonstrated that what observers attend to at a given time affects how their attention is deployed in the few moments that follow. When an observer searches for a discrepant target, repetition of the target feature from the previous trial speeds search, an effect known as priming of pop-out (PoP). Previous PoP studies have relied exclusively on spatial search tasks. Here, using a rapid serial visual presentation task, we show that PoP also occurs when temporal uncertainty makes search necessary, and that when spatial and temporal search trials are interleaved, the PoP effect transfers from one task to the other. The results suggest that common mechanisms of target-feature activation and distractor-feature inhibition underlie spatial and temporal visual search. They elucidate the role of PoP in visual search by showing that it speeds engagement of attention to the selected target, rather than earlier stages involving target localization and attention focusing.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/DDLE9FY7/Yashar and Lamy - 2010 - Intertrial Repetition Facilitates Selection in Tim.pdf}
}

@article{ye_how_2020,
  title = {How the Known Reference Weakens the Visual Oblique Effect: A {{Bayesian}} Account of Cognitive Improvement by Cue Influence},
  shorttitle = {How the Known Reference Weakens the Visual Oblique Effect},
  author = {Ye, Renyu and Liu, Xinsheng},
  year = {2020},
  month = nov,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {20269},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-76911-8},
  abstract = {This paper investigates the influence of a known cue on the oblique effect in orientation identification and explains how subjects integrate cue information to identify target orientations. We design the psychophysical task in which subjects estimate target orientations in the presence of a known oriented reference line. For comparison the control experiments without the reference are conducted. Under Bayesian inference framework, a cue integration model is proposed to explain the perceptual improvement in the presence of the reference. The maximum likelihood estimates of the parameters of our model are obtained. In the presence of the reference, the variability and biases of identification are significantly reduced and the oblique effect of orientation identification is obviously weakened. Moreover, the identification of orientation in the vicinity of the reference line is consistently biased away from~the reference line (i.e., reference repulsion). Comparing the predictions of the model with the experimental results, the Bayesian Least Squares estimator under the Variable-Precision encoding (BLS\_VP) provides a better description of the experimental outcomes and captures the trade-off relationship of bias and precision of identification. Our results provide a useful step toward a better understanding of human visual perception in context of the known cues.},
  copyright = {2020 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Decision;Neural decoding;Neuroscience;Perception Subject\_term\_id: decision;neural-decoding;neuroscience;perception},
  file = {/Users/xzfang/Zotero/storage/EPTMTXGM/Ye and Liu - 2020 - How the known reference weakens the visual oblique.pdf;/Users/xzfang/Zotero/storage/ZRMCCXTI/s41598-020-76911-8.html}
}

@article{yeung_detection_2004,
  title = {Detection of Synchronized Oscillations in the Electroencephalogram: {{An}} Evaluation of Methods},
  shorttitle = {Detection of Synchronized Oscillations in the Electroencephalogram},
  author = {Yeung, Nick and Bogacz, Rafal and Holroyd, Clay B. and Cohen, Jonathan D.},
  year = {2004},
  journal = {Psychophysiology},
  volume = {41},
  number = {6},
  pages = {822--832},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.2004.00239.x},
  abstract = {The signal averaging approach typically used in ERP research assumes that peaks in ERP waveforms reflect neural activity that is uncorrelated with activity in the ongoing EEG. However, this assumption has been challenged by research suggesting that ERP peaks reflect event-related synchronization of ongoing EEG oscillations. In this study, we investigated the validity of a set of methods that have been used to demonstrate that particular ERP peaks result from synchronized EEG oscillations. We simulated epochs of EEG data by superimposing phasic peaks on noise characterized by the power spectrum of the EEG. When applied to the simulated data, the methods in question produced results that have previously been interpreted as evidence of synchronized oscillations, even though no such synchrony was present. These findings suggest that proposed analysis methods may not effectively disambiguate competing views of ERP generation.},
  langid = {english},
  keywords = {Electroencephalography,Event-related potential,Oscillations,Phase resetting,Synchrony},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2004.00239.x},
  file = {/Users/xzfang/Zotero/storage/X5BVX646/Yeung et al. - 2004 - Detection of synchronized oscillations in the elec.pdf;/Users/xzfang/Zotero/storage/LDE79DWI/j.1469-8986.2004.00239.html}
}

@article{yi_encoding_2019,
  title = {The {{Encoding}} of {{Speech Sounds}} in the {{Superior Temporal Gyrus}}},
  author = {Yi, Han Gyol and Leonard, Matthew K. and Chang, Edward F.},
  year = {2019},
  month = jun,
  journal = {Neuron},
  volume = {102},
  number = {6},
  pages = {1096--1110},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.04.023},
  abstract = {The human superior temporal gyrus (STG) is critical for extracting meaningful linguistic features from speech input. Local neural populations are tuned to acoustic-phonetic features of all consonants and vowels and to dynamic cues for intonational pitch. These populations are embedded throughout broader functional zones that are sensitive to amplitude-based temporal cues. Beyond speech features, STG representations are strongly modulated by learned knowledge and perceptual goals. Currently, a major challenge is to understand how these features are integrated across space and time in the brain during natural speech comprehension. We present a theory that temporally recurrent connections within STG generate context-dependent phonological representations, spanning longer temporal sequences relevant for coherent percepts of syllables, words, and phrases.},
  langid = {english},
  keywords = {acoustic-phonetic features,auditory cortex,context-dependent representation,electrocorticography,phonological sequence,speech processing,superior temporal gyrus,temporal integration,temporal landmarks,temporally recurrent connections},
  file = {/Users/xzfang/Zotero/storage/WLHPFSM9/Yi et al. - 2019 - The Encoding of Speech Sounds in the Superior Temp.pdf}
}

@article{yildirim_talkerspecificity_2016,
  title = {Talker-Specificity and Adaptation in Quantifier Interpretation},
  author = {Yildirim, Ilker and Degen, Judith and Tanenhaus, Michael K. and Jaeger, T. Florian},
  year = {2016},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {87},
  pages = {128--143},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.08.003},
  abstract = {Linguistic meaning has long been recognized to be highly context-dependent. Quantifiers like many and some provide a particularly clear example of context-dependence. For example, the interpretation of quantifiers requires listeners to determine the relevant domain and scale. We focus on another type of context-dependence that quantifiers share with other lexical items: talker variability. Different talkers might use quantifiers with different interpretations in mind. We used a web-based crowdsourcing paradigm to study participants' expectations about the use of many and some based on recent exposure. We first established that the mapping of some and many onto quantities (candies in a bowl) is variable both within and between participants. We then examined whether and how listeners' expectations about quantifier use adapts with exposure to talkers who use quantifiers in different ways. The results demonstrate that listeners can adapt to talker-specific biases in both how often and with what intended meaning many and some are used.},
  langid = {english},
  keywords = {Adaptation,Pragmatics,Quantifiers,Semantics,Talker-specificity},
  file = {/Users/xzfang/Zotero/storage/HGHKUBJX/Yildirim et al. - 2016 - Talker-specificity and adaptation in quantifier in.pdf;/Users/xzfang/Zotero/storage/9EGA8Y7X/S0749596X15000996.html}
}

@article{yoo_uncertainty_2021,
  title = {Uncertainty Is Maintained and Used in Working Memory},
  author = {Yoo, Aspen H. and Acerbi, Luigi and Ma, Wei Ji},
  year = {2021},
  month = aug,
  journal = {Journal of Vision},
  volume = {21},
  number = {8},
  pages = {13--13},
  publisher = {{The Association for Research in Vision and Ophthalmology}},
  issn = {1534-7362},
  doi = {10.1167/jov.21.8.13},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/385QS6LX/Yoo et al. - 2021 - Uncertainty is maintained and used in working memo.pdf;/Users/xzfang/Zotero/storage/8DW54Z46/article.html}
}

@article{yoon_adjusting_2014,
  title = {Adjusting Conceptual Pacts in Three-Party Conversation},
  author = {Yoon, Si On and {Brown-Schmidt}, Sarah},
  year = {2014},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {40},
  number = {4},
  pages = {919--937},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1285},
  doi = {10.1037/a0036161},
  abstract = {During conversation, partners develop representations of jointly known information\textemdash the common ground\textemdash and use this knowledge to guide subsequent linguistic exchanges. Extensive research on 2-party conversation has offered key insights into this process, in particular, its partner-specificity: Common ground that is shared with 1 partner is not always assumed to be shared with other partners. Conversation often involves multiple pairs of individuals who differ in common ground. Yet, little is known about common ground processes in multi-party conversation. Here, we take a 1st step toward understanding this problem by examining situations in which simple dyadic representations of common ground might cause difficulty\textemdash situations in which dialogue partners develop shared labels (entrained terms), and then a 3rd (na\"ive) party joins the conversation. Experiment 1 examined unscripted, task-based conversation in which 2 partners entrained on terms. At test, speakers referenced game-pieces in a dialogue with their partner, or in a 3-party conversation including a new, na\"ive listener. Speakers were sensitive to the 3rd party, using longer, disfluent expressions when additionally addressing the new partner. By contrast, analysis of listener eye-fixations did not suggest sensitivity. Experiment 2 provided a stronger test of sensitivity and revealed that listeners do cancel expectations for terms that had been entrained before when a 3rd, na\"ive party joins the conversation. These findings shed light on the mechanisms underlying common ground, showing that rather than a unitary construct, common ground is flexibly adapted to the needs of a na\"ive 3rd party. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Concepts,Conversation,Role Taking,Visual Tracking},
  file = {/Users/xzfang/Zotero/storage/BACF9XDK/doiLanding.html}
}

@article{yoon_audience_2019,
  title = {Audience {{Design}} in {{Multiparty Conversation}}},
  author = {Yoon, Si On and Brown-Schmidt, Sarah},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {8},
  pages = {e12774},
  issn = {1551-6709},
  doi = {10.1111/cogs.12774},
  abstract = {How do speakers design what they say in order to communicate effectively with groups of addressees who vary in their background knowledge of the topic at hand? Prior findings indicate that when a speaker addresses a pair of listeners with discrepant knowledge, that speakers Aim Low, designing their utterances for the least knowledgeable of the two addressees. Here, we test the hypothesis that speakers will depart from an Aim Low approach in order to efficiently communicate with larger groups of interacting partners. Further, we ask whether the cognitive demands of tracking multiple conversational partners' perspectives places limitations on successful audience design. We find that speakers can successfully track information about what up to four of their partners do and do not know in conversation. When addressing groups of 3\textendash 4 addressees at once, speakers design language based on the combined knowledge of the group. These findings point to an audience design process that simultaneously represents the perspectives of multiple other individuals and combines these representations in order to design utterances that strike a balance between the different needs of the individuals within the group.},
  copyright = {\textcopyright{} 2019 Cognitive Science Society, Inc},
  langid = {english},
  keywords = {Audience design,Common ground,Language production,Multiparty conversation,Reference},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12774},
  file = {/Users/xzfang/Zotero/storage/XWT5IA3S/Yoon and Brownâ€Schmidt - 2019 - Audience Design in Multiparty Conversation.pdf;/Users/xzfang/Zotero/storage/E292T2Y8/cogs.html}
}

@article{yoon_historical_2016,
  title = {The Historical Context in Conversation: {{Lexical}} Differentiation and Memory for the Discourse History},
  shorttitle = {The Historical Context in Conversation},
  author = {Yoon, Si On and Benjamin, Aaron S. and {Brown-Schmidt}, Sarah},
  year = {2016},
  month = sep,
  journal = {Cognition},
  volume = {154},
  pages = {102--117},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2016.05.011},
  abstract = {When designing a definite referring expression, speakers take into account both the local context and certain aspects of the historical context, including whether similar referents have been mentioned in the past. When a similar item has been mentioned previously, speakers tend to elaborate their referring expression in order to differentiate the two items, a phenomenon called lexical differentiation. The present research examines the locus of the lexical differentiation effect and its relationship with memory for the discourse. In three experiments, we demonstrate that speakers differentiate to distinguish current from past referents; there was no evidence that speakers differentiate in order to avoid giving two items the same label. Post-task memory tests also revealed a high level of memory for the discourse history, a finding that is inconsistent with the view that failures of memory underlie low differentiation rates. Instead, memory for the discourse history, while necessary, is not sufficient for speakers to design language with respect to the historical context. Speakers must additionally view the discourse history as relevant to design language with respect to this broader context. Finally, measures of memory for past referents point to asymmetries between speakers and listeners in their memory for the discourse, with speakers typically remembering the discourse history better.},
  langid = {english},
  keywords = {Dialogue,Discourse,Lexical differentiation,Memory}
}

@article{yoon_learning_2017,
  title = {Learning and Using Knowledge about What Other People Do and Don't Know despite Amnesia},
  author = {Yoon, S. and Duff, M. C. and {Brown-Schmidt}, S.},
  year = {2017},
  month = sep,
  journal = {Cortex; a journal devoted to the study of the nervous system and behavior},
  volume = {94},
  pages = {164--175},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2017.06.020},
  abstract = {Successful communication requires keeping track of what other people do and do not know, and how this differs from our own knowledge. Here we ask how knowledge of what others know is stored in memory. We take a neuropsychological approach, comparing healthy adults to patients with severe declarative memory impairment (amnesia). We evaluate whether this memory impairment disrupts the ability to successfully acquire and use knowledge about what other people know when communicating with them. We tested participants in a referential communication task in which the participants described a series of abstract ``tangram'' images for a partner. Participants then repeated the task with the same partner or a new partner. Findings show that much like healthy individuals, individuals with amnesia successfully tailored their communicative language to the knowledge shared with their conversational partner\textemdash their common ground. They produced brief descriptions of the tangram images for the familiar partner and provided more descriptive, longer expressions for the new partner. These findings demonstrate remarkable sparing in amnesia of the acquisition and use of partner-specific knowledge that underlies common ground, and have important implications for understanding the memory systems that support conversational language.},
  pmcid = {PMC5567824},
  pmid = {28768183},
  file = {/Users/xzfang/Zotero/storage/ZI8ITHXM/Yoon et al. - 2017 - Learning and using knowledge about what other peop.pdf}
}

@article{yoon_partnerspecific_2020,
  title = {Partner-Specific Adaptation in Disfluency Processing},
  author = {Yoon, Si On and {Brown-Schmidt}, Sarah},
  year = {2020},
  file = {/Users/xzfang/Zotero/storage/HDW2FYXR/0211.pdf}
}

@article{yosinski_understanding_2015,
  title = {Understanding {{Neural Networks Through Deep Visualization}}},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  year = {2015},
  month = jun,
  abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9H8IY9RR/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visuali.pdf}
}

@article{yousif_how_2021,
  title = {How {{We See Area}} and {{Why It Matters}}},
  author = {Yousif, Sami R. and Keil, Frank C.},
  year = {2021},
  month = may,
  journal = {Trends in Cognitive Sciences},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2021.03.017},
  abstract = {A large and growing literature examines how we see the visual quantities of number, area, and density. The literature rests on an untested assumption: that our perception of area is veridical. Here, we discuss a systematic distortion of perceived area and its implications for quantity perception more broadly.},
  langid = {english},
  keywords = {additive-area heuristic,area,density,number,perception}
}

@article{yue_nonperceptual_2019,
  title = {Non-Perceptual {{Regions}} in the {{Left Inferior Parietal Lobe Support Phonological Short-term Memory}}: {{Evidence}} for a {{Buffer Account}}?},
  shorttitle = {Non-Perceptual {{Regions}} in the {{Left Inferior Parietal Lobe Support Phonological Short-term Memory}}},
  author = {Yue, Qiuhai and Martin, Randi C and Hamilton, A Cris and Rose, Nathan S},
  year = {2019},
  month = apr,
  journal = {Cerebral Cortex},
  volume = {29},
  number = {4},
  pages = {1398--1413},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhy037},
  abstract = {Buffer versus embedded processes accounts of short-term memory (STM) for phonological information were addressed by testing subjects' perception and memory for speech and non-speech auditory stimuli. Univariate and multivariate (MVPA) approaches were used to assess whether brain regions recruited in recognizing speech were involved in maintaining speech representations over a delay. As expected, a left superior temporal region was found to support speech perception. However, contrary to the embedded processes approach, this region failed to show a load effect, or any sustained activation, during a maintenance delay. Moreover, MVPA decoding during the maintenance stage was unsuccessful in this region by a perception classifier or an encoding classifier. In contrast, the left supramarginal gyrus showed both sustained activation and a load effect. Using MVPA, stimulus decoding was successful during the delay period. In addition, a functional connectivity analysis showed that, as memory load increased, the left temporal lobe involved in perception became more strongly connected with the parietal region involved in maintenance. Taken together, the findings provide greater support for a buffer than embedded processes account of phonological STM.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/3GTH76GF/Yue et al. - 2019 - Non-perceptual Regions in the Left Inferior Pariet.pdf}
}

@article{yuille_vision_2006,
  title = {Vision as {{Bayesian}} Inference: Analysis by Synthesis?},
  shorttitle = {Vision as {{Bayesian}} Inference},
  author = {Yuille, Alan and Kersten, Daniel},
  year = {2006},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  series = {Special Issue: {{Probabilistic}} Models of Cognition},
  volume = {10},
  number = {7},
  pages = {301--308},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.05.002},
  abstract = {We argue that the study of human vision should be aimed at determining how humans perform natural tasks with natural images. Attempts to understand the phenomenology of vision from artificial stimuli, although worthwhile as a starting point, can lead to faulty generalizations about visual systems, because of the enormous complexity of natural images. Dealing with this complexity is daunting, but Bayesian inference on structured probability distributions offers the ability to design theories of vision that can deal with the complexity of natural images, and that use `analysis by synthesis' strategies with intriguing similarities to the brain. We examine these strategies using recent examples from computer vision, and outline some important imlications for cognitive science.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/ITP6YUWY/Yuille and Kersten - 2006 - Vision as Bayesian inference analysis by synthesi.pdf;/Users/xzfang/Zotero/storage/89MIY9ED/S1364661306001264.html}
}

@misc{zadbood_here_2021,
  title = {Here's the Twist: {{How}} the Brain Updates Naturalistic Event Memories as Our Understanding of the Past Changes},
  shorttitle = {Here's the Twist},
  author = {Zadbood, Asieh and Nastase, Samuel A. and Chen, Janice and Norman, Kenneth A. and Hasson, Uri},
  year = {2021},
  month = sep,
  pages = {2021.09.28.462068},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.09.28.462068},
  abstract = {The brain actively reshapes past memories in light of new incoming information. In the current study, we ask how the brain supports this updatinge process during the encoding and recall of naturalistic stimuli. One group of participants watched a movie (``The Sixth Sense'') with a cinematic ``twist'' at the end that dramatically changed the interpretation of previous events. Next, participants were asked to verbally recall the movie events, taking into account the new ``twist'' information. Most participants updated their recall to incorporate the twist. Two additional groups recalled the movie without having to update their memories during recall: one group never saw the twist; another group was exposed to the twist prior to the beginning of the movie, and thus the twist information was incorporated both during encoding and recall. We found that providing participants with information about the twist beforehand altered neural response patterns during movie-viewing in the default mode network (DMN). Moreover, presenting participants with the twist at the end of the movie changed the neural representation of the previously-encoded information during recall in a subset of DMN regions. Further evidence for this transformation was obtained by comparing the neural activation patterns during encoding and recall and correlating them with behavioral signatures of memory updating. Our results demonstrate that neural representations of past events encoded in the DMN are dynamically integrated with new information that reshapes our memory in natural contexts.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/7BNZQ6L5/Zadbood et al. - 2021 - Hereâ€™s the twist How the brain updates naturalist.pdf;/Users/xzfang/Zotero/storage/ECQTP7P2/2021.09.28.html}
}

@article{zaidel_decoupled_2017,
  title = {Decoupled Choice-Driven and Stimulus-Related Activity in Parietal Neurons May Be Misrepresented by Choice Probabilities},
  author = {Zaidel, Adam and DeAngelis, Gregory C. and Angelaki, Dora E.},
  year = {2017},
  month = sep,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {715},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-00766-3},
  abstract = {Trial-by-trial correlations between neural responses and choices (choice probabilities) are often interpreted to reflect a causal contribution of neurons to task performance. However, choice probabilities may arise from top-down, rather than bottom-up, signals. We isolated distinct sensory and decision contributions to single-unit activity recorded from the dorsal medial superior temporal (MSTd) and ventral intraparietal (VIP) areas of monkeys during perception of self-motion. Superficially, neurons in both areas show similar tuning curves during task performance. However, tuning in MSTd neurons primarily reflects sensory inputs, whereas choice-related signals dominate tuning in VIP neurons. Importantly, the choice-related activity of VIP neurons is not predictable from their stimulus tuning, and these factors are often confounded in choice probability measurements. This finding was confirmed in a subset of neurons for which stimulus tuning was measured during passive fixation. Our findings reveal decoupled stimulus and choice signals in the VIP area, and challenge our understanding of choice signals in the brain.},
  copyright = {2017 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Decision;Extrastriate cortex;Sensory processing Subject\_term\_id: decision;extrastriate-cortex;sensory-processing},
  file = {/Users/xzfang/Zotero/storage/JUMERKJM/Zaidel et al. - 2017 - Decoupled choice-driven and stimulus-related activ.pdf;/Users/xzfang/Zotero/storage/M4G3DV8R/s41467-017-00766-3.html}
}

@article{zamaniesfahlani_highamplitude_2020,
  title = {High-Amplitude Cofluctuations in Cortical Activity Drive Functional Connectivity},
  author = {Zamani Esfahlani, Farnaz and Jo, Youngheun and Faskowitz, Joshua and Byrge, Lisa and Kennedy, Daniel P. and Sporns, Olaf and Betzel, Richard F.},
  year = {2020},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {45},
  pages = {28393--28401},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2005531117},
  abstract = {Resting-state functional connectivity is used throughout neuroscience to study brain organization and to generate biomarkers of development, disease, and cognition. The processes that give rise to correlated activity are, however, poorly understood. Here we decompose resting-state functional connectivity using a temporal unwrapping procedure to assess the contributions of moment-to-moment activity cofluctuations to the overall connectivity pattern. This approach temporally resolves functional connectivity at a timescale of single frames, which enables us to make direct comparisons of cofluctuations of network organization with fluctuations in the blood oxygen level-dependent (BOLD) time series. We show that surprisingly, only a small fraction of frames exhibiting the strongest cofluctuation amplitude are required to explain a significant fraction of variance in the overall pattern of connection weights as well as the network's modular structure. These frames coincide with frames of high BOLD activity amplitude, corresponding to activity patterns that are remarkably consistent across individuals and identify fluctuations in default mode and control network activity as the primary driver of resting-state functional connectivity. Finally, we demonstrate that cofluctuation amplitude synchronizes across subjects during movie watching and that high-amplitude frames carry detailed information about individual subjects (whereas low-amplitude frames carry little). Our approach reveals fine-scale temporal structure of resting-state functional connectivity and discloses that frame-wise contributions vary across time. These observations illuminate the relation of brain activity to functional connectivity and open a number of directions for future research.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/EA8TR4PS/Zamani Esfahlani et al. - 2020 - High-amplitude cofluctuations in cortical activity.pdf}
}

@article{zatorre_functional_1998,
  title = {Functional Anatomy of Musical Processing in Listeners with Absolute Pitch and Relative Pitch},
  author = {Zatorre, R. J. and Perry, D. W. and Beckett, C. A. and Westbury, C. F. and Evans, A. C.},
  year = {1998},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {95},
  number = {6},
  pages = {3172--3177},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.95.6.3172},
  abstract = {We used both structural and functional brain imaging techniques to investigate the neural basis of absolute pitch (AP), a specialized skill present in some musicians. By using positron emission tomography, we measured cerebral blood f low during the presentation of musical tones to AP possessors and to control musicians without AP. Listening to musical tones resulted in similar patterns of increased cerebral blood f low in auditory cortical areas in both groups, as expected. The AP group also demonstrated activation of the left posterior dorsolateral frontal cortex, an area thought to be related to learning conditional associations. However, a similar pattern of left dorsolateral frontal activity was also observed in non-AP subjects when they made relative pitch judgments of intervals, such as minor or major. Conversely, activity within the right inferior frontal cortex was observed in control but not in AP subjects during the interval-judgment task, suggesting that AP possessors need not access working memory mechanisms in this task. MRI measures of cortical volume indicated a larger left planum temporale in the AP group, which correlated with performance on an pitch-naming task. Our findings suggest that AP may not be associated with a unique pattern of cerebral activity but rather may depend on the recruitment of a specialized network involved in the retrieval and manipulation of verbal\textendash tonal associations.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BQNXNMGS/Zatorre et al. - 1998 - Functional anatomy of musical processing in listen.pdf}
}

@article{zellou_listeners_2019,
  title = {Listeners Maintain Phonological Uncertainty over Time and across Words: {{The}} Case of Vowel Nasality in {{English}}},
  shorttitle = {Listeners Maintain Phonological Uncertainty over Time and across Words},
  author = {Zellou, Georgia and Dahan, Delphine},
  year = {2019},
  month = sep,
  journal = {Journal of Phonetics},
  volume = {76},
  pages = {100910},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2019.06.001},
  abstract = {While the fact that phonetic information is evaluated in a non-discrete, probabilistic fashion is well established, there is less consensus regarding how long such encoding is maintained. Here, we examined whether people maintain in memory the amount of vowel nasality present in a word when processing a subsequent word that holds a semantic dependency with the first one. Vowel nasality in English is an acoustic correlate of the oral vs. nasal status of an adjacent consonant, and sometimes it is the only distinguishing phonetic feature (e.g., bet vs. bent). In Experiment 1, we show that people can perceive differences in nasality between two vowels above and beyond differences in the categorization of those vowels. In Experiment 2, we tracked listeners' eye-movements as they heard a sentence that mentioned one of four displayed images (e.g., `money') following a prime word (e.g., `bet') that held a semantic relationship with the target word. Recognition of the target was found to be modulated by the degree of nasality in the first word's vowel: Slightly greater uncertainty regarding the oral status of the post-vocalic consonant in the first word translated into a weaker semantic cue for the identification of the second word. Thus, listeners appear to maintain in memory the degree of vowel nasality they perceived on the first word and bring this information to bear onto the interpretation of a subsequent, semantically-dependent word. Probabilistic cue integration across words that hold semantic coherence, we argue, contributes to achieving robust language comprehension despite the inherent ambiguity of the speech signal.},
  langid = {english},
  keywords = {Discrimination,Eye movements,Predictability,Semantic priming,Spoken word recognition},
  file = {/Users/xzfang/Zotero/storage/NALRKTUY/Zellou and Dahan - 2019 - Listeners maintain phonological uncertainty over t.pdf;/Users/xzfang/Zotero/storage/AWNJINKI/S0095447018300123.html}
}

@article{zendel_attending_2015,
  title = {Attending to {{Pitch Information Inhibits Processing}} of {{Pitch Information}}: {{The Curious Case}} of {{Amusia}}},
  shorttitle = {Attending to {{Pitch Information Inhibits Processing}} of {{Pitch Information}}},
  author = {Zendel, B. R. and Lagrois, M.-E. and Robitaille, N. and Peretz, I.},
  year = {2015},
  month = mar,
  journal = {Journal of Neuroscience},
  volume = {35},
  number = {9},
  pages = {3815--3824},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3766-14.2015},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/9Q4CGZ2L/Zendel et al. - 2015 - Attending to Pitch Information Inhibits Processing.pdf}
}

@article{zhang_bottomup_2021,
  title = {Bottom-up but Not {{Top-down Attention Dominates}} the {{Value Representation}} in the {{Orbitofrontal Cortex}}},
  author = {Zhang, Wenyi and Xie, Yang and Yang, Tianming},
  year = {2021},
  month = jun,
  journal = {bioRxiv},
  pages = {2021.06.14.448326},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.14.448326},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The orbitofrontal cortex (OFC) is essential for value-based learning and decision making. Understanding the attentional modulation of the representation of value in the OFC provides us key information on its functional roles and links the OFC to other cognitive processes. We examined how top-down and bottom-up attention modulates the value encoding in the OFC. Two macaque monkeys were trained to detect a luminance change at a cued location between a pair of visual stimuli, which were over-trained pictures associated with different amount of juice rewards and, thus, different salience. While the monkeys' behavior and the DLPFC neuronal activities indicated that the monkeys actively directed their attention toward the cued location during the task, the OFC neurons' value encoding, however, was dominated by the bottom-up attention based on stimulus salience and only reflected the top-down attention weakly. The disassociation between the top-down and bottom-up attention signals in the OFC indicates that the OFC occupies an early stage of value information processing in the brain.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BMCD5Z5K/Zhang et al. - 2021 - Bottom-up but not Top-down Attention Dominates the.pdf;/Users/xzfang/Zotero/storage/WBRGC6EK/2021.06.14.html}
}

@article{zhang_choosing_2013,
  title = {Choosing the {{Rules}}: {{Distinct}} and {{Overlapping Frontoparietal Representations}} of {{Task Rules}} for {{Perceptual Decisions}}},
  shorttitle = {Choosing the {{Rules}}},
  author = {Zhang, Jiaxiang and Kriegeskorte, Nikolaus and Carlin, Johan D. and Rowe, James B.},
  year = {2013},
  month = jul,
  journal = {Journal of Neuroscience},
  volume = {33},
  number = {29},
  pages = {11852--11862},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5193-12.2013},
  abstract = {Behavior is governed by rules that associate stimuli with responses and outcomes. Human and monkey studies have shown that rule-specific information is widely represented in the frontoparietal cortex. However, it is not known how establishing a rule under different contexts affects its neural representation. Here, we use event-related functional MRI (fMRI) and multivoxel pattern classification methods to investigate the human brain's mechanisms of establishing and maintaining rules for multiple perceptual decision tasks. Rules were either chosen by participants or specifically instructed to them, and the fMRI activation patterns representing rule-specific information were compared between these contexts. We show that frontoparietal regions differ in the properties of their rule representations during active maintenance before execution. First, rule-specific information maintained in the dorsolateral and medial frontal cortex depends on the context in which it was established (chosen vs specified). Second, rule representations maintained in the ventrolateral frontal and parietal cortex are independent of the context in which they were established. Furthermore, we found that the rule-specific coding maintained in anticipation of stimuli may change with execution of the rule: representations in context-independent regions remain invariant from maintenance to execution stages, whereas rule representations in context-dependent regions do not generalize to execution stage. The identification of distinct frontoparietal systems with context-independent and context-dependent task rule representations, and the distinction between anticipatory and executive rule representations, provide new insights into the functional architecture of goal-directed behavior.},
  copyright = {Copyright \textcopyright{} 2013 the authors 0270-6474/13/3311852-11\$15.00/0. This article is freely available online through the J Neurosci Author Open Choice option.},
  langid = {english},
  pmid = {23864675},
  file = {/Users/xzfang/Zotero/storage/SUH8AUS3/Zhang et al. - 2013 - Choosing the Rules Distinct and Overlapping Front.pdf;/Users/xzfang/Zotero/storage/Y74UZC6U/11852.html}
}

@article{zhang_decoding_2021,
  title = {Decoding the Silence: {{Neural}} Bases of Zero Pronoun Resolution in {{Chinese}}},
  shorttitle = {Decoding the Silence},
  author = {Zhang, Shulin and Li, Jixing and Yang, Yiming and Hale, John},
  year = {2021},
  month = may,
  journal = {bioRxiv},
  pages = {2021.05.06.442989},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.05.06.442989},
  abstract = {{$<$}p{$>$}Chinese is one of many languages that can drop subjects. We report an fMRI study of language comprehension processes in these "zero pronoun" cases. The fMRI data come from Chinese speakers who listened to an audiobook. We conducted both univariate GLM and multivariate pattern analysis (MVPA) on these data time-locked to each verb with a zero pronoun subject. We found increased left middle temporal gyrus activity for zero pronouns compared to overt subjects, suggesting additional effort searching for an antecedent during zero pronoun resolution. MVPA further revealed that the intended referent of a zero pronoun seems to be physically represented in the Precuneus and the Parahippocampal Gyrus shortly after its presentation. This highlights the role of memory and discourse-level processing in resolving referential expressions, including unspoken ones, in naturalistic language comprehension.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/BZN96T2T/Zhang et al. - 2021 - Decoding the silence Neural bases of zero pronoun.pdf;/Users/xzfang/Zotero/storage/X4K3HNMY/2021.05.06.442989v1.html}
}

@article{zhang_finding_2018,
  title = {Finding Any {{Waldo}} with Zero-Shot Invariant and Efficient Visual Search},
  author = {Zhang, Mengmi and Feng, Jiashi and Ma, Keng Teck and Lim, Joo Hwee and Zhao, Qi and Kreiman, Gabriel},
  year = {2018},
  month = sep,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {3730},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-06217-x},
  abstract = {Searching for a target object in a cluttered scene constitutes a fundamental challenge in daily vision. Visual search must be selective enough to discriminate the target from distractors, invariant to changes in the appearance of the target, efficient to avoid exhaustive exploration of the image, and must generalize to locate novel target objects with zero-shot training. Previous work on visual search has focused on searching for perfect matches of a target after extensive category-specific training. Here, we show for the first time that humans can efficiently and invariantly search for natural objects in complex scenes. To gain insight into the mechanisms that guide visual search, we propose a biologically inspired computational model that can locate targets without exhaustive sampling and which can generalize to novel objects. The model provides an approximation to the mechanisms integrating bottom-up and top-down signals during search in natural scenes.},
  copyright = {2018 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Attention;Computer science;Network models;Perception Subject\_term\_id: attention;computer-science;network-models;perception},
  file = {/Users/xzfang/Zotero/storage/I3AYAQ3Q/Zhang et al. - 2018 - Finding any Waldo with zero-shot invariant and eff.pdf;/Users/xzfang/Zotero/storage/XR7ELRSF/s41467-018-06217-x.html}
}

@misc{zhang_learning_2019,
  title = {Learning to {{Speak Fluently}} in a {{Foreign Language}}: {{Multilingual Speech Synthesis}} and {{Cross-Language Voice Cloning}}},
  shorttitle = {Learning to {{Speak Fluently}} in a {{Foreign Language}}},
  author = {Zhang, Yu and Weiss, Ron J. and Zen, Heiga and Wu, Yonghui and Chen, Zhifeng and {Skerry-Ryan}, R. J. and Jia, Ye and Rosenberg, Andrew and Ramabhadran, Bhuvana},
  year = {2019},
  month = jul,
  number = {arXiv:1907.04448},
  eprint = {1907.04448},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1907.04448},
  abstract = {We present a multispeaker, multilingual text-to-speech (TTS) synthesis model based on Tacotron that is able to produce high quality speech in multiple languages. Moreover, the model is able to transfer voices across languages, e.g. synthesize fluent Spanish speech using an English speaker's voice, without training on any bilingual or parallel examples. Such transfer works across distantly related languages, e.g. English and Mandarin. Critical to achieving this result are: 1. using a phonemic input representation to encourage sharing of model capacity across languages, and 2. incorporating an adversarial loss term to encourage the model to disentangle its representation of speaker identity (which is perfectly correlated with language in the training data) from the speech content. Further scaling up the model by training on multiple speakers of each language, and incorporating an autoencoding input to help stabilize attention during training, results in a model which can be used to consistently synthesize intelligible speech for training speakers in all languages seen during training, and in native or foreign accents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/xzfang/Zotero/storage/8WJ3XEUL/Zhang et al. - 2019 - Learning to Speak Fluently in a Foreign Language .pdf;/Users/xzfang/Zotero/storage/L76Z4PK8/1907.html}
}

@inproceedings{zhang_learning_2020,
  title = {Learning {{Noise Invariant Features Through Transfer Learning For Robust End-to-End Speech Recognition}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Shucong and Do, Cong-Thanh and Doddipatla, Rama and Renals, Steve},
  year = {2020},
  month = may,
  pages = {7024--7028},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9053169},
  abstract = {End-to-end models yield impressive speech recognition results on clean datasets while having inferior performance on noisy datasets. To address this, we propose transfer learning from a clean dataset (WSJ) to a noisy dataset (CHiME4) for connectionist temporal classification models. We argue that the clean classifier (the upper layers of a neural network trained on clean data) can force the feature extractor (the lower layers) to learn the underlying noise invariant patterns in the noisy dataset. While training on the noisy dataset, the clean classifier is either frozen or trained with a small learning rate. The feature extractor is trained with no learning rate re-scaling. The proposed method gives up to 15.5\% relative character error rate (CER) reduction compared to models trained only on CHiME-4. Furthermore, we use the test sets of Aurora-4 to perform evaluation on unseen noisy conditions. Our method has significantly lower CERs (11.3\% relative on average) on all 14 Aurora-4 test sets compared to the conventional transfer learning method (no learning rate rescale for any layer), indicating our method enables the model to learn noise invariant features.},
  keywords = {Data models,end-to-end,Error analysis,Feature extraction,Noise measurement,robust speech recognition,Speech processing,Speech recognition,transfer learning,Transfer learning},
  file = {/Users/xzfang/Zotero/storage/5IN2HGU4/Zhang et al. - 2020 - Learning Noise Invariant Features Through Transfer.pdf;/Users/xzfang/Zotero/storage/MMQZRKVQ/9053169.html}
}

@article{zhang_object_2011,
  title = {Object Decoding with Attention in Inferior Temporal Cortex},
  author = {Zhang, Y. and Meyers, E. M. and Bichot, N. P. and Serre, T. and Poggio, T. A. and Desimone, R.},
  year = {2011},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {21},
  pages = {8850--8855},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1100999108},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/8LB2CLB8/Zhang et al. - 2011 - Object decoding with attention in inferior tempora.pdf}
}

@article{zhao_attention_2013,
  title = {Attention {{Is Spontaneously Biased Toward Regularities}}},
  author = {Zhao, Jiaying and {Al-Aidroos}, Naseem and {Turk-Browne}, Nicholas B.},
  year = {2013},
  month = may,
  journal = {Psychological Science},
  volume = {24},
  number = {5},
  pages = {667--677},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797612460407},
  abstract = {Knowledge about regularities in the environment can be used to facilitate perception, memory, and language acquisition. Given this usefulness, we hypothesized that statistically structured sources of information receive attentional priority over noisier sources, independent of their intrinsic salience or goal relevance. We report three experiments that support this hypothesis. Experiment 1 shows that regularities bias spatial attention: Visual search was facilitated at a location containing temporal regularities, even though these regularities did not predict target location, timing, or identity. Experiments 2 and 3 show that regularities bias feature attention: Attentional capture doubled in magnitude when singletons appeared, respectively, in a color or dimension with temporal regularities among task-irrelevant stimuli. Prioritization of the locations and features of regularities is not easily accounted for in the conventional dichotomy between stimulus-driven and goal-directed attention. This prioritization may in turn promote further statistical learning, helping the mind to acquire knowledge about stable aspects of the environment.},
  langid = {english},
  keywords = {attentional capture,cognitive control,feature-based attention,spatial attention,statistical learning,visual search},
  file = {/Users/xzfang/Zotero/storage/YWCQ2652/Zhao et al. - 2013 - Attention Is Spontaneously Biased Toward Regularit.pdf}
}

@inproceedings{zhao_pose_2018,
  title = {Towards {{Pose Invariant Face Recognition}} in the {{Wild}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhao, Jian and Cheng, Yu and Xu, Yan and Xiong, Lin and Li, Jianshu and Zhao, Fang and Jayashree, Karlekar and Pranata, Sugiri and Shen, Shengmei and Xing, Junliang and Yan, Shuicheng and Feng, Jiashi},
  year = {2018},
  month = jun,
  pages = {2207--2216},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00235},
  abstract = {Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a ``learning to learn'' strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/6YTIL3S6/Zhao et al. - 2018 - Towards Pose Invariant Face Recognition in the Wil.pdf}
}

@article{zhao_rapid_2019,
  title = {Rapid {{Ocular Responses Are Modulated}} by {{Bottom-up-Driven Auditory Salience}}},
  author = {Zhao, Sijia and Yum, Nga Wai and Benjamin, Lucas and Benhamou, Elia and Yoneya, Makoto and Furukawa, Shigeto and Dick, Fred and Slaney, Malcolm and Chait, Maria},
  year = {2019},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {39},
  pages = {7703--7714},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0776-19.2019},
  abstract = {Despite the prevalent use of alerting sounds in alarms and human\textendash machine interface systems and the long-hypothesized role of the auditory system as the brain's ``early warning system,'' we have only a rudimentary understanding of what determines auditory salience\textemdash the automatic attraction of attention by sound\textemdash and which brain mechanisms underlie this process. A major roadblock has been the lack of a robust, objective means of quantifying sound-driven attentional capture. Here we demonstrate that: (1) a reliable salience scale can be obtained from crowd-sourcing (N = 911), (2) acoustic roughness appears to be a driving feature behind this scaling, consistent with previous reports implicating roughness in the perceptual distinctiveness of sounds, and (3) crowd-sourced auditory salience correlates with objective autonomic measures. Specifically, we show that a salience ranking obtained from online raters correlated robustly with the superior colliculus-mediated ocular freezing response, microsaccadic inhibition (MSI), measured in naive, passively listening human participants (of either sex). More salient sounds evoked earlier and larger MSI, consistent with a faster orienting response. These results are consistent with the hypothesis that MSI reflects a general reorienting response that is evoked by potentially behaviorally important events regardless of their modality. SIGNIFICANCE STATEMENT Microsaccades are small, rapid, fixational eye movements that are measurable with sensitive eye-tracking equipment. We reveal a novel, robust link between microsaccade dynamics and the subjective salience of brief sounds (salience rankings obtained from a large number of participants in an online experiment): Within 300 ms of sound onset, the eyes of naive, passively listening participants demonstrate different microsaccade patterns as a function of the sound's crowd-sourced salience. These results position the superior colliculus (hypothesized to underlie microsaccade generation) as an important brain area to investigate in the context of a putative multimodal salience hub. They also demonstrate an objective means for quantifying auditory salience.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2019 the authors. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {31391262},
  keywords = {attention,auditory scene analysis,microsaccades,pupil dilation,superior colliculus},
  file = {/Users/xzfang/Zotero/storage/Z84ZIRAH/Zhao et al. - 2019 - Rapid Ocular Responses Are Modulated by Bottom-up-.pdf;/Users/xzfang/Zotero/storage/FVT25HY2/7703.html}
}

@article{zheljazkov_distillation_2015,
  title = {Distillation {{Time}} as {{Tool}} for {{Improved Antimalarial Activity}} and {{Differential Oil Composition}} of {{Cumin Seed Oil}}},
  author = {Zheljazkov, Valtcho D. and Gawde, Archana and Cantrell, Charles L. and Astatkie, Tess and Schlegel, Vicki},
  year = {2015},
  month = dec,
  journal = {PLoS ONE},
  volume = {10},
  number = {12},
  pages = {1--12},
  issn = {19326203},
  doi = {10.1371/journal.pone.0144120},
  abstract = {A steam distillation extraction kinetics experiment was conducted to estimate essential oil yield, composition, antimalarial, and antioxidant capacity of cumin (Cuminum cyminum L.) seed (fruits). Furthermore, regression models were developed to predict essential oil yield and composition for a given duration of the steam distillation time (DT). Ten DT durations were tested in this study: 5, 7.5, 15, 30, 60, 120, 240, 360, 480, and 600 min. Oil yields increased with an increase in the DT. Maximum oil yield (content, 2.3 g/100 seed), was achieved at 480 min; longer DT did not increase oil yields. The concentrations of the major oil constituents {$\alpha$}-pinene (0.14\textendash 0.5\% concentration range), {$\beta$}-pinene (3.7\textendash 10.3\% range), {$\gamma$}-cymene (5\textendash 7.3\% range), {$\gamma$}-terpinene (1.8\textendash 7.2\% range), cumin aldehyde (50\textendash 66\% range), {$\alpha$}-terpinen-7-al (3.8\textendash 16\% range), and {$\beta$}-terpinen-7-al (12\textendash 20\% range) varied as a function of the DT. The concentrations of {$\alpha$}-pinene, {$\beta$}-pinene, {$\gamma$}-cymene, {$\gamma$}-terpinene in the oil increased with the increase of the duration of the DT; {$\alpha$}-pinene was highest in the oil obtained at 600 min DT, {$\beta$}-pinene and {$\gamma$}-terpinene reached maximum concentrations in the oil at 360 min DT; {$\gamma$}-cymene reached a maximum in the oil at 60 min DT, cumin aldehyde was high in the oils obtained at 5\textendash 60 min DT, and low in the oils obtained at 240\textendash 600 min DT, {$\alpha$}-terpinen-7-al reached maximum in the oils obtained at 480 or 600 min DT, whereas {$\beta$}-terpinen-7-al reached a maximum concentration in the oil at 60 min DT. The yield of individual oil constituents (calculated from the oil yields and the concentration of a given compound at a particular DT) increased and reached a maximum at 480 or 600 min DT. The antimalarial activity of the cumin seed oil obtained during the 0\textendash 5 and at 5\textendash 7.5 min DT timeframes was twice higher than the antimalarial activity of the oils obtained at the other DT. This study opens the possibility for distinct marketing and utilization for these improved oils. The antioxidant capacity of the oil was highest in the oil obtained at 30 min DT and lowest in the oil from 360 min DT. The Michaelis-Menton and the Power nonlinear regression models developed in this study can be utilized to predict essential oil yield and composition of cumin seed at any given duration of DT and may also be useful to compare previous reports on cumin oil yield and composition. DT can be utilized to obtain cumin seed oil with improved antimalarial activity, improved antioxidant capacity, and with various compositions.},
  keywords = {ANTIMALARIALS,CUMIN,DISTILLATION,EXTRACTION (Chemistry),Research Article,VEGETABLE oils},
  file = {/Users/xzfang/Zotero/storage/H6C4EHEU/Zheljazkov ç­‰. - 2015 - Distillation Time as Tool for Improved Antimalaria.pdf}
}

@article{zheng_neurons_2022,
  title = {Neurons Detect Cognitive Boundaries to Structure Episodic Memories in Humans},
  author = {Zheng, Jie and Schjetnan, Andrea G. P. and Yebra, Mar and Gomes, Bernard A. and Mosher, Clayton P. and Kalia, Suneil K. and Valiante, Taufik A. and Mamelak, Adam N. and Kreiman, Gabriel and Rutishauser, Ueli},
  year = {2022},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {25},
  number = {3},
  pages = {358--368},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01020-w},
  abstract = {While experience is continuous, memories are organized as discrete events. Cognitive boundaries are thought to segment experience and structure memory, but how this process is implemented remains unclear. We recorded the activity of single neurons in the human medial temporal lobe (MTL) during the formation and retrieval of memories with complex narratives. Here, we show that neurons responded to abstract cognitive boundaries between different episodes. Boundary-induced neural state changes during encoding predicted subsequent recognition accuracy but impaired event order memory, mirroring a fundamental behavioral tradeoff between content and time memory. Furthermore, the neural state following boundaries was reinstated during both successful retrieval and false memories. These findings reveal a neuronal substrate for detecting cognitive boundaries that transform experience into mnemonic episodes and structure mental time travel during retrieval.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cognitive neuroscience,Long-term memory,Neurophysiology},
  file = {/Users/xzfang/Zotero/storage/C84KVSXE/Zheng et al. - 2022 - Neurons detect cognitive boundaries to structure e.pdf;/Users/xzfang/Zotero/storage/TPT5WRKB/s41593-022-01020-w.html}
}

@article{zheng_partially_2021,
  title = {Partially Overlapping Spatial Environments Trigger Reinstatement in Hippocampus and Schema Representations in Prefrontal Cortex},
  author = {Zheng, Li and Gao, Zhiyao and McAvan, Andrew S. and Isham, Eve A. and Ekstrom, Arne D.},
  year = {2021},
  month = oct,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {6231},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-26560-w},
  abstract = {When we remember a city that we have visited, we retrieve places related to finding our goal but also non-target locations within this environment. Yet, understanding how the human brain implements the neural computations underlying holistic retrieval remains unsolved, particularly for shared aspects of environments. Here, human participants learned and retrieved details from three partially overlapping environments while undergoing high-resolution functional magnetic resonance imaging (fMRI). Our findings show reinstatement of stores even when they are not related to a specific trial probe, providing evidence for holistic environmental retrieval. For stores shared between cities, we find evidence for pattern separation (representational orthogonalization) in hippocampal subfield CA2/3/DG and repulsion in CA1 (differentiation beyond orthogonalization). Additionally, our findings demonstrate that medial prefrontal cortex (mPFC) stores representations of the common spatial structure, termed schema, across environments. Together, our findings suggest how unique and common elements of multiple spatial environments are accessed computationally and neurally.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Cognitive neuroscience,Hippocampus,Spatial memory},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Cognitive neuroscience;Hippocampus;Spatial memory Subject\_term\_id: cognitive-neuroscience;hippocampus;spatial-memory},
  file = {/Users/xzfang/Zotero/storage/N7XTBIS2/Zheng et al. - 2021 - Partially overlapping spatial environments trigger.pdf;/Users/xzfang/Zotero/storage/9UHTYQ5C/s41467-021-26560-w.html}
}

@article{zheng_semantic_2019,
  title = {The ``Semantic {{P600}}'' in Second Language Processing: {{When}} Syntax Conflicts with Semantics},
  shorttitle = {The ``Semantic {{P600}}'' in Second Language Processing},
  author = {Zheng, Xiaochen and Lemh{\"o}fer, Kristin},
  year = {2019},
  month = apr,
  journal = {Neuropsychologia},
  volume = {127},
  pages = {131--147},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2019.02.010},
  abstract = {In sentences like ``the mouse that chased the cat was hungry'', the syntactically correct interpretation (the mouse chases the cat) is contradicted by semantic and pragmatic knowledge. Previous research has shown that L1 speakers sometimes base sentence interpretation on this type of knowledge (so-called ``shallow'' or ``good-enough'' processing). We made use of both behavioural and ERP measurements to investigate whether L2 learners differ from native speakers in the extent to which they engage in ``shallow'' syntactic processing. German learners of Dutch as well as Dutch native speakers read sentences containing relative clauses (as in the example above) for which the plausible thematic roles were or were not reversed, and made plausibility judgments. The results show that behaviourally, L2 learners had more difficulties than native speakers to discriminate plausible from implausible sentences. In the ERPs, we replicated the previously reported finding of a ``semantic P600'' for semantic reversal anomalies in native speakers, probably reflecting the effort to resolve the syntax-semantics conflict. In L2 learners, though, this P600 was largely attenuated and surfaced only in those trials that were judged correctly for plausibility. These results generally point at a more prevalent, but not exclusive occurrence of shallow syntactic processing in L2 learners.},
  langid = {english},
  keywords = {ERPs,L2 sentence processing,N400,Semantic P600,Semantic reversal anomalies},
  file = {/Users/xzfang/Zotero/storage/YDG2ME2Q/Zheng and LemhÃ¶fer - 2019 - The â€œsemantic P600â€ in second language processing.pdf;/Users/xzfang/Zotero/storage/LC7QP3N5/S0028393219300429.html}
}

@article{zhou_compressive_2018,
  title = {Compressive {{Temporal Summation}} in {{Human Visual Cortex}}},
  author = {Zhou, Jingyang and Benson, Noah C. and Kay, Kendrick N. and Winawer, Jonathan},
  year = {2018},
  month = jan,
  journal = {The Journal of Neuroscience},
  volume = {38},
  number = {3},
  pages = {691--709},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.1724-17.2017},
  abstract = {Combining sensory inputs over space and time is fundamental to vision. Population receptive field models have been successful in characterizing spatial encoding throughout the human visual pathways. A parallel question, how visual areas in the human brain process information distributed over time, has received less attention. One challenge is that the most widely used neuroimaging method, fMRI, has coarse temporal resolution compared with the time-scale of neural dynamics. Here, via carefully controlled temporally modulated stimuli, we show that information about temporal processing can be readily derived from fMRI signal amplitudes in male and female subjects. We find that all visual areas exhibit subadditive summation, whereby responses to longer stimuli are less than the linear prediction from briefer stimuli. We also find fMRI evidence that the neural response to two stimuli is reduced for brief interstimulus intervals (indicating adaptation). These effects are more pronounced in visual areas anterior to V1-V3. Finally, we develop a general model that shows how these effects can be captured with two simple operations: temporal summation followed by a compressive nonlinearity. This model operates for arbitrary temporal stimulation patterns and provides a simple and interpretable set of computations that can be used to characterize neural response properties across the visual hierarchy. Importantly, compressive temporal summation directly parallels earlier findings of compressive spatial summation in visual cortex describing responses to stimuli distributed across space. This indicates that, for space and time, cortex uses a similar processing strategy to achieve higher-level and increasingly invariant representations of the visual world., SIGNIFICANCE STATEMENT Combining sensory inputs over time is fundamental to seeing. Two important temporal phenomena are summation, the accumulation of sensory inputs over time, and adaptation, a response reduction for repeated or sustained stimuli. We investigated these phenomena in the human visual system using fMRI. We built predictive models that operate on arbitrary temporal patterns of stimulation using two simple computations: temporal summation followed by a compressive nonlinearity. Our new temporal compressive summation model captures (1) subadditive temporal summation, and (2) adaptation. We show that the model accounts for systematic differences in these phenomena across visual areas. Finally, we show that for space and time, the visual system uses a similar strategy to achieve increasingly invariant representations of the visual world.},
  pmcid = {PMC5777115},
  pmid = {29192127},
  file = {/Users/xzfang/Zotero/storage/8R8PWCFV/Zhou et al. - 2018 - Compressive Temporal Summation in Human Visual Cor.pdf}
}

@article{zhou_endtoend_2018,
  title = {End-to-{{End Dense Video Captioning}} with {{Masked Transformer}}},
  author = {Zhou, Luowei and Zhou, Yingbo and Corso, Jason J. and Socher, Richard and Xiong, Caiming},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.00819 [cs]},
  eprint = {1804.00819},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xzfang/Zotero/storage/6WNBVQEI/Zhou et al. - 2018 - End-to-End Dense Video Captioning with Masked Tran.pdf;/Users/xzfang/Zotero/storage/E4BPI2N8/1804.html}
}

@article{zhou_humans_2019,
  title = {Humans Can Decipher Adversarial Images},
  author = {Zhou, Zhenglong and Firestone, Chaz},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1334},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-08931-6},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/FBS24S9Y/Zhou and Firestone - 2019 - Humans can decipher adversarial images.pdf}
}

@article{zhou_imagining_2019,
  title = {Is Imagining a Voice like Listening to It? {{Evidence}} from {{ERPs}}},
  shorttitle = {Is Imagining a Voice like Listening to It?},
  author = {Zhou, Peiyun and Garnsey, Susan and Christianson, Kiel},
  year = {2019},
  month = jan,
  journal = {Cognition},
  volume = {182},
  pages = {227--241},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.10.014},
  abstract = {Readers who have seen the Harry Potter movies before reading the novels may ``hear'' actors' voices in their heads when they later read the books. This phenomenon of mentally simulating the voice of speakers depicted in texts has been referred to as auditory perceptual simulation (APS). How much is this mental simulation of voices like listening to actual voices? Two event-related potential (ERP) experiments examined the auditory perceptual simulation of native and non-native English speech while participants silently read English sentences containing subject-verb agreement errors or pronoun-case errors. The aim was to compare readers' ERPs when imagining native and non-native speech to the results of Hanul\'ikov\'a, van Alphen, van Goch, and Weber (2012), who recorded ERPs while participants listened to native and non-native speech and found that native-speaking listeners ``forgive'' errors (signaled by reduced P600 effects) by non-native speakers. Our participants listened to samples of a native and a non-native English speaker's speech and were then asked to imagine the voice of either one or the other speaker while reading sentences. Results revealed differences in N400 and P600 waveforms when imagining the non-native speaker's voice compared to the native speaker's voice. Importantly, when imagining the non-native speaker committing subject-verb agreement errors, P600 amplitudes were no different from error-free items.},
  langid = {english},
  keywords = {Auditory perceptual simulation,Event-related potentials,Grammaticality,P600,Sentence processing,Subject-verb disagreement},
  file = {/Users/xzfang/Zotero/storage/WK6ZXYLB/Zhou et al. - 2019 - Is imagining a voice like listening to it Evidenc.pdf;/Users/xzfang/Zotero/storage/ZIR5UI9S/S0010027718302725.html}
}

@article{zhou_morphological_1995,
  title = {Morphological {{Structure}} in the {{Chinese Mental Lexicon}}},
  author = {Zhou, Xiaolin and {Marslen-wilson}, William},
  year = {1995},
  month = dec,
  journal = {Language and Cognitive Processes},
  volume = {10},
  number = {6},
  pages = {545--600},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690969508407114},
  langid = {english},
  file = {/Users/xzfang/Zotero/storage/34MWK8CR/Zhou and Marslen-wilson - 1995 - Morphological Structure in the Chinese Mental Lexi.pdf}
}

@article{zhuang_unsupervised_2021,
  title = {Unsupervised Neural Network Models of the Ventral Visual Stream},
  author = {Zhuang, Chengxu and Yan, Siming and Nayebi, Aran and Schrimpf, Martin and Frank, Michael C. and DiCarlo, James J. and Yamins, Daniel L. K.},
  year = {2021},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {3},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2014196118},
  abstract = {Deep neural networks currently provide the best quantitative models of the response patterns of neurons throughout the primate ventral visual stream. However, such networks have remained implausible as a model of the development of the ventral stream, in part because they are trained with supervised methods requiring many more labels than are accessible to infants during development. Here, we report that recent rapid progress in unsupervised learning has largely closed this gap. We find that neural network models learned with deep unsupervised contrastive embedding methods achieve neural prediction accuracy in multiple ventral visual cortical areas that equals or exceeds that of models derived using today's best supervised methods and that the mapping of these neural network models' hidden layers is neuroanatomically consistent across the ventral stream. Strikingly, we find that these methods produce brain-like representations even when trained solely with real human child developmental data collected from head-mounted cameras, despite the fact that these datasets are noisy and limited. We also find that semisupervised deep contrastive embeddings can leverage small numbers of labeled examples to produce representations with substantially improved error-pattern consistency to human behavior. Taken together, these results illustrate a use of unsupervised learning to provide a quantitative model of a multiarea cortical brain system and present a strong candidate for a biologically plausible computational theory of primate sensory learning.},
  chapter = {Biological Sciences},
  copyright = {Copyright \textcopyright{} 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {33431673},
  keywords = {deep neural networks,unsupervised algorithms,ventral visual stream},
  file = {/Users/xzfang/Zotero/storage/D8ECF2GU/Zhuang et al. - 2021 - Unsupervised neural network models of the ventral .pdf;/Users/xzfang/Zotero/storage/MEWNAQVS/e2014196118.html}
}

@article{ziegler_event_2018,
  title = {Event {{Structures Drive Semantic Structural Priming}}, {{Not Thematic Roles}}: {{Evidence From Idioms}} and {{Light Verbs}}},
  shorttitle = {Event {{Structures Drive Semantic Structural Priming}}, {{Not Thematic Roles}}},
  author = {Ziegler, Jayden and Snedeker, Jesse and Wittenberg, Eva},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {8},
  pages = {2918--2949},
  issn = {1551-6709},
  doi = {10.1111/cogs.12687},
  abstract = {What are the semantic representations that underlie language production? We use structural priming to distinguish between two competing theories. Thematic roles define semantic structure in terms of atomic units that specify event participants and are ordered with respect to each other through a hierarchy of roles. Event structures instead instantiate semantic structure as embedded sub-predicates that impose an order on verbal arguments based on their relative positioning in these embeddings. Across two experiments, we found that priming for datives depended on the degree of overlap in event structures. Specifically, while all dative structures showed priming, due to common syntax, there was a boost for compositional datives priming other compositional datives. Here, the two syntactic forms have distinct event structures. In contrast, there was no boost in priming for dative light verbs, where the two forms map onto a single event representation. On the thematic roles hypothesis, we would have expected a similar degree of priming for the two cases. Thus, our results support event structural approaches to semantic representation and not thematic roles.},
  copyright = {\textcopyright{} 2018 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Dative alternation,Event structure,Idioms,Light verbs,Structural priming,Thematic roles},
  file = {/Users/xzfang/Zotero/storage/PYMUJWD2/Ziegler et al. - 2018 - Event Structures Drive Semantic Structural Priming.pdf;/Users/xzfang/Zotero/storage/KPKIZDSA/cogs.html}
}

@article{ziegler_how_2018,
  title = {How Broad Are Thematic Roles? {{Evidence}} from Structural Priming},
  shorttitle = {How Broad Are Thematic Roles?},
  author = {Ziegler, Jayden and Snedeker, Jesse},
  year = {2018},
  month = oct,
  journal = {Cognition},
  volume = {179},
  pages = {221--240},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.06.019},
  abstract = {Verbs that are similar in meaning tend to occur in the same syntactic structures. For example, give and hand, which denote transfer of possession, both appear in the prepositional-object construction: ``The child gave/handed the ball to the dog.'' We can call the child a ``giver'' in one case and a ``hander'' in the other, or we can refer to her more generally as the agent, or doer of the action. Similarly, the dog can be called the recipient, and the ball, the theme. These generalized notions of agent, recipient, and theme are known as thematic roles. An important theoretical question for linguists and psycholinguists is what the set of thematic roles is. Are there a small number of very broad roles, perhaps with each one mapping onto a single canonical syntactic position? Or are there many distinct roles, several mapping to the same syntactic position but conveying subtly different meanings? We investigate this question across eleven structural priming experiments on Amazon Mechanical Turk (total N\,=\,2914), asking whether speakers treat the thematic roles recipient and destination (i.e., location or spatial goal) as interchangeable, suggesting the broad role of goal, or distinct, suggesting two separate roles. To do so, we look for priming between dative sentences (e.g., ``The man gave the ball to the dog''), which have a recipient role (dog), and locative sentences (e.g., ``The man loaded hay onto the wagon''), which instead have a destination role (wagon). Our pattern of findings confirms that thematic role mappings can be primed independent of syntactic structure, lexical content, and animacy. However, we find that this priming does not extend from destinations to recipients (or vice versa), providing evidence that these two roles are distinct.},
  langid = {english},
  keywords = {Animacy,Dative alternation,Locative alternation,Structural priming,Thematic roles},
  file = {/Users/xzfang/Zotero/storage/ZQ5V69WR/Ziegler and Snedeker - 2018 - How broad are thematic roles Evidence from struct.pdf;/Users/xzfang/Zotero/storage/VGGZI8D4/S0010027718301719.html}
}

@article{ziegler_how_2019,
  title = {How Abstract Is Syntax? {{Evidence}} from Structural Priming},
  shorttitle = {How Abstract Is Syntax?},
  author = {Ziegler, Jayden and Bencini, Giulia and Goldberg, Adele and Snedeker, Jesse},
  year = {2019},
  month = dec,
  journal = {Cognition},
  volume = {193},
  pages = {104045},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.104045},
  abstract = {In 1990, Bock and Loebell found that passives (e.g., The 747 was radioed by the airport's control tower) can be primed by intransitive locatives (e.g., The 747 was landing by the airport's control tower). This finding is often taken as strong evidence that structural priming occurs on the basis of a syntactic phrase structure that abstracts across lexical content, including prepositions, and is uninfluenced by the semantic roles of the arguments. However, all of the intransitive locative primes in Bock and Loebell contained the preposition by (by-locatives), just like the passive targets. Therefore, the locative-to-passive priming may have been due to the adjunct headed by by, rather than being a result of purely abstract syntax. The present experiment investigates this possibility. We find that passives and intransitive by-locatives are equivalent primes, but intransitive locatives with other prepositions (e.g., The 747 has landed near the airport control tower) do not prime passives. We conclude that a shared abstract, content-less tree structure is not sufficient for passive priming to occur. We then review the prior results that have been offered in favor of abstract tree priming, and note the range of evidence can be considerably narrowed\textemdash and possibly eliminated\textemdash once effects of animacy, semantic event structure, shared morphology, information structure, and rhythm are taken into account.},
  langid = {english},
  keywords = {Abstract syntax,Passive alternation,Structural priming,Syntactic priming},
  file = {/Users/xzfang/Zotero/storage/I7MZI6TB/Ziegler et al. - 2019 - How abstract is syntax Evidence from structural p.pdf;/Users/xzfang/Zotero/storage/BHHZ5NI7/S0010027719302185.html}
}

@article{ziegler_scalar_2016,
  title = {Scalar Adjectives and the Temporal Unfolding of Semantic Composition: {{An MEG}} Investigation},
  shorttitle = {Scalar Adjectives and the Temporal Unfolding of Semantic Composition},
  author = {Ziegler, Jayden and Pylkk{\"a}nen, Liina},
  year = {2016},
  month = aug,
  journal = {Neuropsychologia},
  volume = {89},
  pages = {161--171},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2016.06.010},
  abstract = {A growing body of research implicates the left anterior temporal lobe (LATL) for combinatorial semantic processing. However, magnetoencephalography (MEG) studies have revealed this activity to be timed quite early, at 200\textendash 250ms, preceding the most common time window for lexical-semantic effects. What type of semantic composition could the LATL perform at 200\textendash 250ms? We hypothesized that the LATL computes an early stage of composition, taking as its input only the most readily available lexical-semantic information. To test this, we varied the context-sensitivity of prenominal adjectives, postulating that only context-insensitive intersective adjectives (e.g., dead, Italian) should compose in an early time window, whereas the composition of context-sensitive scalar adjectives (e.g., fast, large) should be delayed until the interpretation of the subsequent noun is fully determined. Consistent with this, early combinatory effects in left temporal cortex were observed only for intersective adjectives, though in this study the effects were somewhat more posterior than in prior reports. Overall, our results suggest multiple stages of semantic composition, of which the LATL may index the earliest.},
  langid = {english},
  keywords = {Context-sensitivity,Left anterior temporal lobe (LATL),Magnetoencephalography (MEG),Scalar adjectives,Semantic composition},
  file = {/Users/xzfang/Zotero/storage/RVTVREHK/Ziegler and PylkkÃ¤nen - 2016 - Scalar adjectives and the temporal unfolding of se.pdf;/Users/xzfang/Zotero/storage/RLJF7C47/S0028393216302093.html}
}

@article{ziegler_scalar_2016a,
  title = {Scalar Adjectives and the Temporal Unfolding of Semantic Composition: {{An MEG}} Investigation},
  shorttitle = {Scalar Adjectives and the Temporal Unfolding of Semantic Composition},
  author = {Ziegler, Jayden and Pylkk{\"a}nen, Liina},
  year = {2016},
  month = aug,
  journal = {Neuropsychologia},
  volume = {89},
  pages = {161--171},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2016.06.010},
  abstract = {A growing body of research implicates the left anterior temporal lobe (LATL) for combinatorial semantic processing. However, magnetoencephalography (MEG) studies have revealed this activity to be timed quite early, at 200\textendash 250ms, preceding the most common time window for lexical-semantic effects. What type of semantic composition could the LATL perform at 200\textendash 250ms? We hypothesized that the LATL computes an early stage of composition, taking as its input only the most readily available lexical-semantic information. To test this, we varied the context-sensitivity of prenominal adjectives, postulating that only context-insensitive intersective adjectives (e.g., dead, Italian) should compose in an early time window, whereas the composition of context-sensitive scalar adjectives (e.g., fast, large) should be delayed until the interpretation of the subsequent noun is fully determined. Consistent with this, early combinatory effects in left temporal cortex were observed only for intersective adjectives, though in this study the effects were somewhat more posterior than in prior reports. Overall, our results suggest multiple stages of semantic composition, of which the LATL may index the earliest.},
  langid = {english},
  keywords = {Context-sensitivity,Left anterior temporal lobe (LATL),Magnetoencephalography (MEG),Scalar adjectives,Semantic composition},
  file = {/Users/xzfang/Zotero/storage/F22YQLUL/Ziegler and PylkkÃ¤nen - 2016 - Scalar adjectives and the temporal unfolding of se.pdf;/Users/xzfang/Zotero/storage/7NLKCBFS/S0028393216302093.html}
}

@article{zmigrod_cognitive_2021,
  title = {The Cognitive and Perceptual Correlates of Ideological Attitudes: A Data-Driven Approach},
  shorttitle = {The Cognitive and Perceptual Correlates of Ideological Attitudes},
  author = {Zmigrod, Leor and Eisenberg, Ian W. and Bissett, Patrick G. and Robbins, Trevor W. and Poldrack, Russell A.},
  year = {2021},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {376},
  number = {1822},
  pages = {20200424},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2020.0424},
  abstract = {Although human existence is enveloped by ideologies, remarkably little is understood about the relationships between ideological attitudes and psychological traits. Even less is known about how cognitive dispositions\textemdash individual differences in how information is perceived and processed\textemdash{} sculpt individuals' ideological worldviews, proclivities for extremist beliefs and resistance (or receptivity) to evidence. Using an unprecedented number of cognitive tasks (n = 37) and personality surveys (n = 22), along with data-driven analyses including drift-diffusion and Bayesian modelling, we uncovered the specific psychological signatures of political, nationalistic, religious and dogmatic beliefs. Cognitive and personality assessments consistently outperformed demographic predictors in accounting for individual differences in ideological preferences by 4 to 15-fold. Furthermore, data-driven analyses revealed that individuals' ideological attitudes mirrored their cognitive decision-making strategies. Conservatism and nationalism were related to greater caution in perceptual decision-making tasks and to reduced strategic information processing, while dogmatism was associated with slower evidence accumulation and impulsive tendencies. Religiosity was implicated in heightened agreeableness and risk perception. Extreme pro-group attitudes, including violence endorsement against outgroups, were linked to poorer working memory, slower perceptual strategies, and tendencies towards impulsivity and sensation-seeking\textemdash reflecting overlaps with the psychological profiles of conservatism and dogmatism. Cognitive and personality signatures were also generated for ideologies such as authoritarianism, system justification, social dominance orientation, patriotism and receptivity to evidence or alternative viewpoints; elucidating their underpinnings and highlighting avenues for future research. Together these findings suggest that ideological worldviews may be reflective of low-level perceptual and cognitive functions.This article is part of the theme issue `The political brain: neurocognitive and computational mechanisms'.},
  file = {/Users/xzfang/Zotero/storage/GY9EYU6I/Zmigrod et al. - 2021 - The cognitive and perceptual correlates of ideolog.pdf;/Users/xzfang/Zotero/storage/K8AIJFPU/rstb.2020.html}
}

@article{zosh_array_2015,
  title = {Array Heterogeneity Prevents Catastrophic Forgetting in Infants},
  author = {Zosh, Jennifer M. and Feigenson, Lisa},
  year = {2015},
  month = mar,
  journal = {Cognition},
  volume = {136},
  pages = {365--380},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2014.11.042},
  abstract = {Working memory is limited in adults and infants. But unlike adults, infants whose working memory capacity is exceeded often fail in a particularly striking way: they do not represent any of the presented objects, rather than simply remembering as many objects as they can and ignoring anything further (Feigenson \& Carey, 2003, 2005). Here we explored the nature of this ``catastrophic forgetting,'' asking whether stimuli themselves modulate the way in which infants' memory fails. We showed 13-month old infants object arrays that either were within or that exceeded working memory capacity\textemdash but, unlike previous experiments, presented objects with contrasting features. Although previous studies have repeatedly documented infants' failure to represent four identical hidden objects, in Experiments 1 and 2 we found that infants who saw four contrasting objects hidden, and then retrieved just two of the four, successfully continued searching for the missing objects. Perceptual contrast between objects sufficed to drive this success; infants succeeded regardless of whether the different objects were contrastively labeled, and regardless of whether the objects were semantically familiar or completely novel. In Experiment 3 we explored the nature of this surprising success, asking whether array heterogeneity actually expanded infants' working memory capacity or rather prevented catastrophic forgetting. We found that infants successfully continued searching after seeing four contrasting objects hidden and retrieving two of them, but not after retrieving three of them. This suggests that, like adults, infants were able to remember up to, but not beyond, the limits of their working memory capacity when representing heterogeneous arrays.},
  langid = {english},
  keywords = {Development,Heterogeneity,Infants,Objects,Working memory},
  file = {/Users/xzfang/Zotero/storage/WT78F5RM/Zosh and Feigenson - 2015 - Array heterogeneity prevents catastrophic forgetti.pdf;/Users/xzfang/Zotero/storage/E86KQHFK/S0010027714002686.html}
}

@article{zou_thetaband_2021,
  title = {Theta-Band {{Cortical Tracking}} of the {{Speech Envelope Shows}} the {{Linear Phase Property}}},
  author = {Zou, Jiajie and Xu, Chuan and Luo, Cheng and Jin, Peiqing and Gao, Jiaxin and Li, Jingqi and Gao, Jian and Ding, Nai and Luo, Benyan},
  year = {2021},
  month = aug,
  journal = {eNeuro},
  publisher = {{Society for Neuroscience}},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0058-21.2021},
  abstract = {When listening to speech, low-frequency cortical activity tracks the speech envelope. It remains controversial, however, whether such envelope-tracking neural activity reflects entrainment of neural oscillations or superposition of transient responses evoked by sound features. Recently, it is suggested that the phase of envelope-tracking activity can potentially distinguish entrained oscillations and evoked responses. Here, we analyze the phase of envelope-tracking in humans during passive listening, and observe that the phase lag between cortical activity and speech envelope tends to change linearly across frequency in the theta band (4-8 Hz), suggesting that the theta-band envelope-tracking activity can be readily modeled by evoked responses. Significance Statement During speech listening, cortical activity tracks the speech envelope, which is a critical cue for speech recognition. It is debated, however, what is the neural mechanism generating the envelope-tracking responses. Previous work has shown that delta-band envelope tracking responses recorded during music listening cannot be explained by a simple linear-system model. Here, however, we demonstrate that theta-band envelope tracking responses recorded during speech listening shows the linear phase property, which can be well explained by a linear-system model.},
  chapter = {New Research},
  copyright = {Copyright \textcopyright{} 2021 Zou et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {34380659},
  keywords = {EEG,neural entrainment,phase resetting},
  file = {/Users/xzfang/Zotero/storage/EPU7JXG6/Zou et al. - 2021 - Theta-band Cortical Tracking of the Speech Envelop.pdf;/Users/xzfang/Zotero/storage/WIY4ARIQ/ENEURO.0058-21.html}
}


